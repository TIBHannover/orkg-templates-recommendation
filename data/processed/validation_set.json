{
    "entailments": [
        {
            "instance_id": "R40006xR44865",
            "template_id": "R40006",
            "paper_id": "R44865",
            "premise": "basic reproduction number estimate time period basic reproduction number location",
            "hypothesis": "modelling the epidemic trend of the 2019 novel coronavirus outbreak in china we present a timely evaluation of the chinese 2019 ncov epidemic in its initial phase, where 2019 ncov demonstrates comparable transmissibility but lower fatality rates than sars and mers a quick diagnosis that leads to case isolation and integrated interventions will have a major impact on its future trend nevertheless, as china is facing its spring festival travel rush and the epidemic has spread beyond its borders, further investigation on its potential spatiotemporal transmission pattern and novel intervention strategies are warranted",
            "sequence": "[CLS] basic reproduction number estimate time period basic reproduction number location [SEP] modelling the epidemic trend of the 2019 novel coronavirus outbreak in china we present a timely evaluation of the chinese 2019 ncov epidemic in its initial phase, where 2019 ncov demonstrates comparable transmissibility but lower fatality rates than sars and mers a quick diagnosis that leads to case isolation and integrated interventions will have a major impact on its future trend nevertheless, as china is facing its spring festival travel rush and the epidemic has spread beyond its borders, further investigation on its potential spatiotemporal transmission pattern and novel intervention strategies are warranted [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            }
        },
        {
            "instance_id": "R178304xR182352",
            "template_id": "R178304",
            "paper_id": "R182352",
            "premise": "dataset inter annotator agreement alternatename assesses creator description disambiguatingdescription encoding exampleofwork genre inlanguage isbasedon license name sameas size sourceorganization url version",
            "hypothesis": "real time mobile food recognition system \"we propose a mobile food recognition system the poses of which are estimating calorie and nutritious of foods and recording a user's eating habits since all the processes on image recognition performed on a smart phone, the system does not need to send images to a server and runs on an ordinary smartphone in a real time way to recognize food items, a user draws bounding boxes by touching the screen first, and then the system starts food item recognition within the indicated bounding boxes to recognize them more accurately, we segment each food item region by grubcut, extract a color histogram and surf based bag of features, and finally classify it into one of the fifty food categories with linear svm and fast 2 kernel in addition, the system estimates the direction of food regions where the higher svm output score is expected to be obtained, show it as an arrow on the screen in order to ask a user to move a smartphone camera this recognition process is performed repeatedly about once a second we implemented this system as an android smartphone application so as to use multiple cpu cores effectively for real time recognition in the experiments, we have achieved the 81 55% classification rate for the top 5 category candidates when the ground truth bounding boxes are given in addition, we obtained positive evaluation by user study compared to the food recording system without object recognition \"",
            "sequence": "[CLS] dataset inter annotator agreement alternatename assesses creator description disambiguatingdescription encoding exampleofwork genre inlanguage isbasedon license name sameas size sourceorganization url version [SEP] real time mobile food recognition system \"we propose a mobile food recognition system the poses of which are estimating calorie and nutritious of foods and recording a user's eating habits since all the processes on image recognition performed on a smart phone, the system does not need to send images to a server and runs on an ordinary smartphone in a real time way to recognize food items, a user draws bounding boxes by touching the screen first, and then the system starts food item recognition within the indicated bounding boxes to recognize them more accurately, we segment each food item region by grubcut, extract a color histogram and surf based bag of features, and finally classify it into one of the fifty food categories with linear svm and fast 2 kernel in addition, the system estimates the direction of food regions where the higher svm output score is expected to be obtained, show it as an arrow on the screen in order to ask a user to move a smartphone camera this recognition process is performed repeatedly about once a second we implemented this system as an android smartphone application so as to use multiple cpu cores effectively for real time recognition in the experiments, we have achieved the 81 55% classification rate for the top 5 category candidates when the ground truth bounding boxes are given in addition, we obtained positive evaluation by user study compared to the food recording system without object recognition \" [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R133",
                "label": "Artificial Intelligence"
            }
        },
        {
            "instance_id": "R160259xR160349",
            "template_id": "R160259",
            "paper_id": "R160349",
            "premise": "digital city twin review application area",
            "hypothesis": "leveraging digital twin for sustainability assessment of an educational building the eu green deal, beginning in 2019, promoted a roadmap for operating the transition to a sustainable eu economy by turning climate issues and environmental challenges into opportunities in all policy areas and making the transition fair and inclusive for all focusing on the built environment, the voluntary adoption of rating systems for sustainability assessment is growing, with an increasing market value, and is perceived as a social responsibility both by public administration and by private companies this paper proposes a framework for shifting from a static sustainability assessment to a digital twin (dt) based and internet of things (iot) enabled dynamic approach this new approach allows for a real time evaluation and control of a wide range of sustainability criteria with a user centered point of view a pilot building, namely, the elux lab cognitive building in the university of brescia, was used to test the framework with some sample applications the educational building accommodates the daily activities of the engineering students by constantly interacting with the sensorized asset monitoring indoor comfort and air quality conditions as well as the energy behavior of the building in order to optimize the trade off with renewable energy production the framework is the cornerstone of a methodology exploiting the digital twin approach to support the decision processes related to sustainability through the whole building\u2019s life cycle",
            "sequence": "[CLS] digital city twin review application area [SEP] leveraging digital twin for sustainability assessment of an educational building the eu green deal, beginning in 2019, promoted a roadmap for operating the transition to a sustainable eu economy by turning climate issues and environmental challenges into opportunities in all policy areas and making the transition fair and inclusive for all focusing on the built environment, the voluntary adoption of rating systems for sustainability assessment is growing, with an increasing market value, and is perceived as a social responsibility both by public administration and by private companies this paper proposes a framework for shifting from a static sustainability assessment to a digital twin (dt) based and internet of things (iot) enabled dynamic approach this new approach allows for a real time evaluation and control of a wide range of sustainability criteria with a user centered point of view a pilot building, namely, the elux lab cognitive building in the university of brescia, was used to test the framework with some sample applications the educational building accommodates the daily activities of the engineering students by constantly interacting with the sensorized asset monitoring indoor comfort and air quality conditions as well as the energy behavior of the building in order to optimize the trade off with renewable energy production the framework is the cornerstone of a methodology exploiting the digital twin approach to support the decision processes related to sustainability through the whole building\u2019s life cycle [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            }
        },
        {
            "instance_id": "R172526xR159657",
            "template_id": "R172526",
            "paper_id": "R159657",
            "premise": "video process has study research problem application production",
            "hypothesis": "supporting requirements with video based analysis the dealing room study is one of many studies that have used video to support requirements elicitation and the general design process a growing body of experience with video based ethnographies supports technology development in various domains, including air traffic and other control rooms, healthcare, public settings such as museums, and more experimental technologies, including media spaces and ubiquitous computing",
            "sequence": "[CLS] video process has study research problem application production [SEP] supporting requirements with video based analysis the dealing room study is one of many studies that have used video to support requirements elicitation and the general design process a growing body of experience with video based ethnographies supports technology development in various domains, including air traffic and other control rooms, healthcare, public settings such as museums, and more experimental technologies, including media spaces and ubiquitous computing [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R11",
                "label": "Science"
            }
        },
        {
            "instance_id": "R186491xR196021",
            "template_id": "R186491",
            "paper_id": "R196021",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "gamifying collaborative prioritization: does pointsification work? gamification has been applied in software engineering contexts, and more recently in requirements engineering with the purpose of improving the motivation and engagement of people performing specific engineering tasks but often an objective evaluation that the resulting gamified tasks successfully meet the intended goal is missing on the other hand, current practices in designing gamified processes seem to rest on a try, test and learn approach, rather than on first principles design methods thus empirical evaluation should play an even more important role we combined gamification and automated reasoning techniques to support collaborative requirements prioritization in software evolution a first prototype has been evaluated in the context of three industrial use cases to further investigate the impact of specific game elements, namely point based elements, we performed a quasi experiment comparing two versions of the tool, with and without pointsification we present the results from these two empirical evaluations, and discuss lessons learned",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] gamifying collaborative prioritization: does pointsification work? gamification has been applied in software engineering contexts, and more recently in requirements engineering with the purpose of improving the motivation and engagement of people performing specific engineering tasks but often an objective evaluation that the resulting gamified tasks successfully meet the intended goal is missing on the other hand, current practices in designing gamified processes seem to rest on a try, test and learn approach, rather than on first principles design methods thus empirical evaluation should play an even more important role we combined gamification and automated reasoning techniques to support collaborative requirements prioritization in software evolution a first prototype has been evaluated in the context of three industrial use cases to further investigate the impact of specific game elements, namely point based elements, we performed a quasi experiment comparing two versions of the tool, with and without pointsification we present the results from these two empirical evaluations, and discuss lessons learned [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R150089xR146490",
            "template_id": "R150089",
            "paper_id": "R146490",
            "premise": "epidemiological surveillance systems design and implementation software used software development approach related work epidemiological surveillance system purpose epidemiological surveillance process epidemiological surveillance users statistical analysis techniques epidemiological surveillance architecture epidemiological software development approach advantage provided by the system limit of the system",
            "hypothesis": "rapid implementation of mobile technology for real time epidemiology of covid 19 mobile symptom tracking \\n \\n the rapidity with which severe acute respiratory syndrome coronavirus 2 (sars cov 2) spreads through a population is defying attempts at tracking it, and quantitative polymerase chain reaction testing so far has been too slow for real time epidemiology taking advantage of existing longitudinal health care and research patient cohorts, drew\\n et al \\n pushed software updates to participants to encourage reporting of potential coronavirus disease 2019 (covid 19) symptoms the authors recruited about 2 million users (including health care workers) to the covid symptom study (previously known as the covid symptom tracker) from across the united kingdom and the united states the prevalence of combinations of symptoms (three or more), including fatigue and cough, followed by diarrhea, fever, and/or anosmia, was predictive of a positive test verification for sars cov 2 as exemplified by data from wales, united kingdom, mathematical modeling predicted geographical hotspots of incidence 5 to 7 days in advance of official public health reports \\n \\n \\n science \\n , this issue p \\n 1362 \\n",
            "sequence": "[CLS] epidemiological surveillance systems design and implementation software used software development approach related work epidemiological surveillance system purpose epidemiological surveillance process epidemiological surveillance users statistical analysis techniques epidemiological surveillance architecture epidemiological software development approach advantage provided by the system limit of the system [SEP] rapid implementation of mobile technology for real time epidemiology of covid 19 mobile symptom tracking \\n \\n the rapidity with which severe acute respiratory syndrome coronavirus 2 (sars cov 2) spreads through a population is defying attempts at tracking it, and quantitative polymerase chain reaction testing so far has been too slow for real time epidemiology taking advantage of existing longitudinal health care and research patient cohorts, drew\\n et al \\n pushed software updates to participants to encourage reporting of potential coronavirus disease 2019 (covid 19) symptoms the authors recruited about 2 million users (including health care workers) to the covid symptom study (previously known as the covid symptom tracker) from across the united kingdom and the united states the prevalence of combinations of symptoms (three or more), including fatigue and cough, followed by diarrhea, fever, and/or anosmia, was predictive of a positive test verification for sars cov 2 as exemplified by data from wales, united kingdom, mathematical modeling predicted geographical hotspots of incidence 5 to 7 days in advance of official public health reports \\n \\n \\n science \\n , this issue p \\n 1362 \\n [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R278",
                "label": "Information Science"
            }
        },
        {
            "instance_id": "R186491xR200049",
            "template_id": "R186491",
            "paper_id": "R200049",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "assessment of risk perception in security requirements composition \"security requirements analysis depends on how well trained analysts perceive security risk, understand the impact of various vulnerabilities, and mitigate threats when systems are composed of multiple machines, configurations, and software components that interact with each other, risk perception must account for the composition of security requirements in this paper, we report on how changes to security requirements affect analysts risk perceptions and their decisions about how to modify the requirements to reach adequate security levels we conducted two user surveys of 174 participants wherein participants assess security levels across 64 factorial vignettes we analyzed the survey results using multi level modeling to test for the effect of security requirements composition on participants' overall security adequacy ratings and on their ratings of individual requirements we accompanied this analysis with grounded analysis of elicited requirements aimed at lowering the security risk our results suggest that requirements composition affects experts' adequacy ratings on security requirements in addition, we identified three categories of requirements modifications, called refinements, replacements and reinforcements, and we measured how these categories compare with overall perceived security risk finally, we discuss the future impact of our work in security requirements assessment practice \"",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] assessment of risk perception in security requirements composition \"security requirements analysis depends on how well trained analysts perceive security risk, understand the impact of various vulnerabilities, and mitigate threats when systems are composed of multiple machines, configurations, and software components that interact with each other, risk perception must account for the composition of security requirements in this paper, we report on how changes to security requirements affect analysts risk perceptions and their decisions about how to modify the requirements to reach adequate security levels we conducted two user surveys of 174 participants wherein participants assess security levels across 64 factorial vignettes we analyzed the survey results using multi level modeling to test for the effect of security requirements composition on participants' overall security adequacy ratings and on their ratings of individual requirements we accompanied this analysis with grounded analysis of elicited requirements aimed at lowering the security risk our results suggest that requirements composition affects experts' adequacy ratings on security requirements in addition, we identified three categories of requirements modifications, called refinements, replacements and reinforcements, and we measured how these categories compare with overall perceived security risk finally, we discuss the future impact of our work in security requirements assessment practice \" [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R172526xR159596",
            "template_id": "R172526",
            "paper_id": "R159596",
            "premise": "video process has study research problem application production",
            "hypothesis": "at home with agents: exploring attitudes towards future smart energy infrastructures energy systems researchers are proposing a broad range of future \"smart\" energy infrastructures to promote more efficient management of energy resources this paper considers how consumers might relate to these future smart grids within the uk to address this challenge we exploited a combination of demonstration and animated sketches to convey the nature of a future smart energy infrastructure based on software agents users\\' reactions suggested that although they felt an obligation to engage with energy issues, they were principally disinterested users showed a considerable lack of trust in energy companies raising a dilemma of design while users might welcome agents to help in engaging with complex energy infrastructures, they had little faith in those that might provide them this suggests the need to consider how to design software agents to enhance trust in these socio economic settings",
            "sequence": "[CLS] video process has study research problem application production [SEP] at home with agents: exploring attitudes towards future smart energy infrastructures energy systems researchers are proposing a broad range of future \"smart\" energy infrastructures to promote more efficient management of energy resources this paper considers how consumers might relate to these future smart grids within the uk to address this challenge we exploited a combination of demonstration and animated sketches to convey the nature of a future smart energy infrastructure based on software agents users\\' reactions suggested that although they felt an obligation to engage with energy issues, they were principally disinterested users showed a considerable lack of trust in energy companies raising a dilemma of design while users might welcome agents to help in engaging with complex energy infrastructures, they had little faith in those that might provide them this suggests the need to consider how to design software agents to enhance trust in these socio economic settings [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R11",
                "label": "Science"
            }
        },
        {
            "instance_id": "R186491xR198795",
            "template_id": "R186491",
            "paper_id": "R198795",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "trace++: a traceability approach to support transitioning to agile software engineering agile methodologies have been introduced as an alternative to traditional software engineering methodologies however, despite the advantages of using agile methodologies, the transition between traditional and agile methodologies is not an easy task there are several problems associated with the use of agile methodologies examples of these problems are related to (i) lack of metrics to measure the amount of rework that occurs per sprint, (ii) interruption of a project after several iterations, (iii) changes in the requirements, (iv) lack of documentation, and (v) lack of management control in this paper we present trace++, a traceability technique that extends traditional traceability relationships with extra information in order to support the transition between traditional and agile software development the use of trace++ has been evaluated in two real projects of different software development companies to measure the benefits of using trace++ to support agile software development",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] trace++: a traceability approach to support transitioning to agile software engineering agile methodologies have been introduced as an alternative to traditional software engineering methodologies however, despite the advantages of using agile methodologies, the transition between traditional and agile methodologies is not an easy task there are several problems associated with the use of agile methodologies examples of these problems are related to (i) lack of metrics to measure the amount of rework that occurs per sprint, (ii) interruption of a project after several iterations, (iii) changes in the requirements, (iv) lack of documentation, and (v) lack of management control in this paper we present trace++, a traceability technique that extends traditional traceability relationships with extra information in order to support the transition between traditional and agile software development the use of trace++ has been evaluated in two real projects of different software development companies to measure the benefits of using trace++ to support agile software development [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R178304xR163542",
            "template_id": "R178304",
            "paper_id": "R163542",
            "premise": "dataset inter annotator agreement alternatename assesses creator description disambiguatingdescription encoding exampleofwork genre inlanguage isbasedon license name sameas size sourceorganization url version",
            "hypothesis": "overview of the regulatory network of plant seed development (seedev) task at the bionlp shared task 2016 this paper presents the seedev task of the bionlp shared task 2016 the purpose of the seedev task is the extraction from scientific articles of the descriptions of genetic and molecular mechanisms involved in seed development of the model plant, arabidopsis thaliana the seedev task consists in the extraction of many different event types that involve a wide range of entity types so that they accurately reflect the complexity of the biological mechanisms the corpus is composed of paragraphs selected from the full texts of relevant scientific articles in this paper, we describe the organization of the seedev task, the corpus characteristics, and the metrics used for the evaluation of participant systems we analyze and discuss the final results of the seven participant systems to the test the best f score is 0 432, which is similar to the scores achieved in similar tasks on molecular biology",
            "sequence": "[CLS] dataset inter annotator agreement alternatename assesses creator description disambiguatingdescription encoding exampleofwork genre inlanguage isbasedon license name sameas size sourceorganization url version [SEP] overview of the regulatory network of plant seed development (seedev) task at the bionlp shared task 2016 this paper presents the seedev task of the bionlp shared task 2016 the purpose of the seedev task is the extraction from scientific articles of the descriptions of genetic and molecular mechanisms involved in seed development of the model plant, arabidopsis thaliana the seedev task consists in the extraction of many different event types that involve a wide range of entity types so that they accurately reflect the complexity of the biological mechanisms the corpus is composed of paragraphs selected from the full texts of relevant scientific articles in this paper, we describe the organization of the seedev task, the corpus characteristics, and the metrics used for the evaluation of participant systems we analyze and discuss the final results of the seven participant systems to the test the best f score is 0 432, which is similar to the scores achieved in similar tasks on molecular biology [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            }
        },
        {
            "instance_id": "R146876xR148246",
            "template_id": "R146876",
            "paper_id": "R148246",
            "premise": "organic solar cells mobility donor acceptor lumo homo energy band gap open circuit voltage, voc short circuit current density, jsc fill factor, ff power conversion efficiency",
            "hypothesis": "design, synthesis, and structural characterization of the first dithienocyclopentacarbazole based n type organic semiconductor and its application in non fullerene polymer solar cells a novel dithienocyclopentacarbazole containing n type organic semiconductor (dtcc\u2013ic) was designed and synthesized as the acceptor for non fullerene solar cells",
            "sequence": "[CLS] organic solar cells mobility donor acceptor lumo homo energy band gap open circuit voltage, voc short circuit current density, jsc fill factor, ff power conversion efficiency [SEP] design, synthesis, and structural characterization of the first dithienocyclopentacarbazole based n type organic semiconductor and its application in non fullerene polymer solar cells a novel dithienocyclopentacarbazole containing n type organic semiconductor (dtcc\u2013ic) was designed and synthesized as the acceptor for non fullerene solar cells [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            }
        },
        {
            "instance_id": "R186491xR78392",
            "template_id": "R186491",
            "paper_id": "R78392",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "bug report, feature request, or simply praise? on automatically classifying app reviews app stores like google play and apple appstore have over 3 million apps covering nearly every kind of software and service billions of users regularly download, use, and review these apps recent studies have shown that reviews written by the users represent a rich source of information for the app vendors and the developers, as they include information about bugs, ideas for new features, or documentation of released features this paper introduces several probabilistic techniques to classify app reviews into four types: bug reports, feature requests, user experiences, and ratings for this we use review metadata such as the star rating and the tense, as well as, text classification, natural language processing, and sentiment analysis techniques we conducted a series of experiments to compare the accuracy of the techniques and compared them with simple string matching we found that metadata alone results in a poor classification accuracy when combined with natural language processing, the classification precision got between 70 95% while the recall between 80 90% multiple binary classifiers outperformed single multiclass classifiers our results impact the design of review analytics tools which help app vendors, developers, and users to deal with the large amount of reviews, filter critical reviews, and assign them to the appropriate stakeholders",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] bug report, feature request, or simply praise? on automatically classifying app reviews app stores like google play and apple appstore have over 3 million apps covering nearly every kind of software and service billions of users regularly download, use, and review these apps recent studies have shown that reviews written by the users represent a rich source of information for the app vendors and the developers, as they include information about bugs, ideas for new features, or documentation of released features this paper introduces several probabilistic techniques to classify app reviews into four types: bug reports, feature requests, user experiences, and ratings for this we use review metadata such as the star rating and the tense, as well as, text classification, natural language processing, and sentiment analysis techniques we conducted a series of experiments to compare the accuracy of the techniques and compared them with simple string matching we found that metadata alone results in a poor classification accuracy when combined with natural language processing, the classification precision got between 70 95% while the recall between 80 90% multiple binary classifiers outperformed single multiclass classifiers our results impact the design of review analytics tools which help app vendors, developers, and users to deal with the large amount of reviews, filter critical reviews, and assign them to the appropriate stakeholders [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R161736xR161940",
            "template_id": "R161736",
            "paper_id": "R161940",
            "premise": "xray laser applications paper type research objective has system qualities",
            "hypothesis": "picosecond snapshot of the speckles from ferroelectricbatio3by means of x ray lasers a picosecond x ray laser speckle has been conducted to study the dynamics of a disordered surface domain structure (batio3 with 90 degrees c/a domains) as a function of temperature for the first time the transient surface structures induced by ferroelectric domains decrease as temperature increases towards the curie temperature t(c) and completely disappear above t(c) the dramatic change of the spatial configuration of the c/a domains was observed to occur from a temperature 2 degrees c below t(c), near which the average correlated domain size at equilibrium decreases as (t(c) t)(0 37+/ 0 02)",
            "sequence": "[CLS] xray laser applications paper type research objective has system qualities [SEP] picosecond snapshot of the speckles from ferroelectricbatio3by means of x ray lasers a picosecond x ray laser speckle has been conducted to study the dynamics of a disordered surface domain structure (batio3 with 90 degrees c/a domains) as a function of temperature for the first time the transient surface structures induced by ferroelectric domains decrease as temperature increases towards the curie temperature t(c) and completely disappear above t(c) the dramatic change of the spatial configuration of the c/a domains was observed to occur from a temperature 2 degrees c below t(c), near which the average correlated domain size at equilibrium decreases as (t(c) t)(0 37+/ 0 02) [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R114008",
                "label": "Applied Physics"
            }
        },
        {
            "instance_id": "R156306xR156404",
            "template_id": "R156306",
            "paper_id": "R156404",
            "premise": "research objective research objective",
            "hypothesis": "amplification of stimulated soft x ray emission in a confined plasma column \"une amplification atteignant 100 de l'emission stimulee sur l'emission spontanee de la raie cvi 182 a a ete mesuree dans une colonne de plasma magnetiquement confinee, par deux methodes independantes utilisant des monochromateurs uv extreme etalonnes en intensite une confirmation supplementaire que l'amplification est due a l'emission stimulee a ete obtenue avec un miroir rx mou: avec 12% de reflectivite du miroir effective mesuree, une augmentation de 120% de l'intensite de la raie cvi 182 a dans la direction axiale a ete observee\"",
            "sequence": "[CLS] research objective research objective [SEP] amplification of stimulated soft x ray emission in a confined plasma column \"une amplification atteignant 100 de l'emission stimulee sur l'emission spontanee de la raie cvi 182 a a ete mesuree dans une colonne de plasma magnetiquement confinee, par deux methodes independantes utilisant des monochromateurs uv extreme etalonnes en intensite une confirmation supplementaire que l'amplification est due a l'emission stimulee a ete obtenue avec un miroir rx mou: avec 12% de reflectivite du miroir effective mesuree, une augmentation de 120% de l'intensite de la raie cvi 182 a dans la direction axiale a ete observee\" [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            }
        },
        {
            "instance_id": "R186491xR194345",
            "template_id": "R186491",
            "paper_id": "R194345",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "a machine learning based approach for demarcating requirements in textual specifications a simple but important task during the analysis of a textual requirements specification is to determine which statements in the specification represent requirements in principle, by following suitable writing and markup conventions, one can provide an immediate and unequivocal demarcation of requirements at the time a specification is being developed however, neither the presence nor a fully accurate enforcement of such conventions is guaranteed the result is that, in many practical situations, analysts end up resorting to after the fact reviews for sifting requirements from other material in a requirements specification this is both tedious and time consuming we propose an automated approach for demarcating requirements in free form requirements specifications the approach, which is based on machine learning, can be applied to a wide variety of specifications in different domains and with different writing styles we train and evaluate our approach over an independently labeled dataset comprised of 30 industrial requirements specifications over this dataset, our approach yields an average precision of 81 2% and an average recall of 95 7% compared to simple baselines that demarcate requirements based on the presence of modal verbs and identifiers, our approach leads to an average gain of 16 4% in precision and 25 5% in recall",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] a machine learning based approach for demarcating requirements in textual specifications a simple but important task during the analysis of a textual requirements specification is to determine which statements in the specification represent requirements in principle, by following suitable writing and markup conventions, one can provide an immediate and unequivocal demarcation of requirements at the time a specification is being developed however, neither the presence nor a fully accurate enforcement of such conventions is guaranteed the result is that, in many practical situations, analysts end up resorting to after the fact reviews for sifting requirements from other material in a requirements specification this is both tedious and time consuming we propose an automated approach for demarcating requirements in free form requirements specifications the approach, which is based on machine learning, can be applied to a wide variety of specifications in different domains and with different writing styles we train and evaluate our approach over an independently labeled dataset comprised of 30 industrial requirements specifications over this dataset, our approach yields an average precision of 81 2% and an average recall of 95 7% compared to simple baselines that demarcate requirements based on the presence of modal verbs and identifiers, our approach leads to an average gain of 16 4% in precision and 25 5% in recall [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R149061xR145434",
            "template_id": "R149061",
            "paper_id": "R145434",
            "premise": "biodiversity inventories with dna barcoding biogeographical region locus (genetics) higher number estimated species higher number estimated species (method) no of estimated species no of estimated species (method) lower number estimated species lower number estimated species (method) class (taxonomy biology) dna sequencing technology phylum (biology) no of potential undescribed species study location order (taxonomy biology) no of samples (sequences) studied taxonomic group (biology) number of identified species with current taxonomy",
            "hypothesis": "dna barcoding of neotropical sand flies (diptera, psychodidae, phlebotominae): species identification and discovery within brazil dna barcoding has been an effective tool for species identification in several animal groups here, we used dna barcoding to discriminate between 47 morphologically distinct species of brazilian sand flies dna barcodes correctly identified approximately 90% of the sampled taxa (42 morphologically distinct species) using clustering based on neighbor joining distance, of which four species showed comparatively higher maximum values of divergence (range 4 23\u201319 04%), indicating cryptic diversity the dna barcodes also corroborated the resurrection of two species within the shannoni complex and provided an efficient tool to differentiate between morphologically indistinguishable females of closely related species taken together, our results validate the effectiveness of dna barcoding for species identification and the discovery of cryptic diversity in sand flies from brazil",
            "sequence": "[CLS] biodiversity inventories with dna barcoding biogeographical region locus (genetics) higher number estimated species higher number estimated species (method) no of estimated species no of estimated species (method) lower number estimated species lower number estimated species (method) class (taxonomy biology) dna sequencing technology phylum (biology) no of potential undescribed species study location order (taxonomy biology) no of samples (sequences) studied taxonomic group (biology) number of identified species with current taxonomy [SEP] dna barcoding of neotropical sand flies (diptera, psychodidae, phlebotominae): species identification and discovery within brazil dna barcoding has been an effective tool for species identification in several animal groups here, we used dna barcoding to discriminate between 47 morphologically distinct species of brazilian sand flies dna barcodes correctly identified approximately 90% of the sampled taxa (42 morphologically distinct species) using clustering based on neighbor joining distance, of which four species showed comparatively higher maximum values of divergence (range 4 23\u201319 04%), indicating cryptic diversity the dna barcodes also corroborated the resurrection of two species within the shannoni complex and provided an efficient tool to differentiate between morphologically indistinguishable females of closely related species taken together, our results validate the effectiveness of dna barcoding for species identification and the discovery of cryptic diversity in sand flies from brazil [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            }
        },
        {
            "instance_id": "R178304xR182258",
            "template_id": "R178304",
            "paper_id": "R182258",
            "premise": "dataset inter annotator agreement alternatename assesses creator description disambiguatingdescription encoding exampleofwork genre inlanguage isbasedon license name sameas size sourceorganization url version",
            "hypothesis": "a food image recognition system with multiple kernel learning \"since health care on foods is drawing people's attention recently, a system that can record everyday meals easily is being awaited in this paper, we propose an automatic food image recognition system for recording people's eating habits in the proposed system, we use the multiple kernel learning (mkl) method to integrate several kinds of image features such as color, texture and sift adaptively mkl enables to estimate optimal weights to combine image features for each category in addition, we implemented a prototype system to recognize food images taken by cellular phone cameras in the experiment, we have achieved the 61 34% classification rate for 50 kinds of foods to the best of our knowledge, this is the first report of a food image classification system which can be applied for practical use \"",
            "sequence": "[CLS] dataset inter annotator agreement alternatename assesses creator description disambiguatingdescription encoding exampleofwork genre inlanguage isbasedon license name sameas size sourceorganization url version [SEP] a food image recognition system with multiple kernel learning \"since health care on foods is drawing people's attention recently, a system that can record everyday meals easily is being awaited in this paper, we propose an automatic food image recognition system for recording people's eating habits in the proposed system, we use the multiple kernel learning (mkl) method to integrate several kinds of image features such as color, texture and sift adaptively mkl enables to estimate optimal weights to combine image features for each category in addition, we implemented a prototype system to recognize food images taken by cellular phone cameras in the experiment, we have achieved the 61 34% classification rate for 50 kinds of foods to the best of our knowledge, this is the first report of a food image classification system which can be applied for practical use \" [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R133",
                "label": "Artificial Intelligence"
            }
        },
        {
            "instance_id": "R186491xR195578",
            "template_id": "R186491",
            "paper_id": "R195578",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "what requirements knowledge do developers need to manage change in safety critical systems? \"developers maintaining safety critical systems need to assess the impact a proposed change would have upon existing safety controls by leveraging the network of traceability links that are present in most safety critical systems, we can push timely information about related hazards, environmental assumptions, and safety requirements to developers in this work we take a design science approach to discover the informational needs of developers as they engage in software maintenance activities and then propose and evaluate techniques for presenting and visualizing this information through a human centered study involving five safety critical system practitioners and 14 experienced developers, we analyze the way in which developers use requirements knowledge while maintaining safety critical code, identify their informational needs, and propose and evaluate a supporting visualization technique the insights proposed as a result of this study can be used to design requirements based knowledge tools for supporting developers' maintenance tasks \"",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] what requirements knowledge do developers need to manage change in safety critical systems? \"developers maintaining safety critical systems need to assess the impact a proposed change would have upon existing safety controls by leveraging the network of traceability links that are present in most safety critical systems, we can push timely information about related hazards, environmental assumptions, and safety requirements to developers in this work we take a design science approach to discover the informational needs of developers as they engage in software maintenance activities and then propose and evaluate techniques for presenting and visualizing this information through a human centered study involving five safety critical system practitioners and 14 experienced developers, we analyze the way in which developers use requirements knowledge while maintaining safety critical code, identify their informational needs, and propose and evaluate a supporting visualization technique the insights proposed as a result of this study can be used to design requirements based knowledge tools for supporting developers' maintenance tasks \" [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R186491xR113160",
            "template_id": "R186491",
            "paper_id": "R113160",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "customer rating reactions can be predicted purely using app features in this paper we provide empirical evidence that the rating that an app attracts can be accurately predicted from the features it offers our results, based on an analysis of 11,537 apps from the samsung android and blackberry world app stores, indicate that the rating of 89% of these apps can be predicted with 100% accuracy our prediction model is built by using feature and rating information from the existing apps offered in the app store and it yields highly accurate rating predictions, using only a few (11 12) existing apps for case based prediction these findings may have important implications for requirements engineering in app stores: they indicate that app developers may be able to obtain (very accurate) assessments of the customer reaction to their proposed feature sets (requirements), thereby providing new opportunities to support the requirements elicitation process for app developers",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] customer rating reactions can be predicted purely using app features in this paper we provide empirical evidence that the rating that an app attracts can be accurately predicted from the features it offers our results, based on an analysis of 11,537 apps from the samsung android and blackberry world app stores, indicate that the rating of 89% of these apps can be predicted with 100% accuracy our prediction model is built by using feature and rating information from the existing apps offered in the app store and it yields highly accurate rating predictions, using only a few (11 12) existing apps for case based prediction these findings may have important implications for requirements engineering in app stores: they indicate that app developers may be able to obtain (very accurate) assessments of the customer reaction to their proposed feature sets (requirements), thereby providing new opportunities to support the requirements elicitation process for app developers [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R155844xR156292",
            "template_id": "R155844",
            "paper_id": "R156292",
            "premise": "photocatalysts wavelength of maximum absorption energy band gap photocatalyst wavelength of maximum emission emission lifetime ground state oxidation potential ground state reduction potential excited state oxidation potential excited state reduction potential",
            "hypothesis": "simple cu(i) complexes with unprecedented excited state lifetimes this report describes new, readily accessible copper(i) complexes that can exhibit unusually long lived, high quantum yield emissions in fluid solution the complexes are of the form [cu(nn)(pop)]+ where nn denotes 1,10 phenanthroline (phen), 2,9 dimethyl 1,10 phenanthroline (dmp) or 2,9 di n butyl 1,10 phenanthroline (dbp) and pop denotes bis[2 (diphenylphosphino)phenyl] ether modes of characterization include x ray crystallography and cyclic voltammetry the complexes each have a pseudotetrahedral coordination geometry and a cu(ii)/cu(i) potential upward of +1 2 v vs ag/agcl in room temperature dichloromethane solution, charge transfer excited states of the dmp and dbp derivatives exhibit respective emission quantum yields of 0 15 and 0 16 and corresponding excited state lifetimes of 14 3 and 16 1 mus, respectively despite the fact that coordinating solvents usually quench charge transfer emission from copper systems, the photoexcited dmp (dbp) complex retains a lifetime of 2 4 mus (5 4 mus) in methanol",
            "sequence": "[CLS] photocatalysts wavelength of maximum absorption energy band gap photocatalyst wavelength of maximum emission emission lifetime ground state oxidation potential ground state reduction potential excited state oxidation potential excited state reduction potential [SEP] simple cu(i) complexes with unprecedented excited state lifetimes this report describes new, readily accessible copper(i) complexes that can exhibit unusually long lived, high quantum yield emissions in fluid solution the complexes are of the form [cu(nn)(pop)]+ where nn denotes 1,10 phenanthroline (phen), 2,9 dimethyl 1,10 phenanthroline (dmp) or 2,9 di n butyl 1,10 phenanthroline (dbp) and pop denotes bis[2 (diphenylphosphino)phenyl] ether modes of characterization include x ray crystallography and cyclic voltammetry the complexes each have a pseudotetrahedral coordination geometry and a cu(ii)/cu(i) potential upward of +1 2 v vs ag/agcl in room temperature dichloromethane solution, charge transfer excited states of the dmp and dbp derivatives exhibit respective emission quantum yields of 0 15 and 0 16 and corresponding excited state lifetimes of 14 3 and 16 1 mus, respectively despite the fact that coordinating solvents usually quench charge transfer emission from copper systems, the photoexcited dmp (dbp) complex retains a lifetime of 2 4 mus (5 4 mus) in methanol [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R130",
                "label": "Physical Chemistry"
            }
        },
        {
            "instance_id": "R149061xR145491",
            "template_id": "R149061",
            "paper_id": "R145491",
            "premise": "biodiversity inventories with dna barcoding biogeographical region locus (genetics) higher number estimated species higher number estimated species (method) no of estimated species no of estimated species (method) lower number estimated species lower number estimated species (method) class (taxonomy biology) dna sequencing technology phylum (biology) no of potential undescribed species study location order (taxonomy biology) no of samples (sequences) studied taxonomic group (biology) number of identified species with current taxonomy",
            "hypothesis": "dna barcoding of tropical black flies (diptera: simuliidae) of thailand the ecological and medical importance of black flies drives the need for rapid and reliable identification of these minute, structurally uniform insects we assessed the efficiency of dna barcoding for species identification of tropical black flies a total of 351 cytochrome c oxidase subunit 1 sequences were obtained from 41 species in six subgenera of the genus simulium in thailand despite high intraspecific genetic divergence (mean = 2 00%, maximum = 9 27%), dna barcodes provided 96% correct identification barcodes also differentiated cytoforms of selected species complexes, albeit with varying levels of success perfect differentiation was achieved for two cytoforms of simulium feuerborni, and 91% correct identification was obtained for the simulium angulistylum complex low success (33%), however, was obtained for the simulium siamense complex the differential efficiency of dna barcodes to discriminate cytoforms was attributed to different levels of genetic structure and demographic histories of the taxa dna barcode trees were largely congruent with phylogenies based on previous molecular, chromosomal and morphological analyses, but revealed inconsistencies that will require further evaluation",
            "sequence": "[CLS] biodiversity inventories with dna barcoding biogeographical region locus (genetics) higher number estimated species higher number estimated species (method) no of estimated species no of estimated species (method) lower number estimated species lower number estimated species (method) class (taxonomy biology) dna sequencing technology phylum (biology) no of potential undescribed species study location order (taxonomy biology) no of samples (sequences) studied taxonomic group (biology) number of identified species with current taxonomy [SEP] dna barcoding of tropical black flies (diptera: simuliidae) of thailand the ecological and medical importance of black flies drives the need for rapid and reliable identification of these minute, structurally uniform insects we assessed the efficiency of dna barcoding for species identification of tropical black flies a total of 351 cytochrome c oxidase subunit 1 sequences were obtained from 41 species in six subgenera of the genus simulium in thailand despite high intraspecific genetic divergence (mean = 2 00%, maximum = 9 27%), dna barcodes provided 96% correct identification barcodes also differentiated cytoforms of selected species complexes, albeit with varying levels of success perfect differentiation was achieved for two cytoforms of simulium feuerborni, and 91% correct identification was obtained for the simulium angulistylum complex low success (33%), however, was obtained for the simulium siamense complex the differential efficiency of dna barcodes to discriminate cytoforms was attributed to different levels of genetic structure and demographic histories of the taxa dna barcode trees were largely congruent with phylogenies based on previous molecular, chromosomal and morphological analyses, but revealed inconsistencies that will require further evaluation [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            }
        },
        {
            "instance_id": "R172526xR159681",
            "template_id": "R172526",
            "paper_id": "R159681",
            "premise": "video process has study research problem application production",
            "hypothesis": "an experiment in teaching innovation in software engineering: video presentation the dolli project was a large scale educational student project course with a real customer, offered to students in their second year in the time frame of a single semester a functional system was developed and delivered to the customer we experimented with a shift from a traditional life cycle to an agile process during the project, and used video techniques for defining requirements and meeting capture",
            "sequence": "[CLS] video process has study research problem application production [SEP] an experiment in teaching innovation in software engineering: video presentation the dolli project was a large scale educational student project course with a real customer, offered to students in their second year in the time frame of a single semester a functional system was developed and delivered to the customer we experimented with a shift from a traditional life cycle to an agile process during the project, and used video techniques for defining requirements and meeting capture [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R11",
                "label": "Science"
            }
        },
        {
            "instance_id": "R172526xR159762",
            "template_id": "R172526",
            "paper_id": "R159762",
            "premise": "video process has study research problem application production",
            "hypothesis": "video artifacts for design: bridging the gap between abstraction and detail video artifacts help bridge the gap between abstraction and detail in the design process this paper describes how our use and re use of video artifacts affected the re design of a graphical editor for building, simulating, and analyzing coloured petri nets the two primary goals of the project were to create design abstractions that integrate recent advances in graphical interaction techniques and to explicitly support specific patterns of use of petri nets in real world settings \\nusing a participatory design process, we organized a series of video based design activities that helped us manage the tension between finding useful design abstractions and specifying the details of the user interface video artifacts resulting from one activity became the basis for the next, facilitating communication among members of the multi disciplinary design team the video artifacts provided an efficient way of capturing and incorporating subtle aspects of petri nets in use into our design and ensured that the implementation of our design principles was grounded in real world work practices",
            "sequence": "[CLS] video process has study research problem application production [SEP] video artifacts for design: bridging the gap between abstraction and detail video artifacts help bridge the gap between abstraction and detail in the design process this paper describes how our use and re use of video artifacts affected the re design of a graphical editor for building, simulating, and analyzing coloured petri nets the two primary goals of the project were to create design abstractions that integrate recent advances in graphical interaction techniques and to explicitly support specific patterns of use of petri nets in real world settings \\nusing a participatory design process, we organized a series of video based design activities that helped us manage the tension between finding useful design abstractions and specifying the details of the user interface video artifacts resulting from one activity became the basis for the next, facilitating communication among members of the multi disciplinary design team the video artifacts provided an efficient way of capturing and incorporating subtle aspects of petri nets in use into our design and ensured that the implementation of our design principles was grounded in real world work practices [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R11",
                "label": "Science"
            }
        },
        {
            "instance_id": "R186491xR199005",
            "template_id": "R186491",
            "paper_id": "R199005",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "change impact analysis for natural language requirements: an nlp approach requirements are subject to frequent changes as a way to ensure that they reflect the current best understanding of a system, and to respond to factors such as new and evolving needs changing one requirement in a requirements specification may warrant further changes to the specification, so that the overall correctness and consistency of the specification can be maintained a manual analysis of how a change to one requirement impacts other requirements is time consuming and presents a challenge for large requirements specifications we propose an approach based on natural language processing (nlp) for analyzing the impact of change in natural language (nl) requirements our focus on nl requirements is motivated by the prevalent use of these requirements, particularly in industry our approach automatically detects and takes into account the phrasal structure of requirements statements we argue about the importance of capturing the conditions under which change should propagate to enable more accurate change impact analysis we propose a quantitative measure for calculating how likely a requirements statement is to be impacted by a change under given conditions we conduct an evaluation of our approach by applying it to 14 change scenarios from two industrial case studies",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] change impact analysis for natural language requirements: an nlp approach requirements are subject to frequent changes as a way to ensure that they reflect the current best understanding of a system, and to respond to factors such as new and evolving needs changing one requirement in a requirements specification may warrant further changes to the specification, so that the overall correctness and consistency of the specification can be maintained a manual analysis of how a change to one requirement impacts other requirements is time consuming and presents a challenge for large requirements specifications we propose an approach based on natural language processing (nlp) for analyzing the impact of change in natural language (nl) requirements our focus on nl requirements is motivated by the prevalent use of these requirements, particularly in industry our approach automatically detects and takes into account the phrasal structure of requirements statements we argue about the importance of capturing the conditions under which change should propagate to enable more accurate change impact analysis we propose a quantitative measure for calculating how likely a requirements statement is to be impacted by a change under given conditions we conduct an evaluation of our approach by applying it to 14 change scenarios from two industrial case studies [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R149061xR157039",
            "template_id": "R149061",
            "paper_id": "R157039",
            "premise": "biodiversity inventories with dna barcoding biogeographical region locus (genetics) higher number estimated species higher number estimated species (method) no of estimated species no of estimated species (method) lower number estimated species lower number estimated species (method) class (taxonomy biology) dna sequencing technology phylum (biology) no of potential undescribed species study location order (taxonomy biology) no of samples (sequences) studied taxonomic group (biology) number of identified species with current taxonomy",
            "hypothesis": "dna barcode library for european gelechiidae (lepidoptera) suggests greatly underestimated species diversity for the first time, a nearly complete barcode library for european gelechiidae is provided dna barcode sequences (coi gene \u2013 cytochrome c oxidase 1) from 751 out of 865 nominal species, belonging to 105 genera, were successfully recovered a total of 741 species represented by specimens with sequences \u2265 500bp and an additional ten species represented by specimens with shorter sequences were used to produce 53 nj trees intraspecific barcode divergence averaged only 0 54% whereas distance to the nearest neighbour species averaged 5 58% of these, 710 species possessed unique dna barcodes, but 31 species could not be reliably discriminated because of barcode sharing or partial barcode overlap species discrimination based on the barcode index system (bin) was successful for 668 out of 723 species which clustered from minimum one to maximum 22 unique bins fifty five species shared a bin with up to four species and identification from dna barcode data is uncertain finally, 65 clusters with a unique bin remained unidentified to species level these putative taxa, as well as 114 nominal species with more than one bin, suggest the presence of considerable cryptic diversity, cases which should be examined in future revisionary studies",
            "sequence": "[CLS] biodiversity inventories with dna barcoding biogeographical region locus (genetics) higher number estimated species higher number estimated species (method) no of estimated species no of estimated species (method) lower number estimated species lower number estimated species (method) class (taxonomy biology) dna sequencing technology phylum (biology) no of potential undescribed species study location order (taxonomy biology) no of samples (sequences) studied taxonomic group (biology) number of identified species with current taxonomy [SEP] dna barcode library for european gelechiidae (lepidoptera) suggests greatly underestimated species diversity for the first time, a nearly complete barcode library for european gelechiidae is provided dna barcode sequences (coi gene \u2013 cytochrome c oxidase 1) from 751 out of 865 nominal species, belonging to 105 genera, were successfully recovered a total of 741 species represented by specimens with sequences \u2265 500bp and an additional ten species represented by specimens with shorter sequences were used to produce 53 nj trees intraspecific barcode divergence averaged only 0 54% whereas distance to the nearest neighbour species averaged 5 58% of these, 710 species possessed unique dna barcodes, but 31 species could not be reliably discriminated because of barcode sharing or partial barcode overlap species discrimination based on the barcode index system (bin) was successful for 668 out of 723 species which clustered from minimum one to maximum 22 unique bins fifty five species shared a bin with up to four species and identification from dna barcode data is uncertain finally, 65 clusters with a unique bin remained unidentified to species level these putative taxa, as well as 114 nominal species with more than one bin, suggest the presence of considerable cryptic diversity, cases which should be examined in future revisionary studies [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            }
        },
        {
            "instance_id": "R152828xR156448",
            "template_id": "R152828",
            "paper_id": "R156448",
            "premise": "xray laser pumped has system qualities",
            "hypothesis": "soft x ray lasing in neonlike germanium and copper plasmas soft x ray 3p\\\\ensuremath{\\\\rightarrow}3s lasing in neonlike germanium (${\\\\mathrm{ge}}^{22+}$) and copper (${\\\\mathrm{cu}}^{19+}$) in the wavelength interval of 195 to 285 a\\\\r{} is observed for the first time, with gain coefficients ranging from 1 7 to 4 1 ${\\\\mathrm{cm}}^{\\\\mathrm{\\\\ensuremath{ }}1}$, the higher gain with germanium the lasing plasmas are produced by focusing a driving laser beam (\\\\ensuremath{\\\\lambda}=1 05 \\\\ensuremath{\\\\mu}m, 2 ns fwhm) into an 18 mm long line onto thin films and slab targets the measured j=0 to 1 gain coefficients are comparable to those of the j=2 to 1 transitions the measured wavelengths of the six lasing lines compared favorably with recent calculations",
            "sequence": "[CLS] xray laser pumped has system qualities [SEP] soft x ray lasing in neonlike germanium and copper plasmas soft x ray 3p\\\\ensuremath{\\\\rightarrow}3s lasing in neonlike germanium (${\\\\mathrm{ge}}^{22+}$) and copper (${\\\\mathrm{cu}}^{19+}$) in the wavelength interval of 195 to 285 a\\\\r{} is observed for the first time, with gain coefficients ranging from 1 7 to 4 1 ${\\\\mathrm{cm}}^{\\\\mathrm{\\\\ensuremath{ }}1}$, the higher gain with germanium the lasing plasmas are produced by focusing a driving laser beam (\\\\ensuremath{\\\\lambda}=1 05 \\\\ensuremath{\\\\mu}m, 2 ns fwhm) into an 18 mm long line onto thin films and slab targets the measured j=0 to 1 gain coefficients are comparable to those of the j=2 to 1 transitions the measured wavelengths of the six lasing lines compared favorably with recent calculations [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            }
        },
        {
            "instance_id": "R178304xR163109",
            "template_id": "R178304",
            "paper_id": "R163109",
            "premise": "dataset inter annotator agreement alternatename assesses creator description disambiguatingdescription encoding exampleofwork genre inlanguage isbasedon license name sameas size sourceorganization url version",
            "hypothesis": "winer: a wikipedia annotated corpus for named entity recognition we revisit the idea of mining wikipedia in order to generate named entity annotations we propose a new methodology that we applied to english wikipedia to build winer, a large, high quality, annotated corpus we evaluate its usefulness on 6 ner tasks, comparing 4 popular state of the art approaches we show that lstm crf is the approach that benefits the most from our corpus we report impressive gains with this model when using a small portion of winer on top of the conll training material last, we propose a simple but efficient method for exploiting the full range of winer, leading to further improvements",
            "sequence": "[CLS] dataset inter annotator agreement alternatename assesses creator description disambiguatingdescription encoding exampleofwork genre inlanguage isbasedon license name sameas size sourceorganization url version [SEP] winer: a wikipedia annotated corpus for named entity recognition we revisit the idea of mining wikipedia in order to generate named entity annotations we propose a new methodology that we applied to english wikipedia to build winer, a large, high quality, annotated corpus we evaluate its usefulness on 6 ner tasks, comparing 4 popular state of the art approaches we show that lstm crf is the approach that benefits the most from our corpus we report impressive gains with this model when using a small portion of winer on top of the conll training material last, we propose a simple but efficient method for exploiting the full range of winer, leading to further improvements [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            }
        },
        {
            "instance_id": "R138077xR151328",
            "template_id": "R138077",
            "paper_id": "R151328",
            "premise": "ontology learning from data sources has dataset application domain input format output format accuracy f1 score precision knowledge source has data source learning purpose terms learning axiom learning properties learning properties hierarchy learning rule learning class hierarchy learning learning method learning tool evaluation metrics related work validation tool training corpus testing corpus class learning implemented technologies relationships learning instance learning taxonomy learning validation comment assessment of acquired knowledge recall f measure",
            "hypothesis": "neuro symbolic architectures for context understanding \"computational context understanding refers to an agent's ability to fuse disparate sources of information for decision making and is, therefore, generally regarded as a prerequisite for sophisticated machine reasoning capabilities, such as in artificial intelligence (ai) data driven and knowledge driven methods are two classical techniques in the pursuit of such machine sense making capability however, while data driven methods seek to model the statistical regularities of events by making observations in the real world, they remain difficult to interpret and they lack mechanisms for naturally incorporating external knowledge conversely, knowledge driven methods, combine structured knowledge bases, perform symbolic reasoning based on axiomatic principles, and are more interpretable in their inferential processing; however, they often lack the ability to estimate the statistical salience of an inference to combat these issues, we propose the use of hybrid ai methodology as a general framework for combining the strengths of both approaches specifically, we inherit the concept of neuro symbolism as a way of using knowledge bases to guide the learning progress of deep neural networks we further ground our discussion in two applications of neuro symbolism and, in both cases, show that our systems maintain interpretability while achieving comparable performance, relative to the state of the art \"",
            "sequence": "[CLS] ontology learning from data sources has dataset application domain input format output format accuracy f1 score precision knowledge source has data source learning purpose terms learning axiom learning properties learning properties hierarchy learning rule learning class hierarchy learning learning method learning tool evaluation metrics related work validation tool training corpus testing corpus class learning implemented technologies relationships learning instance learning taxonomy learning validation comment assessment of acquired knowledge recall f measure [SEP] neuro symbolic architectures for context understanding \"computational context understanding refers to an agent's ability to fuse disparate sources of information for decision making and is, therefore, generally regarded as a prerequisite for sophisticated machine reasoning capabilities, such as in artificial intelligence (ai) data driven and knowledge driven methods are two classical techniques in the pursuit of such machine sense making capability however, while data driven methods seek to model the statistical regularities of events by making observations in the real world, they remain difficult to interpret and they lack mechanisms for naturally incorporating external knowledge conversely, knowledge driven methods, combine structured knowledge bases, perform symbolic reasoning based on axiomatic principles, and are more interpretable in their inferential processing; however, they often lack the ability to estimate the statistical salience of an inference to combat these issues, we propose the use of hybrid ai methodology as a general framework for combining the strengths of both approaches specifically, we inherit the concept of neuro symbolism as a way of using knowledge bases to guide the learning progress of deep neural networks we further ground our discussion in two applications of neuro symbolism and, in both cases, show that our systems maintain interpretability while achieving comparable performance, relative to the state of the art \" [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R112125",
                "label": "Machine Learning"
            }
        },
        {
            "instance_id": "R186491xR195179",
            "template_id": "R186491",
            "paper_id": "R195179",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "fame: supporting continuous requirements elicitation by combining user feedback and monitoring context: software evolution ensures that software systems in use stay up to date and provide value for end users however, it is challenging for requirements engineers to continuously elicit needs for systems used by heterogeneous end users who are out of organisational reach objective: we aim at supporting continuous requirements elicitation by combining user feedback and usage monitoring online feedback mechanisms enable end users to remotely communicate problems, experiences, and opinions, while monitoring provides valuable information about runtime events it is argued that bringing both information sources together can help requirements engineers to understand end user needs better method/tool: we present fame, a framework for the combined and simultaneous collection of feedback and monitoring data in web and mobile contexts to support continuous requirements elicitation in addition to a detailed discussion of our technical solution, we present the first evidence that fame can be successfully introduced in real world contexts therefore, we deployed fame in a web application of a german small and medium sized enterprise (sme) to collect user feedback and usage data results/conclusion: our results suggest that fame not only can be successfully used in industrial environments but that bringing feedback and monitoring data together helps the sme to improve their understanding of end user needs, ultimately supporting continuous requirements elicitation",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] fame: supporting continuous requirements elicitation by combining user feedback and monitoring context: software evolution ensures that software systems in use stay up to date and provide value for end users however, it is challenging for requirements engineers to continuously elicit needs for systems used by heterogeneous end users who are out of organisational reach objective: we aim at supporting continuous requirements elicitation by combining user feedback and usage monitoring online feedback mechanisms enable end users to remotely communicate problems, experiences, and opinions, while monitoring provides valuable information about runtime events it is argued that bringing both information sources together can help requirements engineers to understand end user needs better method/tool: we present fame, a framework for the combined and simultaneous collection of feedback and monitoring data in web and mobile contexts to support continuous requirements elicitation in addition to a detailed discussion of our technical solution, we present the first evidence that fame can be successfully introduced in real world contexts therefore, we deployed fame in a web application of a german small and medium sized enterprise (sme) to collect user feedback and usage data results/conclusion: our results suggest that fame not only can be successfully used in industrial environments but that bringing feedback and monitoring data together helps the sme to improve their understanding of end user needs, ultimately supporting continuous requirements elicitation [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R186491xR193330",
            "template_id": "R186491",
            "paper_id": "R193330",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "towards achieving trust through transparency and ethics the ubiquitous presence of software in the products we use, together with artificial intelligence in these products, has led to an increasing need for consumer trust consumers often lose faith in products, and the lack of trust propagates to the companies behind them this is even more so in mission critical systems such as autonomous vehicles and clinical support systems this paper follows grounded theory principles to elicit knowledge related to trust, ethics, and transparency we approach these qualities as non functional requirements (nfrs), aiming to build catalogs to subsidize the construction of socially responsible software the corpus we have used was built on a selected collection of literature on corporate social responsibility, with an emphasis on business ethics our challenge is how to encode the social perspective knowledge, mainly through the view of corporate social responsibility, on how organizations or institutions achieve trustworthiness since our ground perspective is that of nfrs, results are presented by a catalogue of trust as a non functional requirement, represented as a softgoal interdependency graph (sig) the sig language helps software engineers in understanding alternatives they have to improve trust in software products",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] towards achieving trust through transparency and ethics the ubiquitous presence of software in the products we use, together with artificial intelligence in these products, has led to an increasing need for consumer trust consumers often lose faith in products, and the lack of trust propagates to the companies behind them this is even more so in mission critical systems such as autonomous vehicles and clinical support systems this paper follows grounded theory principles to elicit knowledge related to trust, ethics, and transparency we approach these qualities as non functional requirements (nfrs), aiming to build catalogs to subsidize the construction of socially responsible software the corpus we have used was built on a selected collection of literature on corporate social responsibility, with an emphasis on business ethics our challenge is how to encode the social perspective knowledge, mainly through the view of corporate social responsibility, on how organizations or institutions achieve trustworthiness since our ground perspective is that of nfrs, results are presented by a catalogue of trust as a non functional requirement, represented as a softgoal interdependency graph (sig) the sig language helps software engineers in understanding alternatives they have to improve trust in software products [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R149061xR145495",
            "template_id": "R149061",
            "paper_id": "R145495",
            "premise": "biodiversity inventories with dna barcoding biogeographical region locus (genetics) higher number estimated species higher number estimated species (method) no of estimated species no of estimated species (method) lower number estimated species lower number estimated species (method) class (taxonomy biology) dna sequencing technology phylum (biology) no of potential undescribed species study location order (taxonomy biology) no of samples (sequences) studied taxonomic group (biology) number of identified species with current taxonomy",
            "hypothesis": "dna barcoding for the identification of sand fly species (diptera, psychodidae, phlebotominae) in colombia sand flies include a group of insects that are of medical importance and that vary in geographic distribution, ecology, and pathogen transmission approximately 163 species of sand flies have been reported in colombia surveillance of the presence of sand fly species and the actualization of species distribution are important for predicting risks for and monitoring the expansion of diseases which sand flies can transmit currently, the identification of phlebotomine sand flies is based on morphological characters however, morphological identification requires considerable skills and taxonomic expertise in addition, significant morphological similarity between some species, especially among females, may cause difficulties during the identification process dna based approaches have become increasingly useful and promising tools for estimating sand fly diversity and for ensuring the rapid and accurate identification of species a partial sequence of the mitochondrial cytochrome oxidase gene subunit i (coi) is currently being used to differentiate species in different animal taxa, including insects, and it is referred as a barcoding sequence the present study explored the utility of the dna barcode approach for the identification of phlebotomine sand flies in colombia we sequenced 700 bp of the coi gene from 36 species collected from different geographic localities the coi barcode sequence divergence within a single species was <2% in most cases, whereas this divergence ranged from 9% to 26 6% among different species these results indicated that the barcoding gene correctly discriminated among the previously morphologically identified species with an efficacy of nearly 100% analyses of the generated sequences indicated that the observed species groupings were consistent with the morphological identifications in conclusion, the barcoding gene was useful for species discrimination in sand flies from colombia",
            "sequence": "[CLS] biodiversity inventories with dna barcoding biogeographical region locus (genetics) higher number estimated species higher number estimated species (method) no of estimated species no of estimated species (method) lower number estimated species lower number estimated species (method) class (taxonomy biology) dna sequencing technology phylum (biology) no of potential undescribed species study location order (taxonomy biology) no of samples (sequences) studied taxonomic group (biology) number of identified species with current taxonomy [SEP] dna barcoding for the identification of sand fly species (diptera, psychodidae, phlebotominae) in colombia sand flies include a group of insects that are of medical importance and that vary in geographic distribution, ecology, and pathogen transmission approximately 163 species of sand flies have been reported in colombia surveillance of the presence of sand fly species and the actualization of species distribution are important for predicting risks for and monitoring the expansion of diseases which sand flies can transmit currently, the identification of phlebotomine sand flies is based on morphological characters however, morphological identification requires considerable skills and taxonomic expertise in addition, significant morphological similarity between some species, especially among females, may cause difficulties during the identification process dna based approaches have become increasingly useful and promising tools for estimating sand fly diversity and for ensuring the rapid and accurate identification of species a partial sequence of the mitochondrial cytochrome oxidase gene subunit i (coi) is currently being used to differentiate species in different animal taxa, including insects, and it is referred as a barcoding sequence the present study explored the utility of the dna barcode approach for the identification of phlebotomine sand flies in colombia we sequenced 700 bp of the coi gene from 36 species collected from different geographic localities the coi barcode sequence divergence within a single species was <2% in most cases, whereas this divergence ranged from 9% to 26 6% among different species these results indicated that the barcoding gene correctly discriminated among the previously morphologically identified species with an efficacy of nearly 100% analyses of the generated sequences indicated that the observed species groupings were consistent with the morphological identifications in conclusion, the barcoding gene was useful for species discrimination in sand flies from colombia [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            }
        },
        {
            "instance_id": "R149061xR145296",
            "template_id": "R149061",
            "paper_id": "R145296",
            "premise": "biodiversity inventories with dna barcoding biogeographical region locus (genetics) higher number estimated species higher number estimated species (method) no of estimated species no of estimated species (method) lower number estimated species lower number estimated species (method) class (taxonomy biology) dna sequencing technology phylum (biology) no of potential undescribed species study location order (taxonomy biology) no of samples (sequences) studied taxonomic group (biology) number of identified species with current taxonomy",
            "hypothesis": "molecular identification of mosquitoes (diptera: culicidae) in southeastern australia abstract dna barcoding is a modern species identification technique that can be used to distinguish morphologically similar species, and is particularly useful when using small amounts of starting material from partial specimens or from immature stages in order to use dna barcoding in a surveillance program, a database containing mosquito barcode sequences is required this study obtained cytochrome oxidase i (coi) sequences for 113 morphologically identified specimens, representing 29 species, six tribes and 12 genera; 17 of these species have not been previously barcoded three of the 29 species \u2500 culex palpalis, macleaya macmillani, and an unknown species originally identified as tripteroides atripes \u2500 were initially misidentified as they are difficult to separate morphologically, highlighting the utility of dna barcoding while most species grouped separately (reciprocally monophyletic), the cx pipiens subgroup could not be genetically separated using coi the average conspecific and congeneric p\u2010distance was 0 8% and 7 6%, respectively in our study, we also demonstrate the utility of dna barcoding in distinguishing exotics from endemic mosquitoes by identifying a single intercepted stegomyia aegypti egg at an international airport the use of dna barcoding dramatically reduced the identification time required compared with rearing specimens through to adults, thereby demonstrating the value of this technique in biosecurity surveillance the dna barcodes produced by this study have been uploaded to the \u2018mosquitoes of australia\u2013victoria\u2019 project on the barcode of life database (bold), which will serve as a resource for the victorian arbovirus disease control program and other national and international mosquito surveillance programs",
            "sequence": "[CLS] biodiversity inventories with dna barcoding biogeographical region locus (genetics) higher number estimated species higher number estimated species (method) no of estimated species no of estimated species (method) lower number estimated species lower number estimated species (method) class (taxonomy biology) dna sequencing technology phylum (biology) no of potential undescribed species study location order (taxonomy biology) no of samples (sequences) studied taxonomic group (biology) number of identified species with current taxonomy [SEP] molecular identification of mosquitoes (diptera: culicidae) in southeastern australia abstract dna barcoding is a modern species identification technique that can be used to distinguish morphologically similar species, and is particularly useful when using small amounts of starting material from partial specimens or from immature stages in order to use dna barcoding in a surveillance program, a database containing mosquito barcode sequences is required this study obtained cytochrome oxidase i (coi) sequences for 113 morphologically identified specimens, representing 29 species, six tribes and 12 genera; 17 of these species have not been previously barcoded three of the 29 species \u2500 culex palpalis, macleaya macmillani, and an unknown species originally identified as tripteroides atripes \u2500 were initially misidentified as they are difficult to separate morphologically, highlighting the utility of dna barcoding while most species grouped separately (reciprocally monophyletic), the cx pipiens subgroup could not be genetically separated using coi the average conspecific and congeneric p\u2010distance was 0 8% and 7 6%, respectively in our study, we also demonstrate the utility of dna barcoding in distinguishing exotics from endemic mosquitoes by identifying a single intercepted stegomyia aegypti egg at an international airport the use of dna barcoding dramatically reduced the identification time required compared with rearing specimens through to adults, thereby demonstrating the value of this technique in biosecurity surveillance the dna barcodes produced by this study have been uploaded to the \u2018mosquitoes of australia\u2013victoria\u2019 project on the barcode of life database (bold), which will serve as a resource for the victorian arbovirus disease control program and other national and international mosquito surveillance programs [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            }
        },
        {
            "instance_id": "R186491xR194922",
            "template_id": "R186491",
            "paper_id": "R194922",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "the next release problem revisited: a new avenue for goal models context goal models have long been critiqued for the time it takes to construct them as well as for their limited cognitive and visual scalability is such criticism general or does it depend on the supported task? objectives we advocate for the latter and the aim of this paper is to demonstrate that the next release problem is a suitable application domain for goal models this hypothesis stems from the fact that product release management is a long term investment, and software products are commonly managed in \"themes\" which are smaller focus areas of the product methods we employ a version of goal models that is tailored for the next release problem by capturing requirements, synergies among them, constraints, and release objectives such goal model allows discovering optimal solutions considering multiple criteria for the next release results a retrospective case study confirms that goal models are easier to read and comprehend when organized in themes, and that the reasoning results help product managers decide for the next release our scalability experiments show that, through reasoning based on optimization modulo theories, the discovery of the optimal solution is fast and scales sufficiently well with respect to the model size, connectivity, and number of alternative solutions",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] the next release problem revisited: a new avenue for goal models context goal models have long been critiqued for the time it takes to construct them as well as for their limited cognitive and visual scalability is such criticism general or does it depend on the supported task? objectives we advocate for the latter and the aim of this paper is to demonstrate that the next release problem is a suitable application domain for goal models this hypothesis stems from the fact that product release management is a long term investment, and software products are commonly managed in \"themes\" which are smaller focus areas of the product methods we employ a version of goal models that is tailored for the next release problem by capturing requirements, synergies among them, constraints, and release objectives such goal model allows discovering optimal solutions considering multiple criteria for the next release results a retrospective case study confirms that goal models are easier to read and comprehend when organized in themes, and that the reasoning results help product managers decide for the next release our scalability experiments show that, through reasoning based on optimization modulo theories, the discovery of the optimal solution is fast and scales sufficiently well with respect to the model size, connectivity, and number of alternative solutions [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R172526xR159700",
            "template_id": "R172526",
            "paper_id": "R159700",
            "premise": "video process has study research problem application production",
            "hypothesis": "feed me, feed me: an exemplar for engineering adaptive software the internet of things (iot) promises to deliver improved quality of life for citizens, through pervasive connectivity and quantified monitoring of devices, people, and their environment as such, the iot presents a major new opportunity for research in adaptive software engineering however, there are currently no shared exemplars that can support software engineering researchers to explore and potentially address the challenges of engineering adaptive software for the iot, and to comparatively evaluate proposed solutions in this paper, we present feed me, feed me, an exemplar that represents an iot based ecosystem to support food security at different levels of granularity: individuals, families, cities, and nations we describe this exemplar using animated videos which highlight the requirements that have been informally observed to play a critical role in the success or failure of iot based software systems these requirements are: security and privacy, interoperability, adaptation, and personalisation to elicit a wide spectrum of user reactions, we created these animated videos based on the contravision empirical methodology, which specifically supports the elicitation of end user requirements for controversial or futuristic technologies our deployment of contravision presented our pilot study subjects with an equal number of utopian and dystopian scenarios, derived from the food security domain, and described them at the different level of granularity our synthesis of the preliminary empirical findings suggests a number of key requirements and software engineering research challenges in this area we offer these to the research community, together with a rich exemplar and associated scenarios available in both their textual form in the paper, and as a series of animated videos (http://sead1 open ac uk/fmfm/)",
            "sequence": "[CLS] video process has study research problem application production [SEP] feed me, feed me: an exemplar for engineering adaptive software the internet of things (iot) promises to deliver improved quality of life for citizens, through pervasive connectivity and quantified monitoring of devices, people, and their environment as such, the iot presents a major new opportunity for research in adaptive software engineering however, there are currently no shared exemplars that can support software engineering researchers to explore and potentially address the challenges of engineering adaptive software for the iot, and to comparatively evaluate proposed solutions in this paper, we present feed me, feed me, an exemplar that represents an iot based ecosystem to support food security at different levels of granularity: individuals, families, cities, and nations we describe this exemplar using animated videos which highlight the requirements that have been informally observed to play a critical role in the success or failure of iot based software systems these requirements are: security and privacy, interoperability, adaptation, and personalisation to elicit a wide spectrum of user reactions, we created these animated videos based on the contravision empirical methodology, which specifically supports the elicitation of end user requirements for controversial or futuristic technologies our deployment of contravision presented our pilot study subjects with an equal number of utopian and dystopian scenarios, derived from the food security domain, and described them at the different level of granularity our synthesis of the preliminary empirical findings suggests a number of key requirements and software engineering research challenges in this area we offer these to the research community, together with a rich exemplar and associated scenarios available in both their textual form in the paper, and as a series of animated videos (http://sead1 open ac uk/fmfm/) [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R11",
                "label": "Science"
            }
        },
        {
            "instance_id": "R146876xR146997",
            "template_id": "R146876",
            "paper_id": "R146997",
            "premise": "organic solar cells mobility donor acceptor lumo homo energy band gap open circuit voltage, voc short circuit current density, jsc fill factor, ff power conversion efficiency",
            "hypothesis": "enhancing the performance of organic solar cells by hierarchically supramolecular self assembly of fused ring electron acceptors three novel non fullerene small molecular acceptors itoic, itoic f, and itoic 2f were designed and synthesized with easy chemistry the concept of supramolecular chemistry was successfully used in the molecular design, which includes noncovalently conformational locking (via intrasupramolecular interaction) to enhance the planarity of backbone and electrostatic interaction (intersupramolecular interaction) to enhance the \u03c0\u2013\u03c0 stacking of terminal groups fluorination can further strengthen the intersupramolecular electrostatic interaction of terminal groups as expected, the designed acceptors exhibited excellent device performance when blended with polymer donor pbdb t in comparison with the parent acceptor molecule dc idt2t reported in the literature with a power conversion efficiency (pce) of 3 93%, itoic with a planar structure exhibited a pce of 8 87% and itoic 2f with a planar structure and enhanced electrostatic interaction showed a quite impressive pce of 12 17% our result demonstrates the import",
            "sequence": "[CLS] organic solar cells mobility donor acceptor lumo homo energy band gap open circuit voltage, voc short circuit current density, jsc fill factor, ff power conversion efficiency [SEP] enhancing the performance of organic solar cells by hierarchically supramolecular self assembly of fused ring electron acceptors three novel non fullerene small molecular acceptors itoic, itoic f, and itoic 2f were designed and synthesized with easy chemistry the concept of supramolecular chemistry was successfully used in the molecular design, which includes noncovalently conformational locking (via intrasupramolecular interaction) to enhance the planarity of backbone and electrostatic interaction (intersupramolecular interaction) to enhance the \u03c0\u2013\u03c0 stacking of terminal groups fluorination can further strengthen the intersupramolecular electrostatic interaction of terminal groups as expected, the designed acceptors exhibited excellent device performance when blended with polymer donor pbdb t in comparison with the parent acceptor molecule dc idt2t reported in the literature with a power conversion efficiency (pce) of 3 93%, itoic with a planar structure exhibited a pce of 8 87% and itoic 2f with a planar structure and enhanced electrostatic interaction showed a quite impressive pce of 12 17% our result demonstrates the import [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            }
        },
        {
            "instance_id": "R184022xR190520",
            "template_id": "R184022",
            "paper_id": "R190520",
            "premise": "xray spectroscopy paper type research objective has system qualities",
            "hypothesis": "high precision spectroscopic studies of few electron ions in this report some experiments are described in which the lyman cap alpha lines of hydrogen like and helium like argon and iron ions have been measured the principle of all these experiments was to study with a crystal spectrometer the x rays emitted in flight by the ions, at 90/sup 0/ with respect to the direction of the beam 5 references, 14 figures, 9 tables",
            "sequence": "[CLS] xray spectroscopy paper type research objective has system qualities [SEP] high precision spectroscopic studies of few electron ions in this report some experiments are described in which the lyman cap alpha lines of hydrogen like and helium like argon and iron ions have been measured the principle of all these experiments was to study with a crystal spectrometer the x rays emitted in flight by the ions, at 90/sup 0/ with respect to the direction of the beam 5 references, 14 figures, 9 tables [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R114010",
                "label": "Atomic Physics"
            }
        },
        {
            "instance_id": "R146876xR148232",
            "template_id": "R146876",
            "paper_id": "R148232",
            "premise": "organic solar cells mobility donor acceptor lumo homo energy band gap open circuit voltage, voc short circuit current density, jsc fill factor, ff power conversion efficiency",
            "hypothesis": "enhancing performance of nonfullerene acceptors via side\u2010chain conjugation strategy a side\u2010chain conjugation strategy in the design of nonfullerene electron acceptors is proposed, with the design and synthesis of a side\u2010chain\u2010conjugated acceptor (itic2) based on a 4,8\u2010bis(5\u2010(2\u2010ethylhexyl)thiophen\u20102\u2010yl)benzo[1,2\u2010b:4,5\u2010b\u2032]di(cyclopenta\u2010dithiophene) electron\u2010donating core and 1,1\u2010dicyanomethylene\u20103\u2010indanone electron\u2010withdrawing end groups itic2 with the conjugated side chains exhibits an absorption peak at 714 nm, which redshifts 12 nm relative to itic1 the absorption extinction coefficient of itic2 is 2 7 \u00d7 105m\u22121 cm\u22121, higher than that of itic1 (1 5 \u00d7 105m\u22121 cm\u22121) itic2 exhibits slightly higher highest occupied molecular orbital (homo) (\u22125 43 ev) and lowest unoccupied molecular orbital (lumo) (\u22123 80 ev) energy levels relative to itic1 (homo: \u22125 48 ev; lumo: \u22123 84 ev), and higher electron mobility (1 3 \u00d7 10\u22123 cm2 v\u22121 s\u22121) than that of itic1 (9 6 \u00d7 10\u22124 cm2 v\u22121 s\u22121) the power conversion efficiency of itic2\u2010based organic solar cells is 11 0%, much higher than that of itic1\u2010based control devices (8 54%) our results demonstrate that side\u2010chain conjugation can tune energy levels, enhance absorption, and electron mobility, and finally enhance photovoltaic performance of nonfullerene acceptors",
            "sequence": "[CLS] organic solar cells mobility donor acceptor lumo homo energy band gap open circuit voltage, voc short circuit current density, jsc fill factor, ff power conversion efficiency [SEP] enhancing performance of nonfullerene acceptors via side\u2010chain conjugation strategy a side\u2010chain conjugation strategy in the design of nonfullerene electron acceptors is proposed, with the design and synthesis of a side\u2010chain\u2010conjugated acceptor (itic2) based on a 4,8\u2010bis(5\u2010(2\u2010ethylhexyl)thiophen\u20102\u2010yl)benzo[1,2\u2010b:4,5\u2010b\u2032]di(cyclopenta\u2010dithiophene) electron\u2010donating core and 1,1\u2010dicyanomethylene\u20103\u2010indanone electron\u2010withdrawing end groups itic2 with the conjugated side chains exhibits an absorption peak at 714 nm, which redshifts 12 nm relative to itic1 the absorption extinction coefficient of itic2 is 2 7 \u00d7 105m\u22121 cm\u22121, higher than that of itic1 (1 5 \u00d7 105m\u22121 cm\u22121) itic2 exhibits slightly higher highest occupied molecular orbital (homo) (\u22125 43 ev) and lowest unoccupied molecular orbital (lumo) (\u22123 80 ev) energy levels relative to itic1 (homo: \u22125 48 ev; lumo: \u22123 84 ev), and higher electron mobility (1 3 \u00d7 10\u22123 cm2 v\u22121 s\u22121) than that of itic1 (9 6 \u00d7 10\u22124 cm2 v\u22121 s\u22121) the power conversion efficiency of itic2\u2010based organic solar cells is 11 0%, much higher than that of itic1\u2010based control devices (8 54%) our results demonstrate that side\u2010chain conjugation can tune energy levels, enhance absorption, and electron mobility, and finally enhance photovoltaic performance of nonfullerene acceptors [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            }
        },
        {
            "instance_id": "R160259xR160422",
            "template_id": "R160259",
            "paper_id": "R160422",
            "premise": "digital city twin review application area",
            "hypothesis": "a novel cloud based framework for the elderly healthcare services using digital twin with the development of technologies, such as big data, cloud computing, and the internet of things (iot), digital twin is being applied in industry as a precision simulation technology from concept to practice further, simulation plays a very important role in the healthcare field, especially in research on medical pathway planning, medical resource allocation, medical activity prediction, etc by combining digital twin and healthcare, there will be a new and efficient way to provide more accurate and fast services for elderly healthcare however, how to achieve personal health management throughout the entire lifecycle of elderly patients, and how to converge the medical physical world and the virtual world to realize real smart healthcare, are still two key challenges in the era of precision medicine in this paper, a framework of the cloud healthcare system is proposed based on digital twin healthcare (clouddth) this is a novel, generalized, and extensible framework in the cloud environment for monitoring, diagnosing and predicting aspects of the health of individuals using, for example, wearable medical devices, toward the goal of personal health management, especially for the elderly clouddth aims to achieve interaction and convergence between medical physical and virtual spaces accordingly, a novel concept of digital twin healthcare (dth) is proposed and discussed, and a dth model is implemented next, a reference framework of clouddth based on dth is constructed, and its key enabling technologies are explored finally, the feasibility of some application scenarios and a case study for real time supervision are demonstrated",
            "sequence": "[CLS] digital city twin review application area [SEP] a novel cloud based framework for the elderly healthcare services using digital twin with the development of technologies, such as big data, cloud computing, and the internet of things (iot), digital twin is being applied in industry as a precision simulation technology from concept to practice further, simulation plays a very important role in the healthcare field, especially in research on medical pathway planning, medical resource allocation, medical activity prediction, etc by combining digital twin and healthcare, there will be a new and efficient way to provide more accurate and fast services for elderly healthcare however, how to achieve personal health management throughout the entire lifecycle of elderly patients, and how to converge the medical physical world and the virtual world to realize real smart healthcare, are still two key challenges in the era of precision medicine in this paper, a framework of the cloud healthcare system is proposed based on digital twin healthcare (clouddth) this is a novel, generalized, and extensible framework in the cloud environment for monitoring, diagnosing and predicting aspects of the health of individuals using, for example, wearable medical devices, toward the goal of personal health management, especially for the elderly clouddth aims to achieve interaction and convergence between medical physical and virtual spaces accordingly, a novel concept of digital twin healthcare (dth) is proposed and discussed, and a dth model is implemented next, a reference framework of clouddth based on dth is constructed, and its key enabling technologies are explored finally, the feasibility of some application scenarios and a case study for real time supervision are demonstrated [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            }
        },
        {
            "instance_id": "R149061xR145506",
            "template_id": "R149061",
            "paper_id": "R145506",
            "premise": "biodiversity inventories with dna barcoding biogeographical region locus (genetics) higher number estimated species higher number estimated species (method) no of estimated species no of estimated species (method) lower number estimated species lower number estimated species (method) class (taxonomy biology) dna sequencing technology phylum (biology) no of potential undescribed species study location order (taxonomy biology) no of samples (sequences) studied taxonomic group (biology) number of identified species with current taxonomy",
            "hypothesis": "identification of nearctic black flies using dna barcodes (diptera: simuliidae) dna barcoding has gained increased recognition as a molecular tool for species identification in various groups of organisms in this preliminary study, we tested the efficacy of a 615\u2010bp fragment of the cytochrome c oxidase i (coi) as a dna barcode in the medically important family simuliidae, or black flies a total of 65 (25%) morphologically distinct species and sibling species in species complexes of the 255 recognized nearctic black fly species were used to create a preliminary barcode profile for the family genetic divergence among congeners averaged 14 93% (range 2 83\u201315 33%), whereas intraspecific genetic divergence between morphologically distinct species averaged 0 72% (range 0\u20133 84%) dna barcodes correctly identified nearly 100% of the morphologically distinct species (87% of the total sampled taxa), whereas in species complexes (13% of the sampled taxa) maximum values of divergence were comparatively higher (max 4 58\u20136 5%), indicating cryptic diversity the existence of sibling species in prosimulium travisi and p neomacropyga was also demonstrated, thus confirming previous cytological evidence about the existence of such cryptic diversity in these two taxa we conclude that dna barcoding is an effective method for species identification and discovery of cryptic diversity in black flies",
            "sequence": "[CLS] biodiversity inventories with dna barcoding biogeographical region locus (genetics) higher number estimated species higher number estimated species (method) no of estimated species no of estimated species (method) lower number estimated species lower number estimated species (method) class (taxonomy biology) dna sequencing technology phylum (biology) no of potential undescribed species study location order (taxonomy biology) no of samples (sequences) studied taxonomic group (biology) number of identified species with current taxonomy [SEP] identification of nearctic black flies using dna barcodes (diptera: simuliidae) dna barcoding has gained increased recognition as a molecular tool for species identification in various groups of organisms in this preliminary study, we tested the efficacy of a 615\u2010bp fragment of the cytochrome c oxidase i (coi) as a dna barcode in the medically important family simuliidae, or black flies a total of 65 (25%) morphologically distinct species and sibling species in species complexes of the 255 recognized nearctic black fly species were used to create a preliminary barcode profile for the family genetic divergence among congeners averaged 14 93% (range 2 83\u201315 33%), whereas intraspecific genetic divergence between morphologically distinct species averaged 0 72% (range 0\u20133 84%) dna barcodes correctly identified nearly 100% of the morphologically distinct species (87% of the total sampled taxa), whereas in species complexes (13% of the sampled taxa) maximum values of divergence were comparatively higher (max 4 58\u20136 5%), indicating cryptic diversity the existence of sibling species in prosimulium travisi and p neomacropyga was also demonstrated, thus confirming previous cytological evidence about the existence of such cryptic diversity in these two taxa we conclude that dna barcoding is an effective method for species identification and discovery of cryptic diversity in black flies [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            }
        }
    ],
    "contradictions": [
        {
            "instance_id": "R138077xR195918",
            "template_id": "R138077",
            "correct_template_id": "R187648",
            "paper_id": "R195918",
            "premise": "ontology learning from data sources has dataset application domain input format output format accuracy f1 score precision knowledge source has data source learning purpose terms learning axiom learning properties learning properties hierarchy learning rule learning class hierarchy learning learning method learning tool evaluation metrics related work validation tool training corpus testing corpus class learning implemented technologies relationships learning instance learning taxonomy learning validation comment assessment of acquired knowledge recall f measure",
            "hypothesis": "online stochastic optimization for unknown linear systems: data driven synthesis and controller analysis this paper proposes a data driven control framework to regulate an unknown, stochastic linear dynamical system to the solution of a (stochastic) convex optimization problem despite the centrality of this problem, most of the available methods critically rely on a precise knowledge of the system dynamics (thus requiring off line system identification and model refinement) to this aim, in this paper we first show that the steady state transfer function of a linear system can be computed directly from control experiments, bypassing explicit model identification then, we leverage the estimated transfer function to design a controller \u2013 which is inspired by stochastic gradient descent methods \u2013 that regulates the system to the solution of the prescribed optimization problem a distinguishing feature of our methods is that they do not require any knowledge of the system dynamics, disturbance terms, or their distributions our technical analysis combines concepts and tools from behavioral system theory, stochastic optimization with decision dependent distributions, and stability analysis we illustrate the applicability of the framework on a case study for mobility on demand ride service scheduling in manhattan, ny",
            "sequence": "[CLS] ontology learning from data sources has dataset application domain input format output format accuracy f1 score precision knowledge source has data source learning purpose terms learning axiom learning properties learning properties hierarchy learning rule learning class hierarchy learning learning method learning tool evaluation metrics related work validation tool training corpus testing corpus class learning implemented technologies relationships learning instance learning taxonomy learning validation comment assessment of acquired knowledge recall f measure [SEP] online stochastic optimization for unknown linear systems: data driven synthesis and controller analysis this paper proposes a data driven control framework to regulate an unknown, stochastic linear dynamical system to the solution of a (stochastic) convex optimization problem despite the centrality of this problem, most of the available methods critically rely on a precise knowledge of the system dynamics (thus requiring off line system identification and model refinement) to this aim, in this paper we first show that the steady state transfer function of a linear system can be computed directly from control experiments, bypassing explicit model identification then, we leverage the estimated transfer function to design a controller \u2013 which is inspired by stochastic gradient descent methods \u2013 that regulates the system to the solution of the prescribed optimization problem a distinguishing feature of our methods is that they do not require any knowledge of the system dynamics, disturbance terms, or their distributions our technical analysis combines concepts and tools from behavioral system theory, stochastic optimization with decision dependent distributions, and stability analysis we illustrate the applicability of the framework on a case study for mobility on demand ride service scheduling in manhattan, ny [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R109",
                "label": "Control Theory"
            }
        },
        {
            "instance_id": "R146876xR195519",
            "template_id": "R146876",
            "correct_template_id": null,
            "paper_id": "R195519",
            "premise": "organic solar cells mobility donor acceptor lumo homo energy band gap open circuit voltage, voc short circuit current density, jsc fill factor, ff power conversion efficiency",
            "hypothesis": "using argumentation to explain ambiguity in requirements elicitation interviews \"the requirements elicitation process often starts with an interview between a customer and a requirements analyst during these interviews, ambiguities in the dialogic discourse may reveal the presence of tacit knowledge that needs to be made explicit it is therefore important to understand the nature of ambiguities in interviews and to provide analysts with cognitive tools to identify and alleviate ambiguities ambiguities perceived by analysts are sometimes triggered by specific categories of terms used by the customer such as pronouns, quantifiers, and vague or under specified terms however, many of the ambiguities that arise in practice cannot be rooted in single terms rather, entire fragments of speech and their relation to the mental state of the analyst need to be considered in this paper, we show that particular types of ambiguities can be characterised by means of argumentation theory argumentation is the study of how conclusions can be reached through logical reasoning in an argumentation theory, statements are represented as arguments, and conflict relations among statements are represented as attacks based on a set of ambiguous fragments extracted from interviews, we define a model of the mental state of the analyst during an interview and translate it into an argumentation theory then, we show that many of the ambiguities can be characterized in terms of 'attacks' on arguments the main novelty of this work is in addressing the problem of explaining fragment level ambiguities in requirements elicitation interviews through the formal modeling of the analyst's mental model using argumentation theory our contribution provides a data grounded, theoretical basis to have a more complete understanding of the ambiguity phenomenon, and lays the foundations to design intelligent computer based agents that are able to automatically identify ambiguities \"",
            "sequence": "[CLS] organic solar cells mobility donor acceptor lumo homo energy band gap open circuit voltage, voc short circuit current density, jsc fill factor, ff power conversion efficiency [SEP] using argumentation to explain ambiguity in requirements elicitation interviews \"the requirements elicitation process often starts with an interview between a customer and a requirements analyst during these interviews, ambiguities in the dialogic discourse may reveal the presence of tacit knowledge that needs to be made explicit it is therefore important to understand the nature of ambiguities in interviews and to provide analysts with cognitive tools to identify and alleviate ambiguities ambiguities perceived by analysts are sometimes triggered by specific categories of terms used by the customer such as pronouns, quantifiers, and vague or under specified terms however, many of the ambiguities that arise in practice cannot be rooted in single terms rather, entire fragments of speech and their relation to the mental state of the analyst need to be considered in this paper, we show that particular types of ambiguities can be characterised by means of argumentation theory argumentation is the study of how conclusions can be reached through logical reasoning in an argumentation theory, statements are represented as arguments, and conflict relations among statements are represented as attacks based on a set of ambiguous fragments extracted from interviews, we define a model of the mental state of the analyst during an interview and translate it into an argumentation theory then, we show that many of the ambiguities can be characterized in terms of 'attacks' on arguments the main novelty of this work is in addressing the problem of explaining fragment level ambiguities in requirements elicitation interviews through the formal modeling of the analyst's mental model using argumentation theory our contribution provides a data grounded, theoretical basis to have a more complete understanding of the ambiguity phenomenon, and lays the foundations to design intelligent computer based agents that are able to automatically identify ambiguities \" [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R146876xR139130",
            "template_id": "R146876",
            "correct_template_id": null,
            "paper_id": "R139130",
            "premise": "organic solar cells mobility donor acceptor lumo homo energy band gap open circuit voltage, voc short circuit current density, jsc fill factor, ff power conversion efficiency",
            "hypothesis": "atmospheric plasma vuv photon emission owing to its distinctive photon energy range, vacuum ultraviolet (vuv) emission plays a key role in diverse photo induced natural and technological processes atmospheric pressure plasma produced vuv is central to resolve long held issues in dynamics of natural (e g , lightning) and laboratory (e g , streamer) plasmas challenging the seemingly unavoidable vacuum systems used to prevent vuv emission quenching by ambient gases, here we report the first observation of vacuum free generation of stable sub 110 nm vuv emission from atmospheric pressure plasmas jetted into open air and atmospheric air plasma emission from atomic helium at 58 4 nm is observed from a nonequilibrium atmospheric pressure plasma jet (n appj), jetted directly into ambient air in a similar experiment, we also report vuv emission from excited nitrogen species in an atmospheric pressure discharge in ambient air the photon emissions detected expand the window of photo induced processes beyond \u223c10 ev commonly achievable by existing non excimer vuv plasma sources, and enables direct photo excitation and ionization of molecular species such as co2 and many others the thus enabled direct photoionization of o2, o, and n species further justifies the role of direct photoionization in the dynamics of natural and laboratory atmospheric pressure plasmas and informs the development of the relevant plasma photoionization models, which currently largely sidestep the sub 110 nm domain these findings can make contribution to the complement of photoionization model of lightning, streamer, and other plasmas, open new avenues to quantify the yet elusive role of photoionization in the plasma dynamics",
            "sequence": "[CLS] organic solar cells mobility donor acceptor lumo homo energy band gap open circuit voltage, voc short circuit current density, jsc fill factor, ff power conversion efficiency [SEP] atmospheric plasma vuv photon emission owing to its distinctive photon energy range, vacuum ultraviolet (vuv) emission plays a key role in diverse photo induced natural and technological processes atmospheric pressure plasma produced vuv is central to resolve long held issues in dynamics of natural (e g , lightning) and laboratory (e g , streamer) plasmas challenging the seemingly unavoidable vacuum systems used to prevent vuv emission quenching by ambient gases, here we report the first observation of vacuum free generation of stable sub 110 nm vuv emission from atmospheric pressure plasmas jetted into open air and atmospheric air plasma emission from atomic helium at 58 4 nm is observed from a nonequilibrium atmospheric pressure plasma jet (n appj), jetted directly into ambient air in a similar experiment, we also report vuv emission from excited nitrogen species in an atmospheric pressure discharge in ambient air the photon emissions detected expand the window of photo induced processes beyond \u223c10 ev commonly achievable by existing non excimer vuv plasma sources, and enables direct photo excitation and ionization of molecular species such as co2 and many others the thus enabled direct photoionization of o2, o, and n species further justifies the role of direct photoionization in the dynamics of natural and laboratory atmospheric pressure plasmas and informs the development of the relevant plasma photoionization models, which currently largely sidestep the sub 110 nm domain these findings can make contribution to the complement of photoionization model of lightning, streamer, and other plasmas, open new avenues to quantify the yet elusive role of photoionization in the plasma dynamics [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            }
        },
        {
            "instance_id": "R52190xR25144",
            "template_id": "R52190",
            "correct_template_id": null,
            "paper_id": "R25144",
            "premise": "transient absorption spectroscopy has participating device has participating person has gaseous sample input has solution sample input has solid sample input has radiant energy input has output",
            "hypothesis": "gps enabled speed control embedded system speed limiting device with display and engine control interface \"in the past decade, there have been close to 350,000 fatal crashes in the united states [1] with various improvements in traffic and vehicle safety, the number of such crashes is decreasing every year one of the ways to reduce vehicle crashes is to prevent excessive speeding in the roads and highways the paper aims to outline the design of an embedded system that will automatically control the speed of a motor vehicle based on its location determined by a gps device the embedded system will make use of an avr atmega128 microcontroller connected to an em 406a gps receiver the large amount of location input data justifies the use of an atmega128 microcontroller which has 128kb of programmable flash memory as well as 4kb sram, and a 4kb eeprom memory [2] the output of the atmega128 will be a dogmi63w a lcd module which will display information of the current and the set point speed of the vehicle at the current position a discrete indicator led will flash at a pre determined frequency when the speed of the vehicle has exceeded the recommended speed limit finally, the system will have outputs that will communicate with the engine control unit (ecu) of the vehicle for the limited scope of this project, the ecu is simulated as an external device with two inputs that will acknowledge pulse trains of particular frequencies to limit the speed of a vehicle the speed control system will be programmed using mixed language c and assembly with the latter in use for some pre written subroutines to drive the lcd module the gps module will transmit national marine electronics association (nmea) data strings to the microcontroller (mcu) using serial peripheral interface (spi) the mcu will use the location coordinates (latitude and longitude) and the speed from the nmea rmc output string the current speed is then compared against the recommended speed for the vehicle's location the memory locations in the atmega128 can be used to store set point speed values against a particular set of location co ordinates apart from its implementation in human operated vehicles, the project can be used to control speed of autonomous cars and to implement the idea of a variable speed limit on roads introduced by the department of transportation [3] \"",
            "sequence": "[CLS] transient absorption spectroscopy has participating device has participating person has gaseous sample input has solution sample input has solid sample input has radiant energy input has output [SEP] gps enabled speed control embedded system speed limiting device with display and engine control interface \"in the past decade, there have been close to 350,000 fatal crashes in the united states [1] with various improvements in traffic and vehicle safety, the number of such crashes is decreasing every year one of the ways to reduce vehicle crashes is to prevent excessive speeding in the roads and highways the paper aims to outline the design of an embedded system that will automatically control the speed of a motor vehicle based on its location determined by a gps device the embedded system will make use of an avr atmega128 microcontroller connected to an em 406a gps receiver the large amount of location input data justifies the use of an atmega128 microcontroller which has 128kb of programmable flash memory as well as 4kb sram, and a 4kb eeprom memory [2] the output of the atmega128 will be a dogmi63w a lcd module which will display information of the current and the set point speed of the vehicle at the current position a discrete indicator led will flash at a pre determined frequency when the speed of the vehicle has exceeded the recommended speed limit finally, the system will have outputs that will communicate with the engine control unit (ecu) of the vehicle for the limited scope of this project, the ecu is simulated as an external device with two inputs that will acknowledge pulse trains of particular frequencies to limit the speed of a vehicle the speed control system will be programmed using mixed language c and assembly with the latter in use for some pre written subroutines to drive the lcd module the gps module will transmit national marine electronics association (nmea) data strings to the microcontroller (mcu) using serial peripheral interface (spi) the mcu will use the location coordinates (latitude and longitude) and the speed from the nmea rmc output string the current speed is then compared against the recommended speed for the vehicle's location the memory locations in the atmega128 can be used to store set point speed values against a particular set of location co ordinates apart from its implementation in human operated vehicles, the project can be used to control speed of autonomous cars and to implement the idea of a variable speed limit on roads introduced by the department of transportation [3] \" [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R11",
                "label": "Science"
            }
        },
        {
            "instance_id": "R161545xR194431",
            "template_id": "R161545",
            "correct_template_id": null,
            "paper_id": "R194431",
            "premise": "recycling methods method recycling type has property",
            "hypothesis": "do end users want explanations? analyzing the role of explainability as an emerging aspect of non functional requirements \"software systems are getting more and more complex their ubiquitous presence makes users more dependent on them and their correctness in many aspects of daily life thus, there is a rising need to make software systems and their decisions more comprehensible this seems to call for more transparency in software supported decisions therefore, transparency is gaining importance as a non functional requirement however, the abstract quality aspect of transparency needs to be better understood and related to mechanisms that can foster it integrating explanations in software to leverage systems' opacity has been discussed often yet, an important first step is to understand user requirements with respect to explainable software behavior: are users really interested in transparency, and are explanations considered an adequate mechanism to achieve it? we conducted a survey with 107 end users to assess their opinion on the current status of transparency in software systems, and what they consider main advantages and disadvantages of explanations embedded in software the overall attitude towards embedded explanations was positive however, we also identified potential disadvantages we assess the relation between explanations and transparency and analyze its possible impact on software quality \"",
            "sequence": "[CLS] recycling methods method recycling type has property [SEP] do end users want explanations? analyzing the role of explainability as an emerging aspect of non functional requirements \"software systems are getting more and more complex their ubiquitous presence makes users more dependent on them and their correctness in many aspects of daily life thus, there is a rising need to make software systems and their decisions more comprehensible this seems to call for more transparency in software supported decisions therefore, transparency is gaining importance as a non functional requirement however, the abstract quality aspect of transparency needs to be better understood and related to mechanisms that can foster it integrating explanations in software to leverage systems' opacity has been discussed often yet, an important first step is to understand user requirements with respect to explainable software behavior: are users really interested in transparency, and are explanations considered an adequate mechanism to achieve it? we conducted a survey with 107 end users to assess their opinion on the current status of transparency in software systems, and what they consider main advantages and disadvantages of explanations embedded in software the overall attitude towards embedded explanations was positive however, we also identified potential disadvantages we assess the relation between explanations and transparency and analyze its possible impact on software quality \" [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R155844xR195749",
            "template_id": "R155844",
            "correct_template_id": null,
            "paper_id": "R195749",
            "premise": "photocatalysts wavelength of maximum absorption energy band gap photocatalyst wavelength of maximum emission emission lifetime ground state oxidation potential ground state reduction potential excited state oxidation potential excited state reduction potential",
            "hypothesis": "how much undocumented knowledge is there in agile software development?: case study on industrial project using issue tracking system and version control system in agile software development projects, software engineers prioritize implementation over documentation to eliminate needless documentation is the cost of missing documentation greater than the cost of producing unnecessary or unused documentation? even without these documents, software engineers maintain other software artifacts, such as tickets in an issue tracking system (its) or source code committed to a version control system (vcs) do these artifacts contain the necessary knowledge? in this paper, we examine undocumented knowledge in an agile software development project at ntt for our study, we collected 159 commit logs in a vcs and 102 tickets in the its from the three month period of the project we propose a ticket commit network chart (tcc) that visually represents time series commit activities along with filed issue tickets we also implement a tool to generate the tcc using both commit log and ticket data our study revealed that in 16% of all commits, software engineers committed source code to the vcs without a corresponding issue ticket in the its had these commits been based on individual issue tickets, these \"unissued\" tickets would have accounted for 20% of all tickets software users and requirements engineers also evaluated the contents of these commits and found that 42% of the \"unissued\" tickets were required for software operation and 23% of those were required for requirements modification",
            "sequence": "[CLS] photocatalysts wavelength of maximum absorption energy band gap photocatalyst wavelength of maximum emission emission lifetime ground state oxidation potential ground state reduction potential excited state oxidation potential excited state reduction potential [SEP] how much undocumented knowledge is there in agile software development?: case study on industrial project using issue tracking system and version control system in agile software development projects, software engineers prioritize implementation over documentation to eliminate needless documentation is the cost of missing documentation greater than the cost of producing unnecessary or unused documentation? even without these documents, software engineers maintain other software artifacts, such as tickets in an issue tracking system (its) or source code committed to a version control system (vcs) do these artifacts contain the necessary knowledge? in this paper, we examine undocumented knowledge in an agile software development project at ntt for our study, we collected 159 commit logs in a vcs and 102 tickets in the its from the three month period of the project we propose a ticket commit network chart (tcc) that visually represents time series commit activities along with filed issue tickets we also implement a tool to generate the tcc using both commit log and ticket data our study revealed that in 16% of all commits, software engineers committed source code to the vcs without a corresponding issue ticket in the its had these commits been based on individual issue tickets, these \"unissued\" tickets would have accounted for 20% of all tickets software users and requirements engineers also evaluated the contents of these commits and found that 42% of the \"unissued\" tickets were required for software operation and 23% of those were required for requirements modification [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R184022xR113173",
            "template_id": "R184022",
            "correct_template_id": null,
            "paper_id": "R113173",
            "premise": "xray spectroscopy paper type research objective has system qualities",
            "hypothesis": "software feature request detection in issue tracking systems communication about requirements is often handled in issue tracking systems, especially in a distributed setting as issue tracking systems also contain bug reports or programming tasks, the software feature requests of the users are often difficult to identify this paper investigates natural language processing and machine learning features to detect software feature requests in natural language data of issue tracking systems it compares traditional linguistic machine learning features, such as \"bag of words\", with more advanced features, such as subject action object, and evaluates combinations of machine learning features derived from the natural language and features taken from the issue tracking system meta data our investigation shows that some combinations of machine learning features derived from natural language and the issue tracking system meta data outperform traditional approaches we show that issues or data fields (e g descriptions or comments), which contain software feature requests, can be identified reasonably well, but hardly the exact sentence finally, we show that the choice of machine learning algorithms should depend on the goal, e g maximization of the detection rate or balance between detection rate and precision in addition, the paper contributes a double coded gold standard and an open source implementation to further pursue this topic",
            "sequence": "[CLS] xray spectroscopy paper type research objective has system qualities [SEP] software feature request detection in issue tracking systems communication about requirements is often handled in issue tracking systems, especially in a distributed setting as issue tracking systems also contain bug reports or programming tasks, the software feature requests of the users are often difficult to identify this paper investigates natural language processing and machine learning features to detect software feature requests in natural language data of issue tracking systems it compares traditional linguistic machine learning features, such as \"bag of words\", with more advanced features, such as subject action object, and evaluates combinations of machine learning features derived from the natural language and features taken from the issue tracking system meta data our investigation shows that some combinations of machine learning features derived from natural language and the issue tracking system meta data outperform traditional approaches we show that issues or data fields (e g descriptions or comments), which contain software feature requests, can be identified reasonably well, but hardly the exact sentence finally, we show that the choice of machine learning algorithms should depend on the goal, e g maximization of the detection rate or balance between detection rate and precision in addition, the paper contributes a double coded gold standard and an open source implementation to further pursue this topic [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R198658xR138551",
            "template_id": "R198658",
            "correct_template_id": null,
            "paper_id": "R138551",
            "premise": "nacre mechanics template has method has result research problem has material",
            "hypothesis": "probing planetary biodiversity with dna barcodes: the noctuoidea of north america this study reports the assembly of a dna barcode reference library for species in the lepidopteran superfamily noctuoidea from canada and the usa based on the analysis of 69,378 specimens, the library provides coverage for 97 3% of the noctuoid fauna (3565 of 3664 species) in addition to verifying the strong performance of dna barcodes in the discrimination of these species, the results indicate close congruence between the number of species analyzed (3565) and the number of sequence clusters (3816) recognized by the barcode index number (bin) system distributional patterns across 12 north american ecoregions are examined for the 3251 species that have gps data while bin analysis is used to quantify overlap between the noctuoid faunas of north america and other zoogeographic regions this analysis reveals that 90% of north american noctuoids are endemic and that just 7 5% and 1 8% of bins are shared with the neotropics and with the palearctic, respectively one third (29) of the latter species are recent introductions and, as expected, they possess low intraspecific divergences",
            "sequence": "[CLS] nacre mechanics template has method has result research problem has material [SEP] probing planetary biodiversity with dna barcodes: the noctuoidea of north america this study reports the assembly of a dna barcode reference library for species in the lepidopteran superfamily noctuoidea from canada and the usa based on the analysis of 69,378 specimens, the library provides coverage for 97 3% of the noctuoid fauna (3565 of 3664 species) in addition to verifying the strong performance of dna barcodes in the discrimination of these species, the results indicate close congruence between the number of species analyzed (3565) and the number of sequence clusters (3816) recognized by the barcode index number (bin) system distributional patterns across 12 north american ecoregions are examined for the 3251 species that have gps data while bin analysis is used to quantify overlap between the noctuoid faunas of north america and other zoogeographic regions this analysis reveals that 90% of north american noctuoids are endemic and that just 7 5% and 1 8% of bins are shared with the neotropics and with the palearctic, respectively one third (29) of the latter species are recent introductions and, as expected, they possess low intraspecific divergences [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            }
        },
        {
            "instance_id": "R194212xR193336",
            "template_id": "R194212",
            "correct_template_id": null,
            "paper_id": "R193336",
            "premise": "ontology construction method methodology tool has uri data source research problem evaluation process name people involved serialization language",
            "hypothesis": "unsupervised topic discovery in user comments on social media platforms like twitter, users regularly share their opinions and comments with software vendors and service providers popular software products might get thousands of user comments per day research has shown that such comments contain valuable information for stakeholders, such as feature ideas, problem reports, or support inquiries however, it is hard to manually manage and grasp a large amount of user comments, which can be redundant and of a different quality consequently, researchers suggested automated approaches to extract valuable comments, e g , through problem report classifiers however, these approaches do not aggregate semantically similar comments into specific aspects to provide insights like how often users reported a certain problem we introduce an approach for automatically discovering topics composed of semantically similar user comments based on deep bidirectional natural language processing algorithms stakeholders can use our approach without the need to configure critical parameters like the number of clusters we present our approach and report on a rigorous multiple step empirical evaluation to assess how cohesive and meaningful the resulting clusters are each evaluation step was peer coded and resulted in inter coder agreements of up to 98%, giving us high confidence in the approach we also report a thematic analysis on the topics discovered from tweets in the telecommunication domain",
            "sequence": "[CLS] ontology construction method methodology tool has uri data source research problem evaluation process name people involved serialization language [SEP] unsupervised topic discovery in user comments on social media platforms like twitter, users regularly share their opinions and comments with software vendors and service providers popular software products might get thousands of user comments per day research has shown that such comments contain valuable information for stakeholders, such as feature ideas, problem reports, or support inquiries however, it is hard to manually manage and grasp a large amount of user comments, which can be redundant and of a different quality consequently, researchers suggested automated approaches to extract valuable comments, e g , through problem report classifiers however, these approaches do not aggregate semantically similar comments into specific aspects to provide insights like how often users reported a certain problem we introduce an approach for automatically discovering topics composed of semantically similar user comments based on deep bidirectional natural language processing algorithms stakeholders can use our approach without the need to configure critical parameters like the number of clusters we present our approach and report on a rigorous multiple step empirical evaluation to assess how cohesive and meaningful the resulting clusters are each evaluation step was peer coded and resulted in inter coder agreements of up to 98%, giving us high confidence in the approach we also report a thematic analysis on the topics discovered from tweets in the telecommunication domain [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R172526xR38043",
            "template_id": "R172526",
            "correct_template_id": null,
            "paper_id": "R38043",
            "premise": "video process has study research problem application production",
            "hypothesis": "semantic federation of product information from structured and unstructured sources product related information can be found in various data sources and formats across the product lifecycle effectively exploiting this information requires the federation of these sources, the extraction of implicit information, and the efficient access to this comprehensive knowledge base existing solutions for product information management (pim) are usually restricted to structured information, but most of the business critical information resides in unstructured documents we present a generic architecture for federating heterogeneous information from various sources, including the internet of things, and argue how this process benefits from using semantic representations a reference implementation tailor made to business users is explained and evaluated we also discuss several issues we experienced that we believe to be valuable for researchers and implementers of semantic information systems, as well as the information retrieval community",
            "sequence": "[CLS] video process has study research problem application production [SEP] semantic federation of product information from structured and unstructured sources product related information can be found in various data sources and formats across the product lifecycle effectively exploiting this information requires the federation of these sources, the extraction of implicit information, and the efficient access to this comprehensive knowledge base existing solutions for product information management (pim) are usually restricted to structured information, but most of the business critical information resides in unstructured documents we present a generic architecture for federating heterogeneous information from various sources, including the internet of things, and argue how this process benefits from using semantic representations a reference implementation tailor made to business users is explained and evaluated we also discuss several issues we experienced that we believe to be valuable for researchers and implementers of semantic information systems, as well as the information retrieval community [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R278",
                "label": "Information Science"
            }
        },
        {
            "instance_id": "R150089xR76818",
            "template_id": "R150089",
            "correct_template_id": null,
            "paper_id": "R76818",
            "premise": "epidemiological surveillance systems design and implementation software used software development approach related work epidemiological surveillance system purpose epidemiological surveillance process epidemiological surveillance users statistical analysis techniques epidemiological surveillance architecture epidemiological software development approach advantage provided by the system limit of the system",
            "hypothesis": "app review analysis via active learning: reducing supervision effort without compromising classification accuracy automated app review analysis is an important avenue for extracting a variety of requirements related information typically, a first step toward performing such analysis is preparing a training dataset, where developers (experts) identify a set of reviews and, manually, annotate them according to a given task having sufficiently large training data is important for both achieving a high prediction accuracy and avoiding overfitting given millions of reviews, preparing a training set is laborious we propose to incorporate active learning, a machine learning paradigm, in order to reduce the human effort involved in app review analysis our app review classification framework exploits three active learning strategies based on uncertainty sampling we apply these strategies to an existing dataset of 4,400 app reviews for classifying app reviews as features, bugs, rating, and user experience we find that active learning, compared to a training dataset chosen randomly, yields a significantly higher prediction accuracy under multiple scenarios",
            "sequence": "[CLS] epidemiological surveillance systems design and implementation software used software development approach related work epidemiological surveillance system purpose epidemiological surveillance process epidemiological surveillance users statistical analysis techniques epidemiological surveillance architecture epidemiological software development approach advantage provided by the system limit of the system [SEP] app review analysis via active learning: reducing supervision effort without compromising classification accuracy automated app review analysis is an important avenue for extracting a variety of requirements related information typically, a first step toward performing such analysis is preparing a training dataset, where developers (experts) identify a set of reviews and, manually, annotate them according to a given task having sufficiently large training data is important for both achieving a high prediction accuracy and avoiding overfitting given millions of reviews, preparing a training set is laborious we propose to incorporate active learning, a machine learning paradigm, in order to reduce the human effort involved in app review analysis our app review classification framework exploits three active learning strategies based on uncertainty sampling we apply these strategies to an existing dataset of 4,400 app reviews for classifying app reviews as features, bugs, rating, and user experience we find that active learning, compared to a training dataset chosen randomly, yields a significantly higher prediction accuracy under multiple scenarios [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R182248xR108344",
            "template_id": "R182248",
            "correct_template_id": null,
            "paper_id": "R108344",
            "premise": "food photo dataset year name annotation number of images task number of classes acquisition type",
            "hypothesis": "how we refactor, and how we know it much of what we know about how programmers refactor in the wild is based on studies that examine just a few software projects researchers have rarely taken the time to replicate these studies in other contexts or to examine the assumptions on which they are based to help put refactoring research on a sound scientific basis, we draw conclusions using four data sets spanning more than 13 000 developers, 240 000 tool assisted refactorings, 2500 developer hours, and 3400 version control commits using these data, we cast doubt on several previously stated assumptions about how programmers refactor, while validating others for example, we find that programmers frequently do not indicate refactoring activity in commit logs, which contradicts assumptions made by several previous researchers in contrast, we were able to confirm the assumption that programmers do frequently intersperse refactoring with other program changes by confirming assumptions and replicating studies made by other researchers, we can have greater confidence that those researchers' conclusions are generalizable",
            "sequence": "[CLS] food photo dataset year name annotation number of images task number of classes acquisition type [SEP] how we refactor, and how we know it much of what we know about how programmers refactor in the wild is based on studies that examine just a few software projects researchers have rarely taken the time to replicate these studies in other contexts or to examine the assumptions on which they are based to help put refactoring research on a sound scientific basis, we draw conclusions using four data sets spanning more than 13 000 developers, 240 000 tool assisted refactorings, 2500 developer hours, and 3400 version control commits using these data, we cast doubt on several previously stated assumptions about how programmers refactor, while validating others for example, we find that programmers frequently do not indicate refactoring activity in commit logs, which contradicts assumptions made by several previous researchers in contrast, we were able to confirm the assumption that programmers do frequently intersperse refactoring with other program changes by confirming assumptions and replicating studies made by other researchers, we can have greater confidence that those researchers' conclusions are generalizable [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R186491xR156663",
            "template_id": "R186491",
            "correct_template_id": "R156306",
            "paper_id": "R156663",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "short wavelength x\u2010ray laser research at the lawrence livermore national laboratory laboratory x\u2010ray lasers are currently being studied by researchers worldwide this paper reviews some of the recent work carried out at lawrence livermore national laboratory laser action has been demonstrated at wavelengths as short as 35 6 a while saturation of the small signal gain has been observed with longer wavelength schemes some of the most successful schemes to date have been collisionally pumped x\u2010ray lasers that use the thermal electron distribution within a laser\u2010produced plasma to excite electrons from closed shells in neon\u2010 and nickel\u2010like ions to metastable levels in the next shell attempts to quantify and improve the longitudinal and transverse coherence of collisionally pumped x\u2010ray lasers are motivated by the desire to produce sources for specific applications toward this goal there is a large effort underway to enhance the power output of the ni\u2010like ta x\u2010ray laser at 44 83 a as a source for x\u2010ray imaging of live cells improving the efficiency of x\u2010ray lasers in order to produce s",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] short wavelength x\u2010ray laser research at the lawrence livermore national laboratory laboratory x\u2010ray lasers are currently being studied by researchers worldwide this paper reviews some of the recent work carried out at lawrence livermore national laboratory laser action has been demonstrated at wavelengths as short as 35 6 a while saturation of the small signal gain has been observed with longer wavelength schemes some of the most successful schemes to date have been collisionally pumped x\u2010ray lasers that use the thermal electron distribution within a laser\u2010produced plasma to excite electrons from closed shells in neon\u2010 and nickel\u2010like ions to metastable levels in the next shell attempts to quantify and improve the longitudinal and transverse coherence of collisionally pumped x\u2010ray lasers are motivated by the desire to produce sources for specific applications toward this goal there is a large effort underway to enhance the power output of the ni\u2010like ta x\u2010ray laser at 44 83 a as a source for x\u2010ray imaging of live cells improving the efficiency of x\u2010ray lasers in order to produce s [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            }
        },
        {
            "instance_id": "R161736xR140043",
            "template_id": "R161736",
            "correct_template_id": null,
            "paper_id": "R140043",
            "premise": "xray laser applications paper type research objective has system qualities",
            "hypothesis": "unleashing innovation through internal hackathons hackathons have become an increasingly popular approach for organizations to both test their new products and services as well as to generate new ideas most events either focus on attracting external developers or requesting employees of the organization to focus on a specific problem in this paper we describe extensions to this paradigm that open up the event to internal employees and preserve the open ended nature of the hackathon itself in this paper we describe our initial motivation and objectives for conducting an internal hackathon, our experience in pioneering an internal hackathon at at&t including specific things we did to make the internal hackathon successful we conclude with the benefits (both expected and unexpected) we achieved from the internal hackathon approach, and recommendations for continuing the use of this valuable tool within at&t",
            "sequence": "[CLS] xray laser applications paper type research objective has system qualities [SEP] unleashing innovation through internal hackathons hackathons have become an increasingly popular approach for organizations to both test their new products and services as well as to generate new ideas most events either focus on attracting external developers or requesting employees of the organization to focus on a specific problem in this paper we describe extensions to this paradigm that open up the event to internal employees and preserve the open ended nature of the hackathon itself in this paper we describe our initial motivation and objectives for conducting an internal hackathon, our experience in pioneering an internal hackathon at at&t including specific things we did to make the internal hackathon successful we conclude with the benefits (both expected and unexpected) we achieved from the internal hackathon approach, and recommendations for continuing the use of this valuable tool within at&t [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            }
        },
        {
            "instance_id": "R198658xR111111",
            "template_id": "R198658",
            "correct_template_id": null,
            "paper_id": "R111111",
            "premise": "nacre mechanics template has method has result research problem has material",
            "hypothesis": "beitr\u00e4ge zur chemie und biochemie der \u201ecobalamine\u201d, ii mitteil : \u00fcber den abbau der \u201ecobalamine\u201d mit cer (iii) hydroxyd 7 [d ribofuranosido] adenin, ein abbauprodukt des pseudovitamins b12 unter der katalytischen wirkung des cer(iii) hydroxyds in wasrigem medium bei 95\u00b0 und neutralem ph werden die meisten vitamine der b12 gruppe rasch zu atiocobalamin, nucleosid und phosphorsaure abgebaut ein zusatz von cn\u2296 beschleunigt den abbau und beseitigt weitgehend die unterschiede in der abbaugeschwindigkeit der einzelnen b12 arten der cer abbau ist eine sehr schonende methode zur gewinnung von reinstem atiocobalamin und vor allem von schwer zuganglichen nucleosiden der b12 faktoren das nucleosid des pseudovitamins b12 wird in kristallisiertem zustand gewonnen und als 7 [d ribofuranosido] adenin charakterisiert",
            "sequence": "[CLS] nacre mechanics template has method has result research problem has material [SEP] beitr\u00e4ge zur chemie und biochemie der \u201ecobalamine\u201d, ii mitteil : \u00fcber den abbau der \u201ecobalamine\u201d mit cer (iii) hydroxyd 7 [d ribofuranosido] adenin, ein abbauprodukt des pseudovitamins b12 unter der katalytischen wirkung des cer(iii) hydroxyds in wasrigem medium bei 95\u00b0 und neutralem ph werden die meisten vitamine der b12 gruppe rasch zu atiocobalamin, nucleosid und phosphorsaure abgebaut ein zusatz von cn\u2296 beschleunigt den abbau und beseitigt weitgehend die unterschiede in der abbaugeschwindigkeit der einzelnen b12 arten der cer abbau ist eine sehr schonende methode zur gewinnung von reinstem atiocobalamin und vor allem von schwer zuganglichen nucleosiden der b12 faktoren das nucleosid des pseudovitamins b12 wird in kristallisiertem zustand gewonnen und als 7 [d ribofuranosido] adenin charakterisiert [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R129",
                "label": "Organic Chemistry"
            }
        },
        {
            "instance_id": "R178304xR139538",
            "template_id": "R178304",
            "correct_template_id": null,
            "paper_id": "R139538",
            "premise": "dataset inter annotator agreement alternatename assesses creator description disambiguatingdescription encoding exampleofwork genre inlanguage isbasedon license name sameas size sourceorganization url version",
            "hypothesis": "high resolution dna barcode library for european butterflies reveals continental patterns of mitochondrial genetic diversity abstract the study of global biodiversity will greatly benefit from access to comprehensive dna barcode libraries at continental scale, but such datasets are still very rare here, we assemble the first high resolution reference library for european butterflies that provides 97% taxon coverage (459 species) and 22,306 coi sequences we estimate that we captured 62% of the total haplotype diversity and show that most species possess a few very common haplotypes and many rare ones specimens in the dataset have an average 95 3% probability of being correctly identified mitochondrial diversity displayed elevated haplotype richness in southern european refugia, establishing the generality of this key biogeographic pattern for an entire taxonomic group fifteen percent of the species are involved in barcode sharing, but two thirds of these cases may reflect the need for further taxonomic research this dataset provides a unique resource for conservation and for studying evolutionary processes, cryptic species, phylogeography, and ecology",
            "sequence": "[CLS] dataset inter annotator agreement alternatename assesses creator description disambiguatingdescription encoding exampleofwork genre inlanguage isbasedon license name sameas size sourceorganization url version [SEP] high resolution dna barcode library for european butterflies reveals continental patterns of mitochondrial genetic diversity abstract the study of global biodiversity will greatly benefit from access to comprehensive dna barcode libraries at continental scale, but such datasets are still very rare here, we assemble the first high resolution reference library for european butterflies that provides 97% taxon coverage (459 species) and 22,306 coi sequences we estimate that we captured 62% of the total haplotype diversity and show that most species possess a few very common haplotypes and many rare ones specimens in the dataset have an average 95 3% probability of being correctly identified mitochondrial diversity displayed elevated haplotype richness in southern european refugia, establishing the generality of this key biogeographic pattern for an entire taxonomic group fifteen percent of the species are involved in barcode sharing, but two thirds of these cases may reflect the need for further taxonomic research this dataset provides a unique resource for conservation and for studying evolutionary processes, cryptic species, phylogeography, and ecology [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            }
        },
        {
            "instance_id": "R154390xR160374",
            "template_id": "R154390",
            "correct_template_id": null,
            "paper_id": "R160374",
            "premise": "lignin decomposition substrate catalyst has temperature value solvent pressure reactor conversion product",
            "hypothesis": "a digital twin paradigm: vehicle to cloud based advanced driver assistance systems digital twin, an emerging representation of cyberphysical systems, has attracted increasing attentions very recently it opens the way to real time monitoring and synchronization of real world activities with the virtual counterparts in this study, we develop a digital twin paradigm using an advanced driver assistance system (adas) for connected vehicles by leveraging vehicle to cloud (v2c) communication, on board devices can upload the data to the server through cellular network the server creates a virtual world based on the received data, processes them with the proposed models, and sends them back to the connected vehicles drivers can benefit from this v2c based adas, even if all computations are conducted on the cloud the cooperative ramp merging case study is conducted, and the field implementation results show the proposed digital twin framework can benefit the transportation systems regarding mobility and environmental sustainability with acceptable communication delays and packet losses",
            "sequence": "[CLS] lignin decomposition substrate catalyst has temperature value solvent pressure reactor conversion product [SEP] a digital twin paradigm: vehicle to cloud based advanced driver assistance systems digital twin, an emerging representation of cyberphysical systems, has attracted increasing attentions very recently it opens the way to real time monitoring and synchronization of real world activities with the virtual counterparts in this study, we develop a digital twin paradigm using an advanced driver assistance system (adas) for connected vehicles by leveraging vehicle to cloud (v2c) communication, on board devices can upload the data to the server through cellular network the server creates a virtual world based on the received data, processes them with the proposed models, and sends them back to the connected vehicles drivers can benefit from this v2c based adas, even if all computations are conducted on the cloud the cooperative ramp merging case study is conducted, and the field implementation results show the proposed digital twin framework can benefit the transportation systems regarding mobility and environmental sustainability with acceptable communication delays and packet losses [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            }
        },
        {
            "instance_id": "R152828xR38074",
            "template_id": "R152828",
            "correct_template_id": null,
            "paper_id": "R38074",
            "premise": "xray laser pumped has system qualities",
            "hypothesis": "ontoimm: an ontology for product intelligent master model information organizing principle is one of the key issues of intelligent master model (imm), which is an enhancement of the master model (mm) based on kbe (knowledge based engineering) despite the fact that the core product model (cpm) has been confirmed to be an organizing mechanism for product master model, the key issue of supporting the information organizing for imm is not yet well addressed, mainly due to the following two reasons; (1) lack of representation of complete information and knowledge with regard to product and process, including the know why, know how, and know what information and knowledge, and (2) lack of semantic richness therefore, a multiaspect extension to cpm was first defined, and then an ontology was constructed to represent the information and design knowledge the extension refers to adding a design process model, context model, product control structure model, and design rationale model to cpm concerning the enhancement of master model, which is to comprehensively represent the reason, process, and result information and knowledge of theproduct the ontology construction refers to representing the concepts, relationships among these concepts and consistency rules of imm information structure finally, an example of barrel design and analysis process is illustrated to verify the effectiveness of proposed method",
            "sequence": "[CLS] xray laser pumped has system qualities [SEP] ontoimm: an ontology for product intelligent master model information organizing principle is one of the key issues of intelligent master model (imm), which is an enhancement of the master model (mm) based on kbe (knowledge based engineering) despite the fact that the core product model (cpm) has been confirmed to be an organizing mechanism for product master model, the key issue of supporting the information organizing for imm is not yet well addressed, mainly due to the following two reasons; (1) lack of representation of complete information and knowledge with regard to product and process, including the know why, know how, and know what information and knowledge, and (2) lack of semantic richness therefore, a multiaspect extension to cpm was first defined, and then an ontology was constructed to represent the information and design knowledge the extension refers to adding a design process model, context model, product control structure model, and design rationale model to cpm concerning the enhancement of master model, which is to comprehensively represent the reason, process, and result information and knowledge of theproduct the ontology construction refers to representing the concepts, relationships among these concepts and consistency rules of imm information structure finally, an example of barrel design and analysis process is illustrated to verify the effectiveness of proposed method [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R278",
                "label": "Information Science"
            }
        },
        {
            "instance_id": "R187648xR25079",
            "template_id": "R187648",
            "correct_template_id": null,
            "paper_id": "R25079",
            "premise": "oco for control research problem theoretical guarantees has application in data driven approach has constraints type of considered system measurement noise process noise",
            "hypothesis": "machine prediction of personality from facebook profiles \"an increasing number of americans use social networking sites such as facebook, but few fully appreciate the amount of information they share with the world as a result although studies exist on the sharing of specific types of information (photos, posts, etc ), one area that has been less explored is how facebook profiles can share personality information in a broad, machine readable fashion in this study, we apply data mining and machine learning techniques to predict users' personality traits (specifically, the traits of the big five personality model) using only demographic and text based attributes extracted from their profiles we then use these predictions to rank individuals in terms of the five traits, predicting which users will appear in the top or bottom 5% or 10% of these traits our results show that when using certain models, we can find the top 10% most open individuals with nearly 75% accuracy, and across all traits and directions, we can predict the top 10% with at least 34 5% accuracy (exceeding 21 8%, which is the best accuracy when using just the best performing profile attribute) these results have privacy implications in terms of allowing advertisers and other groups to focus on a specific subset of individuals based on their personality traits \"",
            "sequence": "[CLS] oco for control research problem theoretical guarantees has application in data driven approach has constraints type of considered system measurement noise process noise [SEP] machine prediction of personality from facebook profiles \"an increasing number of americans use social networking sites such as facebook, but few fully appreciate the amount of information they share with the world as a result although studies exist on the sharing of specific types of information (photos, posts, etc ), one area that has been less explored is how facebook profiles can share personality information in a broad, machine readable fashion in this study, we apply data mining and machine learning techniques to predict users' personality traits (specifically, the traits of the big five personality model) using only demographic and text based attributes extracted from their profiles we then use these predictions to rank individuals in terms of the five traits, predicting which users will appear in the top or bottom 5% or 10% of these traits our results show that when using certain models, we can find the top 10% most open individuals with nearly 75% accuracy, and across all traits and directions, we can predict the top 10% with at least 34 5% accuracy (exceeding 21 8%, which is the best accuracy when using just the best performing profile attribute) these results have privacy implications in terms of allowing advertisers and other groups to focus on a specific subset of individuals based on their personality traits \" [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R11",
                "label": "Science"
            }
        },
        {
            "instance_id": "R150595xR175117",
            "template_id": "R150595",
            "correct_template_id": null,
            "paper_id": "R175117",
            "premise": "tailored forming contribution has result research problem has material realizes",
            "hypothesis": "toward altmetric driven research paper recommender system framework \"the volume of literature and more particularly research oriented publications is growing at an exponential rate, and better tools and methodologies are required to efficiently and effectively retrieve desired documents the development of academic search engines, digital libraries and archives has led to better information filtering mechanisms that has resulted to improved search results however, the state of the art research paper recommender systems are still retrieving research articles without explicitly defining the domain of interest of the researchers also, a rich set of research output (research objects) and their associated metrics are also not being utilized in the process of searching, querying, retrieving and recommending articles consequently, a lot of irrelevant and unrelated information is being presented to the user then again, the use of citation counts to rank and recommend research paper to users is still disputed recommendation metrics like citation counts, ratings in collaborative filtering, and keyword analysis' cannot be fully relied on as the only techniques through which similarity between documents can be computed, and this is because recommendations based on such metrics are not accurate and have lots of biasness henceforth, altmetric based techniques and methodologies are expected to give better recommendations of research papers since the circumstances surrounding a research papers are taken into consideration this paper proposes a research paper recommender system framework that utilizes paper ontology and altmetric from research papers, to enhance the performance of research paper recommender systems \"",
            "sequence": "[CLS] tailored forming contribution has result research problem has material realizes [SEP] toward altmetric driven research paper recommender system framework \"the volume of literature and more particularly research oriented publications is growing at an exponential rate, and better tools and methodologies are required to efficiently and effectively retrieve desired documents the development of academic search engines, digital libraries and archives has led to better information filtering mechanisms that has resulted to improved search results however, the state of the art research paper recommender systems are still retrieving research articles without explicitly defining the domain of interest of the researchers also, a rich set of research output (research objects) and their associated metrics are also not being utilized in the process of searching, querying, retrieving and recommending articles consequently, a lot of irrelevant and unrelated information is being presented to the user then again, the use of citation counts to rank and recommend research paper to users is still disputed recommendation metrics like citation counts, ratings in collaborative filtering, and keyword analysis' cannot be fully relied on as the only techniques through which similarity between documents can be computed, and this is because recommendations based on such metrics are not accurate and have lots of biasness henceforth, altmetric based techniques and methodologies are expected to give better recommendations of research papers since the circumstances surrounding a research papers are taken into consideration this paper proposes a research paper recommender system framework that utilizes paper ontology and altmetric from research papers, to enhance the performance of research paper recommender systems \" [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R141823",
                "label": "Semantic Web"
            }
        },
        {
            "instance_id": "R184022xR139100",
            "template_id": "R184022",
            "correct_template_id": null,
            "paper_id": "R139100",
            "premise": "xray spectroscopy paper type research objective has system qualities",
            "hypothesis": "impact of plasma jet vacuum ultraviolet radiation on reactive oxygen species generation in bio relevant liquids plasma medicine utilizes the combined interaction of plasma produced reactive components these are reactive atoms, molecules, ions, metastable species, and radiation here, ultraviolet (uv, 100\u2013400\\u2009nm) and, in particular, vacuum ultraviolet (vuv, 10\u2013200\\u2009nm) radiation generated by an atmospheric pressure argon plasma jet were investigated regarding plasma emission, absorption in a humidified atmosphere and in solutions relevant for plasma medicine the energy absorption was obtained for simple solutions like distilled water (dh2o) or ultrapure water and sodium chloride (nacl) solution as well as for more complex ones, for example, rosewell park memorial institute (rpmi 1640) cell culture media as moderate stable reactive oxygen species, hydrogen peroxide (h2o2) was studied highly reactive oxygen radicals, namely, superoxide anion (o2\u2022\u2212) and hydroxyl radicals (\u2022oh), were investigated by the use of electron paramagnetic resonance spectroscopy all species amounts were detected for three different treatmen",
            "sequence": "[CLS] xray spectroscopy paper type research objective has system qualities [SEP] impact of plasma jet vacuum ultraviolet radiation on reactive oxygen species generation in bio relevant liquids plasma medicine utilizes the combined interaction of plasma produced reactive components these are reactive atoms, molecules, ions, metastable species, and radiation here, ultraviolet (uv, 100\u2013400\\u2009nm) and, in particular, vacuum ultraviolet (vuv, 10\u2013200\\u2009nm) radiation generated by an atmospheric pressure argon plasma jet were investigated regarding plasma emission, absorption in a humidified atmosphere and in solutions relevant for plasma medicine the energy absorption was obtained for simple solutions like distilled water (dh2o) or ultrapure water and sodium chloride (nacl) solution as well as for more complex ones, for example, rosewell park memorial institute (rpmi 1640) cell culture media as moderate stable reactive oxygen species, hydrogen peroxide (h2o2) was studied highly reactive oxygen radicals, namely, superoxide anion (o2\u2022\u2212) and hydroxyl radicals (\u2022oh), were investigated by the use of electron paramagnetic resonance spectroscopy all species amounts were detected for three different treatmen [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            }
        },
        {
            "instance_id": "R52190xR194428",
            "template_id": "R52190",
            "correct_template_id": null,
            "paper_id": "R194428",
            "premise": "transient absorption spectroscopy has participating device has participating person has gaseous sample input has solution sample input has solid sample input has radiant energy input has output",
            "hypothesis": "predicting how to test requirements: an automated approach an important task in requirements engineering is to identify and determine how to verify a requirement (e g , by manual review, testing, or simulation; also called potential verification method) this information is required to effectively create test cases and verification plans for requirements [objective] in this paper, we propose an automatic approach to classify natural language requirements with respect to their potential verification methods (pvm) [method] our approach uses a convolutional neural network architecture to implement a multiclass and multilabel classifier that assigns probabilities to a predefined set of six possible verification methods, which we derived from an industrial guideline additionally, we implemented a backtracing approach to analyze and visualize the reasons for the network\u2019s decisions [results] in a 10 fold cross validation on a set of about 27,000 industrial requirements, our approach achieved a macro averaged f1 score of 0 79 across all labels for the classification into test or non test, the approach achieves an even higher f1 score of 0 94 [conclusions] the results show that our approach might help to increase the quality of requirements specifications with respect to the pvm attribute and guide engineers in effectively deriving test cases and verification plans",
            "sequence": "[CLS] transient absorption spectroscopy has participating device has participating person has gaseous sample input has solution sample input has solid sample input has radiant energy input has output [SEP] predicting how to test requirements: an automated approach an important task in requirements engineering is to identify and determine how to verify a requirement (e g , by manual review, testing, or simulation; also called potential verification method) this information is required to effectively create test cases and verification plans for requirements [objective] in this paper, we propose an automatic approach to classify natural language requirements with respect to their potential verification methods (pvm) [method] our approach uses a convolutional neural network architecture to implement a multiclass and multilabel classifier that assigns probabilities to a predefined set of six possible verification methods, which we derived from an industrial guideline additionally, we implemented a backtracing approach to analyze and visualize the reasons for the network\u2019s decisions [results] in a 10 fold cross validation on a set of about 27,000 industrial requirements, our approach achieved a macro averaged f1 score of 0 79 across all labels for the classification into test or non test, the approach achieves an even higher f1 score of 0 94 [conclusions] the results show that our approach might help to increase the quality of requirements specifications with respect to the pvm attribute and guide engineers in effectively deriving test cases and verification plans [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R160259xR75435",
            "template_id": "R160259",
            "correct_template_id": null,
            "paper_id": "R75435",
            "premise": "digital city twin review application area",
            "hypothesis": "mapreduce: simplified data processing on large clusters \" \\n mapreduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real world tasks users specify the computation in terms of a\\n map \\n and a\\n reduce \\n function, and the underlying runtime system automatically parallelizes the computation across large scale clusters of machines, handles machine failures, and schedules inter machine communication to make efficient use of the network and disks programmers find the system easy to use: more than ten thousand distinct mapreduce programs have been implemented internally at google over the past four years, and an average of one hundred thousand mapreduce jobs are executed on google's clusters every day, processing a total of more than twenty petabytes of data per day \\n \"",
            "sequence": "[CLS] digital city twin review application area [SEP] mapreduce: simplified data processing on large clusters \" \\n mapreduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real world tasks users specify the computation in terms of a\\n map \\n and a\\n reduce \\n function, and the underlying runtime system automatically parallelizes the computation across large scale clusters of machines, handles machine failures, and schedules inter machine communication to make efficient use of the network and disks programmers find the system easy to use: more than ten thousand distinct mapreduce programs have been implemented internally at google over the past four years, and an average of one hundred thousand mapreduce jobs are executed on google's clusters every day, processing a total of more than twenty petabytes of data per day \\n \" [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R152828xR159456",
            "template_id": "R152828",
            "correct_template_id": null,
            "paper_id": "R159456",
            "premise": "xray laser pumped has system qualities",
            "hypothesis": "geospatial artificial intelligence: potentials of machine learning for 3d point clouds and geospatial digital twins abstract artificial intelligence (ai) is changing fundamentally the way how it solutions are implemented and operated across all application domains, including the geospatial domain this contribution outlines ai based techniques for 3d point clouds and geospatial digital twins as generic components of geospatial ai first, we briefly reflect on the term \u201cai\u201d and outline technology developments needed to apply ai to it solutions, seen from a software engineering perspective next, we characterize 3d point clouds as key category of geodata and their role for creating the basis for geospatial digital twins; we explain the feasibility of machine learning (ml) and deep learning (dl) approaches for 3d point clouds in particular, we argue that 3d point clouds can be seen as a corpus with similar properties as natural language corpora and formulate a \u201cnaturalness hypothesis\u201d for 3d point clouds in the main part, we introduce a workflow for interpreting 3d point clouds based on ml/dl approaches that derive domain specific and application specific semantics for 3d point clouds without having to create explicit spatial 3d models or explicit rule sets finally, examples are shown how ml/dl enables us to efficiently build and maintain base data for geospatial digital twins such as virtual 3d city models, indoor models, or building information models",
            "sequence": "[CLS] xray laser pumped has system qualities [SEP] geospatial artificial intelligence: potentials of machine learning for 3d point clouds and geospatial digital twins abstract artificial intelligence (ai) is changing fundamentally the way how it solutions are implemented and operated across all application domains, including the geospatial domain this contribution outlines ai based techniques for 3d point clouds and geospatial digital twins as generic components of geospatial ai first, we briefly reflect on the term \u201cai\u201d and outline technology developments needed to apply ai to it solutions, seen from a software engineering perspective next, we characterize 3d point clouds as key category of geodata and their role for creating the basis for geospatial digital twins; we explain the feasibility of machine learning (ml) and deep learning (dl) approaches for 3d point clouds in particular, we argue that 3d point clouds can be seen as a corpus with similar properties as natural language corpora and formulate a \u201cnaturalness hypothesis\u201d for 3d point clouds in the main part, we introduce a workflow for interpreting 3d point clouds based on ml/dl approaches that derive domain specific and application specific semantics for 3d point clouds without having to create explicit spatial 3d models or explicit rule sets finally, examples are shown how ml/dl enables us to efficiently build and maintain base data for geospatial digital twins such as virtual 3d city models, indoor models, or building information models [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            }
        },
        {
            "instance_id": "R194212xR41079",
            "template_id": "R194212",
            "correct_template_id": null,
            "paper_id": "R41079",
            "premise": "ontology construction method methodology tool has uri data source research problem evaluation process name people involved serialization language",
            "hypothesis": "speech recognition using deep neural networks: a systematic review over the past decades, a tremendous amount of research has been done on the use of machine learning for speech processing applications, especially speech recognition however, in the past few years, research has focused on utilizing deep learning for speech related applications this new area of machine learning has yielded far better results when compared to others in a variety of applications including speech, and thus became a very attractive area of research this paper provides a thorough examination of the different studies that have been conducted since 2006, when deep learning first arose as a new area of machine learning, for speech applications a thorough statistical analysis is provided in this review which was conducted by extracting specific information from 174 papers published between the years 2006 and 2018 the results provided in this paper shed light on the trends of research in this area as well as bring focus to new research topics",
            "sequence": "[CLS] ontology construction method methodology tool has uri data source research problem evaluation process name people involved serialization language [SEP] speech recognition using deep neural networks: a systematic review over the past decades, a tremendous amount of research has been done on the use of machine learning for speech processing applications, especially speech recognition however, in the past few years, research has focused on utilizing deep learning for speech related applications this new area of machine learning has yielded far better results when compared to others in a variety of applications including speech, and thus became a very attractive area of research this paper provides a thorough examination of the different studies that have been conducted since 2006, when deep learning first arose as a new area of machine learning, for speech applications a thorough statistical analysis is provided in this review which was conducted by extracting specific information from 174 papers published between the years 2006 and 2018 the results provided in this paper shed light on the trends of research in this area as well as bring focus to new research topics [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R133",
                "label": "Artificial Intelligence"
            }
        },
        {
            "instance_id": "R146876xR137404",
            "template_id": "R146876",
            "correct_template_id": null,
            "paper_id": "R137404",
            "premise": "organic solar cells mobility donor acceptor lumo homo energy band gap open circuit voltage, voc short circuit current density, jsc fill factor, ff power conversion efficiency",
            "hypothesis": "characteristics of an atmospheric pressure argon plasma jet excited by a dc voltage a dc excited plasma jet is developed to generate a diffuse plasma plume in flowing argon the discharge characteristics of the plasma jet are investigated by optical and electrical methods the results show that the plasma plume is a pulsed discharge even when a dc voltage is applied the discharge frequency varies with a change in the applied voltage, the gas flow rate and the gas gap width it is found that the discharges at different positions of the plasma plume are initiated and quenched almost at the same time with a jitter of about 10 ns by the spatially resolved measurement optical emission spectroscopy is used to investigate the excited electron temperature of the plasma plume the results show that the excited electron temperature decreases with increasing applied voltage, gas flow rate or gas gap width these results are analyzed qualitatively",
            "sequence": "[CLS] organic solar cells mobility donor acceptor lumo homo energy band gap open circuit voltage, voc short circuit current density, jsc fill factor, ff power conversion efficiency [SEP] characteristics of an atmospheric pressure argon plasma jet excited by a dc voltage a dc excited plasma jet is developed to generate a diffuse plasma plume in flowing argon the discharge characteristics of the plasma jet are investigated by optical and electrical methods the results show that the plasma plume is a pulsed discharge even when a dc voltage is applied the discharge frequency varies with a change in the applied voltage, the gas flow rate and the gas gap width it is found that the discharges at different positions of the plasma plume are initiated and quenched almost at the same time with a jitter of about 10 ns by the spatially resolved measurement optical emission spectroscopy is used to investigate the excited electron temperature of the plasma plume the results show that the excited electron temperature decreases with increasing applied voltage, gas flow rate or gas gap width these results are analyzed qualitatively [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R114008",
                "label": "Applied Physics"
            }
        },
        {
            "instance_id": "R154390xR186134",
            "template_id": "R154390",
            "correct_template_id": null,
            "paper_id": "R186134",
            "premise": "lignin decomposition substrate catalyst has temperature value solvent pressure reactor conversion product",
            "hypothesis": "exploiting declarative mapping rules for generating graphql servers with morph graphql in the last decade, rest has become the most common approach to provide web services, yet it was not originally designed to handle typical modern applications (e g mobile apps) graphql was proposed to reduce the number of queries and data exchanged in comparison with rest since its release in 2015, it has gained momentum as an alternative approach to rest however, generating and maintaining graphql resolvers is not a simple task first, a domain expert has to analyze a dataset, design the corresponding graphql schema and map the dataset to the schema then, a software engineer (e g graphql developer) implements the corresponding graphql resolvers in a specific programming language in this paper, we present an approach to exploit the information from mappings rules (relation between target and source schema) and generate a graphql server these mapping rules construct a virtual knowledge graph which is accessed by the generated graphql resolvers these resolvers translate the input graphql queries into the queries supported by the underlying dataset domain experts or software developers may benefit from our approach: a domain expert does not need to involve software developers to implement the resolvers, and software developers can generate the initial version of the resolvers to be implemented we implemented our approach in the morph graphql framework and evaluated it using the lingbm benchmark",
            "sequence": "[CLS] lignin decomposition substrate catalyst has temperature value solvent pressure reactor conversion product [SEP] exploiting declarative mapping rules for generating graphql servers with morph graphql in the last decade, rest has become the most common approach to provide web services, yet it was not originally designed to handle typical modern applications (e g mobile apps) graphql was proposed to reduce the number of queries and data exchanged in comparison with rest since its release in 2015, it has gained momentum as an alternative approach to rest however, generating and maintaining graphql resolvers is not a simple task first, a domain expert has to analyze a dataset, design the corresponding graphql schema and map the dataset to the schema then, a software engineer (e g graphql developer) implements the corresponding graphql resolvers in a specific programming language in this paper, we present an approach to exploit the information from mappings rules (relation between target and source schema) and generate a graphql server these mapping rules construct a virtual knowledge graph which is accessed by the generated graphql resolvers these resolvers translate the input graphql queries into the queries supported by the underlying dataset domain experts or software developers may benefit from our approach: a domain expert does not need to involve software developers to implement the resolvers, and software developers can generate the initial version of the resolvers to be implemented we implemented our approach in the morph graphql framework and evaluated it using the lingbm benchmark [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R141823",
                "label": "Semantic Web"
            }
        },
        {
            "instance_id": "R161545xR138070",
            "template_id": "R161545",
            "correct_template_id": null,
            "paper_id": "R138070",
            "premise": "recycling methods method recycling type has property",
            "hypothesis": "grand challenges in model driven engineering: an analysis of the state of the research abstract in 2017 and 2018, two events were held\u2014in marburg, germany, and san vigilio di marebbe, italy, respectively\u2014focusing on an analysis of the state of research, state of practice, and state of the art in model driven engineering (mde) the events brought together experts from industry, academia, and the open source community to assess what has changed in research in mde over the last 10\\xa0years, what challenges remain, and what new challenges have arisen this article reports on the results of those meetings, and presents a set of grand challenges that emerged from discussions and synthesis these challenges could lead to research initiatives for the community going forward \\n",
            "sequence": "[CLS] recycling methods method recycling type has property [SEP] grand challenges in model driven engineering: an analysis of the state of the research abstract in 2017 and 2018, two events were held\u2014in marburg, germany, and san vigilio di marebbe, italy, respectively\u2014focusing on an analysis of the state of research, state of practice, and state of the art in model driven engineering (mde) the events brought together experts from industry, academia, and the open source community to assess what has changed in research in mde over the last 10\\xa0years, what challenges remain, and what new challenges have arisen this article reports on the results of those meetings, and presents a set of grand challenges that emerged from discussions and synthesis these challenges could lead to research initiatives for the community going forward \\n [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R184022xR149916",
            "template_id": "R184022",
            "correct_template_id": "R138077",
            "paper_id": "R149916",
            "premise": "xray spectroscopy paper type research objective has system qualities",
            "hypothesis": "image domain ontology fusion approach using multi level inference mechanism one of the main challenges in content based or semantic image retrieval is still to bridge the gap between low level features and semantic information in this paper, an approach is presented using integrated multi level image features in ontology fusion construction by a fusion framework, which based on the latent semantic analysis the proposed method promotes images ontology fusion efficiently and broadens the application fields of image ontology retrieval system the relevant experiment shows that this method ameliorates the problem, such as too many redundant data and relations, in the traditional ontology system construction, as well as improves the performance of semantic images retrieval",
            "sequence": "[CLS] xray spectroscopy paper type research objective has system qualities [SEP] image domain ontology fusion approach using multi level inference mechanism one of the main challenges in content based or semantic image retrieval is still to bridge the gap between low level features and semantic information in this paper, an approach is presented using integrated multi level image features in ontology fusion construction by a fusion framework, which based on the latent semantic analysis the proposed method promotes images ontology fusion efficiently and broadens the application fields of image ontology retrieval system the relevant experiment shows that this method ameliorates the problem, such as too many redundant data and relations, in the traditional ontology system construction, as well as improves the performance of semantic images retrieval [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R141823",
                "label": "Semantic Web"
            }
        },
        {
            "instance_id": "R172526xR34944",
            "template_id": "R172526",
            "correct_template_id": null,
            "paper_id": "R34944",
            "premise": "video process has study research problem application production",
            "hypothesis": "a comprehensive survey of graph embedding: problems, techniques and applications graph is an important data representation which appears in a wide diversity of real world scenarios effective graph analytics provides users a deeper understanding of what is behind the data, and thus can benefit a lot of useful applications such as node classification, node recommendation, link prediction, etc however, most graph analytics methods suffer the high computation and space cost graph embedding is an effective yet efficient way to solve the graph analytics problem it converts the graph data into a low dimensional space in which the graph structural information and graph properties are maximumly preserved in this survey, we conduct a comprehensive review of the literature in graph embedding we first introduce the formal definition of graph embedding as well as the related concepts after that, we propose two taxonomies of graph embedding which correspond to what challenges exist in different graph embedding problem settings and how the existing work addresses these challenges in their solutions finally, we summarize the applications that graph embedding enables and suggest four promising future research directions in terms of computation efficiency, problem settings, techniques, and application scenarios",
            "sequence": "[CLS] video process has study research problem application production [SEP] a comprehensive survey of graph embedding: problems, techniques and applications graph is an important data representation which appears in a wide diversity of real world scenarios effective graph analytics provides users a deeper understanding of what is behind the data, and thus can benefit a lot of useful applications such as node classification, node recommendation, link prediction, etc however, most graph analytics methods suffer the high computation and space cost graph embedding is an effective yet efficient way to solve the graph analytics problem it converts the graph data into a low dimensional space in which the graph structural information and graph properties are maximumly preserved in this survey, we conduct a comprehensive review of the literature in graph embedding we first introduce the formal definition of graph embedding as well as the related concepts after that, we propose two taxonomies of graph embedding which correspond to what challenges exist in different graph embedding problem settings and how the existing work addresses these challenges in their solutions finally, we summarize the applications that graph embedding enables and suggest four promising future research directions in terms of computation efficiency, problem settings, techniques, and application scenarios [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R278",
                "label": "Information Science"
            }
        },
        {
            "instance_id": "R161545xR146646",
            "template_id": "R161545",
            "correct_template_id": null,
            "paper_id": "R146646",
            "premise": "recycling methods method recycling type has property",
            "hypothesis": "comprehensive evaluation of dna barcoding for the molecular species identification of forensically important australian sarcophagidae (diptera) carrion breeding sarcophagidae (diptera) can be used to estimate the post mortem interval in forensic cases difficulties with accurate morphological identifications at any life stage and a lack of documented thermobiological profiles have limited their current usefulness the molecular based approach of dna barcoding, which utilises a 648 bp fragment of the mitochondrial cytochrome oxidase subuniti gene, was evaluated in a pilot study for discrimination between 16 australian sarcophagids the current study comprehensively evaluated barcoding for a larger taxon set of 588 australian sarcophagids in total, 39 of the 84 known australian species were represented by 580 specimens, which includes 92% of potentially forensically important species a further eight specimens could not be identified, but were included nonetheless as six unidentifiable taxa a neighbour joining tree was generated and nucleotide sequence divergences were calculated all species except sarcophaga (fergusonimyia) bancroftorum, known for high morphological variability, were resolved as monophyletic (99 2% of cases), with bootstrap support of 100 excluding s bancroftorum, the mean intraspecific and interspecific variation ranged from 1 12% and 2 81\u201311 23%, respectively, allowing for species discrimination dna barcoding was therefore validated as a suitable method for molecular identification of australian sarcophagidae, which will aid in the implementation of this fauna in forensic entomology",
            "sequence": "[CLS] recycling methods method recycling type has property [SEP] comprehensive evaluation of dna barcoding for the molecular species identification of forensically important australian sarcophagidae (diptera) carrion breeding sarcophagidae (diptera) can be used to estimate the post mortem interval in forensic cases difficulties with accurate morphological identifications at any life stage and a lack of documented thermobiological profiles have limited their current usefulness the molecular based approach of dna barcoding, which utilises a 648 bp fragment of the mitochondrial cytochrome oxidase subuniti gene, was evaluated in a pilot study for discrimination between 16 australian sarcophagids the current study comprehensively evaluated barcoding for a larger taxon set of 588 australian sarcophagids in total, 39 of the 84 known australian species were represented by 580 specimens, which includes 92% of potentially forensically important species a further eight specimens could not be identified, but were included nonetheless as six unidentifiable taxa a neighbour joining tree was generated and nucleotide sequence divergences were calculated all species except sarcophaga (fergusonimyia) bancroftorum, known for high morphological variability, were resolved as monophyletic (99 2% of cases), with bootstrap support of 100 excluding s bancroftorum, the mean intraspecific and interspecific variation ranged from 1 12% and 2 81\u201311 23%, respectively, allowing for species discrimination dna barcoding was therefore validated as a suitable method for molecular identification of australian sarcophagidae, which will aid in the implementation of this fauna in forensic entomology [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            }
        },
        {
            "instance_id": "R161545xR159779",
            "template_id": "R161545",
            "correct_template_id": null,
            "paper_id": "R159779",
            "premise": "recycling methods method recycling type has property",
            "hypothesis": "return of the vision video: can corporate vision videos serve as setting for participation? this paper examines the role of corporate vision videos as a possible setting for participation when exploring the future potentials (and pitfalls) of new technological concepts we propose that through the recent decade\u2019s rise web 2 0 platforms, and the viral effects of user sharing, the corporate vision video of today might take on a significantly different role than before, and act as a participatory design approach this address the changing landscaping for participatory and user involved design processes, in the wake of new digital forms of participation, communication and collaboration, which have radically changed the possible power dynamics of the production life cycle of new product developments through a case study, we pose the question of whether the online engagements around corporate vision videos can be viewed as a form of participation in a design process, and thus revitalize the relevance of vision videos as a design resource?",
            "sequence": "[CLS] recycling methods method recycling type has property [SEP] return of the vision video: can corporate vision videos serve as setting for participation? this paper examines the role of corporate vision videos as a possible setting for participation when exploring the future potentials (and pitfalls) of new technological concepts we propose that through the recent decade\u2019s rise web 2 0 platforms, and the viral effects of user sharing, the corporate vision video of today might take on a significantly different role than before, and act as a participatory design approach this address the changing landscaping for participatory and user involved design processes, in the wake of new digital forms of participation, communication and collaboration, which have radically changed the possible power dynamics of the production life cycle of new product developments through a case study, we pose the question of whether the online engagements around corporate vision videos can be viewed as a form of participation in a design process, and thus revitalize the relevance of vision videos as a design resource? [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R265",
                "label": "Computer-Aided Engineering and Design"
            }
        },
        {
            "instance_id": "R154390xR140070",
            "template_id": "R154390",
            "correct_template_id": null,
            "paper_id": "R140070",
            "premise": "lignin decomposition substrate catalyst has temperature value solvent pressure reactor conversion product",
            "hypothesis": "hackathons as co optation ritual: socializing workers and institutionalizing innovation in the \u201cnew\u201d economy abstract \\nhackathons, time bounded events where participants write computer code and build apps, have become a popular means of socializing tech students and workers to produce \u201cinnovation\u201d despite little promise of material reward although they offer participants opportunities for learning new skills and face to face networking and set up interaction rituals that create an emotional \u201chigh,\u201d potential advantage is even greater for the events\u2019 corporate sponsors, who use them to outsource work, crowdsource innovation, and enhance their reputation ethnographic observations and informal interviews at seven hackathons held in new york during the course of a single school year show how the format of the event and sponsors\u2019 discursive tropes, within a dominant cultural frame reflecting the appeal of silicon valley, reshape unpaid and precarious work as an extraordinary opportunity, a ritual of ecstatic labor, and a collective imaginary for fictional expectations of innovation that benefits all, a powerful strategy for manufacturing workers\u2019 consent in the \u201cnew\u201d economy",
            "sequence": "[CLS] lignin decomposition substrate catalyst has temperature value solvent pressure reactor conversion product [SEP] hackathons as co optation ritual: socializing workers and institutionalizing innovation in the \u201cnew\u201d economy abstract \\nhackathons, time bounded events where participants write computer code and build apps, have become a popular means of socializing tech students and workers to produce \u201cinnovation\u201d despite little promise of material reward although they offer participants opportunities for learning new skills and face to face networking and set up interaction rituals that create an emotional \u201chigh,\u201d potential advantage is even greater for the events\u2019 corporate sponsors, who use them to outsource work, crowdsource innovation, and enhance their reputation ethnographic observations and informal interviews at seven hackathons held in new york during the course of a single school year show how the format of the event and sponsors\u2019 discursive tropes, within a dominant cultural frame reflecting the appeal of silicon valley, reshape unpaid and precarious work as an extraordinary opportunity, a ritual of ecstatic labor, and a collective imaginary for fictional expectations of innovation that benefits all, a powerful strategy for manufacturing workers\u2019 consent in the \u201cnew\u201d economy [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            }
        },
        {
            "instance_id": "R40006xR159484",
            "template_id": "R40006",
            "correct_template_id": null,
            "paper_id": "R159484",
            "premise": "basic reproduction number estimate time period basic reproduction number location",
            "hypothesis": "smart city dvelopment with digital twin technology growing urban areas are major consumers of natural resources, energy and raw materials understanding cities\u00b4 urban metabolism is salient when developing sustainable and resilient cities this paper addresses concepts of smart city and digital twin technology as means to foster more sustainable urban development smart city has globally been well adopted concept in urban development with smart city development cities aim to optimize overall performance of the city, its infrastructures, processes and services, but also to improve socio economic wellbeing dynamic digital twins are constituted to form real time connectivity between virtual and physical objects digital twin combines virtual objects to its physical counterparts this conceptual paper provides additionally examples from dynamic digital twin platforms and digital twin of helsinki, finland",
            "sequence": "[CLS] basic reproduction number estimate time period basic reproduction number location [SEP] smart city dvelopment with digital twin technology growing urban areas are major consumers of natural resources, energy and raw materials understanding cities\u00b4 urban metabolism is salient when developing sustainable and resilient cities this paper addresses concepts of smart city and digital twin technology as means to foster more sustainable urban development smart city has globally been well adopted concept in urban development with smart city development cities aim to optimize overall performance of the city, its infrastructures, processes and services, but also to improve socio economic wellbeing dynamic digital twins are constituted to form real time connectivity between virtual and physical objects digital twin combines virtual objects to its physical counterparts this conceptual paper provides additionally examples from dynamic digital twin platforms and digital twin of helsinki, finland [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            }
        },
        {
            "instance_id": "R155844xR140197",
            "template_id": "R155844",
            "correct_template_id": null,
            "paper_id": "R140197",
            "premise": "photocatalysts wavelength of maximum absorption energy band gap photocatalyst wavelength of maximum emission emission lifetime ground state oxidation potential ground state reduction potential excited state oxidation potential excited state reduction potential",
            "hypothesis": "dna barcodes distinguish species of tropical lepidoptera although central to much biological research, the identification of species is often difficult the use of dna barcodes, short dna sequences from a standardized region of the genome, has recently been proposed as a tool to facilitate species identification and discovery however, the effectiveness of dna barcoding for identifying specimens in species rich tropical biotas is unknown here we show that cytochrome c oxidase i dna barcodes effectively discriminate among species in three lepidoptera families from area de conservaci\u00f3n guanacaste in northwestern costa rica we found that 97 9% of the 521 species recognized by prior taxonomic work possess distinctive cytochrome c oxidase i barcodes and that the few instances of interspecific sequence overlap involve very similar species we also found two or more barcode clusters within each of 13 supposedly single species covariation between these clusters and morphological and/or ecological traits indicates overlooked species complexes if these results are general, dna barcoding will significantly aid species identification and discovery in tropical settings",
            "sequence": "[CLS] photocatalysts wavelength of maximum absorption energy band gap photocatalyst wavelength of maximum emission emission lifetime ground state oxidation potential ground state reduction potential excited state oxidation potential excited state reduction potential [SEP] dna barcodes distinguish species of tropical lepidoptera although central to much biological research, the identification of species is often difficult the use of dna barcodes, short dna sequences from a standardized region of the genome, has recently been proposed as a tool to facilitate species identification and discovery however, the effectiveness of dna barcoding for identifying specimens in species rich tropical biotas is unknown here we show that cytochrome c oxidase i dna barcodes effectively discriminate among species in three lepidoptera families from area de conservaci\u00f3n guanacaste in northwestern costa rica we found that 97 9% of the 521 species recognized by prior taxonomic work possess distinctive cytochrome c oxidase i barcodes and that the few instances of interspecific sequence overlap involve very similar species we also found two or more barcode clusters within each of 13 supposedly single species covariation between these clusters and morphological and/or ecological traits indicates overlooked species complexes if these results are general, dna barcoding will significantly aid species identification and discovery in tropical settings [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            }
        },
        {
            "instance_id": "R159441xR139112",
            "template_id": "R159441",
            "correct_template_id": null,
            "paper_id": "R139112",
            "premise": "city digital twin potentials visualization situational awareness planning and prediction integration and collaboration data management",
            "hypothesis": "absolute ozone densities in a radio frequency driven atmospheric pressure plasma using two beam uv led absorption spectroscopy and numerical simulations the efficient generation of reactive oxygen species (ros) in cold atmospheric pressure plasma jets (appjs) is an increasingly important topic, e g for the treatment of temperature sensitive biological samples in the field of plasma medicine a 13 56 mhz radio frequency (rf) driven appj device operated with helium feed gas and small admixtures of oxygen (up to 1%), generating a homogeneous glow mode plasma at low gas temperatures, was investigated absolute densities of ozone, one of the most prominent ros, were measured across the 11 mm wide discharge channel by means of broadband absorption spectroscopy using the hartley band centred at \u03bb = 255 nm a two beam setup with a reference beam in mach\u2013zehnder configuration is employed for improved signal to noise ratio allowing high sensitivity measurements in the investigated single pass weak absorbance regime the results are correlated to gas temperature measurements, deduced from the rotational temperature of the n2 (c 3 \u03c0 u + \u2192 b 3 \u03c0 g + , \u03c5 = 0 \u2192 2) optical emission from introduced air impurities the observed opposing trends of both quantities as a function of rf power input and oxygen admixture are analysed and explained in terms of a zero dimensional plasma chemical kinetics simulation it is found that the gas temperature as well as the densities of o and o2(b 1 \u03c3 g + ) influence the absolute o3 densities when the rf power is varied",
            "sequence": "[CLS] city digital twin potentials visualization situational awareness planning and prediction integration and collaboration data management [SEP] absolute ozone densities in a radio frequency driven atmospheric pressure plasma using two beam uv led absorption spectroscopy and numerical simulations the efficient generation of reactive oxygen species (ros) in cold atmospheric pressure plasma jets (appjs) is an increasingly important topic, e g for the treatment of temperature sensitive biological samples in the field of plasma medicine a 13 56 mhz radio frequency (rf) driven appj device operated with helium feed gas and small admixtures of oxygen (up to 1%), generating a homogeneous glow mode plasma at low gas temperatures, was investigated absolute densities of ozone, one of the most prominent ros, were measured across the 11 mm wide discharge channel by means of broadband absorption spectroscopy using the hartley band centred at \u03bb = 255 nm a two beam setup with a reference beam in mach\u2013zehnder configuration is employed for improved signal to noise ratio allowing high sensitivity measurements in the investigated single pass weak absorbance regime the results are correlated to gas temperature measurements, deduced from the rotational temperature of the n2 (c 3 \u03c0 u + \u2192 b 3 \u03c0 g + , \u03c5 = 0 \u2192 2) optical emission from introduced air impurities the observed opposing trends of both quantities as a function of rf power input and oxygen admixture are analysed and explained in terms of a zero dimensional plasma chemical kinetics simulation it is found that the gas temperature as well as the densities of o and o2(b 1 \u03c3 g + ) influence the absolute o3 densities when the rf power is varied [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            }
        },
        {
            "instance_id": "R138077xR25133",
            "template_id": "R138077",
            "correct_template_id": null,
            "paper_id": "R25133",
            "premise": "ontology learning from data sources has dataset application domain input format output format accuracy f1 score precision knowledge source has data source learning purpose terms learning axiom learning properties learning properties hierarchy learning rule learning class hierarchy learning learning method learning tool evaluation metrics related work validation tool training corpus testing corpus class learning implemented technologies relationships learning instance learning taxonomy learning validation comment assessment of acquired knowledge recall f measure",
            "hypothesis": "complementary audio visual collision warnings the growing number of driver assistance systems increases the demand for warnings that are intuitively comprehensible particularly in hazardous situations, such as a threatening collision, a driver must understand the warning immediately for this reason, collision warnings should convey as much information as needed to interpret the situation properly and to prepare preventive actions the present study investigated whether informing about the object and the location of an imminent crash by a multimodal warning (visual and auditory) leads to shorter reaction times and fewer collisions compared to warning signals which only inform about the object of the crash (auditory icons) or give no additional information (simple tone) results reveal that multimodal warnings have the potential to produce a significant advantage over unimodal signals as long as their components complement each other in a way that realistically fits the situation at hand",
            "sequence": "[CLS] ontology learning from data sources has dataset application domain input format output format accuracy f1 score precision knowledge source has data source learning purpose terms learning axiom learning properties learning properties hierarchy learning rule learning class hierarchy learning learning method learning tool evaluation metrics related work validation tool training corpus testing corpus class learning implemented technologies relationships learning instance learning taxonomy learning validation comment assessment of acquired knowledge recall f measure [SEP] complementary audio visual collision warnings the growing number of driver assistance systems increases the demand for warnings that are intuitively comprehensible particularly in hazardous situations, such as a threatening collision, a driver must understand the warning immediately for this reason, collision warnings should convey as much information as needed to interpret the situation properly and to prepare preventive actions the present study investigated whether informing about the object and the location of an imminent crash by a multimodal warning (visual and auditory) leads to shorter reaction times and fewer collisions compared to warning signals which only inform about the object of the crash (auditory icons) or give no additional information (simple tone) results reveal that multimodal warnings have the potential to produce a significant advantage over unimodal signals as long as their components complement each other in a way that realistically fits the situation at hand [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R11",
                "label": "Science"
            }
        },
        {
            "instance_id": "R198658xR161372",
            "template_id": "R198658",
            "correct_template_id": null,
            "paper_id": "R161372",
            "premise": "nacre mechanics template has method has result research problem has material",
            "hypothesis": "biocatalytic degradation efficiency of postconsumer polyethylene terephthalate packaging determined by their polymer microstructures polyethylene terephthalate (pet) is the most important mass\u2010produced thermoplastic polyester used as a packaging material recently, thermophilic polyester hydrolases such as tfcut2 from thermobifida fusca have emerged as promising biocatalysts for an eco\u2010friendly pet recycling process in this study, postconsumer pet food packaging containers are treated with tfcut2 and show weight losses of more than 50% after 96 h of incubation at 70 \u00b0c differential scanning calorimetry analysis indicates that the high linear degradation rates observed in the first 72 h of incubation is due to the high hydrolysis susceptibility of the mobile amorphous fraction (maf) of pet the physical aging process of pet occurring at 70 \u00b0c is shown to gradually convert maf to polymer microstructures with limited accessibility to enzymatic hydrolysis analysis of the chain\u2010length distribution of degraded pet by nuclear magnetic resonance spectroscopy reveals that maf is rapidly hydrolyzed via a combinatorial exo\u2010 and endo\u2010type degradation mechanism whereas the remaining pet microstructures are slowly degraded only by endo\u2010type chain scission causing no detectable weight loss hence, efficient thermostable biocatalysts are required to overcome the competitive physical aging process for the complete degradation of postconsumer pet materials close to the glass transition temperature of pet",
            "sequence": "[CLS] nacre mechanics template has method has result research problem has material [SEP] biocatalytic degradation efficiency of postconsumer polyethylene terephthalate packaging determined by their polymer microstructures polyethylene terephthalate (pet) is the most important mass\u2010produced thermoplastic polyester used as a packaging material recently, thermophilic polyester hydrolases such as tfcut2 from thermobifida fusca have emerged as promising biocatalysts for an eco\u2010friendly pet recycling process in this study, postconsumer pet food packaging containers are treated with tfcut2 and show weight losses of more than 50% after 96 h of incubation at 70 \u00b0c differential scanning calorimetry analysis indicates that the high linear degradation rates observed in the first 72 h of incubation is due to the high hydrolysis susceptibility of the mobile amorphous fraction (maf) of pet the physical aging process of pet occurring at 70 \u00b0c is shown to gradually convert maf to polymer microstructures with limited accessibility to enzymatic hydrolysis analysis of the chain\u2010length distribution of degraded pet by nuclear magnetic resonance spectroscopy reveals that maf is rapidly hydrolyzed via a combinatorial exo\u2010 and endo\u2010type degradation mechanism whereas the remaining pet microstructures are slowly degraded only by endo\u2010type chain scission causing no detectable weight loss hence, efficient thermostable biocatalysts are required to overcome the competitive physical aging process for the complete degradation of postconsumer pet materials close to the glass transition temperature of pet [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R131",
                "label": "Polymer Chemistry"
            }
        },
        {
            "instance_id": "R138077xR74516",
            "template_id": "R138077",
            "correct_template_id": null,
            "paper_id": "R74516",
            "premise": "ontology learning from data sources has dataset application domain input format output format accuracy f1 score precision knowledge source has data source learning purpose terms learning axiom learning properties learning properties hierarchy learning rule learning class hierarchy learning learning method learning tool evaluation metrics related work validation tool training corpus testing corpus class learning implemented technologies relationships learning instance learning taxonomy learning validation comment assessment of acquired knowledge recall f measure",
            "hypothesis": "measuring human values in software engineering \"background: human values, such as prestige, social justice, and financial success, influence software production decision making processes while their subjectivity makes some values difficult to measure, their impact on software motivates our research aim: to contribute to the scientific understanding and the empirical investigation of human values in software engineering (se) approach: drawing from social psychology, we consider values as mental representations to be investigated on three levels: at a system (l1), personal (l2), and instantiation level (l3) method: we design and develop a selection of tools for the investigation of values at each level, and focus on the design, development, and use of the values q sort results: from our study with 12 software practitioners, it is possible to extract three values `prototypes' indicative of an emergent typology of values considerations in se conclusions: the values q sort generates quantitative values prototypes indicating values relations (l1) as well as rich personal narratives (l2) that reflect specific software practices (l3) it thus offers a systematic, empirical approach to capturing values in se \"",
            "sequence": "[CLS] ontology learning from data sources has dataset application domain input format output format accuracy f1 score precision knowledge source has data source learning purpose terms learning axiom learning properties learning properties hierarchy learning rule learning class hierarchy learning learning method learning tool evaluation metrics related work validation tool training corpus testing corpus class learning implemented technologies relationships learning instance learning taxonomy learning validation comment assessment of acquired knowledge recall f measure [SEP] measuring human values in software engineering \"background: human values, such as prestige, social justice, and financial success, influence software production decision making processes while their subjectivity makes some values difficult to measure, their impact on software motivates our research aim: to contribute to the scientific understanding and the empirical investigation of human values in software engineering (se) approach: drawing from social psychology, we consider values as mental representations to be investigated on three levels: at a system (l1), personal (l2), and instantiation level (l3) method: we design and develop a selection of tools for the investigation of values at each level, and focus on the design, development, and use of the values q sort results: from our study with 12 software practitioners, it is possible to extract three values `prototypes' indicative of an emergent typology of values considerations in se conclusions: the values q sort generates quantitative values prototypes indicating values relations (l1) as well as rich personal narratives (l2) that reflect specific software practices (l3) it thus offers a systematic, empirical approach to capturing values in se \" [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R155844xR140187",
            "template_id": "R155844",
            "correct_template_id": null,
            "paper_id": "R140187",
            "premise": "photocatalysts wavelength of maximum absorption energy band gap photocatalyst wavelength of maximum emission emission lifetime ground state oxidation potential ground state reduction potential excited state oxidation potential excited state reduction potential",
            "hypothesis": "dna barcoding the geometrid fauna of bavaria (lepidoptera): successes, surprises, and questions background the state of bavaria is involved in a research program that will lead to the construction of a dna barcode library for all animal species within its territorial boundaries the present study provides a comprehensive dna barcode library for the geometridae, one of the most diverse of insect families methodology/principal findings this study reports dna barcodes for 400 bavarian geometrid species, 98 per cent of the known fauna, and approximately one per cent of all bavarian animal species although 98 5% of these species possess diagnostic barcode sequences in bavaria, records from neighbouring countries suggest that species level resolution may be compromised in up to 3 5% of cases all taxa which apparently share barcodes are discussed in detail one case of modest divergence (1 4%) revealed a species overlooked by the current taxonomic system: eupithecia goossensiata mabille, 1869 stat n is raised from synonymy with eupithecia absinthiata (clerck, 1759) to species rank deep intraspecific sequence divergences (>2%) were detected in 20 traditionally recognized species conclusions/significance the study emphasizes the effectiveness of dna barcoding as a tool for monitoring biodiversity open access is provided to a data set that includes records for 1,395 geometrid specimens (331 species) from bavaria, with 69 additional species from neighbouring regions taxa with deep intraspecific sequence divergences are undergoing more detailed analysis to ascertain if they represent cases of cryptic diversity",
            "sequence": "[CLS] photocatalysts wavelength of maximum absorption energy band gap photocatalyst wavelength of maximum emission emission lifetime ground state oxidation potential ground state reduction potential excited state oxidation potential excited state reduction potential [SEP] dna barcoding the geometrid fauna of bavaria (lepidoptera): successes, surprises, and questions background the state of bavaria is involved in a research program that will lead to the construction of a dna barcode library for all animal species within its territorial boundaries the present study provides a comprehensive dna barcode library for the geometridae, one of the most diverse of insect families methodology/principal findings this study reports dna barcodes for 400 bavarian geometrid species, 98 per cent of the known fauna, and approximately one per cent of all bavarian animal species although 98 5% of these species possess diagnostic barcode sequences in bavaria, records from neighbouring countries suggest that species level resolution may be compromised in up to 3 5% of cases all taxa which apparently share barcodes are discussed in detail one case of modest divergence (1 4%) revealed a species overlooked by the current taxonomic system: eupithecia goossensiata mabille, 1869 stat n is raised from synonymy with eupithecia absinthiata (clerck, 1759) to species rank deep intraspecific sequence divergences (>2%) were detected in 20 traditionally recognized species conclusions/significance the study emphasizes the effectiveness of dna barcoding as a tool for monitoring biodiversity open access is provided to a data set that includes records for 1,395 geometrid specimens (331 species) from bavaria, with 69 additional species from neighbouring regions taxa with deep intraspecific sequence divergences are undergoing more detailed analysis to ascertain if they represent cases of cryptic diversity [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            }
        },
        {
            "instance_id": "R184022xR159789",
            "template_id": "R184022",
            "correct_template_id": null,
            "paper_id": "R159789",
            "premise": "xray spectroscopy paper type research objective has system qualities",
            "hypothesis": "the potential of using vision videos for crowdre: video comments as a source of feedback vision videos are established for soliciting feedback and stimulating discussions in requirements engineering (re) practices such as focus groups different researchers motivated the transfer of these benefits into crowd based re (crowdre) by using vision videos on social media platforms so far, however, little research explored the potential of using vision videos for crowdre in detail in this paper, we analyze and assess this potential, in particular, focusing on video comments as a source of feedback in a case study, we analyzed 4505 comments on a vision video from youtube we found that the video solicited 2770 comments from 2660 viewers in four days this is more than 50% of all comments the video received in four years even though only a certain fraction of these comments are relevant to re, the relevant comments address typical intentions and topics of user feedback, such as feature request or problem report besides the typical user feedback categories, we found more than 300 comments that address the topic safety which has not appeared in previous analyses of user feedback in an automated analysis, we compared the performance of three machine learning algorithms on classifying the video comments despite certain differences, the algorithms classified the video comments well based on these findings, we conclude that the use of vision videos for crowdre has a large potential despite the preliminary nature of the case study, we are optimistic that vision videos can motivate stakeholders to actively participate in a crowd and solicit numerous of video comments as a valuable source of feedback",
            "sequence": "[CLS] xray spectroscopy paper type research objective has system qualities [SEP] the potential of using vision videos for crowdre: video comments as a source of feedback vision videos are established for soliciting feedback and stimulating discussions in requirements engineering (re) practices such as focus groups different researchers motivated the transfer of these benefits into crowd based re (crowdre) by using vision videos on social media platforms so far, however, little research explored the potential of using vision videos for crowdre in detail in this paper, we analyze and assess this potential, in particular, focusing on video comments as a source of feedback in a case study, we analyzed 4505 comments on a vision video from youtube we found that the video solicited 2770 comments from 2660 viewers in four days this is more than 50% of all comments the video received in four years even though only a certain fraction of these comments are relevant to re, the relevant comments address typical intentions and topics of user feedback, such as feature request or problem report besides the typical user feedback categories, we found more than 300 comments that address the topic safety which has not appeared in previous analyses of user feedback in an automated analysis, we compared the performance of three machine learning algorithms on classifying the video comments despite certain differences, the algorithms classified the video comments well based on these findings, we conclude that the use of vision videos for crowdre has a large potential despite the preliminary nature of the case study, we are optimistic that vision videos can motivate stakeholders to actively participate in a crowd and solicit numerous of video comments as a valuable source of feedback [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R152828xR171846",
            "template_id": "R152828",
            "correct_template_id": null,
            "paper_id": "R171846",
            "premise": "xray laser pumped has system qualities",
            "hypothesis": "investigation of the material combination 20mncr5 and x45crsi9 3 in the tailored forming of shafts with bearing seats abstract the tailored forming process chain is used to manufacture hybrid components and consists of a joining process or additive manufacturing for various materials (e g deposition welding), subsequent hot forming, machining and heat treatment in this way, components can be produced with materials adapted to the load case for this paper, hybrid shafts are produced by deposition welding of a cladding made of x45crsi9 3 onto a workpiece made from 20mncr5 the hybrid shafts are then formed by means of cross wedge rolling it is investigated, how the thickness of the cladding and the type of cooling after hot forming (in air or in water) affect the properties of the cladding the hybrid shafts are formed without layer separation however, slight core loosening occurres in the area of the bearing seat due to the mannesmann effect the microhardness of the cladding is only slightly effected by the cooling strategy, while the microhardness of the base material is significantly higher in water cooled shafts the microstructure of the cladding after both cooling strategies consists mainly of martensite in the base material, air cooling results in a mainly ferritic microstructure with grains of ferrite pearlite quenching in water results in a microstructure containing mainly martensite",
            "sequence": "[CLS] xray laser pumped has system qualities [SEP] investigation of the material combination 20mncr5 and x45crsi9 3 in the tailored forming of shafts with bearing seats abstract the tailored forming process chain is used to manufacture hybrid components and consists of a joining process or additive manufacturing for various materials (e g deposition welding), subsequent hot forming, machining and heat treatment in this way, components can be produced with materials adapted to the load case for this paper, hybrid shafts are produced by deposition welding of a cladding made of x45crsi9 3 onto a workpiece made from 20mncr5 the hybrid shafts are then formed by means of cross wedge rolling it is investigated, how the thickness of the cladding and the type of cooling after hot forming (in air or in water) affect the properties of the cladding the hybrid shafts are formed without layer separation however, slight core loosening occurres in the area of the bearing seat due to the mannesmann effect the microhardness of the cladding is only slightly effected by the cooling strategy, while the microhardness of the base material is significantly higher in water cooled shafts the microstructure of the cladding after both cooling strategies consists mainly of martensite in the base material, air cooling results in a mainly ferritic microstructure with grains of ferrite pearlite quenching in water results in a microstructure containing mainly martensite [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R137654",
                "label": "Mechanical Process Engineering"
            }
        },
        {
            "instance_id": "R150089xR111923",
            "template_id": "R150089",
            "correct_template_id": null,
            "paper_id": "R111923",
            "premise": "epidemiological surveillance systems design and implementation software used software development approach related work epidemiological surveillance system purpose epidemiological surveillance process epidemiological surveillance users statistical analysis techniques epidemiological surveillance architecture epidemiological software development approach advantage provided by the system limit of the system",
            "hypothesis": "same app, different app stores: a comparative study to attract more users, implementing the same mobile app for different platforms has become a common industry practice app stores provide a unique channel for users to share feedback on the acquired apps through ratings and textual reviews however, each mobile platform has its own online store for distributing apps to users to understand the characteristics of and discrepancies in how users perceive the same app implemented for and distributed through different platforms, we present a large scale comparative study of cross platform apps we mine the characteristics of 80,000 app pairs (160k apps in total) from a corpus of 2 4 million apps collected from the apple and google play app stores we quantitatively compare their app store attributes, such as stars, versions, and prices we measure the aggregated user perceived ratings and find many discrepancies across the platforms further, we employ machine learning to classify 1 7 million textual user reviews obtained from 2,000 of the mined app pairs we analyze discrepancies and root causes of user complaints to understand cross platform development challenges that impact cross platform user perceived ratings we also follow up with the developers to understand the reasons behind identified discrepancies",
            "sequence": "[CLS] epidemiological surveillance systems design and implementation software used software development approach related work epidemiological surveillance system purpose epidemiological surveillance process epidemiological surveillance users statistical analysis techniques epidemiological surveillance architecture epidemiological software development approach advantage provided by the system limit of the system [SEP] same app, different app stores: a comparative study to attract more users, implementing the same mobile app for different platforms has become a common industry practice app stores provide a unique channel for users to share feedback on the acquired apps through ratings and textual reviews however, each mobile platform has its own online store for distributing apps to users to understand the characteristics of and discrepancies in how users perceive the same app implemented for and distributed through different platforms, we present a large scale comparative study of cross platform apps we mine the characteristics of 80,000 app pairs (160k apps in total) from a corpus of 2 4 million apps collected from the apple and google play app stores we quantitatively compare their app store attributes, such as stars, versions, and prices we measure the aggregated user perceived ratings and find many discrepancies across the platforms further, we employ machine learning to classify 1 7 million textual user reviews obtained from 2,000 of the mined app pairs we analyze discrepancies and root causes of user complaints to understand cross platform development challenges that impact cross platform user perceived ratings we also follow up with the developers to understand the reasons behind identified discrepancies [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R187648xR157039",
            "template_id": "R187648",
            "correct_template_id": null,
            "paper_id": "R157039",
            "premise": "oco for control research problem theoretical guarantees has application in data driven approach has constraints type of considered system measurement noise process noise",
            "hypothesis": "dna barcode library for european gelechiidae (lepidoptera) suggests greatly underestimated species diversity for the first time, a nearly complete barcode library for european gelechiidae is provided dna barcode sequences (coi gene \u2013 cytochrome c oxidase 1) from 751 out of 865 nominal species, belonging to 105 genera, were successfully recovered a total of 741 species represented by specimens with sequences \u2265 500bp and an additional ten species represented by specimens with shorter sequences were used to produce 53 nj trees intraspecific barcode divergence averaged only 0 54% whereas distance to the nearest neighbour species averaged 5 58% of these, 710 species possessed unique dna barcodes, but 31 species could not be reliably discriminated because of barcode sharing or partial barcode overlap species discrimination based on the barcode index system (bin) was successful for 668 out of 723 species which clustered from minimum one to maximum 22 unique bins fifty five species shared a bin with up to four species and identification from dna barcode data is uncertain finally, 65 clusters with a unique bin remained unidentified to species level these putative taxa, as well as 114 nominal species with more than one bin, suggest the presence of considerable cryptic diversity, cases which should be examined in future revisionary studies",
            "sequence": "[CLS] oco for control research problem theoretical guarantees has application in data driven approach has constraints type of considered system measurement noise process noise [SEP] dna barcode library for european gelechiidae (lepidoptera) suggests greatly underestimated species diversity for the first time, a nearly complete barcode library for european gelechiidae is provided dna barcode sequences (coi gene \u2013 cytochrome c oxidase 1) from 751 out of 865 nominal species, belonging to 105 genera, were successfully recovered a total of 741 species represented by specimens with sequences \u2265 500bp and an additional ten species represented by specimens with shorter sequences were used to produce 53 nj trees intraspecific barcode divergence averaged only 0 54% whereas distance to the nearest neighbour species averaged 5 58% of these, 710 species possessed unique dna barcodes, but 31 species could not be reliably discriminated because of barcode sharing or partial barcode overlap species discrimination based on the barcode index system (bin) was successful for 668 out of 723 species which clustered from minimum one to maximum 22 unique bins fifty five species shared a bin with up to four species and identification from dna barcode data is uncertain finally, 65 clusters with a unique bin remained unidentified to species level these putative taxa, as well as 114 nominal species with more than one bin, suggest the presence of considerable cryptic diversity, cases which should be examined in future revisionary studies [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            }
        },
        {
            "instance_id": "R184022xR38493",
            "template_id": "R184022",
            "correct_template_id": null,
            "paper_id": "R38493",
            "premise": "xray spectroscopy paper type research objective has system qualities",
            "hypothesis": "current challenges for studying search as learning processes search of resources and information is among the most frequent activities on the web while established information retrieval approaches address the relevance of search results to an information need, the actual learning scope of a user is normally disregarded recent research in the search as learning (sal) area has recognized the importance of learning scopes and focused on observing and detecting learning needs the article at hand takes a critical look at existing works in sal and related research disciplines it aims to give a concise, interdisplinary overview which allows for the deduction of possible directions and necessary actions for prospective research works it becomes apparent that past research employs a strong emphasis on textual resources, neglecting the diversity of online multimedia contents for learning and the impact of multimodal features on the learning process we argue that exploring multimodal learning resources should be one focus of future sal projects",
            "sequence": "[CLS] xray spectroscopy paper type research objective has system qualities [SEP] current challenges for studying search as learning processes search of resources and information is among the most frequent activities on the web while established information retrieval approaches address the relevance of search results to an information need, the actual learning scope of a user is normally disregarded recent research in the search as learning (sal) area has recognized the importance of learning scopes and focused on observing and detecting learning needs the article at hand takes a critical look at existing works in sal and related research disciplines it aims to give a concise, interdisplinary overview which allows for the deduction of possible directions and necessary actions for prospective research works it becomes apparent that past research employs a strong emphasis on textual resources, neglecting the diversity of online multimedia contents for learning and the impact of multimodal features on the learning process we argue that exploring multimodal learning resources should be one focus of future sal projects [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R278",
                "label": "Information Science"
            }
        },
        {
            "instance_id": "R159441xR186093",
            "template_id": "R159441",
            "correct_template_id": null,
            "paper_id": "R186093",
            "premise": "city digital twin potentials visualization situational awareness planning and prediction integration and collaboration data management",
            "hypothesis": "assessing business it allignment maturity strategic alignment focuses on the activities that management performs to achieve cohesive goals across the it (information technology) and other functional organizations (e g , finance, marketing, h/r, r&amp;d, manufacturing) therefore, alignment addresses both how it is in harmony with the business, and how the business should, or could, be in harmony with it alignment evolves into a relationship where the function of it and other business functions adapt their strategies together achieving alignment is evolutionary and dynamic it requires strong support from senior management, good working relationships, strong leadership, appropriate prioritization, trust, and effective communication, as well as a thorough understanding of the business and technical environments the strategic alignment maturity assessment provides organizations with a vehicle to evaluate these activities knowing the maturity of its strategic choices and alignment practices make it possible for a firm to see where it stands and how it can improve this chapter discusses an approach for assessing the maturity of the business it alignment once maturity is understood, an organization can identify opportunities for enhancing the harmonious relationship of business and it",
            "sequence": "[CLS] city digital twin potentials visualization situational awareness planning and prediction integration and collaboration data management [SEP] assessing business it allignment maturity strategic alignment focuses on the activities that management performs to achieve cohesive goals across the it (information technology) and other functional organizations (e g , finance, marketing, h/r, r&amp;d, manufacturing) therefore, alignment addresses both how it is in harmony with the business, and how the business should, or could, be in harmony with it alignment evolves into a relationship where the function of it and other business functions adapt their strategies together achieving alignment is evolutionary and dynamic it requires strong support from senior management, good working relationships, strong leadership, appropriate prioritization, trust, and effective communication, as well as a thorough understanding of the business and technical environments the strategic alignment maturity assessment provides organizations with a vehicle to evaluate these activities knowing the maturity of its strategic choices and alignment practices make it possible for a firm to see where it stands and how it can improve this chapter discusses an approach for assessing the maturity of the business it alignment once maturity is understood, an organization can identify opportunities for enhancing the harmonious relationship of business and it [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R160259xR139118",
            "template_id": "R160259",
            "correct_template_id": null,
            "paper_id": "R139118",
            "premise": "digital city twin review application area",
            "hypothesis": "determination of no densities in a surface dielectric barrier discharge using optical emission spectroscopy a new computationally assisted diagnostic to measure no densities in atmospheric pressure microplasmas by optical emission spectroscopy (oes) is developed and validated against absorption spectroscopy in a volume dielectric barrier discharge (dbd) the oes method is then applied to a twin surface dbd operated in n 2 to measure the no density as a function of the o 2 admixture ( 0 1%\u2013 1%) the underlying rate equation model reveals that no ( a 2 \u03c3 + ) is primarily excited by reactions of the ground state no ( x 2 \u03c0 ) with metastables n 2 ( a 3 \u03c3 u + ) a new computationally assisted diagnostic to measure no densities in atmospheric pressure microplasmas by optical emission spectroscopy (oes) is developed and validated against absorption spectroscopy in a volume dielectric barrier discharge (dbd) the oes method is then applied to a twin surface dbd operated in n 2 to measure the no density as a function of the o 2 admixture ( 0 1%\u2013 1%) the underlying rate equation model reveals that no ( a 2 \u03c3 + ) is primarily excited by reactions of the ground state no ( x 2 \u03c0 ) with metastables n 2 ( a 3 \u03c3 u + )",
            "sequence": "[CLS] digital city twin review application area [SEP] determination of no densities in a surface dielectric barrier discharge using optical emission spectroscopy a new computationally assisted diagnostic to measure no densities in atmospheric pressure microplasmas by optical emission spectroscopy (oes) is developed and validated against absorption spectroscopy in a volume dielectric barrier discharge (dbd) the oes method is then applied to a twin surface dbd operated in n 2 to measure the no density as a function of the o 2 admixture ( 0 1%\u2013 1%) the underlying rate equation model reveals that no ( a 2 \u03c3 + ) is primarily excited by reactions of the ground state no ( x 2 \u03c0 ) with metastables n 2 ( a 3 \u03c3 u + ) a new computationally assisted diagnostic to measure no densities in atmospheric pressure microplasmas by optical emission spectroscopy (oes) is developed and validated against absorption spectroscopy in a volume dielectric barrier discharge (dbd) the oes method is then applied to a twin surface dbd operated in n 2 to measure the no density as a function of the o 2 admixture ( 0 1%\u2013 1%) the underlying rate equation model reveals that no ( a 2 \u03c3 + ) is primarily excited by reactions of the ground state no ( x 2 \u03c0 ) with metastables n 2 ( a 3 \u03c3 u + ) [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            }
        },
        {
            "instance_id": "R150595xR49480",
            "template_id": "R150595",
            "correct_template_id": null,
            "paper_id": "R49480",
            "premise": "tailored forming contribution has result research problem has material realizes",
            "hypothesis": "software architecture optimization methods: a systematic literature review due to significant industrial demands toward software systems with increasing complexity and challenging quality requirements, software architecture design has become an important development activity and the research domain is rapidly evolving in the last decades, software architecture optimization methods, which aim to automate the search for an optimal architecture design with respect to a (set of) quality attribute(s), have proliferated however, the reported results are fragmented over different research communities, multiple system domains, and multiple quality attributes to integrate the existing research results, we have performed a systematic literature review and analyzed the results of 188 research papers from the different research communities based on this survey, a taxonomy has been created which is used to classify the existing research furthermore, the systematic analysis of the research literature provided in this review aims to help the research community in consolidating the existing research efforts and deriving a research agenda for future developments",
            "sequence": "[CLS] tailored forming contribution has result research problem has material realizes [SEP] software architecture optimization methods: a systematic literature review due to significant industrial demands toward software systems with increasing complexity and challenging quality requirements, software architecture design has become an important development activity and the research domain is rapidly evolving in the last decades, software architecture optimization methods, which aim to automate the search for an optimal architecture design with respect to a (set of) quality attribute(s), have proliferated however, the reported results are fragmented over different research communities, multiple system domains, and multiple quality attributes to integrate the existing research results, we have performed a systematic literature review and analyzed the results of 188 research papers from the different research communities based on this survey, a taxonomy has been created which is used to classify the existing research furthermore, the systematic analysis of the research literature provided in this review aims to help the research community in consolidating the existing research efforts and deriving a research agenda for future developments [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R152828xR39210",
            "template_id": "R152828",
            "correct_template_id": null,
            "paper_id": "R39210",
            "premise": "xray laser pumped has system qualities",
            "hypothesis": "a survey of recommender systems based on deep learning in recent years, deep learning\u2019s revolutionary advances in speech recognition, image analysis, and natural language processing have gained significant attention deep learning technology has become a hotspot research field in the artificial intelligence and has been applied into recommender system in contrast to traditional recommendation models, deep learning is able to effectively capture the non linear and non trivial user item relationships and enables the codification of more complex abstractions as data representations in the higher layers in this paper, we provide a comprehensive review of the related research contents of deep learning based recommender systems first, we introduce the basic terminologies and the background concepts of recommender systems and deep learning technology second, we describe the main current research on deep learning based recommender systems third, we provide the possible research directions of deep learning based recommender systems in the future finally, concludes this paper",
            "sequence": "[CLS] xray laser pumped has system qualities [SEP] a survey of recommender systems based on deep learning in recent years, deep learning\u2019s revolutionary advances in speech recognition, image analysis, and natural language processing have gained significant attention deep learning technology has become a hotspot research field in the artificial intelligence and has been applied into recommender system in contrast to traditional recommendation models, deep learning is able to effectively capture the non linear and non trivial user item relationships and enables the codification of more complex abstractions as data representations in the higher layers in this paper, we provide a comprehensive review of the related research contents of deep learning based recommender systems first, we introduce the basic terminologies and the background concepts of recommender systems and deep learning technology second, we describe the main current research on deep learning based recommender systems third, we provide the possible research directions of deep learning based recommender systems in the future finally, concludes this paper [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R133",
                "label": "Artificial Intelligence"
            }
        },
        {
            "instance_id": "R149061xR144074",
            "template_id": "R149061",
            "correct_template_id": null,
            "paper_id": "R144074",
            "premise": "biodiversity inventories with dna barcoding biogeographical region locus (genetics) higher number estimated species higher number estimated species (method) no of estimated species no of estimated species (method) lower number estimated species lower number estimated species (method) class (taxonomy biology) dna sequencing technology phylum (biology) no of potential undescribed species study location order (taxonomy biology) no of samples (sequences) studied taxonomic group (biology) number of identified species with current taxonomy",
            "hypothesis": "mitochondria targeted fluorescent thermometer monitors intracellular temperature gradient a small molecule fluorescent thermometer targeting mitochondria (mito thermo yellow) enables us to monitor the intracellular temperature gradient, generated by exogenous heating in various cells",
            "sequence": "[CLS] biodiversity inventories with dna barcoding biogeographical region locus (genetics) higher number estimated species higher number estimated species (method) no of estimated species no of estimated species (method) lower number estimated species lower number estimated species (method) class (taxonomy biology) dna sequencing technology phylum (biology) no of potential undescribed species study location order (taxonomy biology) no of samples (sequences) studied taxonomic group (biology) number of identified species with current taxonomy [SEP] mitochondria targeted fluorescent thermometer monitors intracellular temperature gradient a small molecule fluorescent thermometer targeting mitochondria (mito thermo yellow) enables us to monitor the intracellular temperature gradient, generated by exogenous heating in various cells [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R130",
                "label": "Physical Chemistry"
            }
        },
        {
            "instance_id": "R178304xR195679",
            "template_id": "R178304",
            "correct_template_id": null,
            "paper_id": "R195679",
            "premise": "dataset inter annotator agreement alternatename assesses creator description disambiguatingdescription encoding exampleofwork genre inlanguage isbasedon license name sameas size sourceorganization url version",
            "hypothesis": "reinforcing security requirements with multifactor quality measurement choosing how to write natural language scenarios is challenging, because stakeholders may over generalize their descriptions or overlook or be unaware of alternate scenarios in security, for example, this can result in weak security constraints that are too general, or missing constraints another challenge is that analysts are unclear on where to stop generating new scenarios in this paper, we introduce the multifactor quality method (mqm) to help requirements analysts to empirically collect system constraints in scenarios based on elicited expert preferences the method combines quantitative statistical analysis to measure system quality with qualitative coding to extract new requirements the method is bootstrapped with minimal analyst expertise in the domain affected by the quality area, and then guides an analyst toward selecting expert recommended requirements to monotonically increase system quality we report the results of applying the method to security this include 550 requirements elicited from 69 security experts during a bootstrapping stage, and subsequent evaluation of these results in a verification stage with 45 security experts to measure the overall improvement of the new requirements security experts in our studies have an average of 10 years of experience our results show that using our method, we detect an increase in the security quality ratings collected in the verification stage finally, we discuss how our proposed method helps to improve security requirements elicitation, analysis, and measurement",
            "sequence": "[CLS] dataset inter annotator agreement alternatename assesses creator description disambiguatingdescription encoding exampleofwork genre inlanguage isbasedon license name sameas size sourceorganization url version [SEP] reinforcing security requirements with multifactor quality measurement choosing how to write natural language scenarios is challenging, because stakeholders may over generalize their descriptions or overlook or be unaware of alternate scenarios in security, for example, this can result in weak security constraints that are too general, or missing constraints another challenge is that analysts are unclear on where to stop generating new scenarios in this paper, we introduce the multifactor quality method (mqm) to help requirements analysts to empirically collect system constraints in scenarios based on elicited expert preferences the method combines quantitative statistical analysis to measure system quality with qualitative coding to extract new requirements the method is bootstrapped with minimal analyst expertise in the domain affected by the quality area, and then guides an analyst toward selecting expert recommended requirements to monotonically increase system quality we report the results of applying the method to security this include 550 requirements elicited from 69 security experts during a bootstrapping stage, and subsequent evaluation of these results in a verification stage with 45 security experts to measure the overall improvement of the new requirements security experts in our studies have an average of 10 years of experience our results show that using our method, we detect an increase in the security quality ratings collected in the verification stage finally, we discuss how our proposed method helps to improve security requirements elicitation, analysis, and measurement [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R149061xR195107",
            "template_id": "R149061",
            "correct_template_id": null,
            "paper_id": "R195107",
            "premise": "biodiversity inventories with dna barcoding biogeographical region locus (genetics) higher number estimated species higher number estimated species (method) no of estimated species no of estimated species (method) lower number estimated species lower number estimated species (method) class (taxonomy biology) dna sequencing technology phylum (biology) no of potential undescribed species study location order (taxonomy biology) no of samples (sequences) studied taxonomic group (biology) number of identified species with current taxonomy",
            "hypothesis": "on systems of systems engineering: a requirements engineering perspective and research agenda \"the emergence of systems of systems (soss) and systems of systems engineering (sose) is largely driven by global societal needs including energy water food nexus, population demographics, global climate, integrated transport, security and social activity however, due to their scale, structural and functional complexity and emergent properties, these global spanning cyber physical systems of systems are becoming increasingly complex and more difficult for current requirements engineering (re) practices to handle in this paper, we firstly introduce sose as an emerging discipline and key characteristics of soss we then highlight the challenges that the re discipline must respond to we discuss some weaknesses of current re techniques and approaches to cope with the complexity of soss we then argue that there is a need for the global re community to evolve current re approaches and to develop new ways of thinking, new re capabilities and possibly a new re science as a key mechanism for addressing requirements engineering complexities posed by systems of systems we then outline a requirements engineering perspective and research agenda that identifies 'top 10' research themes informed by a cluster of four systems of systems engineering projects funded by the european commission's horizon 2020 research programme \"",
            "sequence": "[CLS] biodiversity inventories with dna barcoding biogeographical region locus (genetics) higher number estimated species higher number estimated species (method) no of estimated species no of estimated species (method) lower number estimated species lower number estimated species (method) class (taxonomy biology) dna sequencing technology phylum (biology) no of potential undescribed species study location order (taxonomy biology) no of samples (sequences) studied taxonomic group (biology) number of identified species with current taxonomy [SEP] on systems of systems engineering: a requirements engineering perspective and research agenda \"the emergence of systems of systems (soss) and systems of systems engineering (sose) is largely driven by global societal needs including energy water food nexus, population demographics, global climate, integrated transport, security and social activity however, due to their scale, structural and functional complexity and emergent properties, these global spanning cyber physical systems of systems are becoming increasingly complex and more difficult for current requirements engineering (re) practices to handle in this paper, we firstly introduce sose as an emerging discipline and key characteristics of soss we then highlight the challenges that the re discipline must respond to we discuss some weaknesses of current re techniques and approaches to cope with the complexity of soss we then argue that there is a need for the global re community to evolve current re approaches and to develop new ways of thinking, new re capabilities and possibly a new re science as a key mechanism for addressing requirements engineering complexities posed by systems of systems we then outline a requirements engineering perspective and research agenda that identifies 'top 10' research themes informed by a cluster of four systems of systems engineering projects funded by the european commission's horizon 2020 research programme \" [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R149061xR185335",
            "template_id": "R149061",
            "correct_template_id": null,
            "paper_id": "R185335",
            "premise": "biodiversity inventories with dna barcoding biogeographical region locus (genetics) higher number estimated species higher number estimated species (method) no of estimated species no of estimated species (method) lower number estimated species lower number estimated species (method) class (taxonomy biology) dna sequencing technology phylum (biology) no of potential undescribed species study location order (taxonomy biology) no of samples (sequences) studied taxonomic group (biology) number of identified species with current taxonomy",
            "hypothesis": "ontology learning process as a bottom up strategy for building domain specific ontology from legal texts the objective of this paper is to present the role of ontology learning process in supporting an ontology engineer for creating and maintaining ontologies from textual resources the knowledge structures that interest us are legal domain specific ontologies we will use these ontologies to build legal domain ontology for a lebanese legal knowledge based system the domain application of this work is the lebanese criminal system ontologies can be learnt from various sources, such as databases, structured and unstructured documents here, the focus is on the acquisition of ontologies from unstructured text, provided as input in this work, the ontology learning process represents a knowledge extraction phase using natural language processing techniques the resulted ontology is considered as inexpressive ontology there is a need to reengineer it in order to build a complete, correct and more expressive domain specific ontology",
            "sequence": "[CLS] biodiversity inventories with dna barcoding biogeographical region locus (genetics) higher number estimated species higher number estimated species (method) no of estimated species no of estimated species (method) lower number estimated species lower number estimated species (method) class (taxonomy biology) dna sequencing technology phylum (biology) no of potential undescribed species study location order (taxonomy biology) no of samples (sequences) studied taxonomic group (biology) number of identified species with current taxonomy [SEP] ontology learning process as a bottom up strategy for building domain specific ontology from legal texts the objective of this paper is to present the role of ontology learning process in supporting an ontology engineer for creating and maintaining ontologies from textual resources the knowledge structures that interest us are legal domain specific ontologies we will use these ontologies to build legal domain ontology for a lebanese legal knowledge based system the domain application of this work is the lebanese criminal system ontologies can be learnt from various sources, such as databases, structured and unstructured documents here, the focus is on the acquisition of ontologies from unstructured text, provided as input in this work, the ontology learning process represents a knowledge extraction phase using natural language processing techniques the resulted ontology is considered as inexpressive ontology there is a need to reengineer it in order to build a complete, correct and more expressive domain specific ontology [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R141823",
                "label": "Semantic Web"
            }
        },
        {
            "instance_id": "R198658xR145296",
            "template_id": "R198658",
            "correct_template_id": null,
            "paper_id": "R145296",
            "premise": "nacre mechanics template has method has result research problem has material",
            "hypothesis": "molecular identification of mosquitoes (diptera: culicidae) in southeastern australia abstract dna barcoding is a modern species identification technique that can be used to distinguish morphologically similar species, and is particularly useful when using small amounts of starting material from partial specimens or from immature stages in order to use dna barcoding in a surveillance program, a database containing mosquito barcode sequences is required this study obtained cytochrome oxidase i (coi) sequences for 113 morphologically identified specimens, representing 29 species, six tribes and 12 genera; 17 of these species have not been previously barcoded three of the 29 species \u2500 culex palpalis, macleaya macmillani, and an unknown species originally identified as tripteroides atripes \u2500 were initially misidentified as they are difficult to separate morphologically, highlighting the utility of dna barcoding while most species grouped separately (reciprocally monophyletic), the cx pipiens subgroup could not be genetically separated using coi the average conspecific and congeneric p\u2010distance was 0 8% and 7 6%, respectively in our study, we also demonstrate the utility of dna barcoding in distinguishing exotics from endemic mosquitoes by identifying a single intercepted stegomyia aegypti egg at an international airport the use of dna barcoding dramatically reduced the identification time required compared with rearing specimens through to adults, thereby demonstrating the value of this technique in biosecurity surveillance the dna barcodes produced by this study have been uploaded to the \u2018mosquitoes of australia\u2013victoria\u2019 project on the barcode of life database (bold), which will serve as a resource for the victorian arbovirus disease control program and other national and international mosquito surveillance programs",
            "sequence": "[CLS] nacre mechanics template has method has result research problem has material [SEP] molecular identification of mosquitoes (diptera: culicidae) in southeastern australia abstract dna barcoding is a modern species identification technique that can be used to distinguish morphologically similar species, and is particularly useful when using small amounts of starting material from partial specimens or from immature stages in order to use dna barcoding in a surveillance program, a database containing mosquito barcode sequences is required this study obtained cytochrome oxidase i (coi) sequences for 113 morphologically identified specimens, representing 29 species, six tribes and 12 genera; 17 of these species have not been previously barcoded three of the 29 species \u2500 culex palpalis, macleaya macmillani, and an unknown species originally identified as tripteroides atripes \u2500 were initially misidentified as they are difficult to separate morphologically, highlighting the utility of dna barcoding while most species grouped separately (reciprocally monophyletic), the cx pipiens subgroup could not be genetically separated using coi the average conspecific and congeneric p\u2010distance was 0 8% and 7 6%, respectively in our study, we also demonstrate the utility of dna barcoding in distinguishing exotics from endemic mosquitoes by identifying a single intercepted stegomyia aegypti egg at an international airport the use of dna barcoding dramatically reduced the identification time required compared with rearing specimens through to adults, thereby demonstrating the value of this technique in biosecurity surveillance the dna barcodes produced by this study have been uploaded to the \u2018mosquitoes of australia\u2013victoria\u2019 project on the barcode of life database (bold), which will serve as a resource for the victorian arbovirus disease control program and other national and international mosquito surveillance programs [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            }
        },
        {
            "instance_id": "R198658xR113160",
            "template_id": "R198658",
            "correct_template_id": null,
            "paper_id": "R113160",
            "premise": "nacre mechanics template has method has result research problem has material",
            "hypothesis": "customer rating reactions can be predicted purely using app features in this paper we provide empirical evidence that the rating that an app attracts can be accurately predicted from the features it offers our results, based on an analysis of 11,537 apps from the samsung android and blackberry world app stores, indicate that the rating of 89% of these apps can be predicted with 100% accuracy our prediction model is built by using feature and rating information from the existing apps offered in the app store and it yields highly accurate rating predictions, using only a few (11 12) existing apps for case based prediction these findings may have important implications for requirements engineering in app stores: they indicate that app developers may be able to obtain (very accurate) assessments of the customer reaction to their proposed feature sets (requirements), thereby providing new opportunities to support the requirements elicitation process for app developers",
            "sequence": "[CLS] nacre mechanics template has method has result research problem has material [SEP] customer rating reactions can be predicted purely using app features in this paper we provide empirical evidence that the rating that an app attracts can be accurately predicted from the features it offers our results, based on an analysis of 11,537 apps from the samsung android and blackberry world app stores, indicate that the rating of 89% of these apps can be predicted with 100% accuracy our prediction model is built by using feature and rating information from the existing apps offered in the app store and it yields highly accurate rating predictions, using only a few (11 12) existing apps for case based prediction these findings may have important implications for requirements engineering in app stores: they indicate that app developers may be able to obtain (very accurate) assessments of the customer reaction to their proposed feature sets (requirements), thereby providing new opportunities to support the requirements elicitation process for app developers [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R160259xR156448",
            "template_id": "R160259",
            "correct_template_id": "R152828",
            "paper_id": "R156448",
            "premise": "digital city twin review application area",
            "hypothesis": "soft x ray lasing in neonlike germanium and copper plasmas soft x ray 3p\\\\ensuremath{\\\\rightarrow}3s lasing in neonlike germanium (${\\\\mathrm{ge}}^{22+}$) and copper (${\\\\mathrm{cu}}^{19+}$) in the wavelength interval of 195 to 285 a\\\\r{} is observed for the first time, with gain coefficients ranging from 1 7 to 4 1 ${\\\\mathrm{cm}}^{\\\\mathrm{\\\\ensuremath{ }}1}$, the higher gain with germanium the lasing plasmas are produced by focusing a driving laser beam (\\\\ensuremath{\\\\lambda}=1 05 \\\\ensuremath{\\\\mu}m, 2 ns fwhm) into an 18 mm long line onto thin films and slab targets the measured j=0 to 1 gain coefficients are comparable to those of the j=2 to 1 transitions the measured wavelengths of the six lasing lines compared favorably with recent calculations",
            "sequence": "[CLS] digital city twin review application area [SEP] soft x ray lasing in neonlike germanium and copper plasmas soft x ray 3p\\\\ensuremath{\\\\rightarrow}3s lasing in neonlike germanium (${\\\\mathrm{ge}}^{22+}$) and copper (${\\\\mathrm{cu}}^{19+}$) in the wavelength interval of 195 to 285 a\\\\r{} is observed for the first time, with gain coefficients ranging from 1 7 to 4 1 ${\\\\mathrm{cm}}^{\\\\mathrm{\\\\ensuremath{ }}1}$, the higher gain with germanium the lasing plasmas are produced by focusing a driving laser beam (\\\\ensuremath{\\\\lambda}=1 05 \\\\ensuremath{\\\\mu}m, 2 ns fwhm) into an 18 mm long line onto thin films and slab targets the measured j=0 to 1 gain coefficients are comparable to those of the j=2 to 1 transitions the measured wavelengths of the six lasing lines compared favorably with recent calculations [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            }
        },
        {
            "instance_id": "R172526xR3000",
            "template_id": "R172526",
            "correct_template_id": null,
            "paper_id": "R3000",
            "premise": "video process has study research problem application production",
            "hypothesis": "a model for contextual data sharing in smartphone applications \\n purpose \\n the purpose of this paper is to introduce a model for identifying, storing and sharing contextual information across smartphone apps that uses the native device services the authors present the idea of using user input and interaction within an app as contextual information, and how each app can identify and store contextual information \\n \\n \\n design/methodology/approach \\n contexts are modeled as hierarchical objects that can be stored and shared by applications using native mechanisms a proof of concept implementation of the model for the android platform demonstrates contexts modelled as hierarchical objects stored and shared by applications using native mechanisms \\n \\n \\n findings \\n the model was found to be practically viable by implemented sample apps that share context and through a performance analysis of the system \\n \\n \\n practical implications \\n the contextual data sharing model enables the creation of smart apps and services without being tied to any vendor\u2019s cloud services \\n \\n \\n originality/value \\n this paper introduces a new approach for sharing context in smartphone applications that does not require cloud services \\n",
            "sequence": "[CLS] video process has study research problem application production [SEP] a model for contextual data sharing in smartphone applications \\n purpose \\n the purpose of this paper is to introduce a model for identifying, storing and sharing contextual information across smartphone apps that uses the native device services the authors present the idea of using user input and interaction within an app as contextual information, and how each app can identify and store contextual information \\n \\n \\n design/methodology/approach \\n contexts are modeled as hierarchical objects that can be stored and shared by applications using native mechanisms a proof of concept implementation of the model for the android platform demonstrates contexts modelled as hierarchical objects stored and shared by applications using native mechanisms \\n \\n \\n findings \\n the model was found to be practically viable by implemented sample apps that share context and through a performance analysis of the system \\n \\n \\n practical implications \\n the contextual data sharing model enables the creation of smart apps and services without being tied to any vendor\u2019s cloud services \\n \\n \\n originality/value \\n this paper introduces a new approach for sharing context in smartphone applications that does not require cloud services \\n [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R278",
                "label": "Information Science"
            }
        },
        {
            "instance_id": "R40006xR111072",
            "template_id": "R40006",
            "correct_template_id": null,
            "paper_id": "R111072",
            "premise": "basic reproduction number estimate time period basic reproduction number location",
            "hypothesis": "one step synthesis of \u03b1/\u03b2 cyano aqua cobinamides from vitamin b12 with zn(ii) or cu(ii) salts in methanol this short communication describes the screening of various metal salts for the preparation of cyano aqua cobinamides from vitamin b12 in methanol zncl 2 and cu(no 3 ) 2 \u00b73h 2 o have been identified as most active for this purpose and represent useful alternatives to the widely applied ce(iii) method that requires excess cyanide",
            "sequence": "[CLS] basic reproduction number estimate time period basic reproduction number location [SEP] one step synthesis of \u03b1/\u03b2 cyano aqua cobinamides from vitamin b12 with zn(ii) or cu(ii) salts in methanol this short communication describes the screening of various metal salts for the preparation of cyano aqua cobinamides from vitamin b12 in methanol zncl 2 and cu(no 3 ) 2 \u00b73h 2 o have been identified as most active for this purpose and represent useful alternatives to the widely applied ce(iii) method that requires excess cyanide [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R129",
                "label": "Organic Chemistry"
            }
        },
        {
            "instance_id": "R172526xR76126",
            "template_id": "R172526",
            "correct_template_id": null,
            "paper_id": "R76126",
            "premise": "video process has study research problem application production",
            "hypothesis": "crowd centric requirements engineering requirements engineering is a preliminary and crucial phase for the correctness and quality of software systems despite the agreement on the positive correlation between user involvement in requirements engineering and software success, current development methods employ a too narrow concept of that user and rely on a recruited set of users considered to be representative such approaches might not cater for the diversity and dynamism of the actual users and the context of software usage this is especially true in new paradigms such as cloud and mobile computing to overcome these limitations, we propose crowd centric requirements engineering (ccre) as a revised method for requirements engineering where users become primary contributors, resulting in higher quality requirements and increased user satisfaction ccre relies on crowd sourcing to support a broader user involvement, and on gamification to motivate that voluntary involvement",
            "sequence": "[CLS] video process has study research problem application production [SEP] crowd centric requirements engineering requirements engineering is a preliminary and crucial phase for the correctness and quality of software systems despite the agreement on the positive correlation between user involvement in requirements engineering and software success, current development methods employ a too narrow concept of that user and rely on a recruited set of users considered to be representative such approaches might not cater for the diversity and dynamism of the actual users and the context of software usage this is especially true in new paradigms such as cloud and mobile computing to overcome these limitations, we propose crowd centric requirements engineering (ccre) as a revised method for requirements engineering where users become primary contributors, resulting in higher quality requirements and increased user satisfaction ccre relies on crowd sourcing to support a broader user involvement, and on gamification to motivate that voluntary involvement [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R198658xR148663",
            "template_id": "R198658",
            "correct_template_id": "R146876",
            "paper_id": "R148663",
            "premise": "nacre mechanics template has method has result research problem has material",
            "hypothesis": "dithienopicenocarbazole based acceptors for efficient organic solar cells with optoelectronic response over 1000 nm and an extremely low energy loss two cheliform non fullerene acceptors, dtpc ic and dtpc dfic, based on a highly electron rich core, dithienopicenocarbazole (dtpc), are synthesized, showing ultra narrow bandgaps (as low as 1 21 ev) the two dimensional nitrogen containing conjugated dtpc possesses strong electron donating capability, which induces intense intramolecular charge transfer and intermolecular \u03c0 \u03c0 stacking in derived acceptors the solar cell based on dtpc dfic and a spectrally complementary polymer donor, ptb7 th, showed a high power conversion efficiency of 10 21% and an extremely low energy loss of 0 45 ev, which is the lowest among reported efficient oscs",
            "sequence": "[CLS] nacre mechanics template has method has result research problem has material [SEP] dithienopicenocarbazole based acceptors for efficient organic solar cells with optoelectronic response over 1000 nm and an extremely low energy loss two cheliform non fullerene acceptors, dtpc ic and dtpc dfic, based on a highly electron rich core, dithienopicenocarbazole (dtpc), are synthesized, showing ultra narrow bandgaps (as low as 1 21 ev) the two dimensional nitrogen containing conjugated dtpc possesses strong electron donating capability, which induces intense intramolecular charge transfer and intermolecular \u03c0 \u03c0 stacking in derived acceptors the solar cell based on dtpc dfic and a spectrally complementary polymer donor, ptb7 th, showed a high power conversion efficiency of 10 21% and an extremely low energy loss of 0 45 ev, which is the lowest among reported efficient oscs [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            }
        },
        {
            "instance_id": "R186491xR160390",
            "template_id": "R186491",
            "correct_template_id": null,
            "paper_id": "R160390",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "collaborative city digital twin for the covid 19 pandemic: a federated learning solution \"in this work, we propose a collaborative city digital twin based on fl, a novel paradigm that allowing multiple city dt to share the local strategy and status in a timely manner in particular, an fl central server manages the local updates of multiple collaborators (city dt), provides a global model which is trained in multiple iterations at different city dt systems, until the model gains the correlations between various response plan and infection trend that means, a collaborative city dt paradigm based on fl techniques can obtain knowledge and patterns from multiple dts, and eventually establish a `global view' for city crisis management meanwhile, it also helps to improve each city digital twin selves by consolidating other dt's respective data without violating privacy rules to validate the proposed solution, we take covid 19 pandemic as a case study the experimental results on the real dataset with various response plan validate our proposed solution and demonstrate the superior performance \"",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] collaborative city digital twin for the covid 19 pandemic: a federated learning solution \"in this work, we propose a collaborative city digital twin based on fl, a novel paradigm that allowing multiple city dt to share the local strategy and status in a timely manner in particular, an fl central server manages the local updates of multiple collaborators (city dt), provides a global model which is trained in multiple iterations at different city dt systems, until the model gains the correlations between various response plan and infection trend that means, a collaborative city dt paradigm based on fl techniques can obtain knowledge and patterns from multiple dts, and eventually establish a `global view' for city crisis management meanwhile, it also helps to improve each city digital twin selves by consolidating other dt's respective data without violating privacy rules to validate the proposed solution, we take covid 19 pandemic as a case study the experimental results on the real dataset with various response plan validate our proposed solution and demonstrate the superior performance \" [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            }
        },
        {
            "instance_id": "R172526xR144081",
            "template_id": "R172526",
            "correct_template_id": null,
            "paper_id": "R144081",
            "premise": "video process has study research problem application production",
            "hypothesis": "a soluble cryogenic thermometer with high sensitivity based on excited state configuration transformations cryogenic temperature detection plays an irreplaceable role in exploring nature developing high sensitivity, accurate, observable and convenient measurements of cryogenic temperature is not only a challenge but also an opportunity for the thermometer field the small molecule 9 (9,9 dimethyl 9h fluoren 3yl) 14 phenyl 9,14 dihydrodibenzo[a,c]phenazine (fipac) in 2 methyl tetrahydrofuran (methf) solution is utilized for the detection of cryogenic temperature with a wide range from 138 k to 343 k this system possesses significantly high sensitivity at low temperature, which reaches as high as 19 4% k( 1) at 138 k the temperature dependent ratio of the dual emission intensity can be fitted as a single exponential curve as a function of temperature this single exponential curve can be explained by the mechanism that the dual emission feature of fipac results from the excited state configuration transformations upon heating or cooling, which is very different from the previously reported mechanisms here, our work gives an overall interpretation for this mechanism therefore, application of fipac as a cryogenic thermometer is experimentally and theoretically feasible",
            "sequence": "[CLS] video process has study research problem application production [SEP] a soluble cryogenic thermometer with high sensitivity based on excited state configuration transformations cryogenic temperature detection plays an irreplaceable role in exploring nature developing high sensitivity, accurate, observable and convenient measurements of cryogenic temperature is not only a challenge but also an opportunity for the thermometer field the small molecule 9 (9,9 dimethyl 9h fluoren 3yl) 14 phenyl 9,14 dihydrodibenzo[a,c]phenazine (fipac) in 2 methyl tetrahydrofuran (methf) solution is utilized for the detection of cryogenic temperature with a wide range from 138 k to 343 k this system possesses significantly high sensitivity at low temperature, which reaches as high as 19 4% k( 1) at 138 k the temperature dependent ratio of the dual emission intensity can be fitted as a single exponential curve as a function of temperature this single exponential curve can be explained by the mechanism that the dual emission feature of fipac results from the excited state configuration transformations upon heating or cooling, which is very different from the previously reported mechanisms here, our work gives an overall interpretation for this mechanism therefore, application of fipac as a cryogenic thermometer is experimentally and theoretically feasible [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R130",
                "label": "Physical Chemistry"
            }
        },
        {
            "instance_id": "R138077xR25068",
            "template_id": "R138077",
            "correct_template_id": null,
            "paper_id": "R25068",
            "premise": "ontology learning from data sources has dataset application domain input format output format accuracy f1 score precision knowledge source has data source learning purpose terms learning axiom learning properties learning properties hierarchy learning rule learning class hierarchy learning learning method learning tool evaluation metrics related work validation tool training corpus testing corpus class learning implemented technologies relationships learning instance learning taxonomy learning validation comment assessment of acquired knowledge recall f measure",
            "hypothesis": "our twitter profiles, our selves: predicting personality with twitter \"psychological personality has been shown to affect a variety of aspects: preferences for interaction styles in the digital world and for music genres, for example consequently, the design of personalized user interfaces and music recommender systems might benefit from understanding the relationship between personality and use of social media since there has not been a study between personality and use of twitter at large, we set out to analyze the relationship between personality and different types of twitter users, including popular users and influentials for 335 users, we gather personality data, analyze it, and find that both popular users and influentials are extroverts and emotionally stable (low in the trait of neuroticism) interestingly, we also find that popular users are `imaginative' (high in openness), while influentials tend to be `organized' (high in conscientiousness) we then show a way of accurately predicting a user's personality simply based on three counts publicly available on profiles: following, followers, and listed counts knowing these three quantities about an active user, one can predict the user's five personality traits with a root mean squared error below 0 88 on a $[1,5]$ scale based on these promising results, we argue that being able to predict user personality goes well beyond our initial goal of informing the design of new personalized applications as it, for example, expands current studies on privacy in social media \"",
            "sequence": "[CLS] ontology learning from data sources has dataset application domain input format output format accuracy f1 score precision knowledge source has data source learning purpose terms learning axiom learning properties learning properties hierarchy learning rule learning class hierarchy learning learning method learning tool evaluation metrics related work validation tool training corpus testing corpus class learning implemented technologies relationships learning instance learning taxonomy learning validation comment assessment of acquired knowledge recall f measure [SEP] our twitter profiles, our selves: predicting personality with twitter \"psychological personality has been shown to affect a variety of aspects: preferences for interaction styles in the digital world and for music genres, for example consequently, the design of personalized user interfaces and music recommender systems might benefit from understanding the relationship between personality and use of social media since there has not been a study between personality and use of twitter at large, we set out to analyze the relationship between personality and different types of twitter users, including popular users and influentials for 335 users, we gather personality data, analyze it, and find that both popular users and influentials are extroverts and emotionally stable (low in the trait of neuroticism) interestingly, we also find that popular users are `imaginative' (high in openness), while influentials tend to be `organized' (high in conscientiousness) we then show a way of accurately predicting a user's personality simply based on three counts publicly available on profiles: following, followers, and listed counts knowing these three quantities about an active user, one can predict the user's five personality traits with a root mean squared error below 0 88 on a $[1,5]$ scale based on these promising results, we argue that being able to predict user personality goes well beyond our initial goal of informing the design of new personalized applications as it, for example, expands current studies on privacy in social media \" [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R11",
                "label": "Science"
            }
        },
        {
            "instance_id": "R155844xR138562",
            "template_id": "R155844",
            "correct_template_id": null,
            "paper_id": "R138562",
            "premise": "photocatalysts wavelength of maximum absorption energy band gap photocatalyst wavelength of maximum emission emission lifetime ground state oxidation potential ground state reduction potential excited state oxidation potential excited state reduction potential",
            "hypothesis": "fast census of moth diversity in the neotropics: a comparison of field assigned morphospecies and dna barcoding in tiger moths the morphological species delimitations (i e morphospecies) have long been the best way to avoid the taxonomic impediment and compare insect taxa biodiversity in highly diverse tropical and subtropical regions the development of dna barcoding, however, has shown great potential to replace (or at least complement) the morphospecies approach, with the advantage of relying on automated methods implemented in computer programs or even online rather than in often subjective morphological features we sampled moths extensively for two years using light traps in a patch of the highly endangered atlantic forest of brazil to produce a nearly complete census of arctiines (noctuoidea: erebidae), whose species richness was compared using different morphological and molecular approaches (dna barcoding) a total of 1,075 barcode sequences of 286 morphospecies were analyzed based on the clustering method barcode index number (bin) we found a taxonomic bias of approximately 30% in our initial morphological assessment however, a morphological reassessment revealed that the correspondence between morphospecies and molecular operational taxonomic units (motus) can be up to 94% if differences in genitalia morphology are evaluated in individuals of different motus originated from the same morphospecies (putative cases of cryptic species), and by recording if individuals of different genders in different morphospecies merge together in the same motu (putative cases of sexual dimorphism) the results of two other clustering methods (i e automatic barcode gap discovery and 2% threshold) were very similar to those of the bin approach using empirical data we have shown that dna barcoding performed substantially better than the morphospecies approach, based on superficial morphology, to delimit species of a highly diverse moth taxon, and thus should be used in species inventories",
            "sequence": "[CLS] photocatalysts wavelength of maximum absorption energy band gap photocatalyst wavelength of maximum emission emission lifetime ground state oxidation potential ground state reduction potential excited state oxidation potential excited state reduction potential [SEP] fast census of moth diversity in the neotropics: a comparison of field assigned morphospecies and dna barcoding in tiger moths the morphological species delimitations (i e morphospecies) have long been the best way to avoid the taxonomic impediment and compare insect taxa biodiversity in highly diverse tropical and subtropical regions the development of dna barcoding, however, has shown great potential to replace (or at least complement) the morphospecies approach, with the advantage of relying on automated methods implemented in computer programs or even online rather than in often subjective morphological features we sampled moths extensively for two years using light traps in a patch of the highly endangered atlantic forest of brazil to produce a nearly complete census of arctiines (noctuoidea: erebidae), whose species richness was compared using different morphological and molecular approaches (dna barcoding) a total of 1,075 barcode sequences of 286 morphospecies were analyzed based on the clustering method barcode index number (bin) we found a taxonomic bias of approximately 30% in our initial morphological assessment however, a morphological reassessment revealed that the correspondence between morphospecies and molecular operational taxonomic units (motus) can be up to 94% if differences in genitalia morphology are evaluated in individuals of different motus originated from the same morphospecies (putative cases of cryptic species), and by recording if individuals of different genders in different morphospecies merge together in the same motu (putative cases of sexual dimorphism) the results of two other clustering methods (i e automatic barcode gap discovery and 2% threshold) were very similar to those of the bin approach using empirical data we have shown that dna barcoding performed substantially better than the morphospecies approach, based on superficial morphology, to delimit species of a highly diverse moth taxon, and thus should be used in species inventories [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            }
        },
        {
            "instance_id": "R172526xR145468",
            "template_id": "R172526",
            "correct_template_id": null,
            "paper_id": "R145468",
            "premise": "video process has study research problem application production",
            "hypothesis": "dna barcoding of neotropical black flies (diptera: simuliidae): species identification and discovery of cryptic diversity in mesoamerica although correct taxonomy is paramount for disease control programs and epidemiological studies, morphology based taxonomy of black flies is extremely difficult in the present study, the utility of a partial sequence of the coi gene, the dna barcoding region, for the identification of species of black flies from mesoamerica was assessed a total of 32 morphospecies were analyzed, one belonging to the genus gigantodax and 31 species to the genus simulium and six of its subgenera (aspathia, eusimulium, notolepria, psaroniocompsa, psilopelmia, trichodagmia) the neighbour joining tree (nj) derived from the dna barcodes grouped most specimens according to species or species groups recognized by morphotaxonomic studies intraspecific sequence divergences within morphologically distinct species ranged from 0 07% to 1 65%, while higher divergences (2 05% 6 13%) in species complexes suggested the presence of cryptic diversity the existence of well defined groups within s callidum (dyar & shannon), s quadrivittatum loew, and s samboni jennings revealed the likely inclusion of cryptic species within these taxa in addition, the suspected presence of sibling species within s paynei vargas and s tarsatum macquart was supported dna barcodes also showed that specimens of species that are difficult to delimit morphologically such as s callidum, s pseudocallidum d\u00edaz n\u00e1jera, s travisi vargas, vargas & ram\u00edrez p\u00e9rez, relatives of the species complexes such as s metallicum bellardi s l (e g , s horacioi okazawa & onishi, s jobbinsi vargas, mart\u00ednez palacios, d\u00edaz n\u00e1jera, and s puigi vargas, mart\u00ednez palacios & d\u00edaz n\u00e1jera), and s virgatum coquillett complex (e g , s paynei and s tarsatum) grouped together in the nj analysis, suggesting they represent valid species dna barcoding combined with a sound morphotaxonomic framework provided an effective approach for the identification of medically important black flies species in mesoamerica and for the discovery of hidden diversity within this group",
            "sequence": "[CLS] video process has study research problem application production [SEP] dna barcoding of neotropical black flies (diptera: simuliidae): species identification and discovery of cryptic diversity in mesoamerica although correct taxonomy is paramount for disease control programs and epidemiological studies, morphology based taxonomy of black flies is extremely difficult in the present study, the utility of a partial sequence of the coi gene, the dna barcoding region, for the identification of species of black flies from mesoamerica was assessed a total of 32 morphospecies were analyzed, one belonging to the genus gigantodax and 31 species to the genus simulium and six of its subgenera (aspathia, eusimulium, notolepria, psaroniocompsa, psilopelmia, trichodagmia) the neighbour joining tree (nj) derived from the dna barcodes grouped most specimens according to species or species groups recognized by morphotaxonomic studies intraspecific sequence divergences within morphologically distinct species ranged from 0 07% to 1 65%, while higher divergences (2 05% 6 13%) in species complexes suggested the presence of cryptic diversity the existence of well defined groups within s callidum (dyar & shannon), s quadrivittatum loew, and s samboni jennings revealed the likely inclusion of cryptic species within these taxa in addition, the suspected presence of sibling species within s paynei vargas and s tarsatum macquart was supported dna barcodes also showed that specimens of species that are difficult to delimit morphologically such as s callidum, s pseudocallidum d\u00edaz n\u00e1jera, s travisi vargas, vargas & ram\u00edrez p\u00e9rez, relatives of the species complexes such as s metallicum bellardi s l (e g , s horacioi okazawa & onishi, s jobbinsi vargas, mart\u00ednez palacios, d\u00edaz n\u00e1jera, and s puigi vargas, mart\u00ednez palacios & d\u00edaz n\u00e1jera), and s virgatum coquillett complex (e g , s paynei and s tarsatum) grouped together in the nj analysis, suggesting they represent valid species dna barcoding combined with a sound morphotaxonomic framework provided an effective approach for the identification of medically important black flies species in mesoamerica and for the discovery of hidden diversity within this group [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            }
        },
        {
            "instance_id": "R160259xR196046",
            "template_id": "R160259",
            "correct_template_id": "R186491",
            "paper_id": "R196046",
            "premise": "digital city twin review application area",
            "hypothesis": "task interruptions in requirements engineering: reality versus perceptions! \"task switching and interruptions are a daily reality in software development projects: developers switch between requirements engineering (re), coding, testing, daily meetings, and other tasks task switching may increase productivity through increased information flow and effective time management however, it might also cause a cognitive load to reorient the primary task, which accounts for the decrease in developers' productivity and increases in errors this cognitive load is even greater in cases of cognitively demanding tasks as the ones typical for re activities in this paper, to compare the reality of task switching in re with the perception of developers, we conducted two studies: (i) a case study analysis on 5,076 recorded tasks of 19 developers and (ii) a survey of 25 developers the results of our retrospective analysis show that in all of the cases that the disruptiveness of re interruptions is statistically different from other software development tasks, re related tasks are more vulnerable to interruptions compared to other task types moreover, we found that context switching, the priority of the interrupting task, and the interruption source and timing are key factors that impact re interruptions we also provided a set of re task switching patterns along with recommendations for both practitioners and researchers while the results of our retrospective analysis show that self interruptions are more disruptive than external interruptions, developers have different perceptions about the disruptiveness of various sources of interruptions \"",
            "sequence": "[CLS] digital city twin review application area [SEP] task interruptions in requirements engineering: reality versus perceptions! \"task switching and interruptions are a daily reality in software development projects: developers switch between requirements engineering (re), coding, testing, daily meetings, and other tasks task switching may increase productivity through increased information flow and effective time management however, it might also cause a cognitive load to reorient the primary task, which accounts for the decrease in developers' productivity and increases in errors this cognitive load is even greater in cases of cognitively demanding tasks as the ones typical for re activities in this paper, to compare the reality of task switching in re with the perception of developers, we conducted two studies: (i) a case study analysis on 5,076 recorded tasks of 19 developers and (ii) a survey of 25 developers the results of our retrospective analysis show that in all of the cases that the disruptiveness of re interruptions is statistically different from other software development tasks, re related tasks are more vulnerable to interruptions compared to other task types moreover, we found that context switching, the priority of the interrupting task, and the interruption source and timing are key factors that impact re interruptions we also provided a set of re task switching patterns along with recommendations for both practitioners and researchers while the results of our retrospective analysis show that self interruptions are more disruptive than external interruptions, developers have different perceptions about the disruptiveness of various sources of interruptions \" [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R155844xR142471",
            "template_id": "R155844",
            "correct_template_id": null,
            "paper_id": "R142471",
            "premise": "photocatalysts wavelength of maximum absorption energy band gap photocatalyst wavelength of maximum emission emission lifetime ground state oxidation potential ground state reduction potential excited state oxidation potential excited state reduction potential",
            "hypothesis": "dna barcoding of northern nearctic muscidae (diptera) reveals high correspondence between morphological and molecular species limits abstract \\n \\n background \\n various methods have been proposed to assign unknown specimens to known species using their dna barcodes, while others have focused on using genetic divergence thresholds to estimate \u201cspecies\u201d diversity for a taxon, without a well developed taxonomy and/or an extensive reference library of dna barcodes the major goals of the present work were to: a) conduct the largest species level barcoding study of the muscidae to date and characterize the range of genetic divergence values in the northern nearctic fauna; b) evaluate the correspondence between morphospecies and barcode groupings defined using both clustering based and threshold based approaches; and c) use the reference library produced to address taxonomic issues \\n \\n \\n results \\n our data set included 1114 individuals and their coi sequences (951 from churchill, manitoba), representing 160 morphologically determined species from 25 genera, covering 89% of the known fauna of churchill and 23% of the nearctic fauna following an iterative process through which all specimens belonging to taxa with anomalous divergence values and/or monophyly issues were re examined, identity was modified for 9 taxa, including the reinstatement of phaonia luteva (walker) stat nov as a species distinct from phaonia errans (meigen) in the post reassessment data set, no distinct gap was found between maximum pairwise intraspecific distances (range 0 00 3 01%) and minimum interspecific distances (range: 0 77 11 33%) nevertheless, using a clustering based approach, all individuals within 98% of species grouped with their conspecifics with high (&gt;95%) bootstrap support; in contrast, a maximum species discrimination rate of 90% was obtained at the optimal threshold of 1 2% dna barcoding enabled the determination of females from 5 ambiguous species pairs and confirmed that 16 morphospecies were genetically distinct from named taxa there were morphological differences among all distinct genetic clusters; thus, no cases of cryptic species were detected \\n \\n \\n conclusions \\n our findings reveal the great utility of building a well populated, species level reference barcode database against which to compare unknowns when such a library is unavailable, it is still possible to obtain a fairly accurate (within ~10%) rapid assessment of species richness based upon a barcode divergence threshold alone, but this approach is most accurate when the threshold is tuned to a particular taxon \\n",
            "sequence": "[CLS] photocatalysts wavelength of maximum absorption energy band gap photocatalyst wavelength of maximum emission emission lifetime ground state oxidation potential ground state reduction potential excited state oxidation potential excited state reduction potential [SEP] dna barcoding of northern nearctic muscidae (diptera) reveals high correspondence between morphological and molecular species limits abstract \\n \\n background \\n various methods have been proposed to assign unknown specimens to known species using their dna barcodes, while others have focused on using genetic divergence thresholds to estimate \u201cspecies\u201d diversity for a taxon, without a well developed taxonomy and/or an extensive reference library of dna barcodes the major goals of the present work were to: a) conduct the largest species level barcoding study of the muscidae to date and characterize the range of genetic divergence values in the northern nearctic fauna; b) evaluate the correspondence between morphospecies and barcode groupings defined using both clustering based and threshold based approaches; and c) use the reference library produced to address taxonomic issues \\n \\n \\n results \\n our data set included 1114 individuals and their coi sequences (951 from churchill, manitoba), representing 160 morphologically determined species from 25 genera, covering 89% of the known fauna of churchill and 23% of the nearctic fauna following an iterative process through which all specimens belonging to taxa with anomalous divergence values and/or monophyly issues were re examined, identity was modified for 9 taxa, including the reinstatement of phaonia luteva (walker) stat nov as a species distinct from phaonia errans (meigen) in the post reassessment data set, no distinct gap was found between maximum pairwise intraspecific distances (range 0 00 3 01%) and minimum interspecific distances (range: 0 77 11 33%) nevertheless, using a clustering based approach, all individuals within 98% of species grouped with their conspecifics with high (&gt;95%) bootstrap support; in contrast, a maximum species discrimination rate of 90% was obtained at the optimal threshold of 1 2% dna barcoding enabled the determination of females from 5 ambiguous species pairs and confirmed that 16 morphospecies were genetically distinct from named taxa there were morphological differences among all distinct genetic clusters; thus, no cases of cryptic species were detected \\n \\n \\n conclusions \\n our findings reveal the great utility of building a well populated, species level reference barcode database against which to compare unknowns when such a library is unavailable, it is still possible to obtain a fairly accurate (within ~10%) rapid assessment of species richness based upon a barcode divergence threshold alone, but this approach is most accurate when the threshold is tuned to a particular taxon \\n [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            }
        },
        {
            "instance_id": "R159441xR139071",
            "template_id": "R159441",
            "correct_template_id": null,
            "paper_id": "R139071",
            "premise": "city digital twin potentials visualization situational awareness planning and prediction integration and collaboration data management",
            "hypothesis": "on the vacuum ultraviolet radiation of a miniaturized non thermal atmospheric pressure plasma jet the suitability of a miniaturized non thermal appj operating with ar at ambient atmosphere for applications related to surface treatment is demonstrated the vuv emission is measured and the dependence of selected line intensities over the radius of the plasma jet is presented the ar discharge is characterized by an intense vuv radiation, attributed to n, h, and o atomic lines along with an ar2* excimer continuum, which is drastically reduced after adding up to 5% n2 to the ar working gas two absorption dips are found in the vuv spectrum the surface energy enhancement of substrates at temperatures as low as 35\\u2009\u00b0c along with chemical reactivity originating from abundant no and oh free radicals and uv/vuv radiation in the plasma give rise to numerous applications, e g , in the medical and biological field",
            "sequence": "[CLS] city digital twin potentials visualization situational awareness planning and prediction integration and collaboration data management [SEP] on the vacuum ultraviolet radiation of a miniaturized non thermal atmospheric pressure plasma jet the suitability of a miniaturized non thermal appj operating with ar at ambient atmosphere for applications related to surface treatment is demonstrated the vuv emission is measured and the dependence of selected line intensities over the radius of the plasma jet is presented the ar discharge is characterized by an intense vuv radiation, attributed to n, h, and o atomic lines along with an ar2* excimer continuum, which is drastically reduced after adding up to 5% n2 to the ar working gas two absorption dips are found in the vuv spectrum the surface energy enhancement of substrates at temperatures as low as 35\\u2009\u00b0c along with chemical reactivity originating from abundant no and oh free radicals and uv/vuv radiation in the plasma give rise to numerous applications, e g , in the medical and biological field [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            }
        },
        {
            "instance_id": "R186491xR156187",
            "template_id": "R186491",
            "correct_template_id": "R155844",
            "paper_id": "R156187",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "enhanced luminescent iridium(iii) complexes bearing aryltriazole cyclometallated ligands herein we report the synthesis of 4 aryl 1 benzyl 1h 1,2,3 triazoles (atl), made via \"click chemistry\" and their incorporation as cyclometallating ligands into new heteroleptic iridium(iii) complexes containing diimine (n(^)n) ancillary ligands 2,2\\' bipyridine (bpy) and 4,4\\' di tert butyl 2,2\\' bipyridine (dtbubpy) depending on decoration, these complexes emit from the yellow to sky blue in acetonitrile (acn) solution at room temperature (rt) their emission energies are slightly blue shifted and their photoluminescent quantum efficiencies are markedly higher (between 25 and 80%) than analogous (c(^)n)(2)ir(n(^)n)(+) type complexes, where c(^)n is a decorated 2 phenylpyridinato ligand this increased brilliance is in part due to the presence of the benzyl groups, which act to sterically shield the iridium metal center x ray crystallographic analyses of two of the atl complexes corroborate this assertion their electrochemistry is reversible, thus making these complexes amenable for inclusion in light emitting electrochemical cells (leecs) a parallel computational investigation supports the experimental findings and demonstrates that for all complexes included in this study, the highest occupied molecular orbital (homo) is located on both the aryl fragment of the atl ligands and the iridium metal while the lowest unoccupied molecular orbital (lumo) is located essentially exclusively on the ancillary ligand",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] enhanced luminescent iridium(iii) complexes bearing aryltriazole cyclometallated ligands herein we report the synthesis of 4 aryl 1 benzyl 1h 1,2,3 triazoles (atl), made via \"click chemistry\" and their incorporation as cyclometallating ligands into new heteroleptic iridium(iii) complexes containing diimine (n(^)n) ancillary ligands 2,2\\' bipyridine (bpy) and 4,4\\' di tert butyl 2,2\\' bipyridine (dtbubpy) depending on decoration, these complexes emit from the yellow to sky blue in acetonitrile (acn) solution at room temperature (rt) their emission energies are slightly blue shifted and their photoluminescent quantum efficiencies are markedly higher (between 25 and 80%) than analogous (c(^)n)(2)ir(n(^)n)(+) type complexes, where c(^)n is a decorated 2 phenylpyridinato ligand this increased brilliance is in part due to the presence of the benzyl groups, which act to sterically shield the iridium metal center x ray crystallographic analyses of two of the atl complexes corroborate this assertion their electrochemistry is reversible, thus making these complexes amenable for inclusion in light emitting electrochemical cells (leecs) a parallel computational investigation supports the experimental findings and demonstrates that for all complexes included in this study, the highest occupied molecular orbital (homo) is located on both the aryl fragment of the atl ligands and the iridium metal while the lowest unoccupied molecular orbital (lumo) is located essentially exclusively on the ancillary ligand [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R130",
                "label": "Physical Chemistry"
            }
        },
        {
            "instance_id": "R40006xR139115",
            "template_id": "R40006",
            "correct_template_id": null,
            "paper_id": "R139115",
            "premise": "basic reproduction number estimate time period basic reproduction number location",
            "hypothesis": "chemical kinetics in an atmospheric pressure helium plasma containing humidity investigating the formation and kinetics of o and oh in a he\u2013h 2 o plasma jet using absorption spectroscopy and 0d modelling",
            "sequence": "[CLS] basic reproduction number estimate time period basic reproduction number location [SEP] chemical kinetics in an atmospheric pressure helium plasma containing humidity investigating the formation and kinetics of o and oh in a he\u2013h 2 o plasma jet using absorption spectroscopy and 0d modelling [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            }
        },
        {
            "instance_id": "R186491xR163050",
            "template_id": "R186491",
            "correct_template_id": "R178304",
            "paper_id": "R163050",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "named entity recognition in wikipedia \"named entity recognition (ner) is used in many domains beyond the newswire text that comprises current gold standard corpora recent work has used wikipedia's link structure to automatically generate near gold standard annotations until now, these resources have only been evaluated on newswire corpora or themselves \\n \\nwe present the first ner evaluation on a wikipedia gold standard (wg) corpus our analysis of cross corpus performance on wg shows that wikipedia text may be a harder ner domain than newswire we find that an automatic annotation of wikipedia has high agreement with wg and, when used as training data, outperforms newswire models by up to 7 7% \"",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] named entity recognition in wikipedia \"named entity recognition (ner) is used in many domains beyond the newswire text that comprises current gold standard corpora recent work has used wikipedia's link structure to automatically generate near gold standard annotations until now, these resources have only been evaluated on newswire corpora or themselves \\n \\nwe present the first ner evaluation on a wikipedia gold standard (wg) corpus our analysis of cross corpus performance on wg shows that wikipedia text may be a harder ner domain than newswire we find that an automatic annotation of wikipedia has high agreement with wg and, when used as training data, outperforms newswire models by up to 7 7% \" [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            }
        },
        {
            "instance_id": "R159441xR76341",
            "template_id": "R159441",
            "correct_template_id": null,
            "paper_id": "R76341",
            "premise": "city digital twin potentials visualization situational awareness planning and prediction integration and collaboration data management",
            "hypothesis": "the crowd in requirements engineering: the landscape and challenges crowd based requirements engineering (crowdre) could significantly change re performing re activities such as elicitation with the crowd of stakeholders turns re into a participatory effort, leads to more accurate requirements, and ultimately boosts software quality although any stakeholder in the crowd can contribute, crowdre emphasizes one stakeholder group whose role is often trivialized: users crowdre empowers the management of requirements, such as their prioritization and segmentation, in a dynamic, evolved style through collecting and harnessing a continuous flow of user feedback and monitoring data on the usage context to analyze the large amount of data obtained from the crowd, automated approaches are key this article presents current research topics in crowdre; discusses the benefits, challenges, and lessons learned from projects and experiments; and assesses how to apply the methods and tools in industrial contexts this article is part of a special issue on crowdsourcing for software engineering",
            "sequence": "[CLS] city digital twin potentials visualization situational awareness planning and prediction integration and collaboration data management [SEP] the crowd in requirements engineering: the landscape and challenges crowd based requirements engineering (crowdre) could significantly change re performing re activities such as elicitation with the crowd of stakeholders turns re into a participatory effort, leads to more accurate requirements, and ultimately boosts software quality although any stakeholder in the crowd can contribute, crowdre emphasizes one stakeholder group whose role is often trivialized: users crowdre empowers the management of requirements, such as their prioritization and segmentation, in a dynamic, evolved style through collecting and harnessing a continuous flow of user feedback and monitoring data on the usage context to analyze the large amount of data obtained from the crowd, automated approaches are key this article presents current research topics in crowdre; discusses the benefits, challenges, and lessons learned from projects and experiments; and assesses how to apply the methods and tools in industrial contexts this article is part of a special issue on crowdsourcing for software engineering [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R159441xR160377",
            "template_id": "R159441",
            "correct_template_id": null,
            "paper_id": "R160377",
            "premise": "city digital twin potentials visualization situational awareness planning and prediction integration and collaboration data management",
            "hypothesis": "time series behavior modeling with digital twin for internet of vehicles abstract electric vehicle (ev) is considered eco friendly with low carbon emission and maintenance costs given the current battery and charging technology, driving experience of evs relies heavily on the availability and reachability of ev charging infrastructure as the number of charging piles increases, carefully designed arrangement of resources and efficient utilization of the infrastructure is essential to the future development of ev industry the mobility and distribution of evs determine the charging demand and the load of power distribution grid then, dynamic traffic pattern of numerous interconnected evs poses great impact on charging plans and charging infrastructure in this paper, we introduce the digital twin of a real world ev by modeling the mobility based on a time series behaviors of evs to evaluate the charging algorithm and pile arrangement policy the introduced digital twin ev is a virtually simulated equivalence with same traffic behaviors and charging activities as the ev in real world the behavior and route choice of evs is dynamically simulated base on the time varying driving operations, travel intent, and charging plan in a simulated large scale charging scenario composed of concurrently moving evs and correspondingly equipped charging piles different ev navigation algorithms and charging algorithms of internet of vehicle can be exactly evaluated in the dynamic simulation of the digital twins of the moving evs and charging infrastructure then we analyze the collected data such as energy consumption, charging capacity, charging frequency, and waiting time in queue on both the ev side and the charging pile side to evaluate the charging efficiency the simulation is used to study the relations between the scheduled charging operation of evs and the deployment of piles the proposed model helps evaluate and validate the design of the charging recommendation and the deployment plan regarding to the arrangement and distribution of charging piles",
            "sequence": "[CLS] city digital twin potentials visualization situational awareness planning and prediction integration and collaboration data management [SEP] time series behavior modeling with digital twin for internet of vehicles abstract electric vehicle (ev) is considered eco friendly with low carbon emission and maintenance costs given the current battery and charging technology, driving experience of evs relies heavily on the availability and reachability of ev charging infrastructure as the number of charging piles increases, carefully designed arrangement of resources and efficient utilization of the infrastructure is essential to the future development of ev industry the mobility and distribution of evs determine the charging demand and the load of power distribution grid then, dynamic traffic pattern of numerous interconnected evs poses great impact on charging plans and charging infrastructure in this paper, we introduce the digital twin of a real world ev by modeling the mobility based on a time series behaviors of evs to evaluate the charging algorithm and pile arrangement policy the introduced digital twin ev is a virtually simulated equivalence with same traffic behaviors and charging activities as the ev in real world the behavior and route choice of evs is dynamically simulated base on the time varying driving operations, travel intent, and charging plan in a simulated large scale charging scenario composed of concurrently moving evs and correspondingly equipped charging piles different ev navigation algorithms and charging algorithms of internet of vehicle can be exactly evaluated in the dynamic simulation of the digital twins of the moving evs and charging infrastructure then we analyze the collected data such as energy consumption, charging capacity, charging frequency, and waiting time in queue on both the ev side and the charging pile side to evaluate the charging efficiency the simulation is used to study the relations between the scheduled charging operation of evs and the deployment of piles the proposed model helps evaluate and validate the design of the charging recommendation and the deployment plan regarding to the arrangement and distribution of charging piles [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            }
        },
        {
            "instance_id": "R184022xR154399",
            "template_id": "R184022",
            "correct_template_id": "R154390",
            "paper_id": "R154399",
            "premise": "xray spectroscopy paper type research objective has system qualities",
            "hypothesis": "selective catalytic conversion of guaiacol to phenols over a molybdenum carbide catalyst an activated carbon supported \u03b1 molybdenum carbide catalyst (\u03b1 moc 1\u2212x /ac) showed remarkable activity in the selective deoxygenation of guaiacol to substituted mono phenols in low carbon number alcohol solvents",
            "sequence": "[CLS] xray spectroscopy paper type research objective has system qualities [SEP] selective catalytic conversion of guaiacol to phenols over a molybdenum carbide catalyst an activated carbon supported \u03b1 molybdenum carbide catalyst (\u03b1 moc 1\u2212x /ac) showed remarkable activity in the selective deoxygenation of guaiacol to substituted mono phenols in low carbon number alcohol solvents [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R129",
                "label": "Organic Chemistry"
            }
        },
        {
            "instance_id": "R146876xR111988",
            "template_id": "R146876",
            "correct_template_id": null,
            "paper_id": "R111988",
            "premise": "organic solar cells mobility donor acceptor lumo homo energy band gap open circuit voltage, voc short circuit current density, jsc fill factor, ff power conversion efficiency",
            "hypothesis": "a needle in a haystack: what do twitter users say about software? users of the twitter microblogging platform share a vast amount of information about various topics through short messages on a daily basis some of these so called tweets include information that is relevant for software companies and could, for example, help requirements engineers to identify user needs therefore, tweets have the potential to aid in the continuous evolution of software applications despite the existence of such relevant tweets, little is known about their number and content in this paper we report on the results of an exploratory study in which we analyzed the usage characteristics, content and automatic classification potential of tweets about software applications by using descriptive statistics, content analysis and machine learning techniques although the manual search of relevant information within the vast stream of tweets can be compared to looking for a needle in a haystack, our analysis shows that tweets provide a valuable input for software companies furthermore, our results demonstrate that machine learning techniques have the capacity to identify and harvest relevant information automatically",
            "sequence": "[CLS] organic solar cells mobility donor acceptor lumo homo energy band gap open circuit voltage, voc short circuit current density, jsc fill factor, ff power conversion efficiency [SEP] a needle in a haystack: what do twitter users say about software? users of the twitter microblogging platform share a vast amount of information about various topics through short messages on a daily basis some of these so called tweets include information that is relevant for software companies and could, for example, help requirements engineers to identify user needs therefore, tweets have the potential to aid in the continuous evolution of software applications despite the existence of such relevant tweets, little is known about their number and content in this paper we report on the results of an exploratory study in which we analyzed the usage characteristics, content and automatic classification potential of tweets about software applications by using descriptive statistics, content analysis and machine learning techniques although the manual search of relevant information within the vast stream of tweets can be compared to looking for a needle in a haystack, our analysis shows that tweets provide a valuable input for software companies furthermore, our results demonstrate that machine learning techniques have the capacity to identify and harvest relevant information automatically [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R182248xR3076",
            "template_id": "R182248",
            "correct_template_id": null,
            "paper_id": "R3076",
            "premise": "food photo dataset year name annotation number of images task number of classes acquisition type",
            "hypothesis": "an expert system based on texture features and decision tree classifier for diagnosis of tumor in brain mr images in this paper a new tumor classification system has been designed and developed for mri systems the mr imaging is a mostly used scheme for high excellence in medical imaging, it gives clear imageing capability especially in brain imaging where the soft tissues contrast and non invasiveness is a clear advantage the proposed method consists of three stages namely pre processing, feature extraction and classification in the first stage, gausian filter is applied for extracting the noise for experimental image in the second stage, statistical texture features are extracted for the purpose of classification finally, the decision tree classifier is used to classify the type of tumor image in our proposed system classification has two divisions: i) training stage and ii) testing stage in the training stage, various features are extracted from the tumor and non tumor images in testing stage, based on the knowledge base, the classifier classify the image into tumor and non tumor thus, the proposed system has been evaluated on a dataset of 40 patients the proposed system was found efficient in classification with a success of more than 95% of accuracy",
            "sequence": "[CLS] food photo dataset year name annotation number of images task number of classes acquisition type [SEP] an expert system based on texture features and decision tree classifier for diagnosis of tumor in brain mr images in this paper a new tumor classification system has been designed and developed for mri systems the mr imaging is a mostly used scheme for high excellence in medical imaging, it gives clear imageing capability especially in brain imaging where the soft tissues contrast and non invasiveness is a clear advantage the proposed method consists of three stages namely pre processing, feature extraction and classification in the first stage, gausian filter is applied for extracting the noise for experimental image in the second stage, statistical texture features are extracted for the purpose of classification finally, the decision tree classifier is used to classify the type of tumor image in our proposed system classification has two divisions: i) training stage and ii) testing stage in the training stage, various features are extracted from the tumor and non tumor images in testing stage, based on the knowledge base, the classifier classify the image into tumor and non tumor thus, the proposed system has been evaluated on a dataset of 40 patients the proposed system was found efficient in classification with a success of more than 95% of accuracy [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R133",
                "label": "Artificial Intelligence"
            }
        },
        {
            "instance_id": "R198658xR157051",
            "template_id": "R198658",
            "correct_template_id": null,
            "paper_id": "R157051",
            "premise": "nacre mechanics template has method has result research problem has material",
            "hypothesis": "a transcontinental challenge \u2014 a test of dna barcode performance for 1,541 species of canadian noctuoidea (lepidoptera) this study provides a first, comprehensive, diagnostic use of dna barcodes for the canadian fauna of noctuoids or \u201cowlet\u201d moths (lepidoptera: noctuoidea) based on vouchered records for 1,541 species (99 1% species coverage), and more than 30,000 sequences when viewed from a canada wide perspective, dna barcodes unambiguously discriminate 90% of the noctuoid species recognized through prior taxonomic study, and resolution reaches 95 6% when considered at a provincial scale barcode sharing is concentrated in certain lineages with 54% of the cases involving 1 8% of the genera deep intraspecific divergence exists in 7 7% of the species, but further studies are required to clarify whether these cases reflect an overlooked species complex or phylogeographic variation in a single species non native species possess higher nearest neighbour (nn) distances than native taxa, whereas generalist feeders have lower nn distances than those with more specialized feeding habits we found high concordance between taxonomic names and sequence clusters delineated by the barcode index number (bin) system with 1,082 species (70%) assigned to a unique bin the cases of discordance involve both bin mergers and bin splits with 38 species falling into both categories, most likely reflecting bidirectional introgression one fifth of the species are involved in a bin merger reflecting the presence of 158 species sharing their barcode sequence with at least one other taxon, and 189 species with low, but diagnostic coi divergence a very few cases (13) involved species whose members fell into both categories most of the remaining 140 species show a split into two or three bins per species, while virbia ferruginosa was divided into 16 the overall results confirm that dna barcodes are effective for the identification of canadian noctuoids this study also affirms that bins are a strong proxy for species, providing a pathway for a rapid, accurate estimation of animal diversity",
            "sequence": "[CLS] nacre mechanics template has method has result research problem has material [SEP] a transcontinental challenge \u2014 a test of dna barcode performance for 1,541 species of canadian noctuoidea (lepidoptera) this study provides a first, comprehensive, diagnostic use of dna barcodes for the canadian fauna of noctuoids or \u201cowlet\u201d moths (lepidoptera: noctuoidea) based on vouchered records for 1,541 species (99 1% species coverage), and more than 30,000 sequences when viewed from a canada wide perspective, dna barcodes unambiguously discriminate 90% of the noctuoid species recognized through prior taxonomic study, and resolution reaches 95 6% when considered at a provincial scale barcode sharing is concentrated in certain lineages with 54% of the cases involving 1 8% of the genera deep intraspecific divergence exists in 7 7% of the species, but further studies are required to clarify whether these cases reflect an overlooked species complex or phylogeographic variation in a single species non native species possess higher nearest neighbour (nn) distances than native taxa, whereas generalist feeders have lower nn distances than those with more specialized feeding habits we found high concordance between taxonomic names and sequence clusters delineated by the barcode index number (bin) system with 1,082 species (70%) assigned to a unique bin the cases of discordance involve both bin mergers and bin splits with 38 species falling into both categories, most likely reflecting bidirectional introgression one fifth of the species are involved in a bin merger reflecting the presence of 158 species sharing their barcode sequence with at least one other taxon, and 189 species with low, but diagnostic coi divergence a very few cases (13) involved species whose members fell into both categories most of the remaining 140 species show a split into two or three bins per species, while virbia ferruginosa was divided into 16 the overall results confirm that dna barcodes are effective for the identification of canadian noctuoids this study also affirms that bins are a strong proxy for species, providing a pathway for a rapid, accurate estimation of animal diversity [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            }
        },
        {
            "instance_id": "R187648xR160241",
            "template_id": "R187648",
            "correct_template_id": null,
            "paper_id": "R160241",
            "premise": "oco for control research problem theoretical guarantees has application in data driven approach has constraints type of considered system measurement noise process noise",
            "hypothesis": "automatic 3d buildings compact reconstruction from lidar point clouds abstract point clouds generated from aerial lidar and photogrammetric techniques are great ways to obtain valuable spatial insights over large scale however, their nature hinders the direct extraction and sharing of underlying information the generation of consistent large scale 3d city models from this real world data is a major challenge specifically, the integration in workflows usable by decision making scenarios demands that the data is structured, rich and exchangeable citygml permits new advances in terms of interoperable endeavour to use city models in a collaborative way efforts have led to render good looking digital twins of cities but few of them take into account their potential use in finite elements simulations (wind, floods, heat radiation model, etc ) in this paper, we target the automatic reconstruction of consistent 3d city buildings highlighting closed solids, coherent surface junctions, perfect snapping of vertices, etc it specifically investigates the topological and geometrical consistency of generated models from aerial lidar point cloud, formatted following the cityjson specifications these models are then usable to store relevant information and provides geometries usable within complex computations such as computational fluid dynamics, free of local inconsistencies (e g holes and unclosed solids) \\n",
            "sequence": "[CLS] oco for control research problem theoretical guarantees has application in data driven approach has constraints type of considered system measurement noise process noise [SEP] automatic 3d buildings compact reconstruction from lidar point clouds abstract point clouds generated from aerial lidar and photogrammetric techniques are great ways to obtain valuable spatial insights over large scale however, their nature hinders the direct extraction and sharing of underlying information the generation of consistent large scale 3d city models from this real world data is a major challenge specifically, the integration in workflows usable by decision making scenarios demands that the data is structured, rich and exchangeable citygml permits new advances in terms of interoperable endeavour to use city models in a collaborative way efforts have led to render good looking digital twins of cities but few of them take into account their potential use in finite elements simulations (wind, floods, heat radiation model, etc ) in this paper, we target the automatic reconstruction of consistent 3d city buildings highlighting closed solids, coherent surface junctions, perfect snapping of vertices, etc it specifically investigates the topological and geometrical consistency of generated models from aerial lidar point cloud, formatted following the cityjson specifications these models are then usable to store relevant information and provides geometries usable within complex computations such as computational fluid dynamics, free of local inconsistencies (e g holes and unclosed solids) \\n [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            }
        },
        {
            "instance_id": "R172526xR139094",
            "template_id": "R172526",
            "correct_template_id": null,
            "paper_id": "R139094",
            "premise": "video process has study research problem application production",
            "hypothesis": "characterization of transient discharges under atmospheric pressure conditions applying nitrogen photoemission and current measurements the plasma parameters such as electron distribution function and electron density of three atmospheric pressure transient discharges namely filamentary and homogeneous dielectric barrier discharges in air, and the spark discharge of an argon plasma coagulation (apc) system are determined a combination of numerical simulation as well as diagnostic methods including current measurement and optical emission spectroscopy (oes) based on nitrogen emissions is used the applied methods supplement each other and resolve problems, which arise when these methods are used individually nitrogen is used as a sensor gas and is admixed in low amount to argon for characterizing the apc discharge both direct and stepwise electron impact excitation of nitrogen emissions are included in the plasma chemical model applied for characterization of these transient discharges using oes where ambiguity arises in the determination of plasma parameters under specific discharge conditions it is shown that the measured current solves this problem by providing additional information useful for the determination of discharge specific plasma parameters",
            "sequence": "[CLS] video process has study research problem application production [SEP] characterization of transient discharges under atmospheric pressure conditions applying nitrogen photoemission and current measurements the plasma parameters such as electron distribution function and electron density of three atmospheric pressure transient discharges namely filamentary and homogeneous dielectric barrier discharges in air, and the spark discharge of an argon plasma coagulation (apc) system are determined a combination of numerical simulation as well as diagnostic methods including current measurement and optical emission spectroscopy (oes) based on nitrogen emissions is used the applied methods supplement each other and resolve problems, which arise when these methods are used individually nitrogen is used as a sensor gas and is admixed in low amount to argon for characterizing the apc discharge both direct and stepwise electron impact excitation of nitrogen emissions are included in the plasma chemical model applied for characterization of these transient discharges using oes where ambiguity arises in the determination of plasma parameters under specific discharge conditions it is shown that the measured current solves this problem by providing additional information useful for the determination of discharge specific plasma parameters [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            }
        },
        {
            "instance_id": "R184022xR142138",
            "template_id": "R184022",
            "correct_template_id": null,
            "paper_id": "R142138",
            "premise": "xray spectroscopy paper type research objective has system qualities",
            "hypothesis": "mapping intracellular temperature using green fluorescent protein heat is of fundamental importance in many cellular processes such as cell metabolism, cell division and gene expression (1 3) accurate and noninvasive monitoring of temperature changes in individual cells could thus help clarify intricate cellular processes and develop new applications in biology and medicine here we report the use of green fluorescent proteins (gfp) as thermal nanoprobes suited for intracellular temperature mapping temperature probing is achieved by monitoring the fluorescence polarization anisotropy of gfp the method is tested on gfp transfected hela and u 87 mg cancer cell lines where we monitored the heat delivery by photothermal heating of gold nanorods surrounding the cells a spatial resolution of 300 nm and a temperature accuracy of about 0 4 \u00b0c are achieved benefiting from its full compatibility with widely used gfp transfected cells, this approach provides a noninvasive tool for fundamental and applied research in areas ranging from molecular biology to therapeutic and diagnostic studies",
            "sequence": "[CLS] xray spectroscopy paper type research objective has system qualities [SEP] mapping intracellular temperature using green fluorescent protein heat is of fundamental importance in many cellular processes such as cell metabolism, cell division and gene expression (1 3) accurate and noninvasive monitoring of temperature changes in individual cells could thus help clarify intricate cellular processes and develop new applications in biology and medicine here we report the use of green fluorescent proteins (gfp) as thermal nanoprobes suited for intracellular temperature mapping temperature probing is achieved by monitoring the fluorescence polarization anisotropy of gfp the method is tested on gfp transfected hela and u 87 mg cancer cell lines where we monitored the heat delivery by photothermal heating of gold nanorods surrounding the cells a spatial resolution of 300 nm and a temperature accuracy of about 0 4 \u00b0c are achieved benefiting from its full compatibility with widely used gfp transfected cells, this approach provides a noninvasive tool for fundamental and applied research in areas ranging from molecular biology to therapeutic and diagnostic studies [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            }
        },
        {
            "instance_id": "R150089xR113067",
            "template_id": "R150089",
            "correct_template_id": null,
            "paper_id": "R113067",
            "premise": "epidemiological surveillance systems design and implementation software used software development approach related work epidemiological surveillance system purpose epidemiological surveillance process epidemiological surveillance users statistical analysis techniques epidemiological surveillance architecture epidemiological software development approach advantage provided by the system limit of the system",
            "hypothesis": "mining user rationale from software reviews rationale refers to the reasoning and justification behind human decisions, opinions, and beliefs in software engineering, rationale management focuses on capturing design and requirements decisions and on organizing and reusing project knowledge this paper takes a different view on rationale written by users in online reviews we studied 32,414 reviews for 52 software applications in the amazon store through a grounded theory approach and peer content analysis, we investigated how users argue and justify their decisions, e g about upgrading, installing, or switching software applications we also studied the occurrence frequency of rationale concepts such as issues encountered or alternatives considered in the reviews and found that assessment criteria like performance, compatibility, and usability represent the most pervasive concept we then used the truth set of manually labeled review sentences to explore how accurately we can mine rationale concepts from the reviews support vector classifier, naive bayes, and logistic regression, trained on the review metadata, syntax tree of the review text, and influential terms, achieved a precision around 80% for predicting sentences with alternatives and decisions, with top recall values of 98% on the review level, precision was up to 13% higher with recall values reaching 99% we discuss the findings and the rationale importance for supporting deliberation in user communities and synthesizing the reviews for developers",
            "sequence": "[CLS] epidemiological surveillance systems design and implementation software used software development approach related work epidemiological surveillance system purpose epidemiological surveillance process epidemiological surveillance users statistical analysis techniques epidemiological surveillance architecture epidemiological software development approach advantage provided by the system limit of the system [SEP] mining user rationale from software reviews rationale refers to the reasoning and justification behind human decisions, opinions, and beliefs in software engineering, rationale management focuses on capturing design and requirements decisions and on organizing and reusing project knowledge this paper takes a different view on rationale written by users in online reviews we studied 32,414 reviews for 52 software applications in the amazon store through a grounded theory approach and peer content analysis, we investigated how users argue and justify their decisions, e g about upgrading, installing, or switching software applications we also studied the occurrence frequency of rationale concepts such as issues encountered or alternatives considered in the reviews and found that assessment criteria like performance, compatibility, and usability represent the most pervasive concept we then used the truth set of manually labeled review sentences to explore how accurately we can mine rationale concepts from the reviews support vector classifier, naive bayes, and logistic regression, trained on the review metadata, syntax tree of the review text, and influential terms, achieved a precision around 80% for predicting sentences with alternatives and decisions, with top recall values of 98% on the review level, precision was up to 13% higher with recall values reaching 99% we discuss the findings and the rationale importance for supporting deliberation in user communities and synthesizing the reviews for developers [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R161545xR194401",
            "template_id": "R161545",
            "correct_template_id": null,
            "paper_id": "R194401",
            "premise": "recycling methods method recycling type has property",
            "hypothesis": "an approach for reviewing security related aspects in agile requirements specifications of web applications defects in requirements specifications can have severe consequences during the software development lifecycle some of them result in overall project failure due to incorrect or missing quality characteristics such as security there are several concerns that make security difficult to deal with; for instance, (1) when stakeholders discuss general requirements in meetings, they are often unaware that they should also discuss security related topics, and (2) they typically do not have enough expertise in security this often leads to unspecified or ill defined security related aspects these concerns become even more challenging in agile contexts, where lightweight documentation is typically involved the goal of this paper is to design and evaluate an approach for reviewing security related aspects in agile requirements specifications of web applications the approach considers user stories and security specifications as input and relates those user stories to security properties via natural language processing based on the related security properties, our approach then identifies high level security requirements from the open web application security project to be verified and generates a reading technique to support reviewers in detecting defects we evaluate our approach via two controlled experiment trials we compare the effectiveness and efficiency of novice inspectors verifying security aspects in agile requirements using our approach against using the complete list of high level security requirements the (statistically significant) results indicate that using our approach has a positive impact (with large effect size) on the performance of inspectors in terms of effectiveness and efficiency",
            "sequence": "[CLS] recycling methods method recycling type has property [SEP] an approach for reviewing security related aspects in agile requirements specifications of web applications defects in requirements specifications can have severe consequences during the software development lifecycle some of them result in overall project failure due to incorrect or missing quality characteristics such as security there are several concerns that make security difficult to deal with; for instance, (1) when stakeholders discuss general requirements in meetings, they are often unaware that they should also discuss security related topics, and (2) they typically do not have enough expertise in security this often leads to unspecified or ill defined security related aspects these concerns become even more challenging in agile contexts, where lightweight documentation is typically involved the goal of this paper is to design and evaluate an approach for reviewing security related aspects in agile requirements specifications of web applications the approach considers user stories and security specifications as input and relates those user stories to security properties via natural language processing based on the related security properties, our approach then identifies high level security requirements from the open web application security project to be verified and generates a reading technique to support reviewers in detecting defects we evaluate our approach via two controlled experiment trials we compare the effectiveness and efficiency of novice inspectors verifying security aspects in agile requirements using our approach against using the complete list of high level security requirements the (statistically significant) results indicate that using our approach has a positive impact (with large effect size) on the performance of inspectors in terms of effectiveness and efficiency [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R159441xR145296",
            "template_id": "R159441",
            "correct_template_id": null,
            "paper_id": "R145296",
            "premise": "city digital twin potentials visualization situational awareness planning and prediction integration and collaboration data management",
            "hypothesis": "molecular identification of mosquitoes (diptera: culicidae) in southeastern australia abstract dna barcoding is a modern species identification technique that can be used to distinguish morphologically similar species, and is particularly useful when using small amounts of starting material from partial specimens or from immature stages in order to use dna barcoding in a surveillance program, a database containing mosquito barcode sequences is required this study obtained cytochrome oxidase i (coi) sequences for 113 morphologically identified specimens, representing 29 species, six tribes and 12 genera; 17 of these species have not been previously barcoded three of the 29 species \u2500 culex palpalis, macleaya macmillani, and an unknown species originally identified as tripteroides atripes \u2500 were initially misidentified as they are difficult to separate morphologically, highlighting the utility of dna barcoding while most species grouped separately (reciprocally monophyletic), the cx pipiens subgroup could not be genetically separated using coi the average conspecific and congeneric p\u2010distance was 0 8% and 7 6%, respectively in our study, we also demonstrate the utility of dna barcoding in distinguishing exotics from endemic mosquitoes by identifying a single intercepted stegomyia aegypti egg at an international airport the use of dna barcoding dramatically reduced the identification time required compared with rearing specimens through to adults, thereby demonstrating the value of this technique in biosecurity surveillance the dna barcodes produced by this study have been uploaded to the \u2018mosquitoes of australia\u2013victoria\u2019 project on the barcode of life database (bold), which will serve as a resource for the victorian arbovirus disease control program and other national and international mosquito surveillance programs",
            "sequence": "[CLS] city digital twin potentials visualization situational awareness planning and prediction integration and collaboration data management [SEP] molecular identification of mosquitoes (diptera: culicidae) in southeastern australia abstract dna barcoding is a modern species identification technique that can be used to distinguish morphologically similar species, and is particularly useful when using small amounts of starting material from partial specimens or from immature stages in order to use dna barcoding in a surveillance program, a database containing mosquito barcode sequences is required this study obtained cytochrome oxidase i (coi) sequences for 113 morphologically identified specimens, representing 29 species, six tribes and 12 genera; 17 of these species have not been previously barcoded three of the 29 species \u2500 culex palpalis, macleaya macmillani, and an unknown species originally identified as tripteroides atripes \u2500 were initially misidentified as they are difficult to separate morphologically, highlighting the utility of dna barcoding while most species grouped separately (reciprocally monophyletic), the cx pipiens subgroup could not be genetically separated using coi the average conspecific and congeneric p\u2010distance was 0 8% and 7 6%, respectively in our study, we also demonstrate the utility of dna barcoding in distinguishing exotics from endemic mosquitoes by identifying a single intercepted stegomyia aegypti egg at an international airport the use of dna barcoding dramatically reduced the identification time required compared with rearing specimens through to adults, thereby demonstrating the value of this technique in biosecurity surveillance the dna barcodes produced by this study have been uploaded to the \u2018mosquitoes of australia\u2013victoria\u2019 project on the barcode of life database (bold), which will serve as a resource for the victorian arbovirus disease control program and other national and international mosquito surveillance programs [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            }
        },
        {
            "instance_id": "R159441xR141701",
            "template_id": "R159441",
            "correct_template_id": null,
            "paper_id": "R141701",
            "premise": "city digital twin potentials visualization situational awareness planning and prediction integration and collaboration data management",
            "hypothesis": "carbon dot nanothermometry: intracellular photoluminescence lifetime thermal sensing nanoscale biocompatible photoluminescence (pl) thermometers that can be used to accurately and reliably monitor intracellular temperatures have many potential applications in biology and medicine ideally, such nanothermometers should be functional at physiological ph across a wide range of ionic strengths, probe concentrations, and local environments here, we show that water soluble n,s co doped carbon dots (cds) exhibit temperature dependent photoluminescence lifetimes and can serve as highly sensitive and reliable intracellular nanothermometers pl intensity measurements indicate that these cds have many advantages over alternative semiconductor and cd based nanoscale temperature sensors importantly, their pl lifetimes remain constant over wide ranges of ph values (5 12), cd concentrations (1 5 \u00d7 10 5 to 0 5 mg/ml), and environmental ionic strengths (up to 0 7 mol\u00b7l 1 nacl) moreover, they are biocompatible and nontoxic, as demonstrated by cell viability and flow cytometry analyses using nih/3t3 and hela cell lines n,s cd thermal sensors also exhibit good water dispersibility, superior photo and thermostability, extraordinary environment and concentration independence, high storage stability, and reusability their pl decay curves at temperatures between 15 and 45 \u00b0c remained unchanged over seven sequential experiments in vitro pl lifetime based temperature sensing performed with human cervical cancer hela cells demonstrated the great potential of these nanosensors in biomedicine overall, n,s doped cds exhibit excitation independent emission with strongly temperature dependent monoexponential decay, making them suitable for both in vitro and in vivo luminescence lifetime thermometry",
            "sequence": "[CLS] city digital twin potentials visualization situational awareness planning and prediction integration and collaboration data management [SEP] carbon dot nanothermometry: intracellular photoluminescence lifetime thermal sensing nanoscale biocompatible photoluminescence (pl) thermometers that can be used to accurately and reliably monitor intracellular temperatures have many potential applications in biology and medicine ideally, such nanothermometers should be functional at physiological ph across a wide range of ionic strengths, probe concentrations, and local environments here, we show that water soluble n,s co doped carbon dots (cds) exhibit temperature dependent photoluminescence lifetimes and can serve as highly sensitive and reliable intracellular nanothermometers pl intensity measurements indicate that these cds have many advantages over alternative semiconductor and cd based nanoscale temperature sensors importantly, their pl lifetimes remain constant over wide ranges of ph values (5 12), cd concentrations (1 5 \u00d7 10 5 to 0 5 mg/ml), and environmental ionic strengths (up to 0 7 mol\u00b7l 1 nacl) moreover, they are biocompatible and nontoxic, as demonstrated by cell viability and flow cytometry analyses using nih/3t3 and hela cell lines n,s cd thermal sensors also exhibit good water dispersibility, superior photo and thermostability, extraordinary environment and concentration independence, high storage stability, and reusability their pl decay curves at temperatures between 15 and 45 \u00b0c remained unchanged over seven sequential experiments in vitro pl lifetime based temperature sensing performed with human cervical cancer hela cells demonstrated the great potential of these nanosensors in biomedicine overall, n,s doped cds exhibit excitation independent emission with strongly temperature dependent monoexponential decay, making them suitable for both in vitro and in vivo luminescence lifetime thermometry [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            }
        },
        {
            "instance_id": "R184022xR194641",
            "template_id": "R184022",
            "correct_template_id": null,
            "paper_id": "R194641",
            "premise": "xray spectroscopy paper type research objective has system qualities",
            "hypothesis": "digital discrimination in sharing economy a requirements engineering perspective recent evidence has revealed that sharing economy platforms such as uber, airbnb, and taskrabbit, have become active hubs for digital discrimination this new form of discrimination refers to a phenomenon where a business transaction is influenced by race, gender, age, or any other non business related characteristic of providers or consumers existing research often tackles this problem from a socio economic and regulatory points of view however, the research on the design aspects of sharing economy software, which enable such complex sociotechnical problems to emerge online, is still underdeveloped to bridge this gap, in this paper, we propose a new perspective on digital discrimination, tackling the problem from a requirements engineering point of view specifically, we analyze a large dataset of online user feedback as well as synthesize existing literature to identify and classify pervasive discrimination concerns in the sharing economy market based on this analysis, we devise a crowd driven domain model to represent these concerns along with their relations to the functional features and user goals of sharing economy platforms this model is intended to provide requirements engineers, working on sharing economy software, with systematic insights into the complex types of socio technical problems that can emerge in the operational environments of their systems",
            "sequence": "[CLS] xray spectroscopy paper type research objective has system qualities [SEP] digital discrimination in sharing economy a requirements engineering perspective recent evidence has revealed that sharing economy platforms such as uber, airbnb, and taskrabbit, have become active hubs for digital discrimination this new form of discrimination refers to a phenomenon where a business transaction is influenced by race, gender, age, or any other non business related characteristic of providers or consumers existing research often tackles this problem from a socio economic and regulatory points of view however, the research on the design aspects of sharing economy software, which enable such complex sociotechnical problems to emerge online, is still underdeveloped to bridge this gap, in this paper, we propose a new perspective on digital discrimination, tackling the problem from a requirements engineering point of view specifically, we analyze a large dataset of online user feedback as well as synthesize existing literature to identify and classify pervasive discrimination concerns in the sharing economy market based on this analysis, we devise a crowd driven domain model to represent these concerns along with their relations to the functional features and user goals of sharing economy platforms this model is intended to provide requirements engineers, working on sharing economy software, with systematic insights into the complex types of socio technical problems that can emerge in the operational environments of their systems [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R161736xR159779",
            "template_id": "R161736",
            "correct_template_id": null,
            "paper_id": "R159779",
            "premise": "xray laser applications paper type research objective has system qualities",
            "hypothesis": "return of the vision video: can corporate vision videos serve as setting for participation? this paper examines the role of corporate vision videos as a possible setting for participation when exploring the future potentials (and pitfalls) of new technological concepts we propose that through the recent decade\u2019s rise web 2 0 platforms, and the viral effects of user sharing, the corporate vision video of today might take on a significantly different role than before, and act as a participatory design approach this address the changing landscaping for participatory and user involved design processes, in the wake of new digital forms of participation, communication and collaboration, which have radically changed the possible power dynamics of the production life cycle of new product developments through a case study, we pose the question of whether the online engagements around corporate vision videos can be viewed as a form of participation in a design process, and thus revitalize the relevance of vision videos as a design resource?",
            "sequence": "[CLS] xray laser applications paper type research objective has system qualities [SEP] return of the vision video: can corporate vision videos serve as setting for participation? this paper examines the role of corporate vision videos as a possible setting for participation when exploring the future potentials (and pitfalls) of new technological concepts we propose that through the recent decade\u2019s rise web 2 0 platforms, and the viral effects of user sharing, the corporate vision video of today might take on a significantly different role than before, and act as a participatory design approach this address the changing landscaping for participatory and user involved design processes, in the wake of new digital forms of participation, communication and collaboration, which have radically changed the possible power dynamics of the production life cycle of new product developments through a case study, we pose the question of whether the online engagements around corporate vision videos can be viewed as a form of participation in a design process, and thus revitalize the relevance of vision videos as a design resource? [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R265",
                "label": "Computer-Aided Engineering and Design"
            }
        },
        {
            "instance_id": "R156306xR38043",
            "template_id": "R156306",
            "correct_template_id": null,
            "paper_id": "R38043",
            "premise": "research objective research objective",
            "hypothesis": "semantic federation of product information from structured and unstructured sources product related information can be found in various data sources and formats across the product lifecycle effectively exploiting this information requires the federation of these sources, the extraction of implicit information, and the efficient access to this comprehensive knowledge base existing solutions for product information management (pim) are usually restricted to structured information, but most of the business critical information resides in unstructured documents we present a generic architecture for federating heterogeneous information from various sources, including the internet of things, and argue how this process benefits from using semantic representations a reference implementation tailor made to business users is explained and evaluated we also discuss several issues we experienced that we believe to be valuable for researchers and implementers of semantic information systems, as well as the information retrieval community",
            "sequence": "[CLS] research objective research objective [SEP] semantic federation of product information from structured and unstructured sources product related information can be found in various data sources and formats across the product lifecycle effectively exploiting this information requires the federation of these sources, the extraction of implicit information, and the efficient access to this comprehensive knowledge base existing solutions for product information management (pim) are usually restricted to structured information, but most of the business critical information resides in unstructured documents we present a generic architecture for federating heterogeneous information from various sources, including the internet of things, and argue how this process benefits from using semantic representations a reference implementation tailor made to business users is explained and evaluated we also discuss several issues we experienced that we believe to be valuable for researchers and implementers of semantic information systems, as well as the information retrieval community [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R278",
                "label": "Information Science"
            }
        },
        {
            "instance_id": "R161736xR25135",
            "template_id": "R161736",
            "correct_template_id": null,
            "paper_id": "R25135",
            "premise": "xray laser applications paper type research objective has system qualities",
            "hypothesis": "integrating off board cameras and vehicle on board localization for pedestrian safety \"situational awareness for industrial vehicles is crucial to ensure safety of personnel and equipment while human drivers and onboard sensors are able to detect obstacles and pedestrians within line of sight, in complex environments, initially occluded or obscured dynamic objects can unpredictably enter the path of a vehicle we propose a system that integrates a vision based offboard pedestrian tracking subsystem with an onboard localization and navigation subsystem this combination enables warnings to be communicated and effectively extends the vehicle controller's field of view to include areas that would otherwise be blind spots a simple flashing light interface in the vehicle cabin provides a clear and intuitive interface to alert drivers of potential collisions alternatively, the system can be also applied to vehicles that have autonomous navigation capabilities, in which case, instead of alert lights, the vehicle is halted or redirected we implemented and tested the proposed solution on an automated industrial vehicle under autonomous operation and on a human driven vehicle in a full scale production facility, over a period of four months \"",
            "sequence": "[CLS] xray laser applications paper type research objective has system qualities [SEP] integrating off board cameras and vehicle on board localization for pedestrian safety \"situational awareness for industrial vehicles is crucial to ensure safety of personnel and equipment while human drivers and onboard sensors are able to detect obstacles and pedestrians within line of sight, in complex environments, initially occluded or obscured dynamic objects can unpredictably enter the path of a vehicle we propose a system that integrates a vision based offboard pedestrian tracking subsystem with an onboard localization and navigation subsystem this combination enables warnings to be communicated and effectively extends the vehicle controller's field of view to include areas that would otherwise be blind spots a simple flashing light interface in the vehicle cabin provides a clear and intuitive interface to alert drivers of potential collisions alternatively, the system can be also applied to vehicles that have autonomous navigation capabilities, in which case, instead of alert lights, the vehicle is halted or redirected we implemented and tested the proposed solution on an automated industrial vehicle under autonomous operation and on a human driven vehicle in a full scale production facility, over a period of four months \" [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R11",
                "label": "Science"
            }
        },
        {
            "instance_id": "R187648xR113054",
            "template_id": "R187648",
            "correct_template_id": null,
            "paper_id": "R113054",
            "premise": "oco for control research problem theoretical guarantees has application in data driven approach has constraints type of considered system measurement noise process noise",
            "hypothesis": "a gradual approach to crowd based requirements engineering: the case of conference online social networks this paper proposes a gradual approach to crowd based requirements engineering (re) for supporting the establishment of a more engaged crowd, hence, mitigating the low involvement risk in crowd based re our approach advocates involving micro crowds (mcs), where in each micro crowd, the population is relatively cohesive and familiar with each other using this approach, the evolving product is developed iteratively at each iteration, a new mc can join the already established crowd to enhance the requirements for the next version, while adding terminology to an evolving folksonomy we are currently using this approach in an on going research project to develop an online social network (osn) for academic researchers that will facilitate discussions and knowledge sharing around conferences",
            "sequence": "[CLS] oco for control research problem theoretical guarantees has application in data driven approach has constraints type of considered system measurement noise process noise [SEP] a gradual approach to crowd based requirements engineering: the case of conference online social networks this paper proposes a gradual approach to crowd based requirements engineering (re) for supporting the establishment of a more engaged crowd, hence, mitigating the low involvement risk in crowd based re our approach advocates involving micro crowds (mcs), where in each micro crowd, the population is relatively cohesive and familiar with each other using this approach, the evolving product is developed iteratively at each iteration, a new mc can join the already established crowd to enhance the requirements for the next version, while adding terminology to an evolving folksonomy we are currently using this approach in an on going research project to develop an online social network (osn) for academic researchers that will facilitate discussions and knowledge sharing around conferences [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R160259xR138070",
            "template_id": "R160259",
            "correct_template_id": null,
            "paper_id": "R138070",
            "premise": "digital city twin review application area",
            "hypothesis": "grand challenges in model driven engineering: an analysis of the state of the research abstract in 2017 and 2018, two events were held\u2014in marburg, germany, and san vigilio di marebbe, italy, respectively\u2014focusing on an analysis of the state of research, state of practice, and state of the art in model driven engineering (mde) the events brought together experts from industry, academia, and the open source community to assess what has changed in research in mde over the last 10\\xa0years, what challenges remain, and what new challenges have arisen this article reports on the results of those meetings, and presents a set of grand challenges that emerged from discussions and synthesis these challenges could lead to research initiatives for the community going forward \\n",
            "sequence": "[CLS] digital city twin review application area [SEP] grand challenges in model driven engineering: an analysis of the state of the research abstract in 2017 and 2018, two events were held\u2014in marburg, germany, and san vigilio di marebbe, italy, respectively\u2014focusing on an analysis of the state of research, state of practice, and state of the art in model driven engineering (mde) the events brought together experts from industry, academia, and the open source community to assess what has changed in research in mde over the last 10\\xa0years, what challenges remain, and what new challenges have arisen this article reports on the results of those meetings, and presents a set of grand challenges that emerged from discussions and synthesis these challenges could lead to research initiatives for the community going forward \\n [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R150089xR146938",
            "template_id": "R150089",
            "correct_template_id": null,
            "paper_id": "R146938",
            "premise": "epidemiological surveillance systems design and implementation software used software development approach related work epidemiological surveillance system purpose epidemiological surveillance process epidemiological surveillance users statistical analysis techniques epidemiological surveillance architecture epidemiological software development approach advantage provided by the system limit of the system",
            "hypothesis": "evaluation of dna barcoding and identification of new haplomorphs in canadian deerflies and horseflies this paper reports the first tests of the suitability of the standardized mitochondrial cytochrome c oxidase subunit i (coi) barcoding system for the identification of canadian deerflies and horseflies two additional mitochondrial molecular markers were used to determine whether unambiguous species recognition in tabanids can be achieved our 332 canadian tabanid samples yielded 650 sequences from five genera and 42 species standard coi barcodes demonstrated a strong a + t bias (mean 68 1%), especially at third codon positions (mean 93 0%) our preliminary test of this system showed that the standard coi barcode worked well for canadian tabanidae: the target dna can be easily recovered from small amounts of insect tissue and aligned for all tabanid taxa each tabanid species possessed distinctive sets of coi haplotypes which discriminated well among species average conspecific kimura two\u2010parameter (k2p) divergence (0 49%) was 12 times lower than the average divergence within species both the neighbour\u2010joining and the bayesian methods produced trees with identical monophyletic species groups two species, chrysops dawsoni philip and chrysops montanus osten sacken (diptera: tabanidae), showed relatively deep intraspecific sequence divergences (\u223c10 times the average) for all three mitochondrial gene regions analysed we suggest provisional differentiation of ch montanus into two haplotypes, namely, ch montanus haplomorph 1 and ch montanus haplomorph 2, both defined by their molecular sequences and by newly discovered differences in structural features near their ocelli",
            "sequence": "[CLS] epidemiological surveillance systems design and implementation software used software development approach related work epidemiological surveillance system purpose epidemiological surveillance process epidemiological surveillance users statistical analysis techniques epidemiological surveillance architecture epidemiological software development approach advantage provided by the system limit of the system [SEP] evaluation of dna barcoding and identification of new haplomorphs in canadian deerflies and horseflies this paper reports the first tests of the suitability of the standardized mitochondrial cytochrome c oxidase subunit i (coi) barcoding system for the identification of canadian deerflies and horseflies two additional mitochondrial molecular markers were used to determine whether unambiguous species recognition in tabanids can be achieved our 332 canadian tabanid samples yielded 650 sequences from five genera and 42 species standard coi barcodes demonstrated a strong a + t bias (mean 68 1%), especially at third codon positions (mean 93 0%) our preliminary test of this system showed that the standard coi barcode worked well for canadian tabanidae: the target dna can be easily recovered from small amounts of insect tissue and aligned for all tabanid taxa each tabanid species possessed distinctive sets of coi haplotypes which discriminated well among species average conspecific kimura two\u2010parameter (k2p) divergence (0 49%) was 12 times lower than the average divergence within species both the neighbour\u2010joining and the bayesian methods produced trees with identical monophyletic species groups two species, chrysops dawsoni philip and chrysops montanus osten sacken (diptera: tabanidae), showed relatively deep intraspecific sequence divergences (\u223c10 times the average) for all three mitochondrial gene regions analysed we suggest provisional differentiation of ch montanus into two haplotypes, namely, ch montanus haplomorph 1 and ch montanus haplomorph 2, both defined by their molecular sequences and by newly discovered differences in structural features near their ocelli [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            }
        },
        {
            "instance_id": "R40006xR137398",
            "template_id": "R40006",
            "correct_template_id": null,
            "paper_id": "R137398",
            "premise": "basic reproduction number estimate time period basic reproduction number location",
            "hypothesis": "transitions between and control of guided and branching streamers in dc nanosecond pulsed excited plasma jets plasma bullets are ionization fronts created in atmospheric pressure plasma jets the propagation behavior of those bullets is, in the literature, explained by the formation of an interface between the inert gas and the ambient air created by the gas flow of the plasma jet, which guides these discharges in the formed gas channel in this paper, we examine this ionization phenomenon in uniform gases at atmospheric pressure where this interface between two gases is not present by changing electrical parameters and adding admixtures such as oxygen, nitrogen, and air to the gas flow, the conditions for which plasma bullets are present are investigated nanosecond time resolved images have been taken with an iccd camera to observe the propagation behavior of these discharges it is argued that the inhomogeneous spatial concentration of metastable atoms and ions, due to the laminar gas flow and the operation frequency of the discharge in the range of a few kilohertz, is responsible for the guidance of the ionization fronts furthermore, conditions have been observed at where the branching of the discharge is stable and reproducible over time in the case of a helium plasma by adding admixtures of oxygen possible mechanisms for this phenomenon are discussed",
            "sequence": "[CLS] basic reproduction number estimate time period basic reproduction number location [SEP] transitions between and control of guided and branching streamers in dc nanosecond pulsed excited plasma jets plasma bullets are ionization fronts created in atmospheric pressure plasma jets the propagation behavior of those bullets is, in the literature, explained by the formation of an interface between the inert gas and the ambient air created by the gas flow of the plasma jet, which guides these discharges in the formed gas channel in this paper, we examine this ionization phenomenon in uniform gases at atmospheric pressure where this interface between two gases is not present by changing electrical parameters and adding admixtures such as oxygen, nitrogen, and air to the gas flow, the conditions for which plasma bullets are present are investigated nanosecond time resolved images have been taken with an iccd camera to observe the propagation behavior of these discharges it is argued that the inhomogeneous spatial concentration of metastable atoms and ions, due to the laminar gas flow and the operation frequency of the discharge in the range of a few kilohertz, is responsible for the guidance of the ionization fronts furthermore, conditions have been observed at where the branching of the discharge is stable and reproducible over time in the case of a helium plasma by adding admixtures of oxygen possible mechanisms for this phenomenon are discussed [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R114008",
                "label": "Applied Physics"
            }
        },
        {
            "instance_id": "R160259xR162731",
            "template_id": "R160259",
            "correct_template_id": "R150595",
            "paper_id": "R162731",
            "premise": "digital city twin review application area",
            "hypothesis": "cross wedge rolling of pta welded hybrid steel billets with rolling bearing steel and hard material coatings within the collaborative research centre 1153 \u201ctailored forming\u201c a process chain for the manufacturing of hybrid high performance components is developed exemplary process steps consist of deposit welding of high performance steel on low cost steel, pre shaping by cross wedge rolling and finishing by milling hard material coatings such as stellite 6 or delcrome 253 are used as wear or corrosion protection coatings in industrial applications scientists of the institute of material science welded these hard material alloys onto a base material, in this case c22 8, to create a hybrid workpiece scientists of the institut fur integrierte produktion hannover have shown that these hybrid workpieces can be formed without defects (e g detachment of the coating) by cross wedge rolling after forming, the properties of the coatings are retained or in some cases even improved (e g the transition zone between base material and coating) by adjustments in the welding process, it was possible to apply the 100cr6 rolling bearing steel, as of now declared as non weldable, on the low cost steel c22 8 100cr6 was formed afterwards in its hybrid bonding state with c22 8 by cross wedge rolling, thus a component integrated bearing seat was produced even after welding and forming, the rolling bearing steel coating could still be quench hardened to a hardness of over 60 hrc this paper shows the potential of forming hybrid billets to tailored parts since industrially available standard materials can be used for hard material coatings by this approach, even though they are not weldable by conventional methods, it is not necessary to use expensive, for welding designed materials to implement a hybrid component concept within the collaborative research centre 1153 \u201ctailored forming\u201c a process chain for the manufacturing of hybrid high performance components is developed exemplary process steps consist of deposit welding of high performance steel on low cost steel, pre shaping by cross wedge rolling and finishing by milling hard material coatings such as stellite 6 or delcrome 253 are used as wear or corrosion protection coatings in industrial applications scientists of the institute of material science welded these hard material alloys onto a base material, in this case c22 8, to create a hybrid workpiece scientists of the institut fur integrierte produktion hannover have shown that these hybrid workpieces can be formed without defects (e g detachment of the coating) by cross wedge rolling after forming, the properties of the coatings are retained or in some cases even improved (e g the transition zone between base material and coating) by adjustments in the welding process, it was possible to apply the 100cr6 ro",
            "sequence": "[CLS] digital city twin review application area [SEP] cross wedge rolling of pta welded hybrid steel billets with rolling bearing steel and hard material coatings within the collaborative research centre 1153 \u201ctailored forming\u201c a process chain for the manufacturing of hybrid high performance components is developed exemplary process steps consist of deposit welding of high performance steel on low cost steel, pre shaping by cross wedge rolling and finishing by milling hard material coatings such as stellite 6 or delcrome 253 are used as wear or corrosion protection coatings in industrial applications scientists of the institute of material science welded these hard material alloys onto a base material, in this case c22 8, to create a hybrid workpiece scientists of the institut fur integrierte produktion hannover have shown that these hybrid workpieces can be formed without defects (e g detachment of the coating) by cross wedge rolling after forming, the properties of the coatings are retained or in some cases even improved (e g the transition zone between base material and coating) by adjustments in the welding process, it was possible to apply the 100cr6 rolling bearing steel, as of now declared as non weldable, on the low cost steel c22 8 100cr6 was formed afterwards in its hybrid bonding state with c22 8 by cross wedge rolling, thus a component integrated bearing seat was produced even after welding and forming, the rolling bearing steel coating could still be quench hardened to a hardness of over 60 hrc this paper shows the potential of forming hybrid billets to tailored parts since industrially available standard materials can be used for hard material coatings by this approach, even though they are not weldable by conventional methods, it is not necessary to use expensive, for welding designed materials to implement a hybrid component concept within the collaborative research centre 1153 \u201ctailored forming\u201c a process chain for the manufacturing of hybrid high performance components is developed exemplary process steps consist of deposit welding of high performance steel on low cost steel, pre shaping by cross wedge rolling and finishing by milling hard material coatings such as stellite 6 or delcrome 253 are used as wear or corrosion protection coatings in industrial applications scientists of the institute of material science welded these hard material alloys onto a base material, in this case c22 8, to create a hybrid workpiece scientists of the institut fur integrierte produktion hannover have shown that these hybrid workpieces can be formed without defects (e g detachment of the coating) by cross wedge rolling after forming, the properties of the coatings are retained or in some cases even improved (e g the transition zone between base material and coating) by adjustments in the welding process, it was possible to apply the 100cr6 ro [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R137654",
                "label": "Mechanical Process Engineering"
            }
        },
        {
            "instance_id": "R161736xR74688",
            "template_id": "R161736",
            "correct_template_id": null,
            "paper_id": "R74688",
            "premise": "xray laser applications paper type research objective has system qualities",
            "hypothesis": "video as a by product of digital prototyping: capturing the dynamic aspect of interaction requirements engineering provides several practices to analyze how a user wants to interact with a future software mockups, prototypes, and scenarios are suitable to understand usability issues and user requirements early nevertheless, users are often dissatisfied with the usability of a resulting software apparently, previously explored information was lost or no longer accessible during the development phase scenarios are one effective practice to describe behavior however, they are commonly notated in natural language which is often improper to capture and communicate interaction knowledge comprehensible to developers and users the dynamic aspect of interaction is lost if only static descriptions are used digital prototyping enables the creation of interactive prototypes by adding responsive controls to hand or digitally drawn mockups we propose to capture the events of these controls to obtain a representation of the interaction from this data, we generate videos, which demonstrate interaction sequences, as additional support for textual scenarios variants of scenarios can be created by modifying the captured event sequences and mockups any change is unproblematic since videos only need to be regenerated thus, we achieve video as a by product of digital prototyping this reduces the effort compared to video recording such as screencasts a first evaluation showed that such a generated video supports a faster understanding of a textual scenario compared to static mockups",
            "sequence": "[CLS] xray laser applications paper type research objective has system qualities [SEP] video as a by product of digital prototyping: capturing the dynamic aspect of interaction requirements engineering provides several practices to analyze how a user wants to interact with a future software mockups, prototypes, and scenarios are suitable to understand usability issues and user requirements early nevertheless, users are often dissatisfied with the usability of a resulting software apparently, previously explored information was lost or no longer accessible during the development phase scenarios are one effective practice to describe behavior however, they are commonly notated in natural language which is often improper to capture and communicate interaction knowledge comprehensible to developers and users the dynamic aspect of interaction is lost if only static descriptions are used digital prototyping enables the creation of interactive prototypes by adding responsive controls to hand or digitally drawn mockups we propose to capture the events of these controls to obtain a representation of the interaction from this data, we generate videos, which demonstrate interaction sequences, as additional support for textual scenarios variants of scenarios can be created by modifying the captured event sequences and mockups any change is unproblematic since videos only need to be regenerated thus, we achieve video as a by product of digital prototyping this reduces the effort compared to video recording such as screencasts a first evaluation showed that such a generated video supports a faster understanding of a textual scenario compared to static mockups [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R186491xR111441",
            "template_id": "R186491",
            "correct_template_id": null,
            "paper_id": "R111441",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "crowd out the competition myerp is a fictional developer of an enterprise resource planning (erp) system driven by the competition, they face the challenge of losing market share if they fail to de ploy a software as a service (saas) erp system to the european market quickly, but with high quality product this also means that the requirements engineering (re) activities will have to be performed efficiently and provide solid results an additional problem they face is that their (potential) stakeholders are phys ically distributed, it makes sense to consider them a \"crowd\" this competition paper suggests a crowd based re approach that first identifies the crowd, then collects and analyzes their feedback to derive wishes and needs, and validate the results through prototyping for this, techniques are introduced that have so far been rarely employed within re, but more \"traditional\" re techniques, will also be integrated and/or adapted to attain the best possible result in the case of myerp",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] crowd out the competition myerp is a fictional developer of an enterprise resource planning (erp) system driven by the competition, they face the challenge of losing market share if they fail to de ploy a software as a service (saas) erp system to the european market quickly, but with high quality product this also means that the requirements engineering (re) activities will have to be performed efficiently and provide solid results an additional problem they face is that their (potential) stakeholders are phys ically distributed, it makes sense to consider them a \"crowd\" this competition paper suggests a crowd based re approach that first identifies the crowd, then collects and analyzes their feedback to derive wishes and needs, and validate the results through prototyping for this, techniques are introduced that have so far been rarely employed within re, but more \"traditional\" re techniques, will also be integrated and/or adapted to attain the best possible result in the case of myerp [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R155844xR137413",
            "template_id": "R155844",
            "correct_template_id": null,
            "paper_id": "R137413",
            "premise": "photocatalysts wavelength of maximum absorption energy band gap photocatalyst wavelength of maximum emission emission lifetime ground state oxidation potential ground state reduction potential excited state oxidation potential excited state reduction potential",
            "hypothesis": "inactivation of gram positive biofilms by low temperature plasma jet at atmospheric pressure this work is devoted to the evaluation of the efficiency of a new low temperature plasma jet driven in ambient air by a dc corona discharge to inactivate adherent cells and biofilms of gram positive bacteria the selected microorganisms were lactic acid bacteria, a weissella confusa strain which has the particularity to excrete a polysaccharide polymer (dextran) when sucrose is present both adherent cells and biofilms were treated with the low temperature plasma jet for different exposure times the antimicrobial efficiency of the plasma was tested against adherent cells and 48 h old biofilms grown with or without sucrose bacterial survival was estimated using both colony forming unit counts and fluorescence based assays for bacterial cell viability the experiments show the ability of the low temperature plasma jet at atmospheric pressure to inactivate the bacteria an increased resistance of bacteria embedded within biofilms is clearly observed the resistance is also significantly higher with biofilm in the presence of sucrose, which indicates that dextran could play a protective role",
            "sequence": "[CLS] photocatalysts wavelength of maximum absorption energy band gap photocatalyst wavelength of maximum emission emission lifetime ground state oxidation potential ground state reduction potential excited state oxidation potential excited state reduction potential [SEP] inactivation of gram positive biofilms by low temperature plasma jet at atmospheric pressure this work is devoted to the evaluation of the efficiency of a new low temperature plasma jet driven in ambient air by a dc corona discharge to inactivate adherent cells and biofilms of gram positive bacteria the selected microorganisms were lactic acid bacteria, a weissella confusa strain which has the particularity to excrete a polysaccharide polymer (dextran) when sucrose is present both adherent cells and biofilms were treated with the low temperature plasma jet for different exposure times the antimicrobial efficiency of the plasma was tested against adherent cells and 48 h old biofilms grown with or without sucrose bacterial survival was estimated using both colony forming unit counts and fluorescence based assays for bacterial cell viability the experiments show the ability of the low temperature plasma jet at atmospheric pressure to inactivate the bacteria an increased resistance of bacteria embedded within biofilms is clearly observed the resistance is also significantly higher with biofilm in the presence of sucrose, which indicates that dextran could play a protective role [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R114008",
                "label": "Applied Physics"
            }
        },
        {
            "instance_id": "R184022xR194237",
            "template_id": "R184022",
            "correct_template_id": null,
            "paper_id": "R194237",
            "premise": "xray spectroscopy paper type research objective has system qualities",
            "hypothesis": "do we really know what we are building? raising awareness of potential sustainability effects of software systems in requirements engineering integrating novel software systems in our society, economy, and environment can have far reaching effects as a result, software systems should be designed in such a way as to maintain or improve the sustainability of the socio technical system of their destination however, a paradigm shift is required to raise awareness of software professionals on the potential sustainability effects of software systems while requirements engineering is considered the key to driving this change, requirements engineers lack the knowledge, experience and methodological support for doing so this paper presents a question based framework for raising awareness of the potential effects of software systems on sustainability, as the first step towards enabling the required paradigm shift a feasibility study of the framework was carried out with two groups of computer science students the results of the study indicate that the framework helps enable discussions about potential effects that software systems could have on sustainability",
            "sequence": "[CLS] xray spectroscopy paper type research objective has system qualities [SEP] do we really know what we are building? raising awareness of potential sustainability effects of software systems in requirements engineering integrating novel software systems in our society, economy, and environment can have far reaching effects as a result, software systems should be designed in such a way as to maintain or improve the sustainability of the socio technical system of their destination however, a paradigm shift is required to raise awareness of software professionals on the potential sustainability effects of software systems while requirements engineering is considered the key to driving this change, requirements engineers lack the knowledge, experience and methodological support for doing so this paper presents a question based framework for raising awareness of the potential effects of software systems on sustainability, as the first step towards enabling the required paradigm shift a feasibility study of the framework was carried out with two groups of computer science students the results of the study indicate that the framework helps enable discussions about potential effects that software systems could have on sustainability [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R40006xR76341",
            "template_id": "R40006",
            "correct_template_id": null,
            "paper_id": "R76341",
            "premise": "basic reproduction number estimate time period basic reproduction number location",
            "hypothesis": "the crowd in requirements engineering: the landscape and challenges crowd based requirements engineering (crowdre) could significantly change re performing re activities such as elicitation with the crowd of stakeholders turns re into a participatory effort, leads to more accurate requirements, and ultimately boosts software quality although any stakeholder in the crowd can contribute, crowdre emphasizes one stakeholder group whose role is often trivialized: users crowdre empowers the management of requirements, such as their prioritization and segmentation, in a dynamic, evolved style through collecting and harnessing a continuous flow of user feedback and monitoring data on the usage context to analyze the large amount of data obtained from the crowd, automated approaches are key this article presents current research topics in crowdre; discusses the benefits, challenges, and lessons learned from projects and experiments; and assesses how to apply the methods and tools in industrial contexts this article is part of a special issue on crowdsourcing for software engineering",
            "sequence": "[CLS] basic reproduction number estimate time period basic reproduction number location [SEP] the crowd in requirements engineering: the landscape and challenges crowd based requirements engineering (crowdre) could significantly change re performing re activities such as elicitation with the crowd of stakeholders turns re into a participatory effort, leads to more accurate requirements, and ultimately boosts software quality although any stakeholder in the crowd can contribute, crowdre emphasizes one stakeholder group whose role is often trivialized: users crowdre empowers the management of requirements, such as their prioritization and segmentation, in a dynamic, evolved style through collecting and harnessing a continuous flow of user feedback and monitoring data on the usage context to analyze the large amount of data obtained from the crowd, automated approaches are key this article presents current research topics in crowdre; discusses the benefits, challenges, and lessons learned from projects and experiments; and assesses how to apply the methods and tools in industrial contexts this article is part of a special issue on crowdsourcing for software engineering [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R152828xR25129",
            "template_id": "R152828",
            "correct_template_id": null,
            "paper_id": "R25129",
            "premise": "xray laser pumped has system qualities",
            "hypothesis": "a new driving assistant for automobiles this paper introduces an inexpensive car security system which addresses the needs for broader area coverage around the vehicle and stronger indication signals to drivers the new driving assistant features simple ultrasonic based sensors, implemented at the two front corners and the two blind spots of the vehicle in order to report the close by objects to the driver, the system employs a multitude of feedback devices, including tactile vibrators attached to the steering wheel, audible signals, and an led display mounted on the dash board the sensor system and the feedback devices are controlled in real time by microcontrollers over a wireless communication network the final prototype system was installed and tested on a ride on toy car",
            "sequence": "[CLS] xray laser pumped has system qualities [SEP] a new driving assistant for automobiles this paper introduces an inexpensive car security system which addresses the needs for broader area coverage around the vehicle and stronger indication signals to drivers the new driving assistant features simple ultrasonic based sensors, implemented at the two front corners and the two blind spots of the vehicle in order to report the close by objects to the driver, the system employs a multitude of feedback devices, including tactile vibrators attached to the steering wheel, audible signals, and an led display mounted on the dash board the sensor system and the feedback devices are controlled in real time by microcontrollers over a wireless communication network the final prototype system was installed and tested on a ride on toy car [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R11",
                "label": "Science"
            }
        },
        {
            "instance_id": "R161736xR160395",
            "template_id": "R161736",
            "correct_template_id": null,
            "paper_id": "R160395",
            "premise": "xray laser applications paper type research objective has system qualities",
            "hypothesis": "building and exploiting a digital twin for the management of drinking water distribution networks abstract digital twins (dts) are starting to be exploited to improve the management of water distribution systems (wdss) and, in the future, they will be crucial for decision making in this paper, the authors propose several requirements that a dt of a water distribution system should accomplish developing a dt is a challenge, and a continuous process of adjustments and learning is required due to the advantages of having a dt of the wds always available, during the last years a strategy to build and maintain a dt of the water distribution network of valencia (spain) and its metropolitan area (1 6 million inhabitants) was developed this is one of the first dts built of a water utility, being currently in operation the great benefits of their use in the daily operation of the system ensure that they will begin to be usual in the most advanced smart cities",
            "sequence": "[CLS] xray laser applications paper type research objective has system qualities [SEP] building and exploiting a digital twin for the management of drinking water distribution networks abstract digital twins (dts) are starting to be exploited to improve the management of water distribution systems (wdss) and, in the future, they will be crucial for decision making in this paper, the authors propose several requirements that a dt of a water distribution system should accomplish developing a dt is a challenge, and a continuous process of adjustments and learning is required due to the advantages of having a dt of the wds always available, during the last years a strategy to build and maintain a dt of the water distribution network of valencia (spain) and its metropolitan area (1 6 million inhabitants) was developed this is one of the first dts built of a water utility, being currently in operation the great benefits of their use in the daily operation of the system ensure that they will begin to be usual in the most advanced smart cities [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            }
        },
        {
            "instance_id": "R182248xR160319",
            "template_id": "R182248",
            "correct_template_id": null,
            "paper_id": "R160319",
            "premise": "food photo dataset year name annotation number of images task number of classes acquisition type",
            "hypothesis": "the circular economy: a new development strategy in china activities over the past several years, however, clearly show that ce is emerging as an economic strategy rather than a purely environmental strategy the major objective of the government is to promote the sustainable development of economy and society, while it also helps to achieve sustainable environmental protection powers, increasing the wealth of the population and providing employment and business opportunities the rapid economic growth, however, has engendered serious natural resource depletion and environmental pollution, and the continuing increase of population has exacerbated this situation greatly recent research has pointed out that growth of the gross domestic product (gdp) in china has significantly reduced the opportunities of future generations to enjoy natural and environmental resources 1 the central government promised in 2002 to build a prosperous society in a comprehensive way by 2020 by then, gdp per capita is anticipated to reach u s $3,000 and the total gdp to quadruple obviously, it is unrealistic for china to expect to realize this ambitious objective in terms of natural resource use if it continues its current development pathway, with population increasing to 1 45 billion in 2020 (qu 2004), low productivity, and the absence of eco efficiency",
            "sequence": "[CLS] food photo dataset year name annotation number of images task number of classes acquisition type [SEP] the circular economy: a new development strategy in china activities over the past several years, however, clearly show that ce is emerging as an economic strategy rather than a purely environmental strategy the major objective of the government is to promote the sustainable development of economy and society, while it also helps to achieve sustainable environmental protection powers, increasing the wealth of the population and providing employment and business opportunities the rapid economic growth, however, has engendered serious natural resource depletion and environmental pollution, and the continuing increase of population has exacerbated this situation greatly recent research has pointed out that growth of the gross domestic product (gdp) in china has significantly reduced the opportunities of future generations to enjoy natural and environmental resources 1 the central government promised in 2002 to build a prosperous society in a comprehensive way by 2020 by then, gdp per capita is anticipated to reach u s $3,000 and the total gdp to quadruple obviously, it is unrealistic for china to expect to realize this ambitious objective in terms of natural resource use if it continues its current development pathway, with population increasing to 1 45 billion in 2020 (qu 2004), low productivity, and the absence of eco efficiency [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            }
        },
        {
            "instance_id": "R182248xR193108",
            "template_id": "R182248",
            "correct_template_id": null,
            "paper_id": "R193108",
            "premise": "food photo dataset year name annotation number of images task number of classes acquisition type",
            "hypothesis": "perspectives on regulatory compliance in software engineering \"compliance reviews within a software organization are internal attempts to verify regulatory and security requirements during product development before its release however, these reviews are not enough to adequately assess and address regulatory and security requirements throughout a software's development lifecycle we believe requirements engineers can benefit from an improved understanding of how software practitioners treat and perceive compliance requirements this paper describes an interview study seeking to understand how regulatory and security standard requirements are addressed, how burdensome they may be for businesses, and how our participants perceived them in the software development lifecycle we interviewed 15 software practitioners from 13 organizations with different roles in the software development process and working in various industry domains, including big tech, healthcare, data analysis, finance, and small businesses our findings suggest that, for our participants, the software release process is the ultimate focus for regulatory and security compliance reviews also, most participants suggested that having a defined process for addressing compliance requirements was freeing rather than burdensome finally, participants generally saw compliance requirements as an investment for both employees and customers these findings may be unintuitive, and we discuss seven lessons this work may hold for requirements engineering \"",
            "sequence": "[CLS] food photo dataset year name annotation number of images task number of classes acquisition type [SEP] perspectives on regulatory compliance in software engineering \"compliance reviews within a software organization are internal attempts to verify regulatory and security requirements during product development before its release however, these reviews are not enough to adequately assess and address regulatory and security requirements throughout a software's development lifecycle we believe requirements engineers can benefit from an improved understanding of how software practitioners treat and perceive compliance requirements this paper describes an interview study seeking to understand how regulatory and security standard requirements are addressed, how burdensome they may be for businesses, and how our participants perceived them in the software development lifecycle we interviewed 15 software practitioners from 13 organizations with different roles in the software development process and working in various industry domains, including big tech, healthcare, data analysis, finance, and small businesses our findings suggest that, for our participants, the software release process is the ultimate focus for regulatory and security compliance reviews also, most participants suggested that having a defined process for addressing compliance requirements was freeing rather than burdensome finally, participants generally saw compliance requirements as an investment for both employees and customers these findings may be unintuitive, and we discuss seven lessons this work may hold for requirements engineering \" [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R178304xR193473",
            "template_id": "R178304",
            "correct_template_id": null,
            "paper_id": "R193473",
            "premise": "dataset inter annotator agreement alternatename assesses creator description disambiguatingdescription encoding exampleofwork genre inlanguage isbasedon license name sameas size sourceorganization url version",
            "hypothesis": "continual human value analysis in software development: a goal model based approach software failures that demonstrate violations of human values can result in financial losses, reputation damages and social implications therefore, integrating human values into software is vital to satisfy stakeholder needs however, developing methodological approaches that allow systematic integration of human values throughout the software development life cycle is an open challenge this paper proposes the continual value(s) assessment (cva) framework that uses extended goal and feature modeling techniques to support systematic integration, tracing and evaluation of human values in software systems the cva framework prescribes (i) brainstorming of value implications of system features based on conventional system artefacts and (ii) the expansion of the existing set of system features to better serve stakeholder values expectations in a pilot study, we use an emergency alarm system for the elderly to demonstrate the feasibility of the framework we further discuss the challenges we faced while applying the framework and present the lessons learned from the pilot study",
            "sequence": "[CLS] dataset inter annotator agreement alternatename assesses creator description disambiguatingdescription encoding exampleofwork genre inlanguage isbasedon license name sameas size sourceorganization url version [SEP] continual human value analysis in software development: a goal model based approach software failures that demonstrate violations of human values can result in financial losses, reputation damages and social implications therefore, integrating human values into software is vital to satisfy stakeholder needs however, developing methodological approaches that allow systematic integration of human values throughout the software development life cycle is an open challenge this paper proposes the continual value(s) assessment (cva) framework that uses extended goal and feature modeling techniques to support systematic integration, tracing and evaluation of human values in software systems the cva framework prescribes (i) brainstorming of value implications of system features based on conventional system artefacts and (ii) the expansion of the existing set of system features to better serve stakeholder values expectations in a pilot study, we use an emergency alarm system for the elderly to demonstrate the feasibility of the framework we further discuss the challenges we faced while applying the framework and present the lessons learned from the pilot study [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R161545xR142218",
            "template_id": "R161545",
            "correct_template_id": null,
            "paper_id": "R142218",
            "premise": "recycling methods method recycling type has property",
            "hypothesis": "scanning thermal imaging of microelectronic circuits with a fluorescent nanoprobe we have developed a scanning thermal imaging method that uses a fluorescent particle as a temperature sensor the particle, which contains rare earth ions, is glued at the end of an atomic force microscope tip and allows the determination of the temperature of its surrounding medium the measurement is performed by comparing the relative integrated intensity of two fluorescence lines that have a well defined temperature dependence as an example of application, we show the temperature map on an operating complementary metal oxide semiconductor integrated circuit",
            "sequence": "[CLS] recycling methods method recycling type has property [SEP] scanning thermal imaging of microelectronic circuits with a fluorescent nanoprobe we have developed a scanning thermal imaging method that uses a fluorescent particle as a temperature sensor the particle, which contains rare earth ions, is glued at the end of an atomic force microscope tip and allows the determination of the temperature of its surrounding medium the measurement is performed by comparing the relative integrated intensity of two fluorescence lines that have a well defined temperature dependence as an example of application, we show the temperature map on an operating complementary metal oxide semiconductor integrated circuit [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            }
        },
        {
            "instance_id": "R161736xR178198",
            "template_id": "R161736",
            "correct_template_id": null,
            "paper_id": "R178198",
            "premise": "xray laser applications paper type research objective has system qualities",
            "hypothesis": "a high level approach towards end user development in the iot \"programming environments for end user personalization in the internet of things (iot) are becoming increasingly common they allow users to define simple iot applications, i e , connections between different iot devices and services unfortunately, the adopted representation models are highly technology dependent, e g , they often categorize devices and services by manufacturer or brand such an approach is not suitable to face the expected growth of the iot, nor it allows to adapt to yet undiscovered iot services in this paper, we present a generic and technology independent representation for iot end user programming environments the aim of this ``high level' representation is to allow end users to create abstract iot applications that adapt to different contextual situations we preliminary evaluated the representation by comparing it with the one used by existing programming environments in a user study with 10 participants results show that the representation is understandable, and it allows users to create iot applications more correctly and quickly \"",
            "sequence": "[CLS] xray laser applications paper type research objective has system qualities [SEP] a high level approach towards end user development in the iot \"programming environments for end user personalization in the internet of things (iot) are becoming increasingly common they allow users to define simple iot applications, i e , connections between different iot devices and services unfortunately, the adopted representation models are highly technology dependent, e g , they often categorize devices and services by manufacturer or brand such an approach is not suitable to face the expected growth of the iot, nor it allows to adapt to yet undiscovered iot services in this paper, we present a generic and technology independent representation for iot end user programming environments the aim of this ``high level' representation is to allow end users to create abstract iot applications that adapt to different contextual situations we preliminary evaluated the representation by comparing it with the one used by existing programming environments in a user study with 10 participants results show that the representation is understandable, and it allows users to create iot applications more correctly and quickly \" [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R141823",
                "label": "Semantic Web"
            }
        },
        {
            "instance_id": "R187648xR159484",
            "template_id": "R187648",
            "correct_template_id": null,
            "paper_id": "R159484",
            "premise": "oco for control research problem theoretical guarantees has application in data driven approach has constraints type of considered system measurement noise process noise",
            "hypothesis": "smart city dvelopment with digital twin technology growing urban areas are major consumers of natural resources, energy and raw materials understanding cities\u00b4 urban metabolism is salient when developing sustainable and resilient cities this paper addresses concepts of smart city and digital twin technology as means to foster more sustainable urban development smart city has globally been well adopted concept in urban development with smart city development cities aim to optimize overall performance of the city, its infrastructures, processes and services, but also to improve socio economic wellbeing dynamic digital twins are constituted to form real time connectivity between virtual and physical objects digital twin combines virtual objects to its physical counterparts this conceptual paper provides additionally examples from dynamic digital twin platforms and digital twin of helsinki, finland",
            "sequence": "[CLS] oco for control research problem theoretical guarantees has application in data driven approach has constraints type of considered system measurement noise process noise [SEP] smart city dvelopment with digital twin technology growing urban areas are major consumers of natural resources, energy and raw materials understanding cities\u00b4 urban metabolism is salient when developing sustainable and resilient cities this paper addresses concepts of smart city and digital twin technology as means to foster more sustainable urban development smart city has globally been well adopted concept in urban development with smart city development cities aim to optimize overall performance of the city, its infrastructures, processes and services, but also to improve socio economic wellbeing dynamic digital twins are constituted to form real time connectivity between virtual and physical objects digital twin combines virtual objects to its physical counterparts this conceptual paper provides additionally examples from dynamic digital twin platforms and digital twin of helsinki, finland [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            }
        },
        {
            "instance_id": "R194212xR140059",
            "template_id": "R194212",
            "correct_template_id": null,
            "paper_id": "R140059",
            "premise": "ontology construction method methodology tool has uri data source research problem evaluation process name people involved serialization language",
            "hypothesis": "open data hackathons: an innovative strategy to enhance entrepreneurial intention \\n purpose \\n in terms of entrepreneurship, open data benefits include economic growth, innovation, empowerment and new or improved products and services hackathons encourage the development of new applications using open data and the creation of startups based on these applications researchers focus on factors that affect nascent entrepreneurs\u2019 decision to create a startup but researches in the field of open data hackathons have not been fully investigated yet this paper aims to suggest a model that incorporates factors that affect the decision of establishing a startup by developers who have participated in open data hackathons \\n \\n \\n design/methodology/approach \\n in total, 70 papers were examined and analyzed using a three phased literature review methodology, which was suggested by webster and watson (2002) these surveys investigated several factors that affect a nascent entrepreneur to create a startup \\n \\n \\n findings \\n eventually, by identifying the motivations for developers to participate in a hackathon, and understanding the benefits of the use of open data, researchers will be able to elaborate the proposed model and evaluate if the contest has contributed to the decision of establish a startup and what factors affect the decision to establish a startup apply to open data developers, and if the participants of the contest agree with these factors \\n \\n \\n originality/value \\n the paper expands the scope of open data research on entrepreneurship field, stating the need for more research to be conducted regarding the open data in entrepreneurship through hackathons \\n",
            "sequence": "[CLS] ontology construction method methodology tool has uri data source research problem evaluation process name people involved serialization language [SEP] open data hackathons: an innovative strategy to enhance entrepreneurial intention \\n purpose \\n in terms of entrepreneurship, open data benefits include economic growth, innovation, empowerment and new or improved products and services hackathons encourage the development of new applications using open data and the creation of startups based on these applications researchers focus on factors that affect nascent entrepreneurs\u2019 decision to create a startup but researches in the field of open data hackathons have not been fully investigated yet this paper aims to suggest a model that incorporates factors that affect the decision of establishing a startup by developers who have participated in open data hackathons \\n \\n \\n design/methodology/approach \\n in total, 70 papers were examined and analyzed using a three phased literature review methodology, which was suggested by webster and watson (2002) these surveys investigated several factors that affect a nascent entrepreneur to create a startup \\n \\n \\n findings \\n eventually, by identifying the motivations for developers to participate in a hackathon, and understanding the benefits of the use of open data, researchers will be able to elaborate the proposed model and evaluate if the contest has contributed to the decision of establish a startup and what factors affect the decision to establish a startup apply to open data developers, and if the participants of the contest agree with these factors \\n \\n \\n originality/value \\n the paper expands the scope of open data research on entrepreneurship field, stating the need for more research to be conducted regarding the open data in entrepreneurship through hackathons \\n [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            }
        },
        {
            "instance_id": "R194212xR176039",
            "template_id": "R194212",
            "correct_template_id": null,
            "paper_id": "R176039",
            "premise": "ontology construction method methodology tool has uri data source research problem evaluation process name people involved serialization language",
            "hypothesis": "attention is all you need the dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder decoder configuration the best performing models also connect the encoder and decoder through an attention mechanism we propose a new simple network architecture, the transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train our model achieves 28 4 bleu on the wmt 2014 english to german translation task, improving over the existing best results, including ensembles by over 2 bleu on the wmt 2014 english to french translation task, our model establishes a new single model state of the art bleu score of 41 8 after training for 3 5 days on eight gpus, a small fraction of the training costs of the best models from the literature we show that the transformer generalizes well to other tasks by applying it successfully to english constituency parsing both with large and limited training data",
            "sequence": "[CLS] ontology construction method methodology tool has uri data source research problem evaluation process name people involved serialization language [SEP] attention is all you need the dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder decoder configuration the best performing models also connect the encoder and decoder through an attention mechanism we propose a new simple network architecture, the transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train our model achieves 28 4 bleu on the wmt 2014 english to german translation task, improving over the existing best results, including ensembles by over 2 bleu on the wmt 2014 english to french translation task, our model establishes a new single model state of the art bleu score of 41 8 after training for 3 5 days on eight gpus, a small fraction of the training costs of the best models from the literature we show that the transformer generalizes well to other tasks by applying it successfully to english constituency parsing both with large and limited training data [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R112125",
                "label": "Machine Learning"
            }
        },
        {
            "instance_id": "R184022xR141661",
            "template_id": "R184022",
            "correct_template_id": null,
            "paper_id": "R141661",
            "premise": "xray spectroscopy paper type research objective has system qualities",
            "hypothesis": "fluorescent n doped carbon dots as in vitro and in vivo nanothermometer the fluorescent n doped carbon dots (n cds) obtained from c3n4 emit strong blue fluorescence, which is stable with different ionic strengths and time the fluorescence intensity of n cds decreases with the temperature increasing, while it can recover to the initial one with the temperature decreasing it is an accurate linear response of fluorescence intensity to temperature, which may be attributed to the synergistic effect of abundant oxygen containing functional groups and hydrogen bonds further experiments also demonstrate that n cds can serve as effective in vitro and in vivo fluorescence based nanothermometer",
            "sequence": "[CLS] xray spectroscopy paper type research objective has system qualities [SEP] fluorescent n doped carbon dots as in vitro and in vivo nanothermometer the fluorescent n doped carbon dots (n cds) obtained from c3n4 emit strong blue fluorescence, which is stable with different ionic strengths and time the fluorescence intensity of n cds decreases with the temperature increasing, while it can recover to the initial one with the temperature decreasing it is an accurate linear response of fluorescence intensity to temperature, which may be attributed to the synergistic effect of abundant oxygen containing functional groups and hydrogen bonds further experiments also demonstrate that n cds can serve as effective in vitro and in vivo fluorescence based nanothermometer [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            }
        },
        {
            "instance_id": "R146876xR140059",
            "template_id": "R146876",
            "correct_template_id": null,
            "paper_id": "R140059",
            "premise": "organic solar cells mobility donor acceptor lumo homo energy band gap open circuit voltage, voc short circuit current density, jsc fill factor, ff power conversion efficiency",
            "hypothesis": "open data hackathons: an innovative strategy to enhance entrepreneurial intention \\n purpose \\n in terms of entrepreneurship, open data benefits include economic growth, innovation, empowerment and new or improved products and services hackathons encourage the development of new applications using open data and the creation of startups based on these applications researchers focus on factors that affect nascent entrepreneurs\u2019 decision to create a startup but researches in the field of open data hackathons have not been fully investigated yet this paper aims to suggest a model that incorporates factors that affect the decision of establishing a startup by developers who have participated in open data hackathons \\n \\n \\n design/methodology/approach \\n in total, 70 papers were examined and analyzed using a three phased literature review methodology, which was suggested by webster and watson (2002) these surveys investigated several factors that affect a nascent entrepreneur to create a startup \\n \\n \\n findings \\n eventually, by identifying the motivations for developers to participate in a hackathon, and understanding the benefits of the use of open data, researchers will be able to elaborate the proposed model and evaluate if the contest has contributed to the decision of establish a startup and what factors affect the decision to establish a startup apply to open data developers, and if the participants of the contest agree with these factors \\n \\n \\n originality/value \\n the paper expands the scope of open data research on entrepreneurship field, stating the need for more research to be conducted regarding the open data in entrepreneurship through hackathons \\n",
            "sequence": "[CLS] organic solar cells mobility donor acceptor lumo homo energy band gap open circuit voltage, voc short circuit current density, jsc fill factor, ff power conversion efficiency [SEP] open data hackathons: an innovative strategy to enhance entrepreneurial intention \\n purpose \\n in terms of entrepreneurship, open data benefits include economic growth, innovation, empowerment and new or improved products and services hackathons encourage the development of new applications using open data and the creation of startups based on these applications researchers focus on factors that affect nascent entrepreneurs\u2019 decision to create a startup but researches in the field of open data hackathons have not been fully investigated yet this paper aims to suggest a model that incorporates factors that affect the decision of establishing a startup by developers who have participated in open data hackathons \\n \\n \\n design/methodology/approach \\n in total, 70 papers were examined and analyzed using a three phased literature review methodology, which was suggested by webster and watson (2002) these surveys investigated several factors that affect a nascent entrepreneur to create a startup \\n \\n \\n findings \\n eventually, by identifying the motivations for developers to participate in a hackathon, and understanding the benefits of the use of open data, researchers will be able to elaborate the proposed model and evaluate if the contest has contributed to the decision of establish a startup and what factors affect the decision to establish a startup apply to open data developers, and if the participants of the contest agree with these factors \\n \\n \\n originality/value \\n the paper expands the scope of open data research on entrepreneurship field, stating the need for more research to be conducted regarding the open data in entrepreneurship through hackathons \\n [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            }
        },
        {
            "instance_id": "R149061xR194610",
            "template_id": "R149061",
            "correct_template_id": null,
            "paper_id": "R194610",
            "premise": "biodiversity inventories with dna barcoding biogeographical region locus (genetics) higher number estimated species higher number estimated species (method) no of estimated species no of estimated species (method) lower number estimated species lower number estimated species (method) class (taxonomy biology) dna sequencing technology phylum (biology) no of potential undescribed species study location order (taxonomy biology) no of samples (sequences) studied taxonomic group (biology) number of identified species with current taxonomy",
            "hypothesis": "requirements classification with interpretable machine learning and dependency parsing \"requirements classification is a traditional application of machine learning (ml) to re that helps handle large requirements datasets a prime example of an re classification problem is the distinction between functional and non functional (quality) requirements state of the art classifiers build their effectiveness on a large set of word features like text n grams or pos n grams, which do not fully capture the essence of a requirement as a result, it is arduous for human analysts to interpret the classification results by exploring the classifier's inner workings we propose the use of more general linguistic features, such as dependency types, for the construction of interpretable ml classifiers for re through a feature engineering effort, in which we are assisted by modern introspection tools that reveal the hidden inner workings of ml classifiers, we derive a set of 17 linguistic features while classifiers that use our proposed features fit the training set slightly worse than those that use high dimensional feature sets, our approach performs generally better on validation datasets and it is more interpretable \"",
            "sequence": "[CLS] biodiversity inventories with dna barcoding biogeographical region locus (genetics) higher number estimated species higher number estimated species (method) no of estimated species no of estimated species (method) lower number estimated species lower number estimated species (method) class (taxonomy biology) dna sequencing technology phylum (biology) no of potential undescribed species study location order (taxonomy biology) no of samples (sequences) studied taxonomic group (biology) number of identified species with current taxonomy [SEP] requirements classification with interpretable machine learning and dependency parsing \"requirements classification is a traditional application of machine learning (ml) to re that helps handle large requirements datasets a prime example of an re classification problem is the distinction between functional and non functional (quality) requirements state of the art classifiers build their effectiveness on a large set of word features like text n grams or pos n grams, which do not fully capture the essence of a requirement as a result, it is arduous for human analysts to interpret the classification results by exploring the classifier's inner workings we propose the use of more general linguistic features, such as dependency types, for the construction of interpretable ml classifiers for re through a feature engineering effort, in which we are assisted by modern introspection tools that reveal the hidden inner workings of ml classifiers, we derive a set of 17 linguistic features while classifiers that use our proposed features fit the training set slightly worse than those that use high dimensional feature sets, our approach performs generally better on validation datasets and it is more interpretable \" [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R154390xR25097",
            "template_id": "R154390",
            "correct_template_id": null,
            "paper_id": "R25097",
            "premise": "lignin decomposition substrate catalyst has temperature value solvent pressure reactor conversion product",
            "hypothesis": "a retrospective look at pd projects w h i l e m o d e r n m e t h o d s f o r i n f o r m a t i o n s y s t e m d e v e l o p m e n t g e n e r a l l y a c c e p t t h a t u s e r s s h o u l d b e i n v o l v e d in s o m e w a y [15], t h e f o r m o f t h e i n v o l v e m e n t d i f f e r s c o n s i d e r a b l y m o s t l y , u s e r s a r e v i e w e d a s r e l a t i v e l y p a s s i v e s o u r c e s o f i n f o r m a t i o n , a n d t h e i n v o l v e m e n t is r e g a r d e d a s \" f u n c t i o n a l , \" in t h e s e n s e t h a t i t s h o u l d y i e l d b e t t e r s y s t e m r e q u i r e m e n t s a n d i n c r e a s e d a c c e p t a n c e b y u s e r s",
            "sequence": "[CLS] lignin decomposition substrate catalyst has temperature value solvent pressure reactor conversion product [SEP] a retrospective look at pd projects w h i l e m o d e r n m e t h o d s f o r i n f o r m a t i o n s y s t e m d e v e l o p m e n t g e n e r a l l y a c c e p t t h a t u s e r s s h o u l d b e i n v o l v e d in s o m e w a y [15], t h e f o r m o f t h e i n v o l v e m e n t d i f f e r s c o n s i d e r a b l y m o s t l y , u s e r s a r e v i e w e d a s r e l a t i v e l y p a s s i v e s o u r c e s o f i n f o r m a t i o n , a n d t h e i n v o l v e m e n t is r e g a r d e d a s \" f u n c t i o n a l , \" in t h e s e n s e t h a t i t s h o u l d y i e l d b e t t e r s y s t e m r e q u i r e m e n t s a n d i n c r e a s e d a c c e p t a n c e b y u s e r s [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R11",
                "label": "Science"
            }
        },
        {
            "instance_id": "R146876xR145509",
            "template_id": "R146876",
            "correct_template_id": null,
            "paper_id": "R145509",
            "premise": "organic solar cells mobility donor acceptor lumo homo energy band gap open circuit voltage, voc short circuit current density, jsc fill factor, ff power conversion efficiency",
            "hypothesis": "identifying canadian mosquito species through dna barcodes abstract a short fragment of mt dna from the cytochrome c oxidase 1 (co1) region was used to provide the first co1 barcodes for 37 species of canadian mosquitoes (diptera: culicidae) from the provinces ontario and new brunswick sequence variation was analysed in a 617\u2010bp fragment from the 5\u2032 end of the co1 region sequences of each mosquito species formed barcode clusters with tight cohesion that were usually clearly distinct from those of allied species co1 sequence divergences were, on average, nearly 20 times higher for congeneric species than for members of a species; divergences between congeneric species averaged 10 4% (range 0 2\u201317 2%), whereas those for conspecific individuals averaged 0 5% (range 0 0\u20133 9%)",
            "sequence": "[CLS] organic solar cells mobility donor acceptor lumo homo energy band gap open circuit voltage, voc short circuit current density, jsc fill factor, ff power conversion efficiency [SEP] identifying canadian mosquito species through dna barcodes abstract a short fragment of mt dna from the cytochrome c oxidase 1 (co1) region was used to provide the first co1 barcodes for 37 species of canadian mosquitoes (diptera: culicidae) from the provinces ontario and new brunswick sequence variation was analysed in a 617\u2010bp fragment from the 5\u2032 end of the co1 region sequences of each mosquito species formed barcode clusters with tight cohesion that were usually clearly distinct from those of allied species co1 sequence divergences were, on average, nearly 20 times higher for congeneric species than for members of a species; divergences between congeneric species averaged 10 4% (range 0 2\u201317 2%), whereas those for conspecific individuals averaged 0 5% (range 0 0\u20133 9%) [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            }
        },
        {
            "instance_id": "R52190xR43000",
            "template_id": "R52190",
            "correct_template_id": null,
            "paper_id": "R43000",
            "premise": "transient absorption spectroscopy has participating device has participating person has gaseous sample input has solution sample input has solid sample input has radiant energy input has output",
            "hypothesis": "pattern based model reuse using colored petri nets colored petri net (cpn) is a graphical modeling language for simulation and modeling and for verification of discrete event systems cpn allows developers to define a model in the form of reusable components a model component is an independent element, which is specified using a formalized description, can conform to a certain component standard, has a well defined interface, and encapsulates certain behavior modern components can help the developer reuse existing models according to their requirement as it reduces the cost and time of development composability is the capability to select and integrate various components to fulfill user requirements composability provides the means to achieve reusability where \"reuse\" is the ability of a simulation component to be reclaimed for various applications we propose a verification framework for developers to select and assemble cpn based components and verify their composability the goal of this paper is to provide a pattern which helps developer in making models of concurrent systems we present a case study of a restaurant model as proof of concept a verified composition affirms reuse of model components in a meaningful manner by satisfying given requirement specifications",
            "sequence": "[CLS] transient absorption spectroscopy has participating device has participating person has gaseous sample input has solution sample input has solid sample input has radiant energy input has output [SEP] pattern based model reuse using colored petri nets colored petri net (cpn) is a graphical modeling language for simulation and modeling and for verification of discrete event systems cpn allows developers to define a model in the form of reusable components a model component is an independent element, which is specified using a formalized description, can conform to a certain component standard, has a well defined interface, and encapsulates certain behavior modern components can help the developer reuse existing models according to their requirement as it reduces the cost and time of development composability is the capability to select and integrate various components to fulfill user requirements composability provides the means to achieve reusability where \"reuse\" is the ability of a simulation component to be reclaimed for various applications we propose a verification framework for developers to select and assemble cpn based components and verify their composability the goal of this paper is to provide a pattern which helps developer in making models of concurrent systems we present a case study of a restaurant model as proof of concept a verified composition affirms reuse of model components in a meaningful manner by satisfying given requirement specifications [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R149061xR137059",
            "template_id": "R149061",
            "correct_template_id": null,
            "paper_id": "R137059",
            "premise": "biodiversity inventories with dna barcoding biogeographical region locus (genetics) higher number estimated species higher number estimated species (method) no of estimated species no of estimated species (method) lower number estimated species lower number estimated species (method) class (taxonomy biology) dna sequencing technology phylum (biology) no of potential undescribed species study location order (taxonomy biology) no of samples (sequences) studied taxonomic group (biology) number of identified species with current taxonomy",
            "hypothesis": "nickel catalyzed cross coupling of aryl methyl ethers with aryl boronic esters palladiumand nickel catalyzed cross coupling reactions have been recognized as an indispensable tool for current organic synthesis among these reactions, suzuki\u2013miyaura coupling is, arguably, of the greatest practical importance of these methods because of the attractive features of organoboronic acids: widespread availability, stability to air and moisture, and low toxicity recently, tremendous progress has been made in the development of more elaborate catalyst systems that allow the couplings to be conducted at room temperature, to use unreactive chlorides, and to use alkyl electrophiles despite these significant advances, the electrophilic coupling partner for use in suzuki\u2013miyaura coupling remains limited, for the most part, to organic halides and sulfonates; although the use of less available electrophiles, including diazonium salts, ammonium salts, aryltriazene/bf3, [7c] azoles, and phosphonium salts has been reported aryl methyl ethers, which are as readily available as aryl halides, have never been used in the suzuki\u2013miyaura coupling reaction, except for the ruthenium catalyzed system, which requires a ligating group at the ortho position for the reaction to proceed herein, we describe a method for the nickel catalyzed cross coupling of aryl methyl ethers with boronic esters [eq (1)] the advantages of using aryl alkyl ethers in the metalcatalyzed cross coupling reaction have been documented by wenkert and dankwardt in the nickel catalyzed reaction with grignard reagents (i e , kumada\u2013tamao\u2013corriutype coupling) although functional group compatibility and availability of the starting grignard reagents for these initial methods are rather limited, these pioneering studies offer a starting point for the development of cross coupling reactions between aryl methyl ethers and organoboron reagents thus, we investigated the reaction of 2 methoxynaphthalene (1a) with organoboron compounds in the presence of a catalytic amount of [ni(cod)2] (cod= cycloocta 1,5 diene) and pcy3 (table 1) whereas attempts with boronic acid (table 1, entry 1) and borates (table 1, entries 2 and 3) were unsuccessful, cross coupling with boronic ester 2a furnished the product in modest yield (table 1, entry 4) although the",
            "sequence": "[CLS] biodiversity inventories with dna barcoding biogeographical region locus (genetics) higher number estimated species higher number estimated species (method) no of estimated species no of estimated species (method) lower number estimated species lower number estimated species (method) class (taxonomy biology) dna sequencing technology phylum (biology) no of potential undescribed species study location order (taxonomy biology) no of samples (sequences) studied taxonomic group (biology) number of identified species with current taxonomy [SEP] nickel catalyzed cross coupling of aryl methyl ethers with aryl boronic esters palladiumand nickel catalyzed cross coupling reactions have been recognized as an indispensable tool for current organic synthesis among these reactions, suzuki\u2013miyaura coupling is, arguably, of the greatest practical importance of these methods because of the attractive features of organoboronic acids: widespread availability, stability to air and moisture, and low toxicity recently, tremendous progress has been made in the development of more elaborate catalyst systems that allow the couplings to be conducted at room temperature, to use unreactive chlorides, and to use alkyl electrophiles despite these significant advances, the electrophilic coupling partner for use in suzuki\u2013miyaura coupling remains limited, for the most part, to organic halides and sulfonates; although the use of less available electrophiles, including diazonium salts, ammonium salts, aryltriazene/bf3, [7c] azoles, and phosphonium salts has been reported aryl methyl ethers, which are as readily available as aryl halides, have never been used in the suzuki\u2013miyaura coupling reaction, except for the ruthenium catalyzed system, which requires a ligating group at the ortho position for the reaction to proceed herein, we describe a method for the nickel catalyzed cross coupling of aryl methyl ethers with boronic esters [eq (1)] the advantages of using aryl alkyl ethers in the metalcatalyzed cross coupling reaction have been documented by wenkert and dankwardt in the nickel catalyzed reaction with grignard reagents (i e , kumada\u2013tamao\u2013corriutype coupling) although functional group compatibility and availability of the starting grignard reagents for these initial methods are rather limited, these pioneering studies offer a starting point for the development of cross coupling reactions between aryl methyl ethers and organoboron reagents thus, we investigated the reaction of 2 methoxynaphthalene (1a) with organoboron compounds in the presence of a catalytic amount of [ni(cod)2] (cod= cycloocta 1,5 diene) and pcy3 (table 1) whereas attempts with boronic acid (table 1, entry 1) and borates (table 1, entries 2 and 3) were unsuccessful, cross coupling with boronic ester 2a furnished the product in modest yield (table 1, entry 4) although the [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R129",
                "label": "Organic Chemistry"
            }
        },
        {
            "instance_id": "R146876xR140252",
            "template_id": "R146876",
            "correct_template_id": null,
            "paper_id": "R140252",
            "premise": "organic solar cells mobility donor acceptor lumo homo energy band gap open circuit voltage, voc short circuit current density, jsc fill factor, ff power conversion efficiency",
            "hypothesis": "species level para and polyphyly in dna barcode gene trees: strong operational bias in european lepidoptera the proliferation of dna data is revolutionizing all fields of systematic research dna barcode sequences, now available for millions of specimens and several hundred thousand species, are increasingly used in algorithmic species delimitations this is complicated by occasional incongruences between species and gene genealogies, as indicated by situations where conspecific individuals do not form a monophyletic cluster in a gene tree in two previous reviews, non monophyly has been reported as being common in mitochondrial dna gene trees we developed a novel web service \u201cmonophylizer\u201d to detect non monophyly in phylogenetic trees and used it to ascertain the incidence of species non monophyly in coi (a k a cox1) barcode sequence data from 4977 species and 41,583 specimens of european lepidoptera, the largest data set of dna barcodes analyzed from this regard particular attention was paid to accurate species identification to ensure data integrity we investigated the effects of tree building method, sampling effort, and other methodological issues, all of which can influence estimates of non monophyly we found a 12% incidence of non monophyly, a value significantly lower than that observed in previous studies neighbor joining (nj) and maximum likelihood (ml) methods yielded almost equal numbers of non monophyletic species, but 24 1% of these cases of non monophyly were only found by one of these methods non monophyletic species tend to show either low genetic distances to their nearest neighbors or exceptionally high levels of intraspecific variability cases of polyphyly in coi trees arising as a result of deep intraspecific divergence are negligible, as the detected cases reflected misidentifications or methodological errors taking into consideration variation in sampling effort, we estimate that the true incidence of non monophyly is \u223c23%, but with operational factors still being included within the operational factors, we separately assessed the frequency of taxonomic limitations (presence of overlooked cryptic and oversplit species) and identification uncertainties we observed that operational factors are potentially present in more than half (58 6%) of the detected cases of non monophyly furthermore, we observed that in about 20% of non monophyletic species and entangled species, the lineages involved are either allopatric or parapatric\u2014conditions where species delimitation is inherently subjective and particularly dependent on the species concept that has been adopted these observations suggest that species level non monophyly in coi gene trees is less common than previously supposed, with many cases reflecting misidentifications, the subjectivity of species delimitation or other operational factors",
            "sequence": "[CLS] organic solar cells mobility donor acceptor lumo homo energy band gap open circuit voltage, voc short circuit current density, jsc fill factor, ff power conversion efficiency [SEP] species level para and polyphyly in dna barcode gene trees: strong operational bias in european lepidoptera the proliferation of dna data is revolutionizing all fields of systematic research dna barcode sequences, now available for millions of specimens and several hundred thousand species, are increasingly used in algorithmic species delimitations this is complicated by occasional incongruences between species and gene genealogies, as indicated by situations where conspecific individuals do not form a monophyletic cluster in a gene tree in two previous reviews, non monophyly has been reported as being common in mitochondrial dna gene trees we developed a novel web service \u201cmonophylizer\u201d to detect non monophyly in phylogenetic trees and used it to ascertain the incidence of species non monophyly in coi (a k a cox1) barcode sequence data from 4977 species and 41,583 specimens of european lepidoptera, the largest data set of dna barcodes analyzed from this regard particular attention was paid to accurate species identification to ensure data integrity we investigated the effects of tree building method, sampling effort, and other methodological issues, all of which can influence estimates of non monophyly we found a 12% incidence of non monophyly, a value significantly lower than that observed in previous studies neighbor joining (nj) and maximum likelihood (ml) methods yielded almost equal numbers of non monophyletic species, but 24 1% of these cases of non monophyly were only found by one of these methods non monophyletic species tend to show either low genetic distances to their nearest neighbors or exceptionally high levels of intraspecific variability cases of polyphyly in coi trees arising as a result of deep intraspecific divergence are negligible, as the detected cases reflected misidentifications or methodological errors taking into consideration variation in sampling effort, we estimate that the true incidence of non monophyly is \u223c23%, but with operational factors still being included within the operational factors, we separately assessed the frequency of taxonomic limitations (presence of overlooked cryptic and oversplit species) and identification uncertainties we observed that operational factors are potentially present in more than half (58 6%) of the detected cases of non monophyly furthermore, we observed that in about 20% of non monophyletic species and entangled species, the lineages involved are either allopatric or parapatric\u2014conditions where species delimitation is inherently subjective and particularly dependent on the species concept that has been adopted these observations suggest that species level non monophyly in coi gene trees is less common than previously supposed, with many cases reflecting misidentifications, the subjectivity of species delimitation or other operational factors [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            }
        },
        {
            "instance_id": "R161736xR160381",
            "template_id": "R161736",
            "correct_template_id": null,
            "paper_id": "R160381",
            "premise": "xray laser applications paper type research objective has system qualities",
            "hypothesis": "roads infrastructure digital twin: a step toward smarter cities realization digital twin is a new concept that consists of creating an up to date virtual asset in cyberspace which mimics the original physical asset in most of its aspects, ultimately to monitor, analyze, test, and optimize the physical asset in this article, we investigate and discuss the use of the digital twin concept of the roads as a step toward realizing the dream of smart cities to this end, we propose the deployment of a digital twin box to the roads that is composed of a 360\u00b0 camera and a set of iot devices connected to a single onboard computer the digital twin box creates a digital twin of the physical road asset by constantly sending real time data to the edge/cloud, including the 360\u00b0 live stream, gps location, and measurements of the temperature and humidity this data will be used for realtime monitoring and other purposes by displaying the live stream via head mounted devices or using a 360\u00b0 web based player additionally, we perform an object detection process to extract all possible objects from the captured stream for some specific objects (person and vehicle), an identification module and a tracking module are employed to identify the corresponding objects and keep track of all video frames where these objects appeared the outcome of the latter step would be of utmost importance to many other services and domains such as national security to show the viability of the proposed solution, we have implemented and conducted real world experiments where we focus more on the detection and recognition processes the achieved results show the effectiveness of the proposed solution in creating a digital twin of the roads, a step forward to enable self driving vehicles as a crucial component of smart mobility, using the digital twin box",
            "sequence": "[CLS] xray laser applications paper type research objective has system qualities [SEP] roads infrastructure digital twin: a step toward smarter cities realization digital twin is a new concept that consists of creating an up to date virtual asset in cyberspace which mimics the original physical asset in most of its aspects, ultimately to monitor, analyze, test, and optimize the physical asset in this article, we investigate and discuss the use of the digital twin concept of the roads as a step toward realizing the dream of smart cities to this end, we propose the deployment of a digital twin box to the roads that is composed of a 360\u00b0 camera and a set of iot devices connected to a single onboard computer the digital twin box creates a digital twin of the physical road asset by constantly sending real time data to the edge/cloud, including the 360\u00b0 live stream, gps location, and measurements of the temperature and humidity this data will be used for realtime monitoring and other purposes by displaying the live stream via head mounted devices or using a 360\u00b0 web based player additionally, we perform an object detection process to extract all possible objects from the captured stream for some specific objects (person and vehicle), an identification module and a tracking module are employed to identify the corresponding objects and keep track of all video frames where these objects appeared the outcome of the latter step would be of utmost importance to many other services and domains such as national security to show the viability of the proposed solution, we have implemented and conducted real world experiments where we focus more on the detection and recognition processes the achieved results show the effectiveness of the proposed solution in creating a digital twin of the roads, a step forward to enable self driving vehicles as a crucial component of smart mobility, using the digital twin box [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            }
        },
        {
            "instance_id": "R194212xR142153",
            "template_id": "R194212",
            "correct_template_id": null,
            "paper_id": "R142153",
            "premise": "ontology construction method methodology tool has uri data source research problem evaluation process name people involved serialization language",
            "hypothesis": "cdse quantum dots for two photon fluorescence thermal imaging the technological development of quantum dots has ushered in a new era in fluorescence bioimaging, which was propelled with the advent of novel multiphoton fluorescence microscopes here, the potential use of cdse quantum dots has been evaluated as fluorescent nanothermometers for two photon fluorescence microscopy in addition to the enhancement in spatial resolution inherent to any multiphoton excitation processes, two photon (near infrared) excitation leads to a temperature sensitivity of the emission intensity much higher than that achieved under one photon (visible) excitation the peak emission wavelength is also temperature sensitive, providing an additional approach for thermal imaging, which is particularly interesting for systems where nanoparticles are not homogeneously dispersed on the basis of these superior thermal sensitivity properties of the two photon excited fluorescence, we have demonstrated the ability of cdse quantum dots to image a temperature gradient artificially created in a biocompatible fluid (phosphate buffered saline) and also their ability to measure an intracellular temperature increase externally induced in a single living cell",
            "sequence": "[CLS] ontology construction method methodology tool has uri data source research problem evaluation process name people involved serialization language [SEP] cdse quantum dots for two photon fluorescence thermal imaging the technological development of quantum dots has ushered in a new era in fluorescence bioimaging, which was propelled with the advent of novel multiphoton fluorescence microscopes here, the potential use of cdse quantum dots has been evaluated as fluorescent nanothermometers for two photon fluorescence microscopy in addition to the enhancement in spatial resolution inherent to any multiphoton excitation processes, two photon (near infrared) excitation leads to a temperature sensitivity of the emission intensity much higher than that achieved under one photon (visible) excitation the peak emission wavelength is also temperature sensitive, providing an additional approach for thermal imaging, which is particularly interesting for systems where nanoparticles are not homogeneously dispersed on the basis of these superior thermal sensitivity properties of the two photon excited fluorescence, we have demonstrated the ability of cdse quantum dots to image a temperature gradient artificially created in a biocompatible fluid (phosphate buffered saline) and also their ability to measure an intracellular temperature increase externally induced in a single living cell [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            }
        },
        {
            "instance_id": "R146876xR142214",
            "template_id": "R146876",
            "correct_template_id": null,
            "paper_id": "R142214",
            "premise": "organic solar cells mobility donor acceptor lumo homo energy band gap open circuit voltage, voc short circuit current density, jsc fill factor, ff power conversion efficiency",
            "hypothesis": "nanoscale thermometry via the fluorescence of yag:ce phosphor particles: measurements from 7 to 77 c the laser induced fluorescence lifetime of 30 nm particles of yag:ce was measured as a function of temperature from 7 to 77\u00b0c the fluorescence decay lifetimes for the nanoparticles of this phosphor varied from \u224818 to 27 ns, i e \u224833% relative to the longest lifetime measured this large variation in lifetime, coupled with the high signal strength that was observed, suggest that yag:ce nanoparticles will be useful thermographic phosphors we describe the material and the apparatus used to characterize its fluorescence, present the results of measurements made over the range of temperatures tested and comment on some possible applications for this novel material",
            "sequence": "[CLS] organic solar cells mobility donor acceptor lumo homo energy band gap open circuit voltage, voc short circuit current density, jsc fill factor, ff power conversion efficiency [SEP] nanoscale thermometry via the fluorescence of yag:ce phosphor particles: measurements from 7 to 77 c the laser induced fluorescence lifetime of 30 nm particles of yag:ce was measured as a function of temperature from 7 to 77\u00b0c the fluorescence decay lifetimes for the nanoparticles of this phosphor varied from \u224818 to 27 ns, i e \u224833% relative to the longest lifetime measured this large variation in lifetime, coupled with the high signal strength that was observed, suggest that yag:ce nanoparticles will be useful thermographic phosphors we describe the material and the apparatus used to characterize its fluorescence, present the results of measurements made over the range of temperatures tested and comment on some possible applications for this novel material [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            }
        },
        {
            "instance_id": "R150595xR193898",
            "template_id": "R150595",
            "correct_template_id": null,
            "paper_id": "R193898",
            "premise": "tailored forming contribution has result research problem has material realizes",
            "hypothesis": "a deep context wise method for coreference detection in natural language requirements requirements are usually written by different stakeholders with diverse backgrounds and skills and evolve continuously therefore inconsistency caused by specialized jargons and different domains, is inevitable in particular, entity coreference in requirement engineering (re) is that different linguistic expressions refer to the same real world entity it leads to misconception about technical terminologies, and impacts the readability and understandability of requirements negatively manual detection entity coreference is labor intensive and time consuming in this paper, we propose a deep context wise semantic method named deepcoref to entity coreference detection it consists of one fine tuning bert model for context representation and a word2vec based network for entity representation we use a multi layer perception in the end to fuse and make a trade off between two representations for obtaining a better representation of entities the input of the network is requirement contextual text and related entities, and the output is the predictive label to infer whether two entities are coreferent the evaluation on industry data shows that our approach significantly outperforms three baselines with average precision and recall of 96 10% and 96 06% respectively we also compare deepcoref with three variants to demonstrate the performance enhancement from different components",
            "sequence": "[CLS] tailored forming contribution has result research problem has material realizes [SEP] a deep context wise method for coreference detection in natural language requirements requirements are usually written by different stakeholders with diverse backgrounds and skills and evolve continuously therefore inconsistency caused by specialized jargons and different domains, is inevitable in particular, entity coreference in requirement engineering (re) is that different linguistic expressions refer to the same real world entity it leads to misconception about technical terminologies, and impacts the readability and understandability of requirements negatively manual detection entity coreference is labor intensive and time consuming in this paper, we propose a deep context wise semantic method named deepcoref to entity coreference detection it consists of one fine tuning bert model for context representation and a word2vec based network for entity representation we use a multi layer perception in the end to fuse and make a trade off between two representations for obtaining a better representation of entities the input of the network is requirement contextual text and related entities, and the output is the predictive label to infer whether two entities are coreferent the evaluation on industry data shows that our approach significantly outperforms three baselines with average precision and recall of 96 10% and 96 06% respectively we also compare deepcoref with three variants to demonstrate the performance enhancement from different components [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R194212xR25000",
            "template_id": "R194212",
            "correct_template_id": null,
            "paper_id": "R25000",
            "premise": "ontology construction method methodology tool has uri data source research problem evaluation process name people involved serialization language",
            "hypothesis": "predatory open access journals in india: a study this paper analyses the list of predatory journals published by jeffrey beall the study found that india is publishing the highest number of predatory journals the state wise analysis shows that the contribution of madhya pradesh is the highest in india a trend reveals that majority of these journals are published after the year 2010",
            "sequence": "[CLS] ontology construction method methodology tool has uri data source research problem evaluation process name people involved serialization language [SEP] predatory open access journals in india: a study this paper analyses the list of predatory journals published by jeffrey beall the study found that india is publishing the highest number of predatory journals the state wise analysis shows that the contribution of madhya pradesh is the highest in india a trend reveals that majority of these journals are published after the year 2010 [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R11",
                "label": "Science"
            }
        },
        {
            "instance_id": "R138077xR25107",
            "template_id": "R138077",
            "correct_template_id": null,
            "paper_id": "R25107",
            "premise": "ontology learning from data sources has dataset application domain input format output format accuracy f1 score precision knowledge source has data source learning purpose terms learning axiom learning properties learning properties hierarchy learning rule learning class hierarchy learning learning method learning tool evaluation metrics related work validation tool training corpus testing corpus class learning implemented technologies relationships learning instance learning taxonomy learning validation comment assessment of acquired knowledge recall f measure",
            "hypothesis": "participants' view on personal gains and pd process \"while it is commonly claimed that users of participatory design projects reap benefits from their participation, little research exists that shows if this truly occurs in the real world in this paper, we introduce the method and results of assessing the participants' perception of their personal benefits and the degree of participation in a large project in the healthcare field our research shows that a well executed participatory design project can produce most of the benefits hypothesized in the literature but also highlights the challenges of assessing individual benefits and the pd process \"",
            "sequence": "[CLS] ontology learning from data sources has dataset application domain input format output format accuracy f1 score precision knowledge source has data source learning purpose terms learning axiom learning properties learning properties hierarchy learning rule learning class hierarchy learning learning method learning tool evaluation metrics related work validation tool training corpus testing corpus class learning implemented technologies relationships learning instance learning taxonomy learning validation comment assessment of acquired knowledge recall f measure [SEP] participants' view on personal gains and pd process \"while it is commonly claimed that users of participatory design projects reap benefits from their participation, little research exists that shows if this truly occurs in the real world in this paper, we introduce the method and results of assessing the participants' perception of their personal benefits and the degree of participation in a large project in the healthcare field our research shows that a well executed participatory design project can produce most of the benefits hypothesized in the literature but also highlights the challenges of assessing individual benefits and the pd process \" [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R11",
                "label": "Science"
            }
        },
        {
            "instance_id": "R160259xR25131",
            "template_id": "R160259",
            "correct_template_id": null,
            "paper_id": "R25131",
            "premise": "digital city twin review application area",
            "hypothesis": "evaluation of six night vision enhancement systems: qualitative and quantitative support for intelligent image processing objective: an evaluation study was conducted to answer the question of which system properties of night vision enhancement systems (nvess) provide a benefit for drivers without increasing their workload background: different infrared sensor, image processing, and display technologies can be integrated into an nves to support nighttime driving because each of these components has its specific strengths and weaknesses, careful testing is required to determine their best combination method: six prototypical systems were assessed in two steps first, a heuristic evaluation with experts from ergonomics, perception, and traffic psychology was conducted it produced a broad overview of possible effects of system properties on driving based on these results, an experimental field study with 15 experienced drivers was performed criteria used to evaluate the development potential of the six prototypes were the usability dimensions of effectiveness, efficiency, and user satisfaction (international organization for standardization, 1998) results: results showed that the intelligibility of information, the easiness with which obstacles could be located in the environment, and the position of the display presenting the output of the system were of crucial importance for the usability of the nves and its acceptance conclusion: all relevant requirements are met best by nvess that are positioned at an unobtrusive location and are equipped with functions for the automatic identification of objects and for event based warnings application: these design recommendations and the presented approach to evaluate the systems can be directly incorporated into the development process of future nvess",
            "sequence": "[CLS] digital city twin review application area [SEP] evaluation of six night vision enhancement systems: qualitative and quantitative support for intelligent image processing objective: an evaluation study was conducted to answer the question of which system properties of night vision enhancement systems (nvess) provide a benefit for drivers without increasing their workload background: different infrared sensor, image processing, and display technologies can be integrated into an nves to support nighttime driving because each of these components has its specific strengths and weaknesses, careful testing is required to determine their best combination method: six prototypical systems were assessed in two steps first, a heuristic evaluation with experts from ergonomics, perception, and traffic psychology was conducted it produced a broad overview of possible effects of system properties on driving based on these results, an experimental field study with 15 experienced drivers was performed criteria used to evaluate the development potential of the six prototypes were the usability dimensions of effectiveness, efficiency, and user satisfaction (international organization for standardization, 1998) results: results showed that the intelligibility of information, the easiness with which obstacles could be located in the environment, and the position of the display presenting the output of the system were of crucial importance for the usability of the nves and its acceptance conclusion: all relevant requirements are met best by nvess that are positioned at an unobtrusive location and are equipped with functions for the automatic identification of objects and for event based warnings application: these design recommendations and the presented approach to evaluate the systems can be directly incorporated into the development process of future nvess [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R11",
                "label": "Science"
            }
        },
        {
            "instance_id": "R52190xR145732",
            "template_id": "R52190",
            "correct_template_id": null,
            "paper_id": "R145732",
            "premise": "transient absorption spectroscopy has participating device has participating person has gaseous sample input has solution sample input has solid sample input has radiant energy input has output",
            "hypothesis": "tribological study on tailored formed axial bearing washers to enhance tribological contacts under cyclic load, high performance materials are required utilizing the same high strength material for the whole machine element is not resource efficient in order to manufacture machine elements with extended functionality and specific properties, a combination of different materials can be used in a single component for a more efficient material utilization by combining different joining techniques with subsequent forming, multi material or tailored components can be manufactured to reduce material costs and energy consumption during the component service life, a less expensive lightweight material should be used for regions remote from the highly stressed zones the scope is not only to obtain the desired shape and dimensions for the finishing process, but also to improve properties like the bond strength between different materials and the microscopic structure of the material the multi material approach can be applied to all components requiring different properties in separate component regions such as shafts, bearings or bushes the current study exemplarily presents the process route for the production of an axial bearing washer by means of tailored forming technology the bearing washers were chosen to fit axial roller bearings (type 81212) the manufacturing process starts with the laser wire cladding of a hard facing made of martensitic chromium silicon steel (1 4718) on a base substrate of s235 (1 0038) steel subsequently, the bearing washers are forged after finishing, the surfaces of the bearing washers were tested in thrust bearings on an fe 8 test rig the operational test of the bearings consists in a run in phase at 250 rpm a bearing failure is determined by a condition monitoring system before and after this, the bearings were inspected by optical and ultrasonic microscopy in order to examine whether the bond of the coat is resistant against rolling contact fatigue the feasibility of the approach could be proven by endurance test the joining zone was able to withstand the rolling contact stresses and the bearing failed due to material induced fatigue with high cycle stability",
            "sequence": "[CLS] transient absorption spectroscopy has participating device has participating person has gaseous sample input has solution sample input has solid sample input has radiant energy input has output [SEP] tribological study on tailored formed axial bearing washers to enhance tribological contacts under cyclic load, high performance materials are required utilizing the same high strength material for the whole machine element is not resource efficient in order to manufacture machine elements with extended functionality and specific properties, a combination of different materials can be used in a single component for a more efficient material utilization by combining different joining techniques with subsequent forming, multi material or tailored components can be manufactured to reduce material costs and energy consumption during the component service life, a less expensive lightweight material should be used for regions remote from the highly stressed zones the scope is not only to obtain the desired shape and dimensions for the finishing process, but also to improve properties like the bond strength between different materials and the microscopic structure of the material the multi material approach can be applied to all components requiring different properties in separate component regions such as shafts, bearings or bushes the current study exemplarily presents the process route for the production of an axial bearing washer by means of tailored forming technology the bearing washers were chosen to fit axial roller bearings (type 81212) the manufacturing process starts with the laser wire cladding of a hard facing made of martensitic chromium silicon steel (1 4718) on a base substrate of s235 (1 0038) steel subsequently, the bearing washers are forged after finishing, the surfaces of the bearing washers were tested in thrust bearings on an fe 8 test rig the operational test of the bearings consists in a run in phase at 250 rpm a bearing failure is determined by a condition monitoring system before and after this, the bearings were inspected by optical and ultrasonic microscopy in order to examine whether the bond of the coat is resistant against rolling contact fatigue the feasibility of the approach could be proven by endurance test the joining zone was able to withstand the rolling contact stresses and the bearing failed due to material induced fatigue with high cycle stability [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R137654",
                "label": "Mechanical Process Engineering"
            }
        },
        {
            "instance_id": "R161736xR175297",
            "template_id": "R161736",
            "correct_template_id": null,
            "paper_id": "R175297",
            "premise": "xray laser applications paper type research objective has system qualities",
            "hypothesis": "ontology learning from text: a look back and into the future ontologies are often viewed as the answer to the need for interoperable semantics in modern information systems the explosion of textual information on the read/write web coupled with the increasing demand for ontologies to power the semantic web have made (semi )automatic ontology learning from text a very promising research area this together with the advanced state in related areas, such as natural language processing, have fueled research into ontology learning over the past decade this survey looks at how far we have come since the turn of the millennium and discusses the remaining challenges that will define the research directions in this area in the near future",
            "sequence": "[CLS] xray laser applications paper type research objective has system qualities [SEP] ontology learning from text: a look back and into the future ontologies are often viewed as the answer to the need for interoperable semantics in modern information systems the explosion of textual information on the read/write web coupled with the increasing demand for ontologies to power the semantic web have made (semi )automatic ontology learning from text a very promising research area this together with the advanced state in related areas, such as natural language processing, have fueled research into ontology learning over the past decade this survey looks at how far we have come since the turn of the millennium and discusses the remaining challenges that will define the research directions in this area in the near future [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R141823",
                "label": "Semantic Web"
            }
        },
        {
            "instance_id": "R40006xR78371",
            "template_id": "R40006",
            "correct_template_id": null,
            "paper_id": "R78371",
            "premise": "basic reproduction number estimate time period basic reproduction number location",
            "hypothesis": "automatic classification of non functional requirements from augmented app user reviews \"context: the leading app distribution platforms, apple app store, google play, and windows phone store, have over 4 million apps research shows that user reviews contain abundant useful information which may help developers to improve their apps extracting and considering non functional requirements (nfrs), which describe a set of quality attributes wanted for an app and are hidden in user reviews, can help developers to deliver a product which meets users' expectations objective: developers need to be aware of the nfrs from massive user reviews during software maintenance and evolution automatic user reviews classification based on an nfr standard provides a feasible way to achieve this goal method: in this paper, user reviews were automatically classified into four types of nfrs (reliability, usability, portability, and performance), functional requirements (frs), and others we combined four classification techniques bow, tf idf, chi2, and aur bow (proposed in this work) with three machine learning algorithms naive bayes, j48, and bagging to classify user reviews we conducted experiments to compare the f measures of the classification results through all the combinations of the techniques and algorithms results: we found that the combination of aur bow with bagging achieves the best result (a precision of 71 4%, a recall of 72 3%, and an f measure of 71 8%) among all the combinations conclusion: our finding shows that augmented user reviews can lead to better classification results, and the machine learning algorithm bagging is more suitable for nfrs classification from user reviews than na\u00efve bayes and j48 \"",
            "sequence": "[CLS] basic reproduction number estimate time period basic reproduction number location [SEP] automatic classification of non functional requirements from augmented app user reviews \"context: the leading app distribution platforms, apple app store, google play, and windows phone store, have over 4 million apps research shows that user reviews contain abundant useful information which may help developers to improve their apps extracting and considering non functional requirements (nfrs), which describe a set of quality attributes wanted for an app and are hidden in user reviews, can help developers to deliver a product which meets users' expectations objective: developers need to be aware of the nfrs from massive user reviews during software maintenance and evolution automatic user reviews classification based on an nfr standard provides a feasible way to achieve this goal method: in this paper, user reviews were automatically classified into four types of nfrs (reliability, usability, portability, and performance), functional requirements (frs), and others we combined four classification techniques bow, tf idf, chi2, and aur bow (proposed in this work) with three machine learning algorithms naive bayes, j48, and bagging to classify user reviews we conducted experiments to compare the f measures of the classification results through all the combinations of the techniques and algorithms results: we found that the combination of aur bow with bagging achieves the best result (a precision of 71 4%, a recall of 72 3%, and an f measure of 71 8%) among all the combinations conclusion: our finding shows that augmented user reviews can lead to better classification results, and the machine learning algorithm bagging is more suitable for nfrs classification from user reviews than na\u00efve bayes and j48 \" [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R154390xR146779",
            "template_id": "R154390",
            "correct_template_id": null,
            "paper_id": "R146779",
            "premise": "lignin decomposition substrate catalyst has temperature value solvent pressure reactor conversion product",
            "hypothesis": "a solution processable electron acceptor based on dibenzosilole and diketopyrrolopyrrole for organic solar cells organic solar cells (oscs) are a promising cost effective alternative for utility of solar energy, and possess low cost, light weight, and fl exibility advantages [ 1\u20137 ] much attention has been focused on the development of oscs which have seen a dramatic rise in effi ciency over the last decade, and the encouraging power conversion effi ciency (pce) over 9% has been achieved from bulk heterojunction (bhj) oscs [ 8 ] with regard to photoactive materials, fullerenes and their derivatives, such as [6,6] phenyl c 61 butyric acid methyl ester (pc 61 bm), have been the dominant electron acceptor materials in bhj oscs, owing to their high electron mobility, large electron affi nity and isotropy of charge transport [ 9 ] however, fullerenes have a few disadvantages, such as restricted electronic tuning and weak absorption in the visible region furthermore, in typical bhj system of poly(3 hexylthiophene) (p3ht):pc 61 bm, mismatching energy levels between donor and acceptor leads to energy loss and low open circuit voltages ( v oc ) to solve these problems, novel electron acceptor materials with strong and broad absorption spectra and appropriate energy levels are necessary for oscs recently, non fullerene small molecule acceptors have been developed [ 10 , 11 ] however, rare reports on the devices based on solution processed non fullerene small molecule acceptors have shown pces approaching or exceeding 1 5%, [ 12\u201319 ] and only one paper reported pces over 2% [ 16 ]",
            "sequence": "[CLS] lignin decomposition substrate catalyst has temperature value solvent pressure reactor conversion product [SEP] a solution processable electron acceptor based on dibenzosilole and diketopyrrolopyrrole for organic solar cells organic solar cells (oscs) are a promising cost effective alternative for utility of solar energy, and possess low cost, light weight, and fl exibility advantages [ 1\u20137 ] much attention has been focused on the development of oscs which have seen a dramatic rise in effi ciency over the last decade, and the encouraging power conversion effi ciency (pce) over 9% has been achieved from bulk heterojunction (bhj) oscs [ 8 ] with regard to photoactive materials, fullerenes and their derivatives, such as [6,6] phenyl c 61 butyric acid methyl ester (pc 61 bm), have been the dominant electron acceptor materials in bhj oscs, owing to their high electron mobility, large electron affi nity and isotropy of charge transport [ 9 ] however, fullerenes have a few disadvantages, such as restricted electronic tuning and weak absorption in the visible region furthermore, in typical bhj system of poly(3 hexylthiophene) (p3ht):pc 61 bm, mismatching energy levels between donor and acceptor leads to energy loss and low open circuit voltages ( v oc ) to solve these problems, novel electron acceptor materials with strong and broad absorption spectra and appropriate energy levels are necessary for oscs recently, non fullerene small molecule acceptors have been developed [ 10 , 11 ] however, rare reports on the devices based on solution processed non fullerene small molecule acceptors have shown pces approaching or exceeding 1 5%, [ 12\u201319 ] and only one paper reported pces over 2% [ 16 ] [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            }
        },
        {
            "instance_id": "R172526xR160319",
            "template_id": "R172526",
            "correct_template_id": null,
            "paper_id": "R160319",
            "premise": "video process has study research problem application production",
            "hypothesis": "the circular economy: a new development strategy in china activities over the past several years, however, clearly show that ce is emerging as an economic strategy rather than a purely environmental strategy the major objective of the government is to promote the sustainable development of economy and society, while it also helps to achieve sustainable environmental protection powers, increasing the wealth of the population and providing employment and business opportunities the rapid economic growth, however, has engendered serious natural resource depletion and environmental pollution, and the continuing increase of population has exacerbated this situation greatly recent research has pointed out that growth of the gross domestic product (gdp) in china has significantly reduced the opportunities of future generations to enjoy natural and environmental resources 1 the central government promised in 2002 to build a prosperous society in a comprehensive way by 2020 by then, gdp per capita is anticipated to reach u s $3,000 and the total gdp to quadruple obviously, it is unrealistic for china to expect to realize this ambitious objective in terms of natural resource use if it continues its current development pathway, with population increasing to 1 45 billion in 2020 (qu 2004), low productivity, and the absence of eco efficiency",
            "sequence": "[CLS] video process has study research problem application production [SEP] the circular economy: a new development strategy in china activities over the past several years, however, clearly show that ce is emerging as an economic strategy rather than a purely environmental strategy the major objective of the government is to promote the sustainable development of economy and society, while it also helps to achieve sustainable environmental protection powers, increasing the wealth of the population and providing employment and business opportunities the rapid economic growth, however, has engendered serious natural resource depletion and environmental pollution, and the continuing increase of population has exacerbated this situation greatly recent research has pointed out that growth of the gross domestic product (gdp) in china has significantly reduced the opportunities of future generations to enjoy natural and environmental resources 1 the central government promised in 2002 to build a prosperous society in a comprehensive way by 2020 by then, gdp per capita is anticipated to reach u s $3,000 and the total gdp to quadruple obviously, it is unrealistic for china to expect to realize this ambitious objective in terms of natural resource use if it continues its current development pathway, with population increasing to 1 45 billion in 2020 (qu 2004), low productivity, and the absence of eco efficiency [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            }
        },
        {
            "instance_id": "R152828xR43000",
            "template_id": "R152828",
            "correct_template_id": null,
            "paper_id": "R43000",
            "premise": "xray laser pumped has system qualities",
            "hypothesis": "pattern based model reuse using colored petri nets colored petri net (cpn) is a graphical modeling language for simulation and modeling and for verification of discrete event systems cpn allows developers to define a model in the form of reusable components a model component is an independent element, which is specified using a formalized description, can conform to a certain component standard, has a well defined interface, and encapsulates certain behavior modern components can help the developer reuse existing models according to their requirement as it reduces the cost and time of development composability is the capability to select and integrate various components to fulfill user requirements composability provides the means to achieve reusability where \"reuse\" is the ability of a simulation component to be reclaimed for various applications we propose a verification framework for developers to select and assemble cpn based components and verify their composability the goal of this paper is to provide a pattern which helps developer in making models of concurrent systems we present a case study of a restaurant model as proof of concept a verified composition affirms reuse of model components in a meaningful manner by satisfying given requirement specifications",
            "sequence": "[CLS] xray laser pumped has system qualities [SEP] pattern based model reuse using colored petri nets colored petri net (cpn) is a graphical modeling language for simulation and modeling and for verification of discrete event systems cpn allows developers to define a model in the form of reusable components a model component is an independent element, which is specified using a formalized description, can conform to a certain component standard, has a well defined interface, and encapsulates certain behavior modern components can help the developer reuse existing models according to their requirement as it reduces the cost and time of development composability is the capability to select and integrate various components to fulfill user requirements composability provides the means to achieve reusability where \"reuse\" is the ability of a simulation component to be reclaimed for various applications we propose a verification framework for developers to select and assemble cpn based components and verify their composability the goal of this paper is to provide a pattern which helps developer in making models of concurrent systems we present a case study of a restaurant model as proof of concept a verified composition affirms reuse of model components in a meaningful manner by satisfying given requirement specifications [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R52190xR38180",
            "template_id": "R52190",
            "correct_template_id": null,
            "paper_id": "R38180",
            "premise": "transient absorption spectroscopy has participating device has participating person has gaseous sample input has solution sample input has solid sample input has radiant energy input has output",
            "hypothesis": "end to end relation extraction using lstms on sequences and tree structures we present a novel end to end neural model to extract entities and relations between them our recurrent neural network based model captures both word sequence and dependency tree substructure information by stacking bidirectional tree structured lstm rnns on bidirectional sequential lstm rnns this allows our model to jointly represent both entities and relations with shared parameters in a single model we further encourage detection of entities during training and use of entity information in relation extraction via entity pretraining and scheduled sampling our model improves over the state of the art feature based model on end to end relation extraction, achieving 12 1% and 5 7% relative error reductions in f1 score on ace2005 and ace2004, respectively we also show that our lstm rnn based model compares favorably to the state of the art cnn based model (in f1 score) on nominal relation classification (semeval 2010 task 8) finally, we present an extensive ablation analysis of several model components",
            "sequence": "[CLS] transient absorption spectroscopy has participating device has participating person has gaseous sample input has solution sample input has solid sample input has radiant energy input has output [SEP] end to end relation extraction using lstms on sequences and tree structures we present a novel end to end neural model to extract entities and relations between them our recurrent neural network based model captures both word sequence and dependency tree substructure information by stacking bidirectional tree structured lstm rnns on bidirectional sequential lstm rnns this allows our model to jointly represent both entities and relations with shared parameters in a single model we further encourage detection of entities during training and use of entity information in relation extraction via entity pretraining and scheduled sampling our model improves over the state of the art feature based model on end to end relation extraction, achieving 12 1% and 5 7% relative error reductions in f1 score on ace2005 and ace2004, respectively we also show that our lstm rnn based model compares favorably to the state of the art cnn based model (in f1 score) on nominal relation classification (semeval 2010 task 8) finally, we present an extensive ablation analysis of several model components [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R133",
                "label": "Artificial Intelligence"
            }
        },
        {
            "instance_id": "R187648xR141701",
            "template_id": "R187648",
            "correct_template_id": null,
            "paper_id": "R141701",
            "premise": "oco for control research problem theoretical guarantees has application in data driven approach has constraints type of considered system measurement noise process noise",
            "hypothesis": "carbon dot nanothermometry: intracellular photoluminescence lifetime thermal sensing nanoscale biocompatible photoluminescence (pl) thermometers that can be used to accurately and reliably monitor intracellular temperatures have many potential applications in biology and medicine ideally, such nanothermometers should be functional at physiological ph across a wide range of ionic strengths, probe concentrations, and local environments here, we show that water soluble n,s co doped carbon dots (cds) exhibit temperature dependent photoluminescence lifetimes and can serve as highly sensitive and reliable intracellular nanothermometers pl intensity measurements indicate that these cds have many advantages over alternative semiconductor and cd based nanoscale temperature sensors importantly, their pl lifetimes remain constant over wide ranges of ph values (5 12), cd concentrations (1 5 \u00d7 10 5 to 0 5 mg/ml), and environmental ionic strengths (up to 0 7 mol\u00b7l 1 nacl) moreover, they are biocompatible and nontoxic, as demonstrated by cell viability and flow cytometry analyses using nih/3t3 and hela cell lines n,s cd thermal sensors also exhibit good water dispersibility, superior photo and thermostability, extraordinary environment and concentration independence, high storage stability, and reusability their pl decay curves at temperatures between 15 and 45 \u00b0c remained unchanged over seven sequential experiments in vitro pl lifetime based temperature sensing performed with human cervical cancer hela cells demonstrated the great potential of these nanosensors in biomedicine overall, n,s doped cds exhibit excitation independent emission with strongly temperature dependent monoexponential decay, making them suitable for both in vitro and in vivo luminescence lifetime thermometry",
            "sequence": "[CLS] oco for control research problem theoretical guarantees has application in data driven approach has constraints type of considered system measurement noise process noise [SEP] carbon dot nanothermometry: intracellular photoluminescence lifetime thermal sensing nanoscale biocompatible photoluminescence (pl) thermometers that can be used to accurately and reliably monitor intracellular temperatures have many potential applications in biology and medicine ideally, such nanothermometers should be functional at physiological ph across a wide range of ionic strengths, probe concentrations, and local environments here, we show that water soluble n,s co doped carbon dots (cds) exhibit temperature dependent photoluminescence lifetimes and can serve as highly sensitive and reliable intracellular nanothermometers pl intensity measurements indicate that these cds have many advantages over alternative semiconductor and cd based nanoscale temperature sensors importantly, their pl lifetimes remain constant over wide ranges of ph values (5 12), cd concentrations (1 5 \u00d7 10 5 to 0 5 mg/ml), and environmental ionic strengths (up to 0 7 mol\u00b7l 1 nacl) moreover, they are biocompatible and nontoxic, as demonstrated by cell viability and flow cytometry analyses using nih/3t3 and hela cell lines n,s cd thermal sensors also exhibit good water dispersibility, superior photo and thermostability, extraordinary environment and concentration independence, high storage stability, and reusability their pl decay curves at temperatures between 15 and 45 \u00b0c remained unchanged over seven sequential experiments in vitro pl lifetime based temperature sensing performed with human cervical cancer hela cells demonstrated the great potential of these nanosensors in biomedicine overall, n,s doped cds exhibit excitation independent emission with strongly temperature dependent monoexponential decay, making them suitable for both in vitro and in vivo luminescence lifetime thermometry [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            }
        },
        {
            "instance_id": "R155844xR4857",
            "template_id": "R155844",
            "correct_template_id": null,
            "paper_id": "R4857",
            "premise": "photocatalysts wavelength of maximum absorption energy band gap photocatalyst wavelength of maximum emission emission lifetime ground state oxidation potential ground state reduction potential excited state oxidation potential excited state reduction potential",
            "hypothesis": "how are topics born? understanding the research dynamics preceding the emergence of new areas the ability to promptly recognise new research trends is strategic for many stakeholders, including universities, institutional funding bodies, academic publishers and companies while the literature describes several approaches which aim to identify the emergence of new research topics early in their lifecycle, these rely on the assumption that the topic in question is already associated with a number of publications and consistently referred to by a community of researchers hence, detecting the emergence of a new research area at an embryonic stage , i e , before the topic has been consistently labelled by a community of researchers and associated with a number of publications, is still an open challenge in this paper, we begin to address this challenge by performing a study of the dynamics preceding the creation of new topics this study indicates that the emergence of a new topic is anticipated by a significant increase in the pace of collaboration between relevant research areas, which can be seen as the \u2018parents\u2019 of the new topic these initial findings (i) confirm our hypothesis that it is possible in principle to detect the emergence of a new topic at the embryonic stage, (ii) provide new empirical evidence supporting relevant theories in philosophy of science, and also (iii) suggest that new topics tend to emerge in an environment in which weakly interconnected research areas begin to cross fertilise",
            "sequence": "[CLS] photocatalysts wavelength of maximum absorption energy band gap photocatalyst wavelength of maximum emission emission lifetime ground state oxidation potential ground state reduction potential excited state oxidation potential excited state reduction potential [SEP] how are topics born? understanding the research dynamics preceding the emergence of new areas the ability to promptly recognise new research trends is strategic for many stakeholders, including universities, institutional funding bodies, academic publishers and companies while the literature describes several approaches which aim to identify the emergence of new research topics early in their lifecycle, these rely on the assumption that the topic in question is already associated with a number of publications and consistently referred to by a community of researchers hence, detecting the emergence of a new research area at an embryonic stage , i e , before the topic has been consistently labelled by a community of researchers and associated with a number of publications, is still an open challenge in this paper, we begin to address this challenge by performing a study of the dynamics preceding the creation of new topics this study indicates that the emergence of a new topic is anticipated by a significant increase in the pace of collaboration between relevant research areas, which can be seen as the \u2018parents\u2019 of the new topic these initial findings (i) confirm our hypothesis that it is possible in principle to detect the emergence of a new topic at the embryonic stage, (ii) provide new empirical evidence supporting relevant theories in philosophy of science, and also (iii) suggest that new topics tend to emerge in an environment in which weakly interconnected research areas begin to cross fertilise [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R133",
                "label": "Artificial Intelligence"
            }
        },
        {
            "instance_id": "R154390xR78371",
            "template_id": "R154390",
            "correct_template_id": null,
            "paper_id": "R78371",
            "premise": "lignin decomposition substrate catalyst has temperature value solvent pressure reactor conversion product",
            "hypothesis": "automatic classification of non functional requirements from augmented app user reviews \"context: the leading app distribution platforms, apple app store, google play, and windows phone store, have over 4 million apps research shows that user reviews contain abundant useful information which may help developers to improve their apps extracting and considering non functional requirements (nfrs), which describe a set of quality attributes wanted for an app and are hidden in user reviews, can help developers to deliver a product which meets users' expectations objective: developers need to be aware of the nfrs from massive user reviews during software maintenance and evolution automatic user reviews classification based on an nfr standard provides a feasible way to achieve this goal method: in this paper, user reviews were automatically classified into four types of nfrs (reliability, usability, portability, and performance), functional requirements (frs), and others we combined four classification techniques bow, tf idf, chi2, and aur bow (proposed in this work) with three machine learning algorithms naive bayes, j48, and bagging to classify user reviews we conducted experiments to compare the f measures of the classification results through all the combinations of the techniques and algorithms results: we found that the combination of aur bow with bagging achieves the best result (a precision of 71 4%, a recall of 72 3%, and an f measure of 71 8%) among all the combinations conclusion: our finding shows that augmented user reviews can lead to better classification results, and the machine learning algorithm bagging is more suitable for nfrs classification from user reviews than na\u00efve bayes and j48 \"",
            "sequence": "[CLS] lignin decomposition substrate catalyst has temperature value solvent pressure reactor conversion product [SEP] automatic classification of non functional requirements from augmented app user reviews \"context: the leading app distribution platforms, apple app store, google play, and windows phone store, have over 4 million apps research shows that user reviews contain abundant useful information which may help developers to improve their apps extracting and considering non functional requirements (nfrs), which describe a set of quality attributes wanted for an app and are hidden in user reviews, can help developers to deliver a product which meets users' expectations objective: developers need to be aware of the nfrs from massive user reviews during software maintenance and evolution automatic user reviews classification based on an nfr standard provides a feasible way to achieve this goal method: in this paper, user reviews were automatically classified into four types of nfrs (reliability, usability, portability, and performance), functional requirements (frs), and others we combined four classification techniques bow, tf idf, chi2, and aur bow (proposed in this work) with three machine learning algorithms naive bayes, j48, and bagging to classify user reviews we conducted experiments to compare the f measures of the classification results through all the combinations of the techniques and algorithms results: we found that the combination of aur bow with bagging achieves the best result (a precision of 71 4%, a recall of 72 3%, and an f measure of 71 8%) among all the combinations conclusion: our finding shows that augmented user reviews can lead to better classification results, and the machine learning algorithm bagging is more suitable for nfrs classification from user reviews than na\u00efve bayes and j48 \" [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R186491xR25142",
            "template_id": "R186491",
            "correct_template_id": null,
            "paper_id": "R25142",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "a large scale led array to support anticipatory driving we present a novel assistance system which supports anticipatory driving by means of fostering early deceleration upcoming technologies like car2x communication provide information about a time interval which is currently uncovered this information shall be used in the proposed system to inform drivers about future situations which require reduced speed such situations include traffic jams, construction sites or speed limits the hmi is an optical output system based on line arrays of rgb leds our contribution presents construction details as well as user evaluations the results show an earlier deceleration of 3 9 \u2013 11 5 s and a shorter deceleration distance of 2 \u2013 166 m",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] a large scale led array to support anticipatory driving we present a novel assistance system which supports anticipatory driving by means of fostering early deceleration upcoming technologies like car2x communication provide information about a time interval which is currently uncovered this information shall be used in the proposed system to inform drivers about future situations which require reduced speed such situations include traffic jams, construction sites or speed limits the hmi is an optical output system based on line arrays of rgb leds our contribution presents construction details as well as user evaluations the results show an earlier deceleration of 3 9 \u2013 11 5 s and a shorter deceleration distance of 2 \u2013 166 m [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R11",
                "label": "Science"
            }
        },
        {
            "instance_id": "R178304xR137380",
            "template_id": "R178304",
            "correct_template_id": null,
            "paper_id": "R137380",
            "premise": "dataset inter annotator agreement alternatename assesses creator description disambiguatingdescription encoding exampleofwork genre inlanguage isbasedon license name sameas size sourceorganization url version",
            "hypothesis": "deposition of a tmdso based film by a non equilibrium atmospheric pressure dc plasma jet: deposition of a tmdso based film\u2026 this work deals with the deposition of thin films using an atmospheric pressure direct current nitrogen plasma jet with tetramethyldisiloxane as precursor the effect of o 2 flow and plasma discharge power on film deposition rate and film chemical characteristics is investigated in detail by surface profilometry, fourier transform infrared spectroscopy, and x ray photoelectron spectroscopy it is found that a higher deposition rate is obtained at higher oxygen flow rates and higher discharge powers increasing discharge power shows a certain amount of capability to transfer low oxygen content bonds to high oxygen content bonds organic films can be deposited in a pure nitrogen atmosphere the film chemical composition can be tuned to a more inorganic structure by admixture of o 2 leading to an increase in sio4 units at high oxygen flow rates",
            "sequence": "[CLS] dataset inter annotator agreement alternatename assesses creator description disambiguatingdescription encoding exampleofwork genre inlanguage isbasedon license name sameas size sourceorganization url version [SEP] deposition of a tmdso based film by a non equilibrium atmospheric pressure dc plasma jet: deposition of a tmdso based film\u2026 this work deals with the deposition of thin films using an atmospheric pressure direct current nitrogen plasma jet with tetramethyldisiloxane as precursor the effect of o 2 flow and plasma discharge power on film deposition rate and film chemical characteristics is investigated in detail by surface profilometry, fourier transform infrared spectroscopy, and x ray photoelectron spectroscopy it is found that a higher deposition rate is obtained at higher oxygen flow rates and higher discharge powers increasing discharge power shows a certain amount of capability to transfer low oxygen content bonds to high oxygen content bonds organic films can be deposited in a pure nitrogen atmosphere the film chemical composition can be tuned to a more inorganic structure by admixture of o 2 leading to an increase in sio4 units at high oxygen flow rates [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R114008",
                "label": "Applied Physics"
            }
        },
        {
            "instance_id": "R52190xR78466",
            "template_id": "R52190",
            "correct_template_id": null,
            "paper_id": "R78466",
            "premise": "transient absorption spectroscopy has participating device has participating person has gaseous sample input has solution sample input has solid sample input has radiant energy input has output",
            "hypothesis": "how can i improve my app? classifying user reviews for software maintenance and evolution app stores, such as google play or the apple store, allow users to provide feedback on apps by posting review comments and giving star ratings these platforms constitute a useful electronic mean in which application developers and users can productively exchange information about apps previous research showed that users feedback contains usage scenarios, bug reports and feature requests, that can help app developers to accomplish software maintenance and evolution tasks however, in the case of the most popular apps, the large amount of received feedback, its unstructured nature and varying quality can make the identification of useful user feedback a very challenging task in this paper we present a taxonomy to classify app reviews into categories relevant to software maintenance and evolution, as well as an approach that merges three techniques: (1) natural language processing, (2) text analysis and (3) sentiment analysis to automatically classify app reviews into the proposed categories we show that the combined use of these techniques allows to achieve better results (a precision of 75% and a recall of 74%) than results obtained using each technique individually (precision of 70% and a recall of 67%)",
            "sequence": "[CLS] transient absorption spectroscopy has participating device has participating person has gaseous sample input has solution sample input has solid sample input has radiant energy input has output [SEP] how can i improve my app? classifying user reviews for software maintenance and evolution app stores, such as google play or the apple store, allow users to provide feedback on apps by posting review comments and giving star ratings these platforms constitute a useful electronic mean in which application developers and users can productively exchange information about apps previous research showed that users feedback contains usage scenarios, bug reports and feature requests, that can help app developers to accomplish software maintenance and evolution tasks however, in the case of the most popular apps, the large amount of received feedback, its unstructured nature and varying quality can make the identification of useful user feedback a very challenging task in this paper we present a taxonomy to classify app reviews into categories relevant to software maintenance and evolution, as well as an approach that merges three techniques: (1) natural language processing, (2) text analysis and (3) sentiment analysis to automatically classify app reviews into the proposed categories we show that the combined use of these techniques allows to achieve better results (a precision of 75% and a recall of 74%) than results obtained using each technique individually (precision of 70% and a recall of 67%) [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R178304xR25103",
            "template_id": "R178304",
            "correct_template_id": null,
            "paper_id": "R25103",
            "premise": "dataset inter annotator agreement alternatename assesses creator description disambiguatingdescription encoding exampleofwork genre inlanguage isbasedon license name sameas size sourceorganization url version",
            "hypothesis": "sustained participatory design: extending the iterative approach with its 10th biennial anniversary conference in 2008, participatory design (pd) was leaving its teens and must now be considered ready to join the adult world and to think big: pd should engage in large scale information systems development and opt for a sustained pd approach applied throughout design and organizational implementation to pursue this aim we extend the iterative pd approach by (1) emphasizing pd experiments that transcend traditional prototyping and evaluate systems during real work; (2) incorporating improvisational change management including anticipated, emergent, and opportunity based change; and (3) extending initial design and development into a sustained, stepwise implementation that constitutes an overall technology driven organizational change sustained pd is exemplified through a pd experiment in the danish healthcare sector we reflect on our experiences from this experiment and discuss four challenges pd must address in dealing with large scale systems development",
            "sequence": "[CLS] dataset inter annotator agreement alternatename assesses creator description disambiguatingdescription encoding exampleofwork genre inlanguage isbasedon license name sameas size sourceorganization url version [SEP] sustained participatory design: extending the iterative approach with its 10th biennial anniversary conference in 2008, participatory design (pd) was leaving its teens and must now be considered ready to join the adult world and to think big: pd should engage in large scale information systems development and opt for a sustained pd approach applied throughout design and organizational implementation to pursue this aim we extend the iterative pd approach by (1) emphasizing pd experiments that transcend traditional prototyping and evaluate systems during real work; (2) incorporating improvisational change management including anticipated, emergent, and opportunity based change; and (3) extending initial design and development into a sustained, stepwise implementation that constitutes an overall technology driven organizational change sustained pd is exemplified through a pd experiment in the danish healthcare sector we reflect on our experiences from this experiment and discuss four challenges pd must address in dealing with large scale systems development [SEP]",
            "target": "contradiction",
            "research_field": {
                "id": "R11",
                "label": "Science"
            }
        }
    ],
    "neutrals": [
        {
            "instance_id": "R194895",
            "template_id": null,
            "paper_id": "R194895",
            "premise": null,
            "hypothesis": "scalable analysis of real time requirements detecting issues in real time requirements is usually a trade off between flexibility and cost: the effort expended depends on how expensive it is to fix a defect introduced by faulty, ambiguous or incomplete requirements the most rigorous techniques for real time requirement analysis depend on the formalisation of these requirements completely formalised real time requirements allow the detection of issues that are hard to find through other means, like real time inconsistency (i e , \"do the requirements lead to deadlocks and starvation of the system?\") or vacuity (i e , \"are some requirements trivially satisfied\") current analysis techniques for real time requirements suffer from scalability issues \u2013 larger sets of such requirements are usually intractable we present a new technique to analyse formalised real time requirements for various properties our technique leverages recent advances in software model checking and automatic theorem proving by converting the analysis problem for real time requirements to a program analysis task we also report preliminary results from an ongoing, large scale application of our technique in the automotive domain at bosch",
            "sequence": "[CLS] None [SEP] scalable analysis of real time requirements detecting issues in real time requirements is usually a trade off between flexibility and cost: the effort expended depends on how expensive it is to fix a defect introduced by faulty, ambiguous or incomplete requirements the most rigorous techniques for real time requirement analysis depend on the formalisation of these requirements completely formalised real time requirements allow the detection of issues that are hard to find through other means, like real time inconsistency (i e , \"do the requirements lead to deadlocks and starvation of the system?\") or vacuity (i e , \"are some requirements trivially satisfied\") current analysis techniques for real time requirements suffer from scalability issues \u2013 larger sets of such requirements are usually intractable we present a new technique to analyse formalised real time requirements for various properties our technique leverages recent advances in software model checking and automatic theorem proving by converting the analysis problem for real time requirements to a program analysis task we also report preliminary results from an ongoing, large scale application of our technique in the automotive domain at bosch [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R186081",
            "template_id": null,
            "paper_id": "R186081",
            "premise": null,
            "hypothesis": "metrics based verification and validation maturity model (mb v2m2) verification and validation (v&v) is only marginally addressed in software process improvement models like cmm and cmmi a roadmap for the establishment of a sound verification and validation process in software development organizations is badly needed this paper presents a basis for a roadmap; it describes a framework for improvement of the v&v process, based on the testing maturity model (tmm), but with considerable enhancements the model, tentatively named mb v/sup 2/m/sup 2/ (metrics based verification and validation maturity model), has been initiated by a consortium of industrial companies, consultancy & service agencies and an academic institute, operating and residing in the netherlands mb v/sup 2/m/sup 2/ is designed to be universally applicable, to unite the strengths of known (verification and validation) improvement models and to reflect proven work practices it recommends a metrics base to select process improvements and to track and control implementation of improvement actions this paper outlines the model and addresses the current status",
            "sequence": "[CLS] None [SEP] metrics based verification and validation maturity model (mb v2m2) verification and validation (v&v) is only marginally addressed in software process improvement models like cmm and cmmi a roadmap for the establishment of a sound verification and validation process in software development organizations is badly needed this paper presents a basis for a roadmap; it describes a framework for improvement of the v&v process, based on the testing maturity model (tmm), but with considerable enhancements the model, tentatively named mb v/sup 2/m/sup 2/ (metrics based verification and validation maturity model), has been initiated by a consortium of industrial companies, consultancy & service agencies and an academic institute, operating and residing in the netherlands mb v/sup 2/m/sup 2/ is designed to be universally applicable, to unite the strengths of known (verification and validation) improvement models and to reflect proven work practices it recommends a metrics base to select process improvements and to track and control implementation of improvement actions this paper outlines the model and addresses the current status [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R195173",
            "template_id": null,
            "paper_id": "R195173",
            "premise": null,
            "hypothesis": "towards ubiquitous re: a perspective on requirements engineering in the era of digital transformation we are now living in the era of digital transformation: innovative and digital business models are transforming the global business world and society however, the authors of this paper have perceived barriers that prevent requirements engineers from contributing properly to the development of the software systems that underpin the digital transformation we also realized that breaking down each of these barriers would contribute to requirements engineering (re) becoming ubiquitous in certain dimensions: re everywhere, with everyone, for everything, automated, accepting openness, and cross domain in this paper, we analyze each dimension of ubiquity in the scope of the interaction between requirements engineers and end users in particular, we point out the transformation that is required to break down each barrier, present the perspective of the scientific community and our own practical perspective, and discuss our vision on how to achieve this dimension of ubiquity our goal is to raise the interest of the research community in providing approaches to address the barriers and move towards ubiquitous re",
            "sequence": "[CLS] None [SEP] towards ubiquitous re: a perspective on requirements engineering in the era of digital transformation we are now living in the era of digital transformation: innovative and digital business models are transforming the global business world and society however, the authors of this paper have perceived barriers that prevent requirements engineers from contributing properly to the development of the software systems that underpin the digital transformation we also realized that breaking down each of these barriers would contribute to requirements engineering (re) becoming ubiquitous in certain dimensions: re everywhere, with everyone, for everything, automated, accepting openness, and cross domain in this paper, we analyze each dimension of ubiquity in the scope of the interaction between requirements engineers and end users in particular, we point out the transformation that is required to break down each barrier, present the perspective of the scientific community and our own practical perspective, and discuss our vision on how to achieve this dimension of ubiquity our goal is to raise the interest of the research community in providing approaches to address the barriers and move towards ubiquitous re [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R25109",
            "template_id": null,
            "paper_id": "R25109",
            "premise": null,
            "hypothesis": "examining participation participatory design (pd) seeks to promote and regulate the negotiation of social change although many methods claim to be participatory, empirical evidence to support them is lacking few comprehensive criteria exist to describe and evaluate participation as experienced by stakeholders there is a need for rigorous research tools to study, validate and improve pd practice this paper presents the development and initial testing of parte (participation evaluation), an interdisciplinary and intercommunity approach to studying and supporting participation in pd semi structured interviews based on the framework showed it to be useful in: a) revealing differences in how stakeholders view participation and design, b) developing a personal frame of participation c) exploration of the future of participatory practices; and d) suggesting actions to resolve specific challenges or contradictions in participation at a broader level the paper discusses the need to move away from considering pd as a practice claimed by designers towards a more open dialogue between all stakeholders to collective redefine \"participation and design\" for social change",
            "sequence": "[CLS] None [SEP] examining participation participatory design (pd) seeks to promote and regulate the negotiation of social change although many methods claim to be participatory, empirical evidence to support them is lacking few comprehensive criteria exist to describe and evaluate participation as experienced by stakeholders there is a need for rigorous research tools to study, validate and improve pd practice this paper presents the development and initial testing of parte (participation evaluation), an interdisciplinary and intercommunity approach to studying and supporting participation in pd semi structured interviews based on the framework showed it to be useful in: a) revealing differences in how stakeholders view participation and design, b) developing a personal frame of participation c) exploration of the future of participatory practices; and d) suggesting actions to resolve specific challenges or contradictions in participation at a broader level the paper discusses the need to move away from considering pd as a practice claimed by designers towards a more open dialogue between all stakeholders to collective redefine \"participation and design\" for social change [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R11",
                "label": "Science"
            }
        },
        {
            "instance_id": "R195495",
            "template_id": null,
            "paper_id": "R195495",
            "premise": null,
            "hypothesis": "safe: a simple approach for feature extraction from app descriptions and app reviews a main advantage of app stores is that they aggregate important information created by both developers and users in the app store product pages, developers usually describe and maintain the features of their apps in the app reviews, users comment these features recent studies focused on mining app features either as described by developers or as reviewed by users however, extracting and matching the features from the app descriptions and the reviews is essential to bear the app store advantages, e g allowing analysts to identify which app features are actually being reviewed and which are not in this paper, we propose safe, a novel uniform approach to extract app features from the single app pages, the single reviews and to match them we manually build 18 part of speech patterns and 5 sentence patterns that are frequently used in text referring to app features we then apply these patterns with several text pre and post processing steps a major advantage of our approach is that it does not require large training and configuration data to evaluate its accuracy, we manually extracted the features mentioned in the pages and reviews of 10 apps the extraction precision and recall outperformed two state of the art approaches for well maintained app pages such as for google drive our approach has a precision of 87% and on average 56% for 10 evaluated apps safe also matches 87% of the features extracted from user reviews to those extracted from the app descriptions",
            "sequence": "[CLS] None [SEP] safe: a simple approach for feature extraction from app descriptions and app reviews a main advantage of app stores is that they aggregate important information created by both developers and users in the app store product pages, developers usually describe and maintain the features of their apps in the app reviews, users comment these features recent studies focused on mining app features either as described by developers or as reviewed by users however, extracting and matching the features from the app descriptions and the reviews is essential to bear the app store advantages, e g allowing analysts to identify which app features are actually being reviewed and which are not in this paper, we propose safe, a novel uniform approach to extract app features from the single app pages, the single reviews and to match them we manually build 18 part of speech patterns and 5 sentence patterns that are frequently used in text referring to app features we then apply these patterns with several text pre and post processing steps a major advantage of our approach is that it does not require large training and configuration data to evaluate its accuracy, we manually extracted the features mentioned in the pages and reviews of 10 apps the extraction precision and recall outperformed two state of the art approaches for well maintained app pages such as for google drive our approach has a precision of 87% and on average 56% for 10 evaluated apps safe also matches 87% of the features extracted from user reviews to those extracted from the app descriptions [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R178155",
            "template_id": null,
            "paper_id": "R178155",
            "premise": null,
            "hypothesis": "a link prediction approach for item recommendation with complex number recommendation can be reduced to a sub problem of link prediction, with specific nodes (users and items) and links (similar relations among users/items, and interactions between users and items) however, the previous link prediction algorithms need to be modified to suit the recommendation cases since they do not consider the separation of these two fundamental relations: similar or dissimilar and like or dislike in this paper, we propose a novel and unified way to solve this problem, which models the relation duality using complex number under this representation, the previous works can directly reuse in experiments with the movie lens dataset and the android software website appchina com, the presented approach achieves significant performance improvement comparing with other popular recommendation algorithms both in accuracy and coverage besides, our results revealed some new findings first, it is observed that the performance is improved when the user and item popularities are taken into account second, the item popularity plays a more important role than the user popularity does in final recommendation since its notable performance, we are working to apply it in a commercial setting, appchina com website, for application recommendation",
            "sequence": "[CLS] None [SEP] a link prediction approach for item recommendation with complex number recommendation can be reduced to a sub problem of link prediction, with specific nodes (users and items) and links (similar relations among users/items, and interactions between users and items) however, the previous link prediction algorithms need to be modified to suit the recommendation cases since they do not consider the separation of these two fundamental relations: similar or dissimilar and like or dislike in this paper, we propose a novel and unified way to solve this problem, which models the relation duality using complex number under this representation, the previous works can directly reuse in experiments with the movie lens dataset and the android software website appchina com, the presented approach achieves significant performance improvement comparing with other popular recommendation algorithms both in accuracy and coverage besides, our results revealed some new findings first, it is observed that the performance is improved when the user and item popularities are taken into account second, the item popularity plays a more important role than the user popularity does in final recommendation since its notable performance, we are working to apply it in a commercial setting, appchina com website, for application recommendation [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            }
        },
        {
            "instance_id": "R145732",
            "template_id": null,
            "paper_id": "R145732",
            "premise": null,
            "hypothesis": "tribological study on tailored formed axial bearing washers to enhance tribological contacts under cyclic load, high performance materials are required utilizing the same high strength material for the whole machine element is not resource efficient in order to manufacture machine elements with extended functionality and specific properties, a combination of different materials can be used in a single component for a more efficient material utilization by combining different joining techniques with subsequent forming, multi material or tailored components can be manufactured to reduce material costs and energy consumption during the component service life, a less expensive lightweight material should be used for regions remote from the highly stressed zones the scope is not only to obtain the desired shape and dimensions for the finishing process, but also to improve properties like the bond strength between different materials and the microscopic structure of the material the multi material approach can be applied to all components requiring different properties in separate component regions such as shafts, bearings or bushes the current study exemplarily presents the process route for the production of an axial bearing washer by means of tailored forming technology the bearing washers were chosen to fit axial roller bearings (type 81212) the manufacturing process starts with the laser wire cladding of a hard facing made of martensitic chromium silicon steel (1 4718) on a base substrate of s235 (1 0038) steel subsequently, the bearing washers are forged after finishing, the surfaces of the bearing washers were tested in thrust bearings on an fe 8 test rig the operational test of the bearings consists in a run in phase at 250 rpm a bearing failure is determined by a condition monitoring system before and after this, the bearings were inspected by optical and ultrasonic microscopy in order to examine whether the bond of the coat is resistant against rolling contact fatigue the feasibility of the approach could be proven by endurance test the joining zone was able to withstand the rolling contact stresses and the bearing failed due to material induced fatigue with high cycle stability",
            "sequence": "[CLS] None [SEP] tribological study on tailored formed axial bearing washers to enhance tribological contacts under cyclic load, high performance materials are required utilizing the same high strength material for the whole machine element is not resource efficient in order to manufacture machine elements with extended functionality and specific properties, a combination of different materials can be used in a single component for a more efficient material utilization by combining different joining techniques with subsequent forming, multi material or tailored components can be manufactured to reduce material costs and energy consumption during the component service life, a less expensive lightweight material should be used for regions remote from the highly stressed zones the scope is not only to obtain the desired shape and dimensions for the finishing process, but also to improve properties like the bond strength between different materials and the microscopic structure of the material the multi material approach can be applied to all components requiring different properties in separate component regions such as shafts, bearings or bushes the current study exemplarily presents the process route for the production of an axial bearing washer by means of tailored forming technology the bearing washers were chosen to fit axial roller bearings (type 81212) the manufacturing process starts with the laser wire cladding of a hard facing made of martensitic chromium silicon steel (1 4718) on a base substrate of s235 (1 0038) steel subsequently, the bearing washers are forged after finishing, the surfaces of the bearing washers were tested in thrust bearings on an fe 8 test rig the operational test of the bearings consists in a run in phase at 250 rpm a bearing failure is determined by a condition monitoring system before and after this, the bearings were inspected by optical and ultrasonic microscopy in order to examine whether the bond of the coat is resistant against rolling contact fatigue the feasibility of the approach could be proven by endurance test the joining zone was able to withstand the rolling contact stresses and the bearing failed due to material induced fatigue with high cycle stability [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R137654",
                "label": "Mechanical Process Engineering"
            }
        },
        {
            "instance_id": "R145720",
            "template_id": null,
            "paper_id": "R145720",
            "premise": null,
            "hypothesis": "investigations on tailored forming of aisi 52100 as rolling bearing raceway hybrid cylindrical roller thrust bearing washers of type 81212 were manufactured by tailored forming an aisi 1022m base material, featuring a sufficient strength for structural loads, was cladded with the bearing steel aisi 52100 by plasma transferred arc welding (pta) though aisi 52100 is generally regarded as non weldable, it could be applied as a cladding material by adjusting pta parameters the cladded parts were investigated after each individual process step and subsequently tested under rolling contact load welding defects that could not be completely eliminated by the subsequent hot forming were characterized by means of scanning acoustic microscopy and micrographs below the surface, pores with a typical size of ten \u00b5m were found to a depth of about 0 45 mm in the material transition zone and between individual weld seams, larger voids were observed grinding of the surface after heat treatment caused compressive residual stresses near the surface with a relatively small depth fatigue tests were carried out on an fe8 test rig eighty two percent of the calculated rating life for conventional bearings was achieved a high failure slope of the weibull regression was determined a relationship between the weld defects and the fatigue behavior is likely",
            "sequence": "[CLS] None [SEP] investigations on tailored forming of aisi 52100 as rolling bearing raceway hybrid cylindrical roller thrust bearing washers of type 81212 were manufactured by tailored forming an aisi 1022m base material, featuring a sufficient strength for structural loads, was cladded with the bearing steel aisi 52100 by plasma transferred arc welding (pta) though aisi 52100 is generally regarded as non weldable, it could be applied as a cladding material by adjusting pta parameters the cladded parts were investigated after each individual process step and subsequently tested under rolling contact load welding defects that could not be completely eliminated by the subsequent hot forming were characterized by means of scanning acoustic microscopy and micrographs below the surface, pores with a typical size of ten \u00b5m were found to a depth of about 0 45 mm in the material transition zone and between individual weld seams, larger voids were observed grinding of the surface after heat treatment caused compressive residual stresses near the surface with a relatively small depth fatigue tests were carried out on an fe8 test rig eighty two percent of the calculated rating life for conventional bearings was achieved a high failure slope of the weibull regression was determined a relationship between the weld defects and the fatigue behavior is likely [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R137654",
                "label": "Mechanical Process Engineering"
            }
        },
        {
            "instance_id": "R41144",
            "template_id": null,
            "paper_id": "R41144",
            "premise": null,
            "hypothesis": "up scalable and controllable electrolytic production of photo responsive nanostructured silicon the electrochemical reduction of solid silica has been investigated in molten cacl2 at 900 \u00b0c for the one step, up scalable, controllable and affordable production of nanostructured silicon with promising photo responsive properties cyclic voltammetry of the metallic cavity electrode loaded with fine silica powder was performed to elaborate the electrochemical reduction mechanism potentiostatic electrolysis of porous and dense silica pellets was carried out at different potentials, focusing on the influences of the electrolysis potential and the microstructure of the precursory silica on the product purity and microstructure the findings suggest a potential range between \u22120 60 and \u22120 95 v (vs ag/agcl) for the production of nanostructured silicon with high purity (>99 wt%) according to the elucidated mechanism on the electro growth of the silicon nanostructures, optimal process parameters for the controllable preparation of high purity silicon nanoparticles and nanowires were identified scaling up the optimal electrolysis was successful at the gram scale for the preparation of high purity silicon nanowires which exhibited promising photo responsive properties",
            "sequence": "[CLS] None [SEP] up scalable and controllable electrolytic production of photo responsive nanostructured silicon the electrochemical reduction of solid silica has been investigated in molten cacl2 at 900 \u00b0c for the one step, up scalable, controllable and affordable production of nanostructured silicon with promising photo responsive properties cyclic voltammetry of the metallic cavity electrode loaded with fine silica powder was performed to elaborate the electrochemical reduction mechanism potentiostatic electrolysis of porous and dense silica pellets was carried out at different potentials, focusing on the influences of the electrolysis potential and the microstructure of the precursory silica on the product purity and microstructure the findings suggest a potential range between \u22120 60 and \u22120 95 v (vs ag/agcl) for the production of nanostructured silicon with high purity (>99 wt%) according to the elucidated mechanism on the electro growth of the silicon nanostructures, optimal process parameters for the controllable preparation of high purity silicon nanoparticles and nanowires were identified scaling up the optimal electrolysis was successful at the gram scale for the preparation of high purity silicon nanowires which exhibited promising photo responsive properties [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            }
        },
        {
            "instance_id": "R53034",
            "template_id": null,
            "paper_id": "R53034",
            "premise": null,
            "hypothesis": "modeling safest and optimal emergency evacuation plan for large scale pedestrians environments large scale events are always vulnerable to natural disasters and man made chaos which poses great threat to crowd safety such events need an appropriate evacuation plan to alleviate the risk of causalities we propose a modeling framework for large scale evacuation of pedestrians during emergency situation proposed framework presents optimal and safest path evacuation for a hypothetical large scale crowd scenario the main aim is to provide the safest and nearest evacuation path because during disastrous situations there is possibility of exit gate blockade and directions of evacuees may have to be changed at run time for this purpose run time diversions are given to evacuees to ensure their quick and safest exit in this work, different evacuation algorithms are implemented and compared to determine the optimal solution in terms of evacuation time and crowd safety the recommended framework incorporates anylogic simulation environment to design complex spatial environment for large scale pedestrians as agents",
            "sequence": "[CLS] None [SEP] modeling safest and optimal emergency evacuation plan for large scale pedestrians environments large scale events are always vulnerable to natural disasters and man made chaos which poses great threat to crowd safety such events need an appropriate evacuation plan to alleviate the risk of causalities we propose a modeling framework for large scale evacuation of pedestrians during emergency situation proposed framework presents optimal and safest path evacuation for a hypothetical large scale crowd scenario the main aim is to provide the safest and nearest evacuation path because during disastrous situations there is possibility of exit gate blockade and directions of evacuees may have to be changed at run time for this purpose run time diversions are given to evacuees to ensure their quick and safest exit in this work, different evacuation algorithms are implemented and compared to determine the optimal solution in terms of evacuation time and crowd safety the recommended framework incorporates anylogic simulation environment to design complex spatial environment for large scale pedestrians as agents [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R160390",
            "template_id": null,
            "paper_id": "R160390",
            "premise": null,
            "hypothesis": "collaborative city digital twin for the covid 19 pandemic: a federated learning solution \"in this work, we propose a collaborative city digital twin based on fl, a novel paradigm that allowing multiple city dt to share the local strategy and status in a timely manner in particular, an fl central server manages the local updates of multiple collaborators (city dt), provides a global model which is trained in multiple iterations at different city dt systems, until the model gains the correlations between various response plan and infection trend that means, a collaborative city dt paradigm based on fl techniques can obtain knowledge and patterns from multiple dts, and eventually establish a `global view' for city crisis management meanwhile, it also helps to improve each city digital twin selves by consolidating other dt's respective data without violating privacy rules to validate the proposed solution, we take covid 19 pandemic as a case study the experimental results on the real dataset with various response plan validate our proposed solution and demonstrate the superior performance \"",
            "sequence": "[CLS] None [SEP] collaborative city digital twin for the covid 19 pandemic: a federated learning solution \"in this work, we propose a collaborative city digital twin based on fl, a novel paradigm that allowing multiple city dt to share the local strategy and status in a timely manner in particular, an fl central server manages the local updates of multiple collaborators (city dt), provides a global model which is trained in multiple iterations at different city dt systems, until the model gains the correlations between various response plan and infection trend that means, a collaborative city dt paradigm based on fl techniques can obtain knowledge and patterns from multiple dts, and eventually establish a `global view' for city crisis management meanwhile, it also helps to improve each city digital twin selves by consolidating other dt's respective data without violating privacy rules to validate the proposed solution, we take covid 19 pandemic as a case study the experimental results on the real dataset with various response plan validate our proposed solution and demonstrate the superior performance \" [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            }
        },
        {
            "instance_id": "R160291",
            "template_id": null,
            "paper_id": "R160291",
            "premise": null,
            "hypothesis": "a smart campus\u2019 digital twin for sustainable comfort monitoring interdisciplinary cross cultural and cross organizational research offers great opportunities for innovative breakthroughs in the field of smart cities, yet it also presents organizational and knowledge development hurdles smart cities must be large towns able to sustain the needs of their citizens while promoting environmental sustainability smart cities foment the widespread use of novel information and communication technologies (icts); however, experimenting with these technologies in such a large geographical area is unfeasible consequently, smart campuses (scs), which are universities where technological devices and applications create new experiences or services and facilitate operational efficiency, allow experimentation on a smaller scale, the concept of scs as a testbed for a smart city is gaining momentum in the research community nevertheless, while universities acknowledge the academic role of a smart and sustainable approach to higher education, campus life and other student activities remain a mystery, which have never been universally solved this paper proposes a sc concept to investigate the integration of building information modeling tools with internet of things (iot) based wireless sensor networks in the fields of environmental monitoring and emotion detection to provide insights into the level of comfort additionally, it explores the ability of universities to contribute to local sustainability projects by sharing knowledge and experience across a multi disciplinary team preliminary results highlight the significance of monitoring workspaces because productivity has been proven to be directly influenced by environment parameters the comfort monitoring infrastructure could also be reused to monitor physical parameters from educational premises to increase energy efficiency",
            "sequence": "[CLS] None [SEP] a smart campus\u2019 digital twin for sustainable comfort monitoring interdisciplinary cross cultural and cross organizational research offers great opportunities for innovative breakthroughs in the field of smart cities, yet it also presents organizational and knowledge development hurdles smart cities must be large towns able to sustain the needs of their citizens while promoting environmental sustainability smart cities foment the widespread use of novel information and communication technologies (icts); however, experimenting with these technologies in such a large geographical area is unfeasible consequently, smart campuses (scs), which are universities where technological devices and applications create new experiences or services and facilitate operational efficiency, allow experimentation on a smaller scale, the concept of scs as a testbed for a smart city is gaining momentum in the research community nevertheless, while universities acknowledge the academic role of a smart and sustainable approach to higher education, campus life and other student activities remain a mystery, which have never been universally solved this paper proposes a sc concept to investigate the integration of building information modeling tools with internet of things (iot) based wireless sensor networks in the fields of environmental monitoring and emotion detection to provide insights into the level of comfort additionally, it explores the ability of universities to contribute to local sustainability projects by sharing knowledge and experience across a multi disciplinary team preliminary results highlight the significance of monitoring workspaces because productivity has been proven to be directly influenced by environment parameters the comfort monitoring infrastructure could also be reused to monitor physical parameters from educational premises to increase energy efficiency [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            }
        },
        {
            "instance_id": "R160415",
            "template_id": null,
            "paper_id": "R160415",
            "premise": null,
            "hypothesis": "digital twins: from personalised medicine to precision public health a digital twin is a virtual model of a physical entity, with dynamic, bi directional links between the physical entity and its corresponding twin in the digital domain digital twins are increasingly used today in different industry sectors applied to medicine and public health, digital twin technology can drive a much needed radical transformation of traditional electronic health/medical records (focusing on individuals) and their aggregates (covering populations) to make them ready for a new era of precision (and accuracy) medicine and public health digital twins enable learning and discovering new knowledge, new hypothesis generation and testing, and in silico experiments and comparisons they are poised to play a key role in formulating highly personalised treatments and interventions in the future this paper provides an overview of the technology\u2019s history and main concepts a number of application examples of digital twins for personalised medicine, public health, and smart healthy cities are presented, followed by a brief discussion of the key technical and other challenges involved in such applications, including ethical issues that arise when digital twins are applied to model humans",
            "sequence": "[CLS] None [SEP] digital twins: from personalised medicine to precision public health a digital twin is a virtual model of a physical entity, with dynamic, bi directional links between the physical entity and its corresponding twin in the digital domain digital twins are increasingly used today in different industry sectors applied to medicine and public health, digital twin technology can drive a much needed radical transformation of traditional electronic health/medical records (focusing on individuals) and their aggregates (covering populations) to make them ready for a new era of precision (and accuracy) medicine and public health digital twins enable learning and discovering new knowledge, new hypothesis generation and testing, and in silico experiments and comparisons they are poised to play a key role in formulating highly personalised treatments and interventions in the future this paper provides an overview of the technology\u2019s history and main concepts a number of application examples of digital twins for personalised medicine, public health, and smart healthy cities are presented, followed by a brief discussion of the key technical and other challenges involved in such applications, including ethical issues that arise when digital twins are applied to model humans [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            }
        },
        {
            "instance_id": "R195986",
            "template_id": null,
            "paper_id": "R195986",
            "premise": null,
            "hypothesis": "do words make a difference? an empirical study on the impact of taxonomies on the classification of requirements \"requirements taxonomies help to classify and channel the requirements in a project a very simple taxonomy is the distinction between functional and non functional requirements furthermore, a taxonomy helps to decide if a statement is a requirement at all or just something else (e g , 'information') the quality of a taxonomy is important as we do not want to put a statement in the wrong category in this paper, we argue that we need to take cognitive psychology into account in this task of requirements classification cognitive psychology focuses on the abilities and limitations of the human mind we present a controlled experiment and a replication in which we compare three requirements taxonomies the participants had to evaluate a set of requirements based on the given taxonomies the results of these experiments show that there are differences between the taxonomies: interestingly, the question whether a statement is identified as a requirement or not depends on the taxonomy these experiments present initial results, we assume that these results are related to phenomena of cognitive psychology we conclude that the wording should be carefully taken into account in the definition of the categories of a high quality requirements taxonomy \"",
            "sequence": "[CLS] None [SEP] do words make a difference? an empirical study on the impact of taxonomies on the classification of requirements \"requirements taxonomies help to classify and channel the requirements in a project a very simple taxonomy is the distinction between functional and non functional requirements furthermore, a taxonomy helps to decide if a statement is a requirement at all or just something else (e g , 'information') the quality of a taxonomy is important as we do not want to put a statement in the wrong category in this paper, we argue that we need to take cognitive psychology into account in this task of requirements classification cognitive psychology focuses on the abilities and limitations of the human mind we present a controlled experiment and a replication in which we compare three requirements taxonomies the participants had to evaluate a set of requirements based on the given taxonomies the results of these experiments show that there are differences between the taxonomies: interestingly, the question whether a statement is identified as a requirement or not depends on the taxonomy these experiments present initial results, we assume that these results are related to phenomena of cognitive psychology we conclude that the wording should be carefully taken into account in the definition of the categories of a high quality requirements taxonomy \" [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R193828",
            "template_id": null,
            "paper_id": "R193828",
            "premise": null,
            "hypothesis": "mining reddit as a new source for software requirements mining app stores and social media has proven to be a good source for collecting user feedback to foster requirements engineering and software evolution recent literature on mining software related data from social platforms, such as twitter and facebook, shows that it complements app store mining however, there are many other platforms where users discuss and provide feedback on software applications that are not thoroughly researched and analysed one of such platforms is reddit in this paper, we introduce reddit as a new potential data source and explore if and how requirements engineering and software evolution can benefit from obtaining user feedback from reddit we also present an exploratory study in which we analysed the usage characteristics (i e , frequency of posts, number of comments, and number of users for each subreddit) of reddit posts about software applications furthermore, we examined the content of the posts and the results reveal that almost 54% of posts contain useful information finally, we investigated the potential of automatic classification and applied machine learning algorithms to unstructured and noisy reddit data to perform automated classification into the categories of bug reports, feature related, and irrelevant we found that the support vector machine algorithm with the f1 score of 84% can be effective in categorizing reddit posts our results show that reddit posts provide useful feedback on software applications that can foster requirements engineering and software evolution",
            "sequence": "[CLS] None [SEP] mining reddit as a new source for software requirements mining app stores and social media has proven to be a good source for collecting user feedback to foster requirements engineering and software evolution recent literature on mining software related data from social platforms, such as twitter and facebook, shows that it complements app store mining however, there are many other platforms where users discuss and provide feedback on software applications that are not thoroughly researched and analysed one of such platforms is reddit in this paper, we introduce reddit as a new potential data source and explore if and how requirements engineering and software evolution can benefit from obtaining user feedback from reddit we also present an exploratory study in which we analysed the usage characteristics (i e , frequency of posts, number of comments, and number of users for each subreddit) of reddit posts about software applications furthermore, we examined the content of the posts and the results reveal that almost 54% of posts contain useful information finally, we investigated the potential of automatic classification and applied machine learning algorithms to unstructured and noisy reddit data to perform automated classification into the categories of bug reports, feature related, and irrelevant we found that the support vector machine algorithm with the f1 score of 84% can be effective in categorizing reddit posts our results show that reddit posts provide useful feedback on software applications that can foster requirements engineering and software evolution [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R140197",
            "template_id": null,
            "paper_id": "R140197",
            "premise": null,
            "hypothesis": "dna barcodes distinguish species of tropical lepidoptera although central to much biological research, the identification of species is often difficult the use of dna barcodes, short dna sequences from a standardized region of the genome, has recently been proposed as a tool to facilitate species identification and discovery however, the effectiveness of dna barcoding for identifying specimens in species rich tropical biotas is unknown here we show that cytochrome c oxidase i dna barcodes effectively discriminate among species in three lepidoptera families from area de conservaci\u00f3n guanacaste in northwestern costa rica we found that 97 9% of the 521 species recognized by prior taxonomic work possess distinctive cytochrome c oxidase i barcodes and that the few instances of interspecific sequence overlap involve very similar species we also found two or more barcode clusters within each of 13 supposedly single species covariation between these clusters and morphological and/or ecological traits indicates overlooked species complexes if these results are general, dna barcoding will significantly aid species identification and discovery in tropical settings",
            "sequence": "[CLS] None [SEP] dna barcodes distinguish species of tropical lepidoptera although central to much biological research, the identification of species is often difficult the use of dna barcodes, short dna sequences from a standardized region of the genome, has recently been proposed as a tool to facilitate species identification and discovery however, the effectiveness of dna barcoding for identifying specimens in species rich tropical biotas is unknown here we show that cytochrome c oxidase i dna barcodes effectively discriminate among species in three lepidoptera families from area de conservaci\u00f3n guanacaste in northwestern costa rica we found that 97 9% of the 521 species recognized by prior taxonomic work possess distinctive cytochrome c oxidase i barcodes and that the few instances of interspecific sequence overlap involve very similar species we also found two or more barcode clusters within each of 13 supposedly single species covariation between these clusters and morphological and/or ecological traits indicates overlooked species complexes if these results are general, dna barcoding will significantly aid species identification and discovery in tropical settings [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            }
        },
        {
            "instance_id": "R194139",
            "template_id": null,
            "paper_id": "R194139",
            "premise": null,
            "hypothesis": "an ai assisted approach for checking the completeness of privacy policies against gdpr privacy policies are critical for helping individuals make informed decisions about their personal data in europe, privacy policies are subject to compliance with the general data protection regulation (gdpr) if done entirely manually, checking whether a given privacy policy complies with gdpr is both time consuming and error prone automated support for this task is thus advantageous at the moment, there is an evident lack of such support on the market in this paper, we tackle an important dimension of gdpr compliance checking for privacy policies specifically, we provide automated support for checking whether the content of a given privacy policy is complete according to the provisions stipulated by gdpr to do so, we present: (1) a conceptual model to characterize the information content envisaged by gdpr for privacy policies, (2) an ai assisted approach for classifying the information content in gdpr privacy policies and subsequently checking how well the classified content meets the completeness criteria of interest; and (3) an evaluation of our approach through a case study over 24 unseen privacy policies for classification, we leverage a combination of natural language processing and supervised machine learning our experimental material is comprised of 234 real privacy policies from the fund industry our empirical results indicate that our approach detected 45 of the total of 47 incompleteness issues in the 24 privacy policies it was applied to over these policies, the approach had eight false positives the approach thus has a precision of 85% and recall of 96% over our case study",
            "sequence": "[CLS] None [SEP] an ai assisted approach for checking the completeness of privacy policies against gdpr privacy policies are critical for helping individuals make informed decisions about their personal data in europe, privacy policies are subject to compliance with the general data protection regulation (gdpr) if done entirely manually, checking whether a given privacy policy complies with gdpr is both time consuming and error prone automated support for this task is thus advantageous at the moment, there is an evident lack of such support on the market in this paper, we tackle an important dimension of gdpr compliance checking for privacy policies specifically, we provide automated support for checking whether the content of a given privacy policy is complete according to the provisions stipulated by gdpr to do so, we present: (1) a conceptual model to characterize the information content envisaged by gdpr for privacy policies, (2) an ai assisted approach for classifying the information content in gdpr privacy policies and subsequently checking how well the classified content meets the completeness criteria of interest; and (3) an evaluation of our approach through a case study over 24 unseen privacy policies for classification, we leverage a combination of natural language processing and supervised machine learning our experimental material is comprised of 234 real privacy policies from the fund industry our empirical results indicate that our approach detected 45 of the total of 47 incompleteness issues in the 24 privacy policies it was applied to over these policies, the approach had eight false positives the approach thus has a precision of 85% and recall of 96% over our case study [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R195005",
            "template_id": null,
            "paper_id": "R195005",
            "premise": null,
            "hypothesis": "catalog of invisibility requirements for ubicomp and iot applications a new set of non functional requirements (nfrs) have appeared with the advent of ubiquitous computing (ubicomp) and more recently internet of things (iot) invisibility is one of these nfrs that means the ability to hide technology from users although invisibility is long seen as an essential characteristic for achieving the goals of ubicomp, it has not been cataloged regarding its subcharacteristics and solutions, making its design and requirements specification in such applications a challenging task considering the softgoal interdependency graph (sig), which is a well known format to catalog nfrs, this work aims at capturing subcharacteristics and solutions for invisibility and cataloging them in a sig since there is no systematic approach on how to build sigs, we also propose to systematize the definition of invisibility sig using the following well defined research methods: snowballing, database search, grounded theory and questionnaires as a result, we got an invisibility sig composed of two main subcharacteristics, twelve sub subcharacteristics, ten general solutions and fifty six specific solutions this organized body of knowledge is useful for supporting software engineers to specify requirements and practical solutions for ubicomp and iot applications furthermore, the proposed methodology used to capture and catalog requirements in a sig can be reused for other nfrs",
            "sequence": "[CLS] None [SEP] catalog of invisibility requirements for ubicomp and iot applications a new set of non functional requirements (nfrs) have appeared with the advent of ubiquitous computing (ubicomp) and more recently internet of things (iot) invisibility is one of these nfrs that means the ability to hide technology from users although invisibility is long seen as an essential characteristic for achieving the goals of ubicomp, it has not been cataloged regarding its subcharacteristics and solutions, making its design and requirements specification in such applications a challenging task considering the softgoal interdependency graph (sig), which is a well known format to catalog nfrs, this work aims at capturing subcharacteristics and solutions for invisibility and cataloging them in a sig since there is no systematic approach on how to build sigs, we also propose to systematize the definition of invisibility sig using the following well defined research methods: snowballing, database search, grounded theory and questionnaires as a result, we got an invisibility sig composed of two main subcharacteristics, twelve sub subcharacteristics, ten general solutions and fifty six specific solutions this organized body of knowledge is useful for supporting software engineers to specify requirements and practical solutions for ubicomp and iot applications furthermore, the proposed methodology used to capture and catalog requirements in a sig can be reused for other nfrs [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R25139",
            "template_id": null,
            "paper_id": "R25139",
            "premise": null,
            "hypothesis": "led a pillars as the chassis of cars become more robust, the pillars of a car become broader in order to increase driver safety as a pillars grow wider, so too does their negative affect on the panoramic view of the driver and with a smaller field of vision, the risk of overlooking a pedestrian or an object outside the car increases in order to deal with a pillar blind spots, this project examined how distances and directions of possible obstacles can be displayed and how different visualization types with led strips on the a pillars can affect drivers perception the result of this study shows that such a prototype improves the panoramic view for car drivers resulting in higher security for road users",
            "sequence": "[CLS] None [SEP] led a pillars as the chassis of cars become more robust, the pillars of a car become broader in order to increase driver safety as a pillars grow wider, so too does their negative affect on the panoramic view of the driver and with a smaller field of vision, the risk of overlooking a pedestrian or an object outside the car increases in order to deal with a pillar blind spots, this project examined how distances and directions of possible obstacles can be displayed and how different visualization types with led strips on the a pillars can affect drivers perception the result of this study shows that such a prototype improves the panoramic view for car drivers resulting in higher security for road users [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R11",
                "label": "Science"
            }
        },
        {
            "instance_id": "R160408",
            "template_id": null,
            "paper_id": "R160408",
            "premise": null,
            "hypothesis": "using smart city technology to make healthcare smarter smart cities use information and communication technologies (icts) to scale services include utilities and transportation to a growing population in this paper, we discuss how smart city icts can also improve healthcare effectiveness and lower healthcare cost for smart city residents we survey current literature and introduce original research to offer an overview of how smart city infrastructure supports strategic healthcare using both mobile and ambient sensors combined with machine learning finally, we consider challenges that will be faced as healthcare providers make use of these opportunities",
            "sequence": "[CLS] None [SEP] using smart city technology to make healthcare smarter smart cities use information and communication technologies (icts) to scale services include utilities and transportation to a growing population in this paper, we discuss how smart city icts can also improve healthcare effectiveness and lower healthcare cost for smart city residents we survey current literature and introduce original research to offer an overview of how smart city infrastructure supports strategic healthcare using both mobile and ambient sensors combined with machine learning finally, we consider challenges that will be faced as healthcare providers make use of these opportunities [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            }
        },
        {
            "instance_id": "R111101",
            "template_id": null,
            "paper_id": "R111101",
            "premise": null,
            "hypothesis": "preparation of dicyano and methylcobinamide from vitamin b12a treatment of vitamin b 12a 1 (hydroxycobalamin hydrochloride, aquocobalamin) with nabh 4 and zncl 2 leads to the selective cleavage of the nucleotide loop and gives dicyanocobinamide 2a in good yield methylcobinamide 4 was prepared from 2 via aquocyanocobinamide 3 the glutathione mediated methylation of 3 in a ph 3 5 buffer solution proceeded with mel, but not with meots",
            "sequence": "[CLS] None [SEP] preparation of dicyano and methylcobinamide from vitamin b12a treatment of vitamin b 12a 1 (hydroxycobalamin hydrochloride, aquocobalamin) with nabh 4 and zncl 2 leads to the selective cleavage of the nucleotide loop and gives dicyanocobinamide 2a in good yield methylcobinamide 4 was prepared from 2 via aquocyanocobinamide 3 the glutathione mediated methylation of 3 in a ph 3 5 buffer solution proceeded with mel, but not with meots [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R129",
                "label": "Organic Chemistry"
            }
        },
        {
            "instance_id": "R41079",
            "template_id": null,
            "paper_id": "R41079",
            "premise": null,
            "hypothesis": "speech recognition using deep neural networks: a systematic review over the past decades, a tremendous amount of research has been done on the use of machine learning for speech processing applications, especially speech recognition however, in the past few years, research has focused on utilizing deep learning for speech related applications this new area of machine learning has yielded far better results when compared to others in a variety of applications including speech, and thus became a very attractive area of research this paper provides a thorough examination of the different studies that have been conducted since 2006, when deep learning first arose as a new area of machine learning, for speech applications a thorough statistical analysis is provided in this review which was conducted by extracting specific information from 174 papers published between the years 2006 and 2018 the results provided in this paper shed light on the trends of research in this area as well as bring focus to new research topics",
            "sequence": "[CLS] None [SEP] speech recognition using deep neural networks: a systematic review over the past decades, a tremendous amount of research has been done on the use of machine learning for speech processing applications, especially speech recognition however, in the past few years, research has focused on utilizing deep learning for speech related applications this new area of machine learning has yielded far better results when compared to others in a variety of applications including speech, and thus became a very attractive area of research this paper provides a thorough examination of the different studies that have been conducted since 2006, when deep learning first arose as a new area of machine learning, for speech applications a thorough statistical analysis is provided in this review which was conducted by extracting specific information from 174 papers published between the years 2006 and 2018 the results provided in this paper shed light on the trends of research in this area as well as bring focus to new research topics [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R133",
                "label": "Artificial Intelligence"
            }
        },
        {
            "instance_id": "R194281",
            "template_id": null,
            "paper_id": "R194281",
            "premise": null,
            "hypothesis": "learning requirements elicitation interviews with role playing, self assessment and peer review interviews are largely used in the practice of requirements elicitation nevertheless, performing an effective interview often depends on soft skills, and on knowledge acquired through experience when it comes to requirements engineering education and training (reet), limited resources and few well founded pedagogical approaches are available to allow students to acquire and improve their skills as interviewers this paper presents a novel pedagogical approach that combines role playing, peer review and self assessment to enable students to reflect on their mistakes, and improve their interview skills we evaluate the approach through a controlled quasi experiment the study shows that the approach significantly reduces the amount of mistakes made by the students feedback from the participants confirms the usefulness and easiness of the proposed training this work contributes to the body of knowledge of reet with an empirically evaluated method for teaching inter views furthermore, we share the pedagogical material used, to enable other educators to apply and possibly tailor the approach",
            "sequence": "[CLS] None [SEP] learning requirements elicitation interviews with role playing, self assessment and peer review interviews are largely used in the practice of requirements elicitation nevertheless, performing an effective interview often depends on soft skills, and on knowledge acquired through experience when it comes to requirements engineering education and training (reet), limited resources and few well founded pedagogical approaches are available to allow students to acquire and improve their skills as interviewers this paper presents a novel pedagogical approach that combines role playing, peer review and self assessment to enable students to reflect on their mistakes, and improve their interview skills we evaluate the approach through a controlled quasi experiment the study shows that the approach significantly reduces the amount of mistakes made by the students feedback from the participants confirms the usefulness and easiness of the proposed training this work contributes to the body of knowledge of reet with an empirically evaluated method for teaching inter views furthermore, we share the pedagogical material used, to enable other educators to apply and possibly tailor the approach [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R193022",
            "template_id": null,
            "paper_id": "R193022",
            "premise": null,
            "hypothesis": "design decisions in the construction of traceability information models for safe automotive systems traceability management relies on a supporting model, the traceability information model (tim), that defines which types of relationships exist between which artifacts and contains additional constraints such as multiplicities constructing a tim that is fit for purpose is crucial to ensure that a traceability strategy yields the desired benefits however, which design decisions are critical in the construction of tims and which impact they have on the usefulness and applicability of traceability is still an open question in this paper, we use two cases of tims constructed for safety critical, automotive systems with industrial safety experts, to identify key design decisions we also propose a comparison scheme for tims based on a systematic literature review and evaluate the two cases as well as tims from the literature according to the scheme based on our analyses, we thus derive key insights into tim construction and the design decisions that ensure that a tim is fit for purpose",
            "sequence": "[CLS] None [SEP] design decisions in the construction of traceability information models for safe automotive systems traceability management relies on a supporting model, the traceability information model (tim), that defines which types of relationships exist between which artifacts and contains additional constraints such as multiplicities constructing a tim that is fit for purpose is crucial to ensure that a traceability strategy yields the desired benefits however, which design decisions are critical in the construction of tims and which impact they have on the usefulness and applicability of traceability is still an open question in this paper, we use two cases of tims constructed for safety critical, automotive systems with industrial safety experts, to identify key design decisions we also propose a comparison scheme for tims based on a systematic literature review and evaluate the two cases as well as tims from the literature according to the scheme based on our analyses, we thus derive key insights into tim construction and the design decisions that ensure that a tim is fit for purpose [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R108983",
            "template_id": null,
            "paper_id": "R108983",
            "premise": null,
            "hypothesis": "barcoding the butterflies of southern south america: species delimitation efficacy, cryptic diversity and geographic patterns of divergence because the tropical regions of america harbor the highest concentration of butterfly species, its fauna has attracted considerable attention much less is known about the butterflies of southern south america, particularly argentina, where over 1,200 species occur to advance understanding of this fauna, we assembled a dna barcode reference library for 417 butterfly species of argentina, focusing on the atlantic forest, a biodiversity hotspot we tested the efficacy of this library for specimen identification, used it to assess the frequency of cryptic species, and examined geographic patterns of genetic variation, making this study the first large scale genetic assessment of the butterflies of southern south america the average sequence divergence to the nearest neighbor (i e minimum interspecific distance) was 6 91%, ten times larger than the mean distance to the furthest conspecific (0 69%), with a clear barcode gap present in all but four of the species represented by two or more specimens as a consequence, the dna barcode library was extremely effective in the discrimination of these species, allowing a correct identification in more than 95% of the cases singletons (i e species represented by a single sequence) were also distinguishable in the gene trees since they all had unique dna barcodes, divergent from those of the closest non conspecific the clustering algorithms implemented recognized from 416 to 444 barcode clusters, suggesting that the actual diversity of butterflies in argentina is 3%\u20139% higher than currently recognized furthermore, our survey added three new records of butterflies for the country (eurema agave, mithras hannelore, melanis hillapana) in summary, this study not only supported the utility of dna barcoding for the identification of the butterfly species of argentina, but also highlighted several cases of both deep intraspecific and shallow interspecific divergence that should be studied in more detail",
            "sequence": "[CLS] None [SEP] barcoding the butterflies of southern south america: species delimitation efficacy, cryptic diversity and geographic patterns of divergence because the tropical regions of america harbor the highest concentration of butterfly species, its fauna has attracted considerable attention much less is known about the butterflies of southern south america, particularly argentina, where over 1,200 species occur to advance understanding of this fauna, we assembled a dna barcode reference library for 417 butterfly species of argentina, focusing on the atlantic forest, a biodiversity hotspot we tested the efficacy of this library for specimen identification, used it to assess the frequency of cryptic species, and examined geographic patterns of genetic variation, making this study the first large scale genetic assessment of the butterflies of southern south america the average sequence divergence to the nearest neighbor (i e minimum interspecific distance) was 6 91%, ten times larger than the mean distance to the furthest conspecific (0 69%), with a clear barcode gap present in all but four of the species represented by two or more specimens as a consequence, the dna barcode library was extremely effective in the discrimination of these species, allowing a correct identification in more than 95% of the cases singletons (i e species represented by a single sequence) were also distinguishable in the gene trees since they all had unique dna barcodes, divergent from those of the closest non conspecific the clustering algorithms implemented recognized from 416 to 444 barcode clusters, suggesting that the actual diversity of butterflies in argentina is 3%\u20139% higher than currently recognized furthermore, our survey added three new records of butterflies for the country (eurema agave, mithras hannelore, melanis hillapana) in summary, this study not only supported the utility of dna barcoding for the identification of the butterfly species of argentina, but also highlighted several cases of both deep intraspecific and shallow interspecific divergence that should be studied in more detail [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            }
        },
        {
            "instance_id": "R137389",
            "template_id": null,
            "paper_id": "R137389",
            "premise": null,
            "hypothesis": "arrays of microplasmas for the controlled production of tunable high fluxes of reactive oxygen species at atmospheric pressure the atmospheric pressure generation of singlet delta oxygen (o2(a 1\u03b4g)) by microplasmas was experimentally studied the remarkable stability of microcathode sustained discharges (mcsds) allowed the operation of dc glow discharges, free from the glow to arc transition, in he/o2/no mixtures at atmospheric pressure from optical diagnostics measurements we deduced the yield of o2(a 1\u03b4g) by operating arrays of several mcsds in series, o2(a 1\u03b4g) densities higher than 1 0 \u00d7 1017 cm\u22123 were efficiently produced and transported over distances longer than 50 cm, corresponding to o2(a 1\u03b4g) partial pressures and production yields greater than 5 mbar and 6%, respectively at such high o2(a 1\u03b4g) densities, the fluorescence of the so called o2(a 1\u03b4g) dimol was observed as a red glow at 634 nm up to 1 m downstream parallel operation of arrays of mcsds was also implemented, generating o2(a 1\u03b4g) fluxes as high as 100 mmol h\u22121 in addition, ozone (o3) densities up to 1016 cm\u22123 were obtained finally, the density ratio of o2(a 1\u03b4g) to o3 was finely and easily tuned in the range [10\u22123\u201310+5], through the values of the discharge current and no concentration this opens up opportunities for a large spectrum of new applications, making this plasma source notably very useful for biomedicine",
            "sequence": "[CLS] None [SEP] arrays of microplasmas for the controlled production of tunable high fluxes of reactive oxygen species at atmospheric pressure the atmospheric pressure generation of singlet delta oxygen (o2(a 1\u03b4g)) by microplasmas was experimentally studied the remarkable stability of microcathode sustained discharges (mcsds) allowed the operation of dc glow discharges, free from the glow to arc transition, in he/o2/no mixtures at atmospheric pressure from optical diagnostics measurements we deduced the yield of o2(a 1\u03b4g) by operating arrays of several mcsds in series, o2(a 1\u03b4g) densities higher than 1 0 \u00d7 1017 cm\u22123 were efficiently produced and transported over distances longer than 50 cm, corresponding to o2(a 1\u03b4g) partial pressures and production yields greater than 5 mbar and 6%, respectively at such high o2(a 1\u03b4g) densities, the fluorescence of the so called o2(a 1\u03b4g) dimol was observed as a red glow at 634 nm up to 1 m downstream parallel operation of arrays of mcsds was also implemented, generating o2(a 1\u03b4g) fluxes as high as 100 mmol h\u22121 in addition, ozone (o3) densities up to 1016 cm\u22123 were obtained finally, the density ratio of o2(a 1\u03b4g) to o3 was finely and easily tuned in the range [10\u22123\u201310+5], through the values of the discharge current and no concentration this opens up opportunities for a large spectrum of new applications, making this plasma source notably very useful for biomedicine [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R114008",
                "label": "Applied Physics"
            }
        },
        {
            "instance_id": "R146932",
            "template_id": null,
            "paper_id": "R146932",
            "premise": null,
            "hypothesis": "dna barcodes reveal cryptic genetic diversity within the blackfly subgenus trichodagmia enderlein (diptera: simuliidae: simulium) and related taxa in the new world in this paper we investigate the utility of the coi dna barcoding region for species identification and for revealing hidden diversity within the subgenus trichodagmia and related taxa in the new world in total, 24 morphospecies within the current expanded taxonomic concept of trichodagmia were analyzed three species in the subgenus aspathia and 10 species in the subgenus simulium s str were also included in the analysis because of their putative phylogenetic relationship with trichodagmia in the neighbour joining analysis tree (nj) derived from the dna barcodes most of the specimens grouped together according to species or species groups as recognized by other morphotaxonomic studies the interspecific genetic divergence averaged 11 2% (range 2 8\u201319 5%), whereas intraspecific genetic divergence within morphologically distinct species averaged 0 5% (range 0\u20131 2%) higher values of genetic divergence (3 2\u20133 7%) in species complexes suggest the presence of cryptic diversity the existence of well defined groups within s piperi, s duodenicornium, s canadense and s rostratum indicate the possible presence of cryptic species within these taxa also, the suspected presence of a sibling species in s tarsatum and s paynei is supported dna barcodes also showed that specimens from species that were taxonomically difficult to delimit such as s hippovorum, s rubrithorax, s paynei, and other related taxa (s solarii), grouped together in the nj analysis, confirming the validity of their species status the recovery of partial barcodes from specimens in collections was time consuming and pcr success was low from specimens more than 10 years old however, when a sequence was obtained, it provided good resolution for species identification larvae preserved in \u2018weak\u2019 carnoy\u2019s solution (9:1 ethanol:acetic acid) provided full dna barcodes adding legs directly to the pcr mix from recently collected and preserved adults was an inexpensive, fast methodology to obtain full barcodes in summary, dna barcoding combined with a sound morphotaxonomic framework provides an effective approach for the delineation of species and for the discovery of hidden diversity in the subgenus trichodagmia",
            "sequence": "[CLS] None [SEP] dna barcodes reveal cryptic genetic diversity within the blackfly subgenus trichodagmia enderlein (diptera: simuliidae: simulium) and related taxa in the new world in this paper we investigate the utility of the coi dna barcoding region for species identification and for revealing hidden diversity within the subgenus trichodagmia and related taxa in the new world in total, 24 morphospecies within the current expanded taxonomic concept of trichodagmia were analyzed three species in the subgenus aspathia and 10 species in the subgenus simulium s str were also included in the analysis because of their putative phylogenetic relationship with trichodagmia in the neighbour joining analysis tree (nj) derived from the dna barcodes most of the specimens grouped together according to species or species groups as recognized by other morphotaxonomic studies the interspecific genetic divergence averaged 11 2% (range 2 8\u201319 5%), whereas intraspecific genetic divergence within morphologically distinct species averaged 0 5% (range 0\u20131 2%) higher values of genetic divergence (3 2\u20133 7%) in species complexes suggest the presence of cryptic diversity the existence of well defined groups within s piperi, s duodenicornium, s canadense and s rostratum indicate the possible presence of cryptic species within these taxa also, the suspected presence of a sibling species in s tarsatum and s paynei is supported dna barcodes also showed that specimens from species that were taxonomically difficult to delimit such as s hippovorum, s rubrithorax, s paynei, and other related taxa (s solarii), grouped together in the nj analysis, confirming the validity of their species status the recovery of partial barcodes from specimens in collections was time consuming and pcr success was low from specimens more than 10 years old however, when a sequence was obtained, it provided good resolution for species identification larvae preserved in \u2018weak\u2019 carnoy\u2019s solution (9:1 ethanol:acetic acid) provided full dna barcodes adding legs directly to the pcr mix from recently collected and preserved adults was an inexpensive, fast methodology to obtain full barcodes in summary, dna barcoding combined with a sound morphotaxonomic framework provides an effective approach for the delineation of species and for the discovery of hidden diversity in the subgenus trichodagmia [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            }
        },
        {
            "instance_id": "R112425",
            "template_id": null,
            "paper_id": "R112425",
            "premise": null,
            "hypothesis": "refinement and resolution of just in time requirements in open source software: a case study just in time (jit) requirements are characterized as not following the traditional requirement engineering approach, instead focusing on elaboration when the implementation begins in this experience report, we analyze both functional and nonfunctional jit requirements from three successful open source software (oss) projects, including firefox, lucene, and mylyn, to explore the common activities that shaped those requirements we identify a novel refinement and resolution process that all studied requirements followed from requirement inception to their complete realization and subsequent release this research provides new insights into how oss project teams create quality features from simple initial descriptions of jit requirements our study also initiates three captivating questions regarding jit requirements and opens new avenues for further research in this emerging field",
            "sequence": "[CLS] None [SEP] refinement and resolution of just in time requirements in open source software: a case study just in time (jit) requirements are characterized as not following the traditional requirement engineering approach, instead focusing on elaboration when the implementation begins in this experience report, we analyze both functional and nonfunctional jit requirements from three successful open source software (oss) projects, including firefox, lucene, and mylyn, to explore the common activities that shaped those requirements we identify a novel refinement and resolution process that all studied requirements followed from requirement inception to their complete realization and subsequent release this research provides new insights into how oss project teams create quality features from simple initial descriptions of jit requirements our study also initiates three captivating questions regarding jit requirements and opens new avenues for further research in this emerging field [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R113008",
            "template_id": null,
            "paper_id": "R113008",
            "premise": null,
            "hypothesis": "canary: extracting requirements related information from online discussions online discussions about software applications generate a large amount of requirements related information this information can potentially be usefully applied in requirements engineering; however currently, there are few systematic approaches for extracting such information to address this gap, we propose canary, an approach for extracting and querying requirements related information in online discussions the highlight of our approach is a high level query language that combines aspects of both requirements and discussion in online forums we give the semantics of the query language in terms of relational databases and sql we demonstrate the usefulness of the language using examples on real data extracted from online discussions our approach relies on human annotations of online discussions we highlight the subtleties involved in interpreting the content in online discussions and the assumptions and choices we made to effectively address them we demonstrate the feasibility of generating high quality annotations by obtaining them from lay amazon mechanical turk users",
            "sequence": "[CLS] None [SEP] canary: extracting requirements related information from online discussions online discussions about software applications generate a large amount of requirements related information this information can potentially be usefully applied in requirements engineering; however currently, there are few systematic approaches for extracting such information to address this gap, we propose canary, an approach for extracting and querying requirements related information in online discussions the highlight of our approach is a high level query language that combines aspects of both requirements and discussion in online forums we give the semantics of the query language in terms of relational databases and sql we demonstrate the usefulness of the language using examples on real data extracted from online discussions our approach relies on human annotations of online discussions we highlight the subtleties involved in interpreting the content in online discussions and the assumptions and choices we made to effectively address them we demonstrate the feasibility of generating high quality annotations by obtaining them from lay amazon mechanical turk users [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R194414",
            "template_id": null,
            "paper_id": "R194414",
            "premise": null,
            "hypothesis": "detecting bad smells in use case descriptions use case modeling is very popular to represent the functionality of the system to be developed, and it consists of two parts: use case diagram and use case description use case descriptions are written in structured natural language (nl), and the usage of nl can lead to poor descriptions such as ambiguous, inconsistent and/or incomplete descriptions, etc poor descriptions lead to missing requirements and eliciting incorrect requirements as well as less comprehensiveness of produced use case models this paper proposes a technique to automate detecting bad smells of use case descriptions, symptoms of poor descriptions at first, to clarify bad smells, we analyzed existing use case models to discover poor use case descriptions concretely and developed the list of bad smells, i e , a catalogue of bad smells some of the bad smells can be refined into measures using the goal question metric paradigm to automate their detection the main contribution of this paper is the automated detection of bad smells we have implemented an automated smell detector for 22 bad smells at first and assessed its usefulness by an experiment as a result, the first version of our tool got a precision ratio of 0 591 and recall ratio of 0 981",
            "sequence": "[CLS] None [SEP] detecting bad smells in use case descriptions use case modeling is very popular to represent the functionality of the system to be developed, and it consists of two parts: use case diagram and use case description use case descriptions are written in structured natural language (nl), and the usage of nl can lead to poor descriptions such as ambiguous, inconsistent and/or incomplete descriptions, etc poor descriptions lead to missing requirements and eliciting incorrect requirements as well as less comprehensiveness of produced use case models this paper proposes a technique to automate detecting bad smells of use case descriptions, symptoms of poor descriptions at first, to clarify bad smells, we analyzed existing use case models to discover poor use case descriptions concretely and developed the list of bad smells, i e , a catalogue of bad smells some of the bad smells can be refined into measures using the goal question metric paradigm to automate their detection the main contribution of this paper is the automated detection of bad smells we have implemented an automated smell detector for 22 bad smells at first and assessed its usefulness by an experiment as a result, the first version of our tool got a precision ratio of 0 591 and recall ratio of 0 981 [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R194607",
            "template_id": null,
            "paper_id": "R194607",
            "premise": null,
            "hypothesis": "extracting and analyzing context information in user support conversations on twitter while many apps include built in options to report bugs or request features, users still provide an increasing amount of feedback via social media, like twitter compared to traditional issue trackers, the reporting process in social media is unstructured and the feedback often lacks basic context information, such as the app version or the device concerned when experiencing the issue to make this feedback actionable to developers, support teams engage in recurring, effortful conversations with app users to clarify missing context items this paper introduces a simple approach that accurately extracts basic context information from unstructured, informal user feedback on mobile apps, including the platform, device, app version, and system version evaluated against a truthset of 3014 tweets from official twitter support accounts of the 3 popular apps netflix, snapchat, and spotify, our approach achieved precisions from 81% to 99% and recalls from 86% to 98% for the different context item types combined with a chatbot that automatically requests missing context items from reporting users, our approach aims at auto populating issue trackers with structured bug reports",
            "sequence": "[CLS] None [SEP] extracting and analyzing context information in user support conversations on twitter while many apps include built in options to report bugs or request features, users still provide an increasing amount of feedback via social media, like twitter compared to traditional issue trackers, the reporting process in social media is unstructured and the feedback often lacks basic context information, such as the app version or the device concerned when experiencing the issue to make this feedback actionable to developers, support teams engage in recurring, effortful conversations with app users to clarify missing context items this paper introduces a simple approach that accurately extracts basic context information from unstructured, informal user feedback on mobile apps, including the platform, device, app version, and system version evaluated against a truthset of 3014 tweets from official twitter support accounts of the 3 popular apps netflix, snapchat, and spotify, our approach achieved precisions from 81% to 99% and recalls from 86% to 98% for the different context item types combined with a chatbot that automatically requests missing context items from reporting users, our approach aims at auto populating issue trackers with structured bug reports [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R178192",
            "template_id": null,
            "paper_id": "R178192",
            "premise": null,
            "hypothesis": "iot lite: a lightweight semantic model for the internet of things over the past few years the semantics community has developed ontologies to describe concepts, relationships between different entities in various application domains, including internet of things (iot) applications a key problem is that most of the iot related semantic descriptions are not as widely adopted as expected one of the main concerns of users, developers is that semantic techniques increase the complexity, processing time, therefore they are unsuitable for dynamic, responsive environments such as the iot to address this concern, we propose iot lite, an instantiation of the semantic sensor network (ssn) ontology to describe key iot concepts allowing interoperability, discovery of sensory data in heterogeneous iot platforms by a lightweight semantics we propose 10 rules for good, scalable semantic model design, follow them to create iot lite we also demonstrate the scalability of iot lite by providing some experimental analysis,, assess iot lite against another solution in terms of round time trip (rtt) performance for query response times",
            "sequence": "[CLS] None [SEP] iot lite: a lightweight semantic model for the internet of things over the past few years the semantics community has developed ontologies to describe concepts, relationships between different entities in various application domains, including internet of things (iot) applications a key problem is that most of the iot related semantic descriptions are not as widely adopted as expected one of the main concerns of users, developers is that semantic techniques increase the complexity, processing time, therefore they are unsuitable for dynamic, responsive environments such as the iot to address this concern, we propose iot lite, an instantiation of the semantic sensor network (ssn) ontology to describe key iot concepts allowing interoperability, discovery of sensory data in heterogeneous iot platforms by a lightweight semantics we propose 10 rules for good, scalable semantic model design, follow them to create iot lite we also demonstrate the scalability of iot lite by providing some experimental analysis,, assess iot lite against another solution in terms of round time trip (rtt) performance for query response times [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R141823",
                "label": "Semantic Web"
            }
        },
        {
            "instance_id": "R161108",
            "template_id": null,
            "paper_id": "R161108",
            "premise": null,
            "hypothesis": "polymers and the environment the traditional polymer materials available today, especially the plastics, are the result of decades of evolution their production is extremely efficient in terms of utilization of raw materials and energy, as well as of waste release the products present a series of excellent properties such as impermeability to water and microorganisms, high mechanical strength, low density (useful for transporting goods), and low cost due to manufacturing scale and process optimization [1] however, some of their most useful features, the chemical, physical and biological inertness, and durability resulted in their accumulation in the environment if not recycled unfortunately, the accumulation of plastics, along with other materials, is becoming a serious problem for all countries in the world these materials occupy significant volume in landfills and dumps today recently, the presence of huge amounts of plastic fragments on the oceans has been observed, considerable part of them coming from the streets, going through the drains with the rain, and then going into the rivers and lakes, and then to the oceans [1] as a result, there is a very strong and irreversible movement, in all countries of the world, to use materials that do not harm the planet, that is, low environmental impact materials",
            "sequence": "[CLS] None [SEP] polymers and the environment the traditional polymer materials available today, especially the plastics, are the result of decades of evolution their production is extremely efficient in terms of utilization of raw materials and energy, as well as of waste release the products present a series of excellent properties such as impermeability to water and microorganisms, high mechanical strength, low density (useful for transporting goods), and low cost due to manufacturing scale and process optimization [1] however, some of their most useful features, the chemical, physical and biological inertness, and durability resulted in their accumulation in the environment if not recycled unfortunately, the accumulation of plastics, along with other materials, is becoming a serious problem for all countries in the world these materials occupy significant volume in landfills and dumps today recently, the presence of huge amounts of plastic fragments on the oceans has been observed, considerable part of them coming from the streets, going through the drains with the rain, and then going into the rivers and lakes, and then to the oceans [1] as a result, there is a very strong and irreversible movement, in all countries of the world, to use materials that do not harm the planet, that is, low environmental impact materials [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R131",
                "label": "Polymer Chemistry"
            }
        },
        {
            "instance_id": "R25105",
            "template_id": null,
            "paper_id": "R25105",
            "premise": null,
            "hypothesis": "user gains and pd aims \"we present a study of user gains from their participation in a participatory design (pd) project at danish primary schools we explore user experiences and reported gains from the project in relation to the multiple aims of pd, based on a series of interviews with pupils, teachers, administrators, and consultants, conducted approximately three years after the end of the project in particular, we reflect on how the pd initiatives were sustained after the project had ended we propose that not only are ideas and initiatives disseminated directly within the organization, but also through networked relationships among people, stretching across organizations and project groups moreover, we demonstrate how users' gains related to their acting within these networks these results suggest a heightened focus on the indirect and distributed channels through which the long term impact of pd emerges \"",
            "sequence": "[CLS] None [SEP] user gains and pd aims \"we present a study of user gains from their participation in a participatory design (pd) project at danish primary schools we explore user experiences and reported gains from the project in relation to the multiple aims of pd, based on a series of interviews with pupils, teachers, administrators, and consultants, conducted approximately three years after the end of the project in particular, we reflect on how the pd initiatives were sustained after the project had ended we propose that not only are ideas and initiatives disseminated directly within the organization, but also through networked relationships among people, stretching across organizations and project groups moreover, we demonstrate how users' gains related to their acting within these networks these results suggest a heightened focus on the indirect and distributed channels through which the long term impact of pd emerges \" [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R11",
                "label": "Science"
            }
        },
        {
            "instance_id": "R75426",
            "template_id": null,
            "paper_id": "R75426",
            "premise": null,
            "hypothesis": "security and cryptographic challenges for authentication based on biometrics data authentication systems based on biometrics characteristics and data represents one of the most important trend in the evolution of the society, e g , smart city, internet of things (iot), cloud computing, big data in the near future, biometrics systems will be everywhere in the society, such as government, education, smart cities, banks etc due to its uniqueness, characteristic, biometrics systems will become more and more vulnerable, privacy being one of the most important challenges the classic cryptographic primitives are not sufficient to assure a strong level of secureness for privacy the current paper has several objectives the main objective consists in creating a framework based on cryptographic modules which can be applied in systems with biometric authentication methods the technologies used in creating the framework are: c#, java, c++, python, and haskell the wide range of technologies for developing the algorithms give the readers the possibility and not only, to choose the proper modules for their own research or business direction the cryptographic modules contain algorithms based on machine learning and modern cryptographic algorithms: aes (advanced encryption system), sha 256, rc4, rc5, rc6, mars, blowfish, twofish, threefish, rsa (rivest shamir adleman), elliptic curve, and diffie hellman as methods for implementing with success the cryptographic modules, we will propose a methodology which can be used as a how to guide the article will focus only on the first category, machine learning, and data clustering, algorithms with applicability in the cloud computing environment for tests we have used a virtual machine (virtual box) with apache hadoop and a biometric analysis tool the weakness of the algorithms and methods implemented within the framework will be evaluated and presented in order for the reader to acknowledge the latest status of the security analysis and the vulnerabilities founded in the mentioned algorithms another important result of the authors consists in creating a scheme for biometric enrollment (in results) the purpose of the scheme is to give a big overview on how to use it, step by step, in real life, and how to use the algorithms in the end, as a conclusion, the current work paper gives a comprehensive background on the most important and challenging aspects on how to design and implement an authentication system based on biometrics characteristics",
            "sequence": "[CLS] None [SEP] security and cryptographic challenges for authentication based on biometrics data authentication systems based on biometrics characteristics and data represents one of the most important trend in the evolution of the society, e g , smart city, internet of things (iot), cloud computing, big data in the near future, biometrics systems will be everywhere in the society, such as government, education, smart cities, banks etc due to its uniqueness, characteristic, biometrics systems will become more and more vulnerable, privacy being one of the most important challenges the classic cryptographic primitives are not sufficient to assure a strong level of secureness for privacy the current paper has several objectives the main objective consists in creating a framework based on cryptographic modules which can be applied in systems with biometric authentication methods the technologies used in creating the framework are: c#, java, c++, python, and haskell the wide range of technologies for developing the algorithms give the readers the possibility and not only, to choose the proper modules for their own research or business direction the cryptographic modules contain algorithms based on machine learning and modern cryptographic algorithms: aes (advanced encryption system), sha 256, rc4, rc5, rc6, mars, blowfish, twofish, threefish, rsa (rivest shamir adleman), elliptic curve, and diffie hellman as methods for implementing with success the cryptographic modules, we will propose a methodology which can be used as a how to guide the article will focus only on the first category, machine learning, and data clustering, algorithms with applicability in the cloud computing environment for tests we have used a virtual machine (virtual box) with apache hadoop and a biometric analysis tool the weakness of the algorithms and methods implemented within the framework will be evaluated and presented in order for the reader to acknowledge the latest status of the security analysis and the vulnerabilities founded in the mentioned algorithms another important result of the authors consists in creating a scheme for biometric enrollment (in results) the purpose of the scheme is to give a big overview on how to use it, step by step, in real life, and how to use the algorithms in the end, as a conclusion, the current work paper gives a comprehensive background on the most important and challenging aspects on how to design and implement an authentication system based on biometrics characteristics [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R192445",
            "template_id": null,
            "paper_id": "R192445",
            "premise": null,
            "hypothesis": "automated traceability for domain modelling decisions empowered by artificial intelligence domain modelling abstracts real world entities and their relationships in the form of class diagrams for a given domain problem space modellers often perform domain modelling to reduce the gap between understanding the problem description which expresses requirements in natural language and the concise interpretation of these requirements however, the manual practice of domain modelling is both time consuming and error prone these issues are further aggravated when problem descriptions are long, which makes it hard to trace modelling decisions from domain models to problem descriptions or vice versa leading to completeness and conciseness issues automated support for tracing domain modelling decisions in both directions is thus advantageous in this paper, we propose an automated approach that uses artificial intelligence techniques to extract domain models along with their trace links we present a traceability information model to enable traceability of modelling decisions in both directions and provide its proof of concept in the form of a tool the evaluation on a set of unseen problem descriptions shows that our approach is promising with an overall median f2 score of 82 04% we conduct an exploratory user study to assess the benefits and limitations of our approach and present the lessons learned from this study",
            "sequence": "[CLS] None [SEP] automated traceability for domain modelling decisions empowered by artificial intelligence domain modelling abstracts real world entities and their relationships in the form of class diagrams for a given domain problem space modellers often perform domain modelling to reduce the gap between understanding the problem description which expresses requirements in natural language and the concise interpretation of these requirements however, the manual practice of domain modelling is both time consuming and error prone these issues are further aggravated when problem descriptions are long, which makes it hard to trace modelling decisions from domain models to problem descriptions or vice versa leading to completeness and conciseness issues automated support for tracing domain modelling decisions in both directions is thus advantageous in this paper, we propose an automated approach that uses artificial intelligence techniques to extract domain models along with their trace links we present a traceability information model to enable traceability of modelling decisions in both directions and provide its proof of concept in the form of a tool the evaluation on a set of unseen problem descriptions shows that our approach is promising with an overall median f2 score of 82 04% we conduct an exploratory user study to assess the benefits and limitations of our approach and present the lessons learned from this study [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R193044",
            "template_id": null,
            "paper_id": "R193044",
            "premise": null,
            "hypothesis": "from ideas to expressed needs: an empirical study on the evolution of requirements during elicitation requirements are elicited from the customer and other stakeholders through an iterative process of interviews, prototyping, and other interactive sessions many communication phenomena may emerge in these early iterations, that lead initial ideas to be transformed, renegotiated, or reframed understanding how this process takes place can help in solving possible communication issues as well as their consequences in this work, we perform an exploratory study of descriptive nature to understand in which way requirements get transformed from initial ideas into documented needs to this end, we select 30 subjects that act as requirements analysts, and we perform a set of elicitation sessions with a fictional customer the customer is required to study a sample requirements document for a system beforehand and to answer the questions of the analysts about the system after the elicitation sessions, the analysts produce user stories for the system these are compared with the original ones by two researchers to assess to which extent and in which way the initial requirements evolved throughout the interactive sessions our results show that between 30% and 38% of the produced user stories include content that can be fully traced to the initial ones, while the rest of the content is dedicated to new requirements we also show what types of requirements are introduced through the elicitation process, and how they vary depending on the analyst our work contributes to theory in requirements engineering, with empirically grounded, quantitative data, concerning the impact of elicitation activities with respect to initial ideas",
            "sequence": "[CLS] None [SEP] from ideas to expressed needs: an empirical study on the evolution of requirements during elicitation requirements are elicited from the customer and other stakeholders through an iterative process of interviews, prototyping, and other interactive sessions many communication phenomena may emerge in these early iterations, that lead initial ideas to be transformed, renegotiated, or reframed understanding how this process takes place can help in solving possible communication issues as well as their consequences in this work, we perform an exploratory study of descriptive nature to understand in which way requirements get transformed from initial ideas into documented needs to this end, we select 30 subjects that act as requirements analysts, and we perform a set of elicitation sessions with a fictional customer the customer is required to study a sample requirements document for a system beforehand and to answer the questions of the analysts about the system after the elicitation sessions, the analysts produce user stories for the system these are compared with the original ones by two researchers to assess to which extent and in which way the initial requirements evolved throughout the interactive sessions our results show that between 30% and 38% of the produced user stories include content that can be fully traced to the initial ones, while the rest of the content is dedicated to new requirements we also show what types of requirements are introduced through the elicitation process, and how they vary depending on the analyst our work contributes to theory in requirements engineering, with empirically grounded, quantitative data, concerning the impact of elicitation activities with respect to initial ideas [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R194345",
            "template_id": null,
            "paper_id": "R194345",
            "premise": null,
            "hypothesis": "a machine learning based approach for demarcating requirements in textual specifications a simple but important task during the analysis of a textual requirements specification is to determine which statements in the specification represent requirements in principle, by following suitable writing and markup conventions, one can provide an immediate and unequivocal demarcation of requirements at the time a specification is being developed however, neither the presence nor a fully accurate enforcement of such conventions is guaranteed the result is that, in many practical situations, analysts end up resorting to after the fact reviews for sifting requirements from other material in a requirements specification this is both tedious and time consuming we propose an automated approach for demarcating requirements in free form requirements specifications the approach, which is based on machine learning, can be applied to a wide variety of specifications in different domains and with different writing styles we train and evaluate our approach over an independently labeled dataset comprised of 30 industrial requirements specifications over this dataset, our approach yields an average precision of 81 2% and an average recall of 95 7% compared to simple baselines that demarcate requirements based on the presence of modal verbs and identifiers, our approach leads to an average gain of 16 4% in precision and 25 5% in recall",
            "sequence": "[CLS] None [SEP] a machine learning based approach for demarcating requirements in textual specifications a simple but important task during the analysis of a textual requirements specification is to determine which statements in the specification represent requirements in principle, by following suitable writing and markup conventions, one can provide an immediate and unequivocal demarcation of requirements at the time a specification is being developed however, neither the presence nor a fully accurate enforcement of such conventions is guaranteed the result is that, in many practical situations, analysts end up resorting to after the fact reviews for sifting requirements from other material in a requirements specification this is both tedious and time consuming we propose an automated approach for demarcating requirements in free form requirements specifications the approach, which is based on machine learning, can be applied to a wide variety of specifications in different domains and with different writing styles we train and evaluate our approach over an independently labeled dataset comprised of 30 industrial requirements specifications over this dataset, our approach yields an average precision of 81 2% and an average recall of 95 7% compared to simple baselines that demarcate requirements based on the presence of modal verbs and identifiers, our approach leads to an average gain of 16 4% in precision and 25 5% in recall [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            }
        },
        {
            "instance_id": "R187232",
            "template_id": null,
            "paper_id": "R187232",
            "premise": null,
            "hypothesis": "explainable cyber physical energy systems based on knowledge graph explainability can help cyber physical systems alleviating risk in automating decisions that are affecting our life building an explainable cyber physical system requires deriving explanations from system events and causality between the system elements cyber physical energy systems such as smart grids involve cyber and physical aspects of energy systems and other elements, namely social and economic moreover, a smart grid scale can range from a small village to a large region across countries therefore, integrating these varieties of data and knowledge is a fundamental challenge to build an explainable cyber physical energy system this paper aims to use knowledge graph based framework to solve this challenge the framework consists of an ontology to model and link data from various sources and graph based algorithm to derive explanations from the events a simulated demand response scenario covering the above aspects further demonstrates the applicability of this framework",
            "sequence": "[CLS] None [SEP] explainable cyber physical energy systems based on knowledge graph explainability can help cyber physical systems alleviating risk in automating decisions that are affecting our life building an explainable cyber physical system requires deriving explanations from system events and causality between the system elements cyber physical energy systems such as smart grids involve cyber and physical aspects of energy systems and other elements, namely social and economic moreover, a smart grid scale can range from a small village to a large region across countries therefore, integrating these varieties of data and knowledge is a fundamental challenge to build an explainable cyber physical energy system this paper aims to use knowledge graph based framework to solve this challenge the framework consists of an ontology to model and link data from various sources and graph based algorithm to derive explanations from the events a simulated demand response scenario covering the above aspects further demonstrates the applicability of this framework [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R141823",
                "label": "Semantic Web"
            }
        }
    ]
}