{
    "entailments": [
        {
            "instance_id": "R138077xR185271",
            "template_id": "R138077",
            "paper_id": "R185271",
            "premise": "ontology learning from data sources has dataset application domain input format output format accuracy f1 score precision knowledge source has data source learning purpose terms learning axiom learning properties learning properties hierarchy learning rule learning class hierarchy learning learning method learning tool evaluation metrics related work validation tool training corpus testing corpus class learning implemented technologies relationships learning instance learning taxonomy learning validation comment assessment of acquired knowledge recall f measure",
            "hypothesis": "multimedia ontology learning for automatic annotation and video browsing in this work, we offer an approach to combine standard multimedia analysis techniques with knowledge drawn from conceptual metadata provided by domain experts of a specialized scholarly domain, to learn a domain specific multimedia ontology from a set of annotated examples a standard bayesian network learning algorithm that learns structure and parameters of a bayesian network is extended to include media observables in the learning an expert group provides domain knowledge to construct a basic ontology of the domain as well as to annotate a set of training videos these annotations help derive the associations between high level semantic concepts of the domain and low level mpeg 7 based features representing audio visual content of the videos we construct a more robust and refined version of this ontology by learning from this set of conceptually annotated videos to encode this knowledge, we use mowl, a multimedia extension of web ontology language (owl) which is capable of describing domain concepts in terms of their media properties and of capturing the inherent uncertainties involved we use the ontology specified knowledge for recognizing concepts relevant to a video to annotate fresh addition to the video database with relevant concepts in the ontology these conceptual annotations are used to create hyperlinks in the video collection, to provide an effective video browsing interface to the user",
            "sequence": "[CLS] ontology learning from data sources has dataset application domain input format output format accuracy f1 score precision knowledge source has data source learning purpose terms learning axiom learning properties learning properties hierarchy learning rule learning class hierarchy learning learning method learning tool evaluation metrics related work validation tool training corpus testing corpus class learning implemented technologies relationships learning instance learning taxonomy learning validation comment assessment of acquired knowledge recall f measure [SEP] multimedia ontology learning for automatic annotation and video browsing in this work, we offer an approach to combine standard multimedia analysis techniques with knowledge drawn from conceptual metadata provided by domain experts of a specialized scholarly domain, to learn a domain specific multimedia ontology from a set of annotated examples a standard bayesian network learning algorithm that learns structure and parameters of a bayesian network is extended to include media observables in the learning an expert group provides domain knowledge to construct a basic ontology of the domain as well as to annotate a set of training videos these annotations help derive the associations between high level semantic concepts of the domain and low level mpeg 7 based features representing audio visual content of the videos we construct a more robust and refined version of this ontology by learning from this set of conceptually annotated videos to encode this knowledge, we use mowl, a multimedia extension of web ontology language (owl) which is capable of describing domain concepts in terms of their media properties and of capturing the inherent uncertainties involved we use the ontology specified knowledge for recognizing concepts relevant to a video to annotate fresh addition to the video database with relevant concepts in the ontology these conceptual annotations are used to create hyperlinks in the video collection, to provide an effective video browsing interface to the user [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R141823",
                "label": "Semantic Web"
            },
            "results": [
                {
                    "template_id": "R138077"
                },
                {
                    "template_id": "R194212"
                }
            ]
        },
        {
            "instance_id": "R146876xR146918",
            "template_id": "R146876",
            "paper_id": "R146918",
            "premise": "organic solar cells mobility donor acceptor lumo homo energy band gap open circuit voltage, voc short circuit current density, jsc fill factor, ff power conversion efficiency",
            "hypothesis": "design and synthesis of a low bandgap small molecule acceptor for efficient polymer solar cells a novel non fullerene acceptor, possessing a very low bandgap of 1 34 ev and a high lying lowest unoccupied molecular orbital level of 3 95 ev, is designed and synthesized by introducing electron donating alkoxy groups to the backbone of a conjugated small molecule impressive power conversion efficiencies of 8 4% and 10 7% are obtained for fabricated single and tandem polymer solar cells",
            "sequence": "[CLS] organic solar cells mobility donor acceptor lumo homo energy band gap open circuit voltage, voc short circuit current density, jsc fill factor, ff power conversion efficiency [SEP] design and synthesis of a low bandgap small molecule acceptor for efficient polymer solar cells a novel non fullerene acceptor, possessing a very low bandgap of 1 34 ev and a high lying lowest unoccupied molecular orbital level of 3 95 ev, is designed and synthesized by introducing electron donating alkoxy groups to the backbone of a conjugated small molecule impressive power conversion efficiencies of 8 4% and 10 7% are obtained for fabricated single and tandem polymer solar cells [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            },
            "results": [
                {
                    "template_id": "R146876"
                }
            ]
        },
        {
            "instance_id": "R149061xR139546",
            "template_id": "R149061",
            "paper_id": "R139546",
            "premise": "biodiversity inventories with dna barcoding biogeographical region locus (genetics) higher number estimated species higher number estimated species (method) no of estimated species no of estimated species (method) lower number estimated species lower number estimated species (method) class (taxonomy biology) dna sequencing technology phylum (biology) no of potential undescribed species study location order (taxonomy biology) no of samples (sequences) studied taxonomic group (biology) number of identified species with current taxonomy",
            "hypothesis": "a dna barcode reference library for swiss butterflies and forester moths as a tool for species identification, systematics and conservation butterfly monitoring and red list programs in switzerland rely on a combination of observations and collection records to document changes in species distributions through time while most butterflies can be identified using morphology, some taxa remain challenging, making it difficult to accurately map their distributions and develop appropriate conservation measures in this paper, we explore the use of the dna barcode (a fragment of the mitochondrial gene coi) as a tool for the identification of swiss butterflies and forester moths (rhopalocera and zygaenidae) we present a national dna barcode reference library including 868 sequences representing 217 out of 224 resident species, or 96 9% of swiss fauna dna barcodes were diagnostic for nearly 90% of swiss species the remaining 10% represent cases of para and polyphyly likely involving introgression or incomplete lineage sorting among closely related taxa we demonstrate that integrative taxonomic methods incorporating a combination of morphological and genetic techniques result in a rate of species identification of over 96% in females and over 98% in males, higher than either morphology or dna barcodes alone we explore the use of the dna barcode for exploring boundaries among taxa, understanding the geographical distribution of cryptic diversity and evaluating the status of purportedly endemic taxa finally, we discuss how dna barcodes may be used to improve field practices and ultimately enhance conservation strategies",
            "sequence": "[CLS] biodiversity inventories with dna barcoding biogeographical region locus (genetics) higher number estimated species higher number estimated species (method) no of estimated species no of estimated species (method) lower number estimated species lower number estimated species (method) class (taxonomy biology) dna sequencing technology phylum (biology) no of potential undescribed species study location order (taxonomy biology) no of samples (sequences) studied taxonomic group (biology) number of identified species with current taxonomy [SEP] a dna barcode reference library for swiss butterflies and forester moths as a tool for species identification, systematics and conservation butterfly monitoring and red list programs in switzerland rely on a combination of observations and collection records to document changes in species distributions through time while most butterflies can be identified using morphology, some taxa remain challenging, making it difficult to accurately map their distributions and develop appropriate conservation measures in this paper, we explore the use of the dna barcode (a fragment of the mitochondrial gene coi) as a tool for the identification of swiss butterflies and forester moths (rhopalocera and zygaenidae) we present a national dna barcode reference library including 868 sequences representing 217 out of 224 resident species, or 96 9% of swiss fauna dna barcodes were diagnostic for nearly 90% of swiss species the remaining 10% represent cases of para and polyphyly likely involving introgression or incomplete lineage sorting among closely related taxa we demonstrate that integrative taxonomic methods incorporating a combination of morphological and genetic techniques result in a rate of species identification of over 96% in females and over 98% in males, higher than either morphology or dna barcodes alone we explore the use of the dna barcode for exploring boundaries among taxa, understanding the geographical distribution of cryptic diversity and evaluating the status of purportedly endemic taxa finally, we discuss how dna barcodes may be used to improve field practices and ultimately enhance conservation strategies [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "results": [
                {
                    "template_id": "R149061"
                }
            ]
        },
        {
            "instance_id": "R150089xR145318",
            "template_id": "R150089",
            "paper_id": "R145318",
            "premise": "epidemiological surveillance systems design and implementation software used software development approach related work epidemiological surveillance system purpose epidemiological surveillance process epidemiological surveillance users statistical analysis techniques epidemiological surveillance architecture epidemiological software development approach advantage provided by the system limit of the system",
            "hypothesis": "electronic surveillance system for the early notification of community based epidemics (essence): overview, components, and public health applications \\n background \\n the electronic surveillance system for the early notification of community based epidemics (essence) is a secure web based tool that enables health care practitioners to monitor health indicators of public health importance for the detection and tracking of disease outbreaks, consequences of severe weather, and other events of concern the essence concept began in an internally funded project at the johns hopkins university applied physics laboratory, advanced with funding from the state of maryland, and broadened in 1999 as a collaboration with the walter reed army institute for research versions of the system have been further developed by johns hopkins university applied physics laboratory in multiple military and civilian programs for the timely detection and tracking of health threats \\n \\n \\n objective \\n this study aims to describe the components and development of a biosurveillance system increasingly coordinating all hazards health surveillance and infectious disease monitoring among large and small health departments, to list the key features and lessons learned in the growth of this system, and to describe the range of initiatives and accomplishments of local epidemiologists using it \\n \\n \\n methods \\n the features of essence include spatial and temporal statistical alerting, custom querying, user defined alert notifications, geographical mapping, remote data capture, and event communications to expedite visualization, configurable and interactive modes of data stratification and filtering, graphical and tabular customization, user preference management, and sharing features allow users to query data and view geographic representations, time series and data details pages, and reports these features allow essence users to gather and organize the resulting wealth of information into a coherent view of population health status and communicate findings among users \\n \\n \\n results \\n the resulting broad utility, applicability, and adaptability of this system led to the adoption of essence by the centers for disease control and prevention, numerous state and local health departments, and the department of defense, both nationally and globally the open source version of suite for automated global electronic biosurveillance is available for global, resource limited settings resourceful users of the us national syndromic surveillance program essence have applied it to the surveillance of infectious diseases, severe weather and natural disaster events, mass gatherings, chronic diseases and mental health, and injury and substance abuse \\n \\n \\n conclusions \\n with emerging high consequence communicable diseases and other health conditions, the continued user requirement\u2013driven enhancements of essence demonstrate an adaptable disease surveillance capability focused on the everyday needs of public health the challenge of a live system for widely distributed users with multiple different data sources and high throughput requirements has driven a novel, evolving architecture design \\n",
            "sequence": "[CLS] epidemiological surveillance systems design and implementation software used software development approach related work epidemiological surveillance system purpose epidemiological surveillance process epidemiological surveillance users statistical analysis techniques epidemiological surveillance architecture epidemiological software development approach advantage provided by the system limit of the system [SEP] electronic surveillance system for the early notification of community based epidemics (essence): overview, components, and public health applications \\n background \\n the electronic surveillance system for the early notification of community based epidemics (essence) is a secure web based tool that enables health care practitioners to monitor health indicators of public health importance for the detection and tracking of disease outbreaks, consequences of severe weather, and other events of concern the essence concept began in an internally funded project at the johns hopkins university applied physics laboratory, advanced with funding from the state of maryland, and broadened in 1999 as a collaboration with the walter reed army institute for research versions of the system have been further developed by johns hopkins university applied physics laboratory in multiple military and civilian programs for the timely detection and tracking of health threats \\n \\n \\n objective \\n this study aims to describe the components and development of a biosurveillance system increasingly coordinating all hazards health surveillance and infectious disease monitoring among large and small health departments, to list the key features and lessons learned in the growth of this system, and to describe the range of initiatives and accomplishments of local epidemiologists using it \\n \\n \\n methods \\n the features of essence include spatial and temporal statistical alerting, custom querying, user defined alert notifications, geographical mapping, remote data capture, and event communications to expedite visualization, configurable and interactive modes of data stratification and filtering, graphical and tabular customization, user preference management, and sharing features allow users to query data and view geographic representations, time series and data details pages, and reports these features allow essence users to gather and organize the resulting wealth of information into a coherent view of population health status and communicate findings among users \\n \\n \\n results \\n the resulting broad utility, applicability, and adaptability of this system led to the adoption of essence by the centers for disease control and prevention, numerous state and local health departments, and the department of defense, both nationally and globally the open source version of suite for automated global electronic biosurveillance is available for global, resource limited settings resourceful users of the us national syndromic surveillance program essence have applied it to the surveillance of infectious diseases, severe weather and natural disaster events, mass gatherings, chronic diseases and mental health, and injury and substance abuse \\n \\n \\n conclusions \\n with emerging high consequence communicable diseases and other health conditions, the continued user requirement\u2013driven enhancements of essence demonstrate an adaptable disease surveillance capability focused on the everyday needs of public health the challenge of a live system for widely distributed users with multiple different data sources and high throughput requirements has driven a novel, evolving architecture design \\n [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R278",
                "label": "Information Science"
            },
            "results": [
                {
                    "template_id": "R150089"
                }
            ]
        },
        {
            "instance_id": "R150595xR145720",
            "template_id": "R150595",
            "paper_id": "R145720",
            "premise": "tailored forming contribution has result research problem has material realizes",
            "hypothesis": "investigations on tailored forming of aisi 52100 as rolling bearing raceway hybrid cylindrical roller thrust bearing washers of type 81212 were manufactured by tailored forming an aisi 1022m base material, featuring a sufficient strength for structural loads, was cladded with the bearing steel aisi 52100 by plasma transferred arc welding (pta) though aisi 52100 is generally regarded as non weldable, it could be applied as a cladding material by adjusting pta parameters the cladded parts were investigated after each individual process step and subsequently tested under rolling contact load welding defects that could not be completely eliminated by the subsequent hot forming were characterized by means of scanning acoustic microscopy and micrographs below the surface, pores with a typical size of ten \u00b5m were found to a depth of about 0 45 mm in the material transition zone and between individual weld seams, larger voids were observed grinding of the surface after heat treatment caused compressive residual stresses near the surface with a relatively small depth fatigue tests were carried out on an fe8 test rig eighty two percent of the calculated rating life for conventional bearings was achieved a high failure slope of the weibull regression was determined a relationship between the weld defects and the fatigue behavior is likely",
            "sequence": "[CLS] tailored forming contribution has result research problem has material realizes [SEP] investigations on tailored forming of aisi 52100 as rolling bearing raceway hybrid cylindrical roller thrust bearing washers of type 81212 were manufactured by tailored forming an aisi 1022m base material, featuring a sufficient strength for structural loads, was cladded with the bearing steel aisi 52100 by plasma transferred arc welding (pta) though aisi 52100 is generally regarded as non weldable, it could be applied as a cladding material by adjusting pta parameters the cladded parts were investigated after each individual process step and subsequently tested under rolling contact load welding defects that could not be completely eliminated by the subsequent hot forming were characterized by means of scanning acoustic microscopy and micrographs below the surface, pores with a typical size of ten \u00b5m were found to a depth of about 0 45 mm in the material transition zone and between individual weld seams, larger voids were observed grinding of the surface after heat treatment caused compressive residual stresses near the surface with a relatively small depth fatigue tests were carried out on an fe8 test rig eighty two percent of the calculated rating life for conventional bearings was achieved a high failure slope of the weibull regression was determined a relationship between the weld defects and the fatigue behavior is likely [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R137654",
                "label": "Mechanical Process Engineering"
            },
            "results": [
                {
                    "template_id": "R150595"
                }
            ]
        },
        {
            "instance_id": "R154390xR154426",
            "template_id": "R154390",
            "paper_id": "R154426",
            "premise": "lignin decomposition substrate catalyst has temperature value solvent pressure reactor conversion product",
            "hypothesis": "catalysis meets nonthermal separation for the production of (alkyl)phenols and hydrocarbons from pyrolysis oil a simple and efficient hydrodeoxygenation strategy is described to selectively generate and separate high value alkylphenols from pyrolysis bio oil, produced directly from lignocellulosic biomass the overall process is efficient and only requires low pressures of hydrogen gas (5\\u2005bar) initially, an investigation using model compounds indicates that mocx /c is a promising catalyst for targeted hydrodeoxygenation, enabling selective retention of the desired ar oh substituents by applying this procedure to pyrolysis bio oil, the primary products (phenol/4 alkylphenols and hydrocarbons) are easily separable from each other by short path column chromatography, serving as potential valuable feedstocks for industry the strategy requires no prior fractionation of the lignocellulosic biomass, no further synthetic steps, and no input of additional (e g , petrochemical) platform molecules",
            "sequence": "[CLS] lignin decomposition substrate catalyst has temperature value solvent pressure reactor conversion product [SEP] catalysis meets nonthermal separation for the production of (alkyl)phenols and hydrocarbons from pyrolysis oil a simple and efficient hydrodeoxygenation strategy is described to selectively generate and separate high value alkylphenols from pyrolysis bio oil, produced directly from lignocellulosic biomass the overall process is efficient and only requires low pressures of hydrogen gas (5\\u2005bar) initially, an investigation using model compounds indicates that mocx /c is a promising catalyst for targeted hydrodeoxygenation, enabling selective retention of the desired ar oh substituents by applying this procedure to pyrolysis bio oil, the primary products (phenol/4 alkylphenols and hydrocarbons) are easily separable from each other by short path column chromatography, serving as potential valuable feedstocks for industry the strategy requires no prior fractionation of the lignocellulosic biomass, no further synthetic steps, and no input of additional (e g , petrochemical) platform molecules [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R129",
                "label": "Organic Chemistry"
            },
            "results": [
                {
                    "template_id": "R154390"
                }
            ]
        },
        {
            "instance_id": "R155844xR156272",
            "template_id": "R155844",
            "paper_id": "R156272",
            "premise": "photocatalysts wavelength of maximum absorption energy band gap photocatalyst wavelength of maximum emission emission lifetime ground state oxidation potential ground state reduction potential excited state oxidation potential excited state reduction potential",
            "hypothesis": "a noble metal free system for photocatalytic hydrogen production from water a series of heteroleptic copper(i) complexes with bidentate pp and nn chelate ligands was prepared and successfully applied as photosensitizers in the light driven production of hydrogen, by using [fe3(co)12] as a water reduction catalyst (wrc) these systems efficiently reduces protons from water/thf/triethylamine mixtures, in which the amine serves as a sacrificial electron donor (sr) turnover numbers (for h) up to 1330 were obtained with these fully noble metal free systems the new complexes were electrochemically and photophysically characterized they exhibited a correlation between the lifetimes of the mlct excited state and their efficiency as photosensitizers in proton reduction systems within these experiments, considerably long excited state lifetimes of up to 54 \u03bcs were observed quenching studies with the sr, in the presence and absence of the wrc, showed that intramolecular deactivation was more efficient in the former case, thus suggesting the predominance of an oxidative quenching pathway",
            "sequence": "[CLS] photocatalysts wavelength of maximum absorption energy band gap photocatalyst wavelength of maximum emission emission lifetime ground state oxidation potential ground state reduction potential excited state oxidation potential excited state reduction potential [SEP] a noble metal free system for photocatalytic hydrogen production from water a series of heteroleptic copper(i) complexes with bidentate pp and nn chelate ligands was prepared and successfully applied as photosensitizers in the light driven production of hydrogen, by using [fe3(co)12] as a water reduction catalyst (wrc) these systems efficiently reduces protons from water/thf/triethylamine mixtures, in which the amine serves as a sacrificial electron donor (sr) turnover numbers (for h) up to 1330 were obtained with these fully noble metal free systems the new complexes were electrochemically and photophysically characterized they exhibited a correlation between the lifetimes of the mlct excited state and their efficiency as photosensitizers in proton reduction systems within these experiments, considerably long excited state lifetimes of up to 54 \u03bcs were observed quenching studies with the sr, in the presence and absence of the wrc, showed that intramolecular deactivation was more efficient in the former case, thus suggesting the predominance of an oxidative quenching pathway [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R130",
                "label": "Physical Chemistry"
            },
            "results": [
                {
                    "template_id": "R155844"
                }
            ]
        },
        {
            "instance_id": "R159441xR159481",
            "template_id": "R159441",
            "paper_id": "R159481",
            "premise": "city digital twin potentials visualization situational awareness planning and prediction integration and collaboration data management",
            "hypothesis": "the digital twin of the city of zurich for urban planning abstract population growth will confront the city of zurich with a variety of challenges in the coming years, as the increase in the number of inhabitants and jobs will lead to densification and competing land uses the tasks for the city administration have become more complex, whereas tools and methods are often based on traditional, static approaches while involving a limited number of citizens and stakeholders in relevant decisions the digital transformation of more and more\\xa0pieces of the planning and decision making process will make both increasingly more illustrative, easier to understand and more comprehensible an important data basis for these processes is the digital twin of the city of zurich 3d spatial data and their models transform themes of the city, such as buildings, bridges, vegetation, etc , to the digital world, are being updated when required, and create advantages in digital space these benefits need to be highlighted and published an important step in public awareness is the release of 3d spatial data under open government data this allows the development of applications, the promotion of understanding, and the simplification of the creation of different collaborative platforms by\\xa0visualization and analysis of digital prototypes and the demonstration of interactions with the built environment, scenarios can be digitally developed and discussed in decision making bodies questions about the urban climate can be simulated with the help of the digital twin and results can be linked to the existing 3d spatial data thus, the 3d spatial data set, the models and their descriptions through\\xa0metadata become the reference and must be updated according to the requirements depending on requirements and questions, further 3d spatial data must be added the description of the 3d spatial data and their models or the lifecycle management of the digital twin must be carried out with great care only in this way, decision processes can be supported in a comprehensible way",
            "sequence": "[CLS] city digital twin potentials visualization situational awareness planning and prediction integration and collaboration data management [SEP] the digital twin of the city of zurich for urban planning abstract population growth will confront the city of zurich with a variety of challenges in the coming years, as the increase in the number of inhabitants and jobs will lead to densification and competing land uses the tasks for the city administration have become more complex, whereas tools and methods are often based on traditional, static approaches while involving a limited number of citizens and stakeholders in relevant decisions the digital transformation of more and more\\xa0pieces of the planning and decision making process will make both increasingly more illustrative, easier to understand and more comprehensible an important data basis for these processes is the digital twin of the city of zurich 3d spatial data and their models transform themes of the city, such as buildings, bridges, vegetation, etc , to the digital world, are being updated when required, and create advantages in digital space these benefits need to be highlighted and published an important step in public awareness is the release of 3d spatial data under open government data this allows the development of applications, the promotion of understanding, and the simplification of the creation of different collaborative platforms by\\xa0visualization and analysis of digital prototypes and the demonstration of interactions with the built environment, scenarios can be digitally developed and discussed in decision making bodies questions about the urban climate can be simulated with the help of the digital twin and results can be linked to the existing 3d spatial data thus, the 3d spatial data set, the models and their descriptions through\\xa0metadata become the reference and must be updated according to the requirements depending on requirements and questions, further 3d spatial data must be added the description of the 3d spatial data and their models or the lifecycle management of the digital twin must be carried out with great care only in this way, decision processes can be supported in a comprehensible way [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "results": [
                {
                    "template_id": "R159441"
                },
                {
                    "template_id": "R160259"
                }
            ]
        },
        {
            "instance_id": "R161545xR161598",
            "template_id": "R161545",
            "paper_id": "R161598",
            "premise": "recycling methods method recycling type has property",
            "hypothesis": "hydrolysis and solvolysis as benign routes for the end of life management of thermoset polymer waste the production of thermoset polymers is increasing globally owing to their advantageous properties, particularly when applied as composite materials though these materials are traditionally used in more durable, longer lasting applications, ultimately, they become waste at the end of their usable lifetimes current recycling practices are not applicable to traditional thermoset waste, owing to their network structures and lack of processability recently, researchers have been developing thermoset polymers with the right functionalities to be chemically degraded under relatively benign conditions postuse, providing a route to future management of thermoset waste this review presents thermosets containing hydrolytically or solvolytically cleavable bonds, such as esters and acetals hydrolysis and solvolysis mechanisms are discussed, and various factors that influence the degradation rates are examined degradable thermosets with impressive mechanical, thermal, and adhesion behavior are discussed, illustrating that the design of material end of life need not limit material performance",
            "sequence": "[CLS] recycling methods method recycling type has property [SEP] hydrolysis and solvolysis as benign routes for the end of life management of thermoset polymer waste the production of thermoset polymers is increasing globally owing to their advantageous properties, particularly when applied as composite materials though these materials are traditionally used in more durable, longer lasting applications, ultimately, they become waste at the end of their usable lifetimes current recycling practices are not applicable to traditional thermoset waste, owing to their network structures and lack of processability recently, researchers have been developing thermoset polymers with the right functionalities to be chemically degraded under relatively benign conditions postuse, providing a route to future management of thermoset waste this review presents thermosets containing hydrolytically or solvolytically cleavable bonds, such as esters and acetals hydrolysis and solvolysis mechanisms are discussed, and various factors that influence the degradation rates are examined degradable thermosets with impressive mechanical, thermal, and adhesion behavior are discussed, illustrating that the design of material end of life need not limit material performance [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R131",
                "label": "Polymer Chemistry"
            },
            "results": [
                {
                    "template_id": "R161545"
                }
            ]
        },
        {
            "instance_id": "R161736xR162021",
            "template_id": "R161736",
            "paper_id": "R162021",
            "premise": "xray laser applications paper type research objective has system qualities",
            "hypothesis": "sub 38 nm resolution tabletop microscopy with 13 nm wavelength laser light we have acquired images with a spatial resolution better than 38 nm by using a tabletop microscope that combines 13 nm wavelength light from a high brightness tabletop laser and fresnel zone plate optics these results open a gateway to the development of compact and widely available extreme ultraviolet imaging tools capable of inspecting samples in a variety of environments with a 15 20 nm spatial resolution and a picosecond time resolution",
            "sequence": "[CLS] xray laser applications paper type research objective has system qualities [SEP] sub 38 nm resolution tabletop microscopy with 13 nm wavelength laser light we have acquired images with a spatial resolution better than 38 nm by using a tabletop microscope that combines 13 nm wavelength light from a high brightness tabletop laser and fresnel zone plate optics these results open a gateway to the development of compact and widely available extreme ultraviolet imaging tools capable of inspecting samples in a variety of environments with a 15 20 nm spatial resolution and a picosecond time resolution [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R114008",
                "label": "Applied Physics"
            },
            "results": [
                {
                    "template_id": "R161736"
                }
            ]
        },
        {
            "instance_id": "R172526xR159642",
            "template_id": "R172526",
            "paper_id": "R159642",
            "premise": "video process has study research problem application production",
            "hypothesis": "requirements elicitation and validation with real world scenes a requirements specification defines the requirements for the future system at a conceptual level (i e , class or type level) in contrast, a scenario represents a concrete example of current or future system usage in early re phases, scenarios are used to support the definition of high level requirements (goals) to be achieved by the new system in many cases, those goals can to a large degree be elicited by observing, documenting and analyzing scenarios about current system usage to support the elicitation and validation of the goals achieved by the existing system and to illustrate problems of the old system, we propose to capture current system usage using rich media (e g , video, speech, pictures, etc ) and to interrelate those observations with the goal definitions thus, we aim at making the abstraction process which leads to the definition of the conceptual models more transparent and traceable we relate the parts of the observations which have caused the definition of a goal or against which a goal was validated with the corresponding goal these interrelations provide the basis for: 1) explaining and illustrating a goal model to, e g , untrained stakeholders and/or new team members; 2) detecting, analyzing, and resolving a different interpretation of the observations; 3) comparing different observations using computed goal annotations; and 4) refining or detailing a goal model during later process phases using the prime implementation framework, we have implemented the prime crews environment, which supports the interrelation of conceptual models and captured system usage observations we report on our experiences with prime crews gained in an experimental case study",
            "sequence": "[CLS] video process has study research problem application production [SEP] requirements elicitation and validation with real world scenes a requirements specification defines the requirements for the future system at a conceptual level (i e , class or type level) in contrast, a scenario represents a concrete example of current or future system usage in early re phases, scenarios are used to support the definition of high level requirements (goals) to be achieved by the new system in many cases, those goals can to a large degree be elicited by observing, documenting and analyzing scenarios about current system usage to support the elicitation and validation of the goals achieved by the existing system and to illustrate problems of the old system, we propose to capture current system usage using rich media (e g , video, speech, pictures, etc ) and to interrelate those observations with the goal definitions thus, we aim at making the abstraction process which leads to the definition of the conceptual models more transparent and traceable we relate the parts of the observations which have caused the definition of a goal or against which a goal was validated with the corresponding goal these interrelations provide the basis for: 1) explaining and illustrating a goal model to, e g , untrained stakeholders and/or new team members; 2) detecting, analyzing, and resolving a different interpretation of the observations; 3) comparing different observations using computed goal annotations; and 4) refining or detailing a goal model during later process phases using the prime implementation framework, we have implemented the prime crews environment, which supports the interrelation of conceptual models and captured system usage observations we report on our experiences with prime crews gained in an experimental case study [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R11",
                "label": "Science"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R178304xR163595",
            "template_id": "R178304",
            "paper_id": "R163595",
            "premise": "dataset inter annotator agreement alternatename assesses creator description disambiguatingdescription encoding exampleofwork genre inlanguage isbasedon license name sameas size sourceorganization url version",
            "hypothesis": "overview of the bacteria biotope task at bionlp shared task 2016 this paper presents the bacteria biotope task of the bionlp shared task 2016, which follows the previous 2013 and 2011 editions the task focuses on the extraction of the locations (biotopes and geographical places) of bacteria from pubme abstracts and the characterization of bacteria and their associated habitats with\\nrespect to reference knowledge sources (ncbi taxonomy, ontobiotope ontology) the task is motivated by the importance of the knowledge on bacteria habitats for fundamental research and applications in microbiology the paper describes the different proposed subtasks, the corpus characteristics, the challenge organization, and the evaluation metrics we also provide an analysis of the results obtained by participants",
            "sequence": "[CLS] dataset inter annotator agreement alternatename assesses creator description disambiguatingdescription encoding exampleofwork genre inlanguage isbasedon license name sameas size sourceorganization url version [SEP] overview of the bacteria biotope task at bionlp shared task 2016 this paper presents the bacteria biotope task of the bionlp shared task 2016, which follows the previous 2013 and 2011 editions the task focuses on the extraction of the locations (biotopes and geographical places) of bacteria from pubme abstracts and the characterization of bacteria and their associated habitats with\\nrespect to reference knowledge sources (ncbi taxonomy, ontobiotope ontology) the task is motivated by the importance of the knowledge on bacteria habitats for fundamental research and applications in microbiology the paper describes the different proposed subtasks, the corpus characteristics, the challenge organization, and the evaluation metrics we also provide an analysis of the results obtained by participants [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R184022xR185136",
            "template_id": "R184022",
            "paper_id": "R185136",
            "premise": "xray spectroscopy paper type research objective has system qualities",
            "hypothesis": "absolute measurement of the resonance lines in heliumlike vanadium on an electron beam ion trap trap are reported the 1 s2p 1 p1!1s 2 ,1 s2p 3 p2!1s 2 ~1s2p 3 p1!1s 2 and 1s2p 3 p0!1s 2 ! blend and 1s2s 3 s1!1s 2 transitions are 5205 10 6 0 14 ev, 5189 12 6 0 21 ev, 5180 22 6 0 17 ev, and 5153 82 6 0 14 ev, respectively this agrees with recent theoretical calculations and the experimental precision lies at the same level as the current uncertainty in theory ~0 1 ev! these measurements represent a 5 7\u20108% determination of the qed contribution to the transition energies and are the most precise measurements of heliumlike resonance lines in the z519\u2010 31 range the measurement of the 1s2s 3 s1!1s 2 transition is also sensitive to the 1 s2s 3 s1 qed contribution at the 40% level the intensity of the strong 1 s2p 3 p1!1s 2 component of the blend compared to the total blend intensity is 94%612% this is in accord with current theoretical predictions but disagrees with an earlier reported ratio",
            "sequence": "[CLS] xray spectroscopy paper type research objective has system qualities [SEP] absolute measurement of the resonance lines in heliumlike vanadium on an electron beam ion trap trap are reported the 1 s2p 1 p1!1s 2 ,1 s2p 3 p2!1s 2 ~1s2p 3 p1!1s 2 and 1s2p 3 p0!1s 2 ! blend and 1s2s 3 s1!1s 2 transitions are 5205 10 6 0 14 ev, 5189 12 6 0 21 ev, 5180 22 6 0 17 ev, and 5153 82 6 0 14 ev, respectively this agrees with recent theoretical calculations and the experimental precision lies at the same level as the current uncertainty in theory ~0 1 ev! these measurements represent a 5 7\u20108% determination of the qed contribution to the transition energies and are the most precise measurements of heliumlike resonance lines in the z519\u2010 31 range the measurement of the 1s2s 3 s1!1s 2 transition is also sensitive to the 1 s2s 3 s1 qed contribution at the 40% level the intensity of the strong 1 s2p 3 p1!1s 2 component of the blend compared to the total blend intensity is 94%612% this is in accord with current theoretical predictions but disagrees with an earlier reported ratio [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R175",
                "label": "Atomic, Molecular and Optical Physics"
            },
            "results": [
                {
                    "template_id": "R184022"
                }
            ]
        },
        {
            "instance_id": "R186491xR108199",
            "template_id": "R186491",
            "paper_id": "R108199",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "a little bird told me: mining tweets for requirements and software evolution \"twitter is one of the most popular social networks previous research found that users employ twitter to communicate about software applications via short messages, commonly referred to as tweets, and that these tweets can be useful for requirements engineering and software evolution however, due to their large number in the range of thousands per day for popular applications a manual analysis is unfeasible in this work we present alertme, an approach to automatically classify, group and rank tweets about software applications we apply machine learning techniques for automatically classifying tweets requesting improvements, topic modeling for grouping semantically related tweets and a weighted function for ranking tweets according to specific attributes, such as content category, sentiment and number of retweets we ran our approach on 68,108 collected tweets from three software applications and compared its results against software practitioners' judgement our results show that alertme is an effective approach for filtering, summarizing and ranking tweets about software applications alertme enables the exploitation of twitter as a feedback channel for information relevant to software evolution, including end user requirements \"",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] a little bird told me: mining tweets for requirements and software evolution \"twitter is one of the most popular social networks previous research found that users employ twitter to communicate about software applications via short messages, commonly referred to as tweets, and that these tweets can be useful for requirements engineering and software evolution however, due to their large number in the range of thousands per day for popular applications a manual analysis is unfeasible in this work we present alertme, an approach to automatically classify, group and rank tweets about software applications we apply machine learning techniques for automatically classifying tweets requesting improvements, topic modeling for grouping semantically related tweets and a weighted function for ranking tweets according to specific attributes, such as content category, sentiment and number of retweets we ran our approach on 68,108 collected tweets from three software applications and compared its results against software practitioners' judgement our results show that alertme is an effective approach for filtering, summarizing and ranking tweets about software applications alertme enables the exploitation of twitter as a feedback channel for information relevant to software evolution, including end user requirements \" [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR112434",
            "template_id": "R186491",
            "paper_id": "R112434",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "users \u2014 the hidden software product quality experts?: a study on how app users report quality aspects in online reviews [context and motivation] research on eliciting requirements from a large number of online reviews using automated means has focused on functional aspects assuring the quality of an app is vital for its success this is why user feedback concerning quality issues should be considered as well [question/problem] but to what extent do online reviews of apps address quality characteristics? and how much potential is there to extract such knowledge through automation? [principal ideas/results] by tagging online reviews, we found that users mainly write about \"usability\" and \"reliability\", but the majority of statements are on a subcharacteristic level, most notably regarding \"operability\", \"adaptability\", \"fault tolerance\", and \"interoperability\" a set of 16 language patterns regarding \"usability\" correctly identified 1,528 statements from a large dataset far more efficiently than our manual analysis of a small subset [contribution] we found that statements can especially be derived from online reviews about qualities by which users are directly affected, although with some ambiguity language patterns can identify statements about qualities with high precision, though the recall is modest at this time nevertheless, our results have shown that online reviews are an unused big data source for quality requirements",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] users \u2014 the hidden software product quality experts?: a study on how app users report quality aspects in online reviews [context and motivation] research on eliciting requirements from a large number of online reviews using automated means has focused on functional aspects assuring the quality of an app is vital for its success this is why user feedback concerning quality issues should be considered as well [question/problem] but to what extent do online reviews of apps address quality characteristics? and how much potential is there to extract such knowledge through automation? [principal ideas/results] by tagging online reviews, we found that users mainly write about \"usability\" and \"reliability\", but the majority of statements are on a subcharacteristic level, most notably regarding \"operability\", \"adaptability\", \"fault tolerance\", and \"interoperability\" a set of 16 language patterns regarding \"usability\" correctly identified 1,528 statements from a large dataset far more efficiently than our manual analysis of a small subset [contribution] we found that statements can especially be derived from online reviews about qualities by which users are directly affected, although with some ambiguity language patterns can identify statements about qualities with high precision, though the recall is modest at this time nevertheless, our results have shown that online reviews are an unused big data source for quality requirements [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR113204",
            "template_id": "R186491",
            "paper_id": "R113204",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "mining android app descriptions for permission requirements recommendation \"during the development or maintenance of an android app, the app developer needs to determine the app's security and privacy requirements such as permission requirements permission requirements include two folds first, what permissions (i e , access to sensitive resources, e g , location or contact list) the app needs to request second, how to explain the reason of permission usages to users in this paper, we focus on the multiple challenges that developers face when creating permission usage explanations we propose a novel framework, clap, that mines potential explanations from the descriptions of similar apps clap leverages information retrieval and text summarization techniques to find frequent permission usages we evaluate clap on a large dataset containing 1 4 million android apps the evaluation results outperform existing state of the art approaches, showing great promise of clap as a tool for assisting developers and permission requirements discovery \"",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] mining android app descriptions for permission requirements recommendation \"during the development or maintenance of an android app, the app developer needs to determine the app's security and privacy requirements such as permission requirements permission requirements include two folds first, what permissions (i e , access to sensitive resources, e g , location or contact list) the app needs to request second, how to explain the reason of permission usages to users in this paper, we focus on the multiple challenges that developers face when creating permission usage explanations we propose a novel framework, clap, that mines potential explanations from the descriptions of similar apps clap leverages information retrieval and text summarization techniques to find frequent permission usages we evaluate clap on a large dataset containing 1 4 million android apps the evaluation results outperform existing state of the art approaches, showing great promise of clap as a tool for assisting developers and permission requirements discovery \" [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR192398",
            "template_id": "R186491",
            "paper_id": "R192398",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "agile teams\u2019 perception in privacy requirements elicitation: lgpd\u2019s compliance in brazil context: the implementation of the brazilian general data protection law (lgpd) may impact activities carried out by the software development teams it is necessary for developers to know the existing techniques and tools to carry out privacy requirements elicitation objectives: in this research, we investigated the perception of agile software development team members from different organizations, regarding the impact that lgpd will have on the activities of the software development process methods: we conducted an online survey and a systematic literature review to identify the techniques, methodologies and tools used in the literature to perform privacy requirements elicitation in the context of agile software development (asd) in addition, we also investigated the perception of an agile team from a federal public administration organization regarding the impacts of the obligation to develop software in accordance with the lgpd results: our findings reveal that agile teams know the concepts related to data privacy legislation, but they do not use the techniques proposed in the literature to perform privacy requirements elicitation in addition, agile teams face problems with outdated software requirements specifications and stakeholders\u2019 lack of knowledge regarding data privacy conclusions: agile teams need to improve their knowledge on privacy requirements",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] agile teams\u2019 perception in privacy requirements elicitation: lgpd\u2019s compliance in brazil context: the implementation of the brazilian general data protection law (lgpd) may impact activities carried out by the software development teams it is necessary for developers to know the existing techniques and tools to carry out privacy requirements elicitation objectives: in this research, we investigated the perception of agile software development team members from different organizations, regarding the impact that lgpd will have on the activities of the software development process methods: we conducted an online survey and a systematic literature review to identify the techniques, methodologies and tools used in the literature to perform privacy requirements elicitation in the context of agile software development (asd) in addition, we also investigated the perception of an agile team from a federal public administration organization regarding the impacts of the obligation to develop software in accordance with the lgpd results: our findings reveal that agile teams know the concepts related to data privacy legislation, but they do not use the techniques proposed in the literature to perform privacy requirements elicitation in addition, agile teams face problems with outdated software requirements specifications and stakeholders\u2019 lack of knowledge regarding data privacy conclusions: agile teams need to improve their knowledge on privacy requirements [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR192428",
            "template_id": "R186491",
            "paper_id": "R192428",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "ambiguity and generality in natural language privacy policies privacy policies are legal documents containing application data practices these documents are well established sources of requirements in software engineering however, privacy policies are written in natural language, thus subject to ambiguity and abstraction eliciting requirements from privacy policies is a challenging task as these ambiguities can result in more than one interpretation of a given information type (e g , ambiguous information type \"device information\" in the statement \"we collect your device information\") to address this challenge, we propose an automated approach to infer semantic relations among information types and construct an ontology to guide requirements authors in the selection of the most appropriate information type terms our solution utilizes word embeddings and convolutional neural networks (cnn) to classify information type pairs as either hypernymy, synonymy, or unknown we evaluate our model on a manually built ontology, yielding predictions that identify hypernymy relations in information type pairs with 0 904 f 1 score, suggesting a large reduction in effort required for ontology construction",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] ambiguity and generality in natural language privacy policies privacy policies are legal documents containing application data practices these documents are well established sources of requirements in software engineering however, privacy policies are written in natural language, thus subject to ambiguity and abstraction eliciting requirements from privacy policies is a challenging task as these ambiguities can result in more than one interpretation of a given information type (e g , ambiguous information type \"device information\" in the statement \"we collect your device information\") to address this challenge, we propose an automated approach to infer semantic relations among information types and construct an ontology to guide requirements authors in the selection of the most appropriate information type terms our solution utilizes word embeddings and convolutional neural networks (cnn) to classify information type pairs as either hypernymy, synonymy, or unknown we evaluate our model on a manually built ontology, yielding predictions that identify hypernymy relations in information type pairs with 0 904 f 1 score, suggesting a large reduction in effort required for ontology construction [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR192445",
            "template_id": "R186491",
            "paper_id": "R192445",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "automated traceability for domain modelling decisions empowered by artificial intelligence domain modelling abstracts real world entities and their relationships in the form of class diagrams for a given domain problem space modellers often perform domain modelling to reduce the gap between understanding the problem description which expresses requirements in natural language and the concise interpretation of these requirements however, the manual practice of domain modelling is both time consuming and error prone these issues are further aggravated when problem descriptions are long, which makes it hard to trace modelling decisions from domain models to problem descriptions or vice versa leading to completeness and conciseness issues automated support for tracing domain modelling decisions in both directions is thus advantageous in this paper, we propose an automated approach that uses artificial intelligence techniques to extract domain models along with their trace links we present a traceability information model to enable traceability of modelling decisions in both directions and provide its proof of concept in the form of a tool the evaluation on a set of unseen problem descriptions shows that our approach is promising with an overall median f2 score of 82 04% we conduct an exploratory user study to assess the benefits and limitations of our approach and present the lessons learned from this study",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] automated traceability for domain modelling decisions empowered by artificial intelligence domain modelling abstracts real world entities and their relationships in the form of class diagrams for a given domain problem space modellers often perform domain modelling to reduce the gap between understanding the problem description which expresses requirements in natural language and the concise interpretation of these requirements however, the manual practice of domain modelling is both time consuming and error prone these issues are further aggravated when problem descriptions are long, which makes it hard to trace modelling decisions from domain models to problem descriptions or vice versa leading to completeness and conciseness issues automated support for tracing domain modelling decisions in both directions is thus advantageous in this paper, we propose an automated approach that uses artificial intelligence techniques to extract domain models along with their trace links we present a traceability information model to enable traceability of modelling decisions in both directions and provide its proof of concept in the form of a tool the evaluation on a set of unseen problem descriptions shows that our approach is promising with an overall median f2 score of 82 04% we conduct an exploratory user study to assess the benefits and limitations of our approach and present the lessons learned from this study [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR193295",
            "template_id": "R186491",
            "paper_id": "R193295",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "the practical role of context modeling in the elicitation of context aware functionalities: a survey context aware functionalities are functionalities that consider the context to produce a certain system behavior, typically an adaptation or recommendation as contextual elements such as time, location, weather, user activity, device characteristics, network status, and countless others are becoming increasingly more accessible, the potential for adding context awareness to applications is enormous identifying novel, unexpected, and even delightful context aware functionalities in practice can be challenging, though: what context information is relevant for a given user task? how can contextual elements be combined? what if there is a large number of contextual elements? context modeling has been described in the literature as an essential aspect in the elicitation of context aware functionalities; however, reports on the state of the practice are rare in this study, we conducted a survey with industrial practitioners, mostly experienced professionals from large enterprises, to investigate how context models and context modeling activities have been used to support the elicitation of context aware functionalities the results indicate a gap between research and industry: context models are rarely used in practice, and context modeling activities such as analysis of relevance and especially analysis of combinations of contextual elements have been overlooked due to their high complexity, despite practitioners recognizing their importance",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] the practical role of context modeling in the elicitation of context aware functionalities: a survey context aware functionalities are functionalities that consider the context to produce a certain system behavior, typically an adaptation or recommendation as contextual elements such as time, location, weather, user activity, device characteristics, network status, and countless others are becoming increasingly more accessible, the potential for adding context awareness to applications is enormous identifying novel, unexpected, and even delightful context aware functionalities in practice can be challenging, though: what context information is relevant for a given user task? how can contextual elements be combined? what if there is a large number of contextual elements? context modeling has been described in the literature as an essential aspect in the elicitation of context aware functionalities; however, reports on the state of the practice are rare in this study, we conducted a survey with industrial practitioners, mostly experienced professionals from large enterprises, to investigate how context models and context modeling activities have been used to support the elicitation of context aware functionalities the results indicate a gap between research and industry: context models are rarely used in practice, and context modeling activities such as analysis of relevance and especially analysis of combinations of contextual elements have been overlooked due to their high complexity, despite practitioners recognizing their importance [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR193308",
            "template_id": "R186491",
            "paper_id": "R193308",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "the role of linguistic relativity on the identification of sustainability requirements: an empirical study linguistic relativity theory states that language and its structure influence people\u2019s world view and cognition we investigate how this theory impacts the identification of requirements in practice to this end, we conducted two controlled experiments with 101 participants we randomly showed participants a set of requirements dimensions (i e a language structure) either with a focus on software quality or on sustainability and asked them to identify the requirements for a grocery shopping app according to these dimensions participants of the control group were not given any dimensions the results show that the use of requirements dimensions significantly increases the number of identified requirements in comparison to the control group furthermore, participants who were given the sustainability dimensions identified more sustainability requirements in follow up interviews with 16 practitioners, the interviewees reported benefits of the dimensions such as a holistic guidance but were also concerned about the customers acceptance furthermore, they stated challenges of implementing sustainability dimensions in the daily business but also suggested solutions like establishing sustainability as a common standard our study indicates that carefully structuring requirements engineering along sustainability dimensions can guide development teams towards considering and ensuring software sustainability",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] the role of linguistic relativity on the identification of sustainability requirements: an empirical study linguistic relativity theory states that language and its structure influence people\u2019s world view and cognition we investigate how this theory impacts the identification of requirements in practice to this end, we conducted two controlled experiments with 101 participants we randomly showed participants a set of requirements dimensions (i e a language structure) either with a focus on software quality or on sustainability and asked them to identify the requirements for a grocery shopping app according to these dimensions participants of the control group were not given any dimensions the results show that the use of requirements dimensions significantly increases the number of identified requirements in comparison to the control group furthermore, participants who were given the sustainability dimensions identified more sustainability requirements in follow up interviews with 16 practitioners, the interviewees reported benefits of the dimensions such as a holistic guidance but were also concerned about the customers acceptance furthermore, they stated challenges of implementing sustainability dimensions in the daily business but also suggested solutions like establishing sustainability as a common standard our study indicates that carefully structuring requirements engineering along sustainability dimensions can guide development teams towards considering and ensuring software sustainability [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR193341",
            "template_id": "R186491",
            "paper_id": "R193341",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "what can open domain model tell us about the missing software requirements: a preliminary study completeness is one of the most important attributes of software requirement specification unfortunately, incompleteness is one of the most difficult violations to detect some approaches have been proposed to detect missing requirements based on the requirement oriented domain model however, these kinds of models are actually lack for lots of domains fortunately, the domain models constructed for different purposes can usually be found online this raises a question: whether or not these domain models are useful for finding the missing functional information in requirement specification? to explore this question, we design and conduct a preliminary study by computing the overlapping rate between the entities in domain models and the concepts of natural language software requirements, and then digging into four regularities of the occurrence of these entities(concepts) based on two example domains the usefulness of these regularities, especially the one based our proposed metric ahme (with 54% and 70% of f2 on the two domains), has been initially evaluated with an additional experiment",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] what can open domain model tell us about the missing software requirements: a preliminary study completeness is one of the most important attributes of software requirement specification unfortunately, incompleteness is one of the most difficult violations to detect some approaches have been proposed to detect missing requirements based on the requirement oriented domain model however, these kinds of models are actually lack for lots of domains fortunately, the domain models constructed for different purposes can usually be found online this raises a question: whether or not these domain models are useful for finding the missing functional information in requirement specification? to explore this question, we design and conduct a preliminary study by computing the overlapping rate between the entities in domain models and the concepts of natural language software requirements, and then digging into four regularities of the occurrence of these entities(concepts) based on two example domains the usefulness of these regularities, especially the one based our proposed metric ahme (with 54% and 70% of f2 on the two domains), has been initially evaluated with an additional experiment [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR193357",
            "template_id": "R186491",
            "paper_id": "R193357",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "what\u2019s up with requirements engineering for artificial intelligence systems? in traditional approaches to building software systems (that do not include an artificial intelligent (ai) or machine learning (ml) component), requirements engineering (re) activities are well established and researched however, building software systems with one or more ai components may depend heavily on data with limited or no insight into the system\u2019s workings therefore, engineering such systems poses significant new challenges to re our search showed that literature has focused on using ai to manage re activities, with limited research on re for ai (re4ai) our study\u2019s main objective was to investigate current approaches in writing requirements for ai/ml systems, identify available tools and techniques used to model requirements, and find existing challenges and limitations we performed a systematic literature review (slr) of current re4ai methods and identified 27 primary studies using these studies, we analysed the key tools and techniques used to specify and model requirements and found several challenges and limitations of existing re4ai practices we further provide recommendations for future research, based on our analysis of the primary studies and mapping to industry guidelines in google pair) the slr findings highlighted that present re applications were not adaptive to manage most ai/ml systems and emphasised the need to provide new techniques and tools to support re4ai",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] what\u2019s up with requirements engineering for artificial intelligence systems? in traditional approaches to building software systems (that do not include an artificial intelligent (ai) or machine learning (ml) component), requirements engineering (re) activities are well established and researched however, building software systems with one or more ai components may depend heavily on data with limited or no insight into the system\u2019s workings therefore, engineering such systems poses significant new challenges to re our search showed that literature has focused on using ai to manage re activities, with limited research on re for ai (re4ai) our study\u2019s main objective was to investigate current approaches in writing requirements for ai/ml systems, identify available tools and techniques used to model requirements, and find existing challenges and limitations we performed a systematic literature review (slr) of current re4ai methods and identified 27 primary studies using these studies, we analysed the key tools and techniques used to specify and model requirements and found several challenges and limitations of existing re4ai practices we further provide recommendations for future research, based on our analysis of the primary studies and mapping to industry guidelines in google pair) the slr findings highlighted that present re applications were not adaptive to manage most ai/ml systems and emphasised the need to provide new techniques and tools to support re4ai [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR193371",
            "template_id": "R186491",
            "paper_id": "R193371",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "automated recommendation of templates for legal requirements [context] in legal requirements elicitation, requirements analysts need to extract obligations from legal texts however, legal texts often express obligations only indirectly, for example, by attributing a right to the counterpart this phenomenon has already been described in the requirements engineering (re) literature [1] [objectives] we investigate the use of requirements templates for the systematic elicitation of legal requirements our work is motivated by two observations: (1) the existing literature does not provide a harmonized view on the requirements templates that are useful for legal re; (2) despite the promising recent advancements in natural language processing (nlp), automated support for legal re through the suggestion of requirements templates has not been achieved yet our objective is to take steps toward addressing these limitations [methods] we review and reconcile the legal requirement templates proposed in re subsequently, we conduct a qualitative study to define nlp rules for template recommendation [results and conclusions] our contributions consist of (a) a harmonized list of requirements templates pertinent to legal re, and (b) rules for the automatic recommendation of such templates we evaluate our rules through a case study on 400 statements from two legal domains the results indicate a recall and precision of 82,3% and 79,8%, respectively we show that introducing some limited interaction with the analyst considerably improves accuracy specifically, our human feedback strategy increases recall by 12% and precision by 10,8%, thus yielding an overall recall of 94,3% and overall precision of 90,6%",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] automated recommendation of templates for legal requirements [context] in legal requirements elicitation, requirements analysts need to extract obligations from legal texts however, legal texts often express obligations only indirectly, for example, by attributing a right to the counterpart this phenomenon has already been described in the requirements engineering (re) literature [1] [objectives] we investigate the use of requirements templates for the systematic elicitation of legal requirements our work is motivated by two observations: (1) the existing literature does not provide a harmonized view on the requirements templates that are useful for legal re; (2) despite the promising recent advancements in natural language processing (nlp), automated support for legal re through the suggestion of requirements templates has not been achieved yet our objective is to take steps toward addressing these limitations [methods] we review and reconcile the legal requirement templates proposed in re subsequently, we conduct a qualitative study to define nlp rules for template recommendation [results and conclusions] our contributions consist of (a) a harmonized list of requirements templates pertinent to legal re, and (b) rules for the automatic recommendation of such templates we evaluate our rules through a case study on 400 statements from two legal domains the results indicate a recall and precision of 82,3% and 79,8%, respectively we show that introducing some limited interaction with the analyst considerably improves accuracy specifically, our human feedback strategy increases recall by 12% and precision by 10,8%, thus yielding an overall recall of 94,3% and overall precision of 90,6% [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR193418",
            "template_id": "R186491",
            "paper_id": "R193418",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "voice of the users: a demographic study of software feedback behaviour user feedback on mobile app stores, product forums, and on social media can contain product development insights there has been a lot of recent research studying this feedback and developing methods to automatically extract requirement related information this feedback is generally considered to be the \u201cvoice of the users\u201d; however, only a subset of software users provide online feedback if the demographics of the online feedback givers are not representative of the user base, this introduces the possibility of developing software that does not meet the needs of all users it is, therefore, important to understand who provides online feedback to ensure the needs of underrepresented groups are not being missed in this work, we directly survey 1040 software users about their feedback habits, software use, and demographic information their responses indicate that there are statistically significant differences in who gives feedback on each online channel, with respect to traditional demographics (gender, age, etc) we also identify key differences in what motivates software users to engage with each of the three channels our findings provide valuable context for requirements elicited from online feedback and show that considering information from all channels will provide a more comprehensive view of user needs",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] voice of the users: a demographic study of software feedback behaviour user feedback on mobile app stores, product forums, and on social media can contain product development insights there has been a lot of recent research studying this feedback and developing methods to automatically extract requirement related information this feedback is generally considered to be the \u201cvoice of the users\u201d; however, only a subset of software users provide online feedback if the demographics of the online feedback givers are not representative of the user base, this introduces the possibility of developing software that does not meet the needs of all users it is, therefore, important to understand who provides online feedback to ensure the needs of underrepresented groups are not being missed in this work, we directly survey 1040 software users about their feedback habits, software use, and demographic information their responses indicate that there are statistically significant differences in who gives feedback on each online channel, with respect to traditional demographics (gender, age, etc) we also identify key differences in what motivates software users to engage with each of the three channels our findings provide valuable context for requirements elicited from online feedback and show that considering information from all channels will provide a more comprehensive view of user needs [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR193467",
            "template_id": "R186491",
            "paper_id": "R193467",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "cutting through the jungle: disambiguating model based traceability terminology traceability, a classic requirements engineering topic, is increasingly used in the context of model based engineering however, researchers and practitioners lack a concise terminology to discuss aspects of requirements traceability in situations in which engineers heavily rely on models and model based engineering while others have previously surveyed the domain, no one has so far provided a clear, unambiguous set of terms that can be used to discuss traceability in such a context we therefore set out to cut a path through the jungle of terminology for model based traceability, ground it in established terminology from requirements engineering, and derive an unambiguous set of relevant terms we also map the terminology used in existing primary and secondary studies to our taxonomy to show differences and commonalities the contribution of this paper is thus a terminology for model based traceability that allows requirements engineers and engineers working with models to unambiguously discuss their joint traceability efforts",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] cutting through the jungle: disambiguating model based traceability terminology traceability, a classic requirements engineering topic, is increasingly used in the context of model based engineering however, researchers and practitioners lack a concise terminology to discuss aspects of requirements traceability in situations in which engineers heavily rely on models and model based engineering while others have previously surveyed the domain, no one has so far provided a clear, unambiguous set of terms that can be used to discuss traceability in such a context we therefore set out to cut a path through the jungle of terminology for model based traceability, ground it in established terminology from requirements engineering, and derive an unambiguous set of relevant terms we also map the terminology used in existing primary and secondary studies to our taxonomy to show differences and commonalities the contribution of this paper is thus a terminology for model based traceability that allows requirements engineers and engineers working with models to unambiguously discuss their joint traceability efforts [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR193481",
            "template_id": "R186491",
            "paper_id": "R193481",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "extracting and classifying requirements from software engineering contracts in this paper, we present our work on extracting and classifying requirements from large software engineering contracts typically, the process of requirements elicitation begins after a contractual agreement is signed by all participants our interactions with the legal compliance team in a large vendor organization reveal that business contracts can help in the identification of high level requirements relevant to the success of software engineering projects we posit that requirements engineering as a discipline has an even wider scope than software engineering of which it is traditionally considered to be a sub discipline this is because software engineering specific requirements are but a part of the success story of any large project the requirements that emerge from contracts are obligatory in nature, whether or not they pertain to core software development therefore, it is important that these are extracted and classified for the benefit of software engineers and other stakeholders responsible for a project we discuss the results of an exploratory study and a range of experiments from the use of regular expressions to bidirectional encoder representations from transformers for automating the extraction and classification of requirements from software engineering contracts with bidirectional encoder representations from transformers, we obtained a high f score of greater than eighty four percent for classification of requirements",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] extracting and classifying requirements from software engineering contracts in this paper, we present our work on extracting and classifying requirements from large software engineering contracts typically, the process of requirements elicitation begins after a contractual agreement is signed by all participants our interactions with the legal compliance team in a large vendor organization reveal that business contracts can help in the identification of high level requirements relevant to the success of software engineering projects we posit that requirements engineering as a discipline has an even wider scope than software engineering of which it is traditionally considered to be a sub discipline this is because software engineering specific requirements are but a part of the success story of any large project the requirements that emerge from contracts are obligatory in nature, whether or not they pertain to core software development therefore, it is important that these are extracted and classified for the benefit of software engineers and other stakeholders responsible for a project we discuss the results of an exploratory study and a range of experiments from the use of regular expressions to bidirectional encoder representations from transformers for automating the extraction and classification of requirements from software engineering contracts with bidirectional encoder representations from transformers, we obtained a high f score of greater than eighty four percent for classification of requirements [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR193490",
            "template_id": "R186491",
            "paper_id": "R193490",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "which app features are being used? learning app feature usages from interaction data in the dynamic and fast growing app market, monitoring and understanding how past releases are actually being used is indispensable for successful app maintenance and evolution current app usage analytics tools either log execution events, e g , in stack traces, or general usage information such as the app activation time, location, and device in this paper, we focus on analyzing the usages of the single app features as described in release notes and app pages we suggest monitoring nine app independent, privacy friendly interaction events for training a machine learning model to learn app feature usages we conducted a crowdsourcing study with 55 participants who labeled 5,815 feature usages of 170 unique apps for 18 days our within apps evaluation shows that we could achieve encouraging precision and recall values already with ten labeled feature usages for certain popular features such as browse newsfeed or send an email, we achieved f1 values above 88% betweenapps feature learning seems feasible with f1 values of up to 86%",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] which app features are being used? learning app feature usages from interaction data in the dynamic and fast growing app market, monitoring and understanding how past releases are actually being used is indispensable for successful app maintenance and evolution current app usage analytics tools either log execution events, e g , in stack traces, or general usage information such as the app activation time, location, and device in this paper, we focus on analyzing the usages of the single app features as described in release notes and app pages we suggest monitoring nine app independent, privacy friendly interaction events for training a machine learning model to learn app feature usages we conducted a crowdsourcing study with 55 participants who labeled 5,815 feature usages of 170 unique apps for 18 days our within apps evaluation shows that we could achieve encouraging precision and recall values already with ten labeled feature usages for certain popular features such as browse newsfeed or send an email, we achieved f1 values above 88% betweenapps feature learning seems feasible with f1 values of up to 86% [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR193798",
            "template_id": "R186491",
            "paper_id": "R193798",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "classifying user requirements from online feedback in small dataset environments using deep learning an overwhelming number of users access app repositories like app store/google play and social media platforms like twitter, where they provide feedback on digital experiences this vast textual corpus comprising user feedback has the potential to unearth detailed insights regarding the users\u2019 opinions on products and services various tools have been proposed that employ natural language processing (nlp) and traditional machine learning (ml) based models as an inexpensive mechanism to identify requirements in user feedback however, they fall short on their classification accuracy over unseen data due to factors like the cost of generating voluminous de biased labeled datasets and general inefficiency recently, van vliet et al [1] achieved state of the art results extracting and classifying requirements from user reviews through traditional crowdsourcing based on their reference classification tasks and outcomes, we successfully developed and validated a deep learning backed artificial intelligence pipeline to achieve a state of the art averaged classification accuracy of \u223c87% on standard tasks for user feedback analysis this approach, which comprises a bert based sequence classifier, proved effective even in extremely low volume dataset environments additionally, our approach drastically reduces the time and costs of evaluation, and improves on the accuracy measures achieved using traditional ml /nlp based techniques",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] classifying user requirements from online feedback in small dataset environments using deep learning an overwhelming number of users access app repositories like app store/google play and social media platforms like twitter, where they provide feedback on digital experiences this vast textual corpus comprising user feedback has the potential to unearth detailed insights regarding the users\u2019 opinions on products and services various tools have been proposed that employ natural language processing (nlp) and traditional machine learning (ml) based models as an inexpensive mechanism to identify requirements in user feedback however, they fall short on their classification accuracy over unseen data due to factors like the cost of generating voluminous de biased labeled datasets and general inefficiency recently, van vliet et al [1] achieved state of the art results extracting and classifying requirements from user reviews through traditional crowdsourcing based on their reference classification tasks and outcomes, we successfully developed and validated a deep learning backed artificial intelligence pipeline to achieve a state of the art averaged classification accuracy of \u223c87% on standard tasks for user feedback analysis this approach, which comprises a bert based sequence classifier, proved effective even in extremely low volume dataset environments additionally, our approach drastically reduces the time and costs of evaluation, and improves on the accuracy measures achieved using traditional ml /nlp based techniques [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR193849",
            "template_id": "R186491",
            "paper_id": "R193849",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "tem: a transparency engineering methodology enabling users\u2019 trust judgement transparency is key to enhancing users\u2019 trust by enabling their judgment on the outcomes and consequences of a system\u2019s operations this paper presents the transparency engineering methodology (tem) to generate transparency requirements that enable users\u2019 trust judgement the idea is to identify where transparency is lacking and to address this through patterns augmenting the specification of data, use case, and process requirements due to the complexity of software, it is impossible (and undesirable) to achieve full transparency throughout the system however, transparency can be improved for selected system aspects this is demonstrated using the results from an industrial case study with a medical technology company where, with the help of tem, existing functional requirements were refined, and transparency requirements generated systematically",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] tem: a transparency engineering methodology enabling users\u2019 trust judgement transparency is key to enhancing users\u2019 trust by enabling their judgment on the outcomes and consequences of a system\u2019s operations this paper presents the transparency engineering methodology (tem) to generate transparency requirements that enable users\u2019 trust judgement the idea is to identify where transparency is lacking and to address this through patterns augmenting the specification of data, use case, and process requirements due to the complexity of software, it is impossible (and undesirable) to achieve full transparency throughout the system however, transparency can be improved for selected system aspects this is demonstrated using the results from an industrial case study with a medical technology company where, with the help of tem, existing functional requirements were refined, and transparency requirements generated systematically [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR193920",
            "template_id": "R186491",
            "paper_id": "R193920",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "same same but different: finding similar user feedback across multiple platforms and languages users submit feedback about the software they use through application distributions platforms, i e , app stores, and social media previous research has found that this type of feedback contains valuable information for software evolution, such as bug reports, or feature requests however, popular applications receive thousands of feedback entities per day, making their manual analysis unrealistic in this work, we present an approach to automatically identify similar user feedback across different languages and platforms at the core of the approach is a word aligner that aligns words based on their semantic similarity and the similarity of their local semantic contexts additionally, we make use of machine translation, sentiment analysis, and text classification, to extract the sentiment polarity and content nature of user feedback written in different languages we use the results of these components to compute a similarity score between user feedback pairs we evaluated our approach on user feedback entities written in four different languages, and retrieved from five different mobile applications obtained from four different app stores and social networking sites the obtained results are encouraging compared to human assessment, the overall performance for monolingual user feedback pairs yielded a strong correlation of 0 79 for the crosslingual feedback pairs the correlation was also strong, with a value of 0 78",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] same same but different: finding similar user feedback across multiple platforms and languages users submit feedback about the software they use through application distributions platforms, i e , app stores, and social media previous research has found that this type of feedback contains valuable information for software evolution, such as bug reports, or feature requests however, popular applications receive thousands of feedback entities per day, making their manual analysis unrealistic in this work, we present an approach to automatically identify similar user feedback across different languages and platforms at the core of the approach is a word aligner that aligns words based on their semantic similarity and the similarity of their local semantic contexts additionally, we make use of machine translation, sentiment analysis, and text classification, to extract the sentiment polarity and content nature of user feedback written in different languages we use the results of these components to compute a similarity score between user feedback pairs we evaluated our approach on user feedback entities written in four different languages, and retrieved from five different mobile applications obtained from four different app stores and social networking sites the obtained results are encouraging compared to human assessment, the overall performance for monolingual user feedback pairs yielded a strong correlation of 0 79 for the crosslingual feedback pairs the correlation was also strong, with a value of 0 78 [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR194084",
            "template_id": "R186491",
            "paper_id": "R194084",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "requirements dependency extraction by integrating active learning with ontology based retrieval context: incomplete or incorrect detection of requirement dependencies has proven to result in reduced release quality and substantial rework additionally, the extraction of dependencies is challenging since requirements are mostly documented in natural language, which makes it a cognitively difficult task moreover, with ever changing and new requirements, a manual analysis process must be repeated, which imposes extra hardship even for domain experts objective: the three main objectives of this research are: 1) proposing a new dependency extraction method using a variant of active learning (al) 2) evaluating this al and ontology based retrieval (obr) as baseline methods for dependency extraction on the two industrial data sets 3) analyzing the value gained from integrating these diverse approaches to form two hybrid methods method: building on the general al, ensemble and semi supervised machine learning, a variant of al was developed, which was further integrated with obr to form two hybrid methods (hybrid1, hybrid2) for extracting three types of dependencies (requires, refines, other): hybrid1 used obr as a substitute for human expert; hybrid2 used dependencies extracted through the obr as an additional input for training set in al results: for two industrial case studies, al extracted more dependencies than obr hybrid1 showed improvement for both data sets for one of them, f1 score increased to 82 6% compared to the al baseline score of 49 9% hybrid2 increased the accuracy by 25% to the level of 75 8% compared to the al baseline accuracy obr also complemented the al approach by reducing 50% of the human effort",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] requirements dependency extraction by integrating active learning with ontology based retrieval context: incomplete or incorrect detection of requirement dependencies has proven to result in reduced release quality and substantial rework additionally, the extraction of dependencies is challenging since requirements are mostly documented in natural language, which makes it a cognitively difficult task moreover, with ever changing and new requirements, a manual analysis process must be repeated, which imposes extra hardship even for domain experts objective: the three main objectives of this research are: 1) proposing a new dependency extraction method using a variant of active learning (al) 2) evaluating this al and ontology based retrieval (obr) as baseline methods for dependency extraction on the two industrial data sets 3) analyzing the value gained from integrating these diverse approaches to form two hybrid methods method: building on the general al, ensemble and semi supervised machine learning, a variant of al was developed, which was further integrated with obr to form two hybrid methods (hybrid1, hybrid2) for extracting three types of dependencies (requires, refines, other): hybrid1 used obr as a substitute for human expert; hybrid2 used dependencies extracted through the obr as an additional input for training set in al results: for two industrial case studies, al extracted more dependencies than obr hybrid1 showed improvement for both data sets for one of them, f1 score increased to 82 6% compared to the al baseline score of 49 9% hybrid2 increased the accuracy by 25% to the level of 75 8% compared to the al baseline accuracy obr also complemented the al approach by reducing 50% of the human effort [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR194093",
            "template_id": "R186491",
            "paper_id": "R194093",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "oasis: weakening user obligations for security critical systems security critical systems typically place some requirements on the behaviour of their users, obliging them to follow certain instructions when using those systems security vulnerabilities can arise when users do not fully satisfy their obligations in this paper, we propose an approach that improves system security by ensuring that attack scenarios are mitigated even when the users deviate from their expected behaviour the approach uses structured transition systems to present and reason about user obligations the aim is to identify potential vulnerabilities by weakening the assumptions on how the user will behave we present an algorithm that combines iterative abstraction and controller synthesis to produce a new software specification that maintains the satisfaction of security requirements while weakening user obligations we demonstrate the feasibility of our approach through two examples from the e voting and e commerce domains",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] oasis: weakening user obligations for security critical systems security critical systems typically place some requirements on the behaviour of their users, obliging them to follow certain instructions when using those systems security vulnerabilities can arise when users do not fully satisfy their obligations in this paper, we propose an approach that improves system security by ensuring that attack scenarios are mitigated even when the users deviate from their expected behaviour the approach uses structured transition systems to present and reason about user obligations the aim is to identify potential vulnerabilities by weakening the assumptions on how the user will behave we present an algorithm that combines iterative abstraction and controller synthesis to produce a new software specification that maintains the satisfaction of security requirements while weakening user obligations we demonstrate the feasibility of our approach through two examples from the e voting and e commerce domains [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR194125",
            "template_id": "R186491",
            "paper_id": "R194125",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "data driven risk management for requirements engineering: an automated approach based on bayesian networks requirements engineering (re) is a means to reduce the risk of delivering a product that does not fulfill the stakeholders\u2019 needs therefore, a major challenge in re is to decide how much re is needed and what re methods to apply the quality of such decisions is strongly based on the re expert\u2019s experience and expertise in carefully analyzing the context and current state of a project recent work, however, shows that lack of experience and qualification are common causes for problems in re we trained a series of bayesian networks on data from the napire survey to model relationships between re problems, their causes, and effects in projects with different contextual characteristics these models were used to conduct (1) a post mortem (diagnostic) analysis, deriving probable causes of suboptimal re performance, and (2) to conduct a preventive analysis, predicting probable issues a young project might encounter the method was subject to a rigorous cross validation procedure for both use cases before assessing its applicability to real world scenarios with a case study",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] data driven risk management for requirements engineering: an automated approach based on bayesian networks requirements engineering (re) is a means to reduce the risk of delivering a product that does not fulfill the stakeholders\u2019 needs therefore, a major challenge in re is to decide how much re is needed and what re methods to apply the quality of such decisions is strongly based on the re expert\u2019s experience and expertise in carefully analyzing the context and current state of a project recent work, however, shows that lack of experience and qualification are common causes for problems in re we trained a series of bayesian networks on data from the napire survey to model relationships between re problems, their causes, and effects in projects with different contextual characteristics these models were used to conduct (1) a post mortem (diagnostic) analysis, deriving probable causes of suboptimal re performance, and (2) to conduct a preventive analysis, predicting probable issues a young project might encounter the method was subject to a rigorous cross validation procedure for both use cases before assessing its applicability to real world scenarios with a case study [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR194139",
            "template_id": "R186491",
            "paper_id": "R194139",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "an ai assisted approach for checking the completeness of privacy policies against gdpr privacy policies are critical for helping individuals make informed decisions about their personal data in europe, privacy policies are subject to compliance with the general data protection regulation (gdpr) if done entirely manually, checking whether a given privacy policy complies with gdpr is both time consuming and error prone automated support for this task is thus advantageous at the moment, there is an evident lack of such support on the market in this paper, we tackle an important dimension of gdpr compliance checking for privacy policies specifically, we provide automated support for checking whether the content of a given privacy policy is complete according to the provisions stipulated by gdpr to do so, we present: (1) a conceptual model to characterize the information content envisaged by gdpr for privacy policies, (2) an ai assisted approach for classifying the information content in gdpr privacy policies and subsequently checking how well the classified content meets the completeness criteria of interest; and (3) an evaluation of our approach through a case study over 24 unseen privacy policies for classification, we leverage a combination of natural language processing and supervised machine learning our experimental material is comprised of 234 real privacy policies from the fund industry our empirical results indicate that our approach detected 45 of the total of 47 incompleteness issues in the 24 privacy policies it was applied to over these policies, the approach had eight false positives the approach thus has a precision of 85% and recall of 96% over our case study",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] an ai assisted approach for checking the completeness of privacy policies against gdpr privacy policies are critical for helping individuals make informed decisions about their personal data in europe, privacy policies are subject to compliance with the general data protection regulation (gdpr) if done entirely manually, checking whether a given privacy policy complies with gdpr is both time consuming and error prone automated support for this task is thus advantageous at the moment, there is an evident lack of such support on the market in this paper, we tackle an important dimension of gdpr compliance checking for privacy policies specifically, we provide automated support for checking whether the content of a given privacy policy is complete according to the provisions stipulated by gdpr to do so, we present: (1) a conceptual model to characterize the information content envisaged by gdpr for privacy policies, (2) an ai assisted approach for classifying the information content in gdpr privacy policies and subsequently checking how well the classified content meets the completeness criteria of interest; and (3) an evaluation of our approach through a case study over 24 unseen privacy policies for classification, we leverage a combination of natural language processing and supervised machine learning our experimental material is comprised of 234 real privacy policies from the fund industry our empirical results indicate that our approach detected 45 of the total of 47 incompleteness issues in the 24 privacy policies it was applied to over these policies, the approach had eight false positives the approach thus has a precision of 85% and recall of 96% over our case study [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR194253",
            "template_id": "R186491",
            "paper_id": "R194253",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "can a conversation paint a picture? mining requirements in software forums the modern software landscape is highly competitive software companies need to quickly fix reported bugs and release requested new features, or they risk negative reviews and reduced market share the amount of online user feedback prevents manual analysis past research has investigated automated requirement mining techniques on online platforms like app stores and twitter, but online product forums have not been studied in this paper, we show that online product forums are a rich source of user feedback that may be used to elicit product requirements the information contained in forum questions is different from what has been described in the related work on app stores or twitter users often provide detailed context to specific problems they encounter with a software product and other users respond with workarounds or to confirm the problem through the analysis of two large forums, we identify 18 different types of information (classifications) contained in forums that can be relevant to maintenance and evolution tasks we show that a state of the art app store tool is unable to accurately classify forum data, which may be due to the differences in content thus, specific techniques are likely needed to mine requirements from product forums in an exploratory study, we developed classifiers with forum specific features promising results are achieved for all classifiers with f measure scores ranging from 70 3% to 89 8%",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] can a conversation paint a picture? mining requirements in software forums the modern software landscape is highly competitive software companies need to quickly fix reported bugs and release requested new features, or they risk negative reviews and reduced market share the amount of online user feedback prevents manual analysis past research has investigated automated requirement mining techniques on online platforms like app stores and twitter, but online product forums have not been studied in this paper, we show that online product forums are a rich source of user feedback that may be used to elicit product requirements the information contained in forum questions is different from what has been described in the related work on app stores or twitter users often provide detailed context to specific problems they encounter with a software product and other users respond with workarounds or to confirm the problem through the analysis of two large forums, we identify 18 different types of information (classifications) contained in forums that can be relevant to maintenance and evolution tasks we show that a state of the art app store tool is unable to accurately classify forum data, which may be due to the differences in content thus, specific techniques are likely needed to mine requirements from product forums in an exploratory study, we developed classifiers with forum specific features promising results are achieved for all classifiers with f measure scores ranging from 70 3% to 89 8% [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR194360",
            "template_id": "R186491",
            "paper_id": "R194360",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "analysis of requirements related arguments in user forums in the past, users were asked to express their needs and intentions by writing a structured requirements document in natural language due to the pervasive use of online forums and social media, user feedback is more accessible today however, the information obtained is often fragmented, involving multipleperspectives from multiple parties on an on going basis in this paper, we propose a crowd based requirements engineering approach by argumentation (crowdre arg), which analyses the conversations from user forum, identifies the arguments in favor or opposing of a given requirements related discussion topic by generating the argumentation model of the involved user statements, we are able to recover the conflicting viewpoints, to reason about the winning arguments for informed requirements decisions the proposed approach is illustrated with a data set of sample conversations about the design of a new google map feature from reddit also, we apply natural language processing techniques and machine learning algorithms to support the automated execution of the crowdre arg approach",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] analysis of requirements related arguments in user forums in the past, users were asked to express their needs and intentions by writing a structured requirements document in natural language due to the pervasive use of online forums and social media, user feedback is more accessible today however, the information obtained is often fragmented, involving multipleperspectives from multiple parties on an on going basis in this paper, we propose a crowd based requirements engineering approach by argumentation (crowdre arg), which analyses the conversations from user forum, identifies the arguments in favor or opposing of a given requirements related discussion topic by generating the argumentation model of the involved user statements, we are able to recover the conflicting viewpoints, to reason about the winning arguments for informed requirements decisions the proposed approach is illustrated with a data set of sample conversations about the design of a new google map feature from reddit also, we apply natural language processing techniques and machine learning algorithms to support the automated execution of the crowdre arg approach [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR194425",
            "template_id": "R186491",
            "paper_id": "R194425",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "visualization requirements for business intelligence analytics: a goal based, iterative framework information visualization plays a key role in business intelligence analytics with ever larger amounts of data that need to be interpreted, using the right visualizations is crucial in order to understand the underlying patterns and results obtained by analysis algorithms despite its importance, defining the right visualization is still a challenging task business users are rarely experts in information visualization, and they may not exactly know the most adequate visualization tools or patterns for their goals consequently, misinterpreted graphs and wrong results can be obtained, leading to missed opportunities and significant losses for companies the main problem underneath is a lack of tools and methodologies that allow non expert users to define their visualization and data analysis goals in business terms in order to tackle this problem, we present an iterative goal oriented approach based on the i* language for the automatic derivation of data visualizations our approach links non expert user requirements to the data to be analyzed, choosing the most suited visualization techniques in a semi automatic way the great advantage of our proposal is that we provide non expert users with the best suited visualizations according to their information needs and their data with little effort and without requiring expertise in information visualization",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] visualization requirements for business intelligence analytics: a goal based, iterative framework information visualization plays a key role in business intelligence analytics with ever larger amounts of data that need to be interpreted, using the right visualizations is crucial in order to understand the underlying patterns and results obtained by analysis algorithms despite its importance, defining the right visualization is still a challenging task business users are rarely experts in information visualization, and they may not exactly know the most adequate visualization tools or patterns for their goals consequently, misinterpreted graphs and wrong results can be obtained, leading to missed opportunities and significant losses for companies the main problem underneath is a lack of tools and methodologies that allow non expert users to define their visualization and data analysis goals in business terms in order to tackle this problem, we present an iterative goal oriented approach based on the i* language for the automatic derivation of data visualizations our approach links non expert user requirements to the data to be analyzed, choosing the most suited visualization techniques in a semi automatic way the great advantage of our proposal is that we provide non expert users with the best suited visualizations according to their information needs and their data with little effort and without requiring expertise in information visualization [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR194445",
            "template_id": "R186491",
            "paper_id": "R194445",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "automated recommendation of software refactorings based on feature requests during software evolution, developers often receive new requirements expressed as feature requests to implement the requested features, developers have to perform necessary modifications (refactorings) to prepare for new adaptation that accommodates the new requirements software refactoring is a well known technique that has been extensively used to improve software quality such as maintainability and extensibility however, it is often challenging to determine which kind of refactorings should be applied consequently, several approaches based on various heuristics have been proposed to recommend refactorings however, there is still lack of automated support to recommend refactorings given a feature request to this end, in this paper, we propose a novel approach that recommends refactorings based on the history of the previously requested features and applied refactorings first, we exploit the stateof the art refactoring detection tools to identify the previous refactorings applied to implement the past feature requests second, we train a machine classifier with the history data of the feature requests and refactorings applied on the commits that implemented the corresponding feature requests the machine classifier is then used to predict refactorings for new feature requests we evaluate the proposed approach on the dataset of 43 open source java projects and the results suggest that the proposed approach can accurately recommend refactorings (average precision 73%)",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] automated recommendation of software refactorings based on feature requests during software evolution, developers often receive new requirements expressed as feature requests to implement the requested features, developers have to perform necessary modifications (refactorings) to prepare for new adaptation that accommodates the new requirements software refactoring is a well known technique that has been extensively used to improve software quality such as maintainability and extensibility however, it is often challenging to determine which kind of refactorings should be applied consequently, several approaches based on various heuristics have been proposed to recommend refactorings however, there is still lack of automated support to recommend refactorings given a feature request to this end, in this paper, we propose a novel approach that recommends refactorings based on the history of the previously requested features and applied refactorings first, we exploit the stateof the art refactoring detection tools to identify the previous refactorings applied to implement the past feature requests second, we train a machine classifier with the history data of the feature requests and refactorings applied on the commits that implemented the corresponding feature requests the machine classifier is then used to predict refactorings for new feature requests we evaluate the proposed approach on the dataset of 43 open source java projects and the results suggest that the proposed approach can accurately recommend refactorings (average precision 73%) [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR194474",
            "template_id": "R186491",
            "paper_id": "R194474",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "how do practitioners capture and utilize user feedback during continuous software engineering? \"continuous software engineering (cse) evolved as a process for rapid software evolution continuous delivery enables developers to frequently retrieve user feedback on the latest software increment developers use these insights for requirements validation and verification despite the importance of users, reports about user feedback in cse practice are sparse we conducted 20 interviews with practitioners from 17 companies that apply cse we asked practitioners how they capture and utilize user feedback in this paper, we detail the practitioners' answers by posing three research questions to improve continuous user feedback capture and utilization with respect to requirements engineering, we derived five recommendations: first, internal sources should be approached, as they provide a rich source of user feedback; second, existing tool support should be adapted and extended to automate user feedback processing; third, a concept of reference points should be established to relate user feedback to requirements; fourth, the utilization of user feedback for requirements validation should be increased; and last, the interaction with user feedback should be enabled and supported by increasing developer user communication we conclude that a continuous user understanding activity can improve requirements engineering by contributing to both the completeness and correctness of requirements \"",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] how do practitioners capture and utilize user feedback during continuous software engineering? \"continuous software engineering (cse) evolved as a process for rapid software evolution continuous delivery enables developers to frequently retrieve user feedback on the latest software increment developers use these insights for requirements validation and verification despite the importance of users, reports about user feedback in cse practice are sparse we conducted 20 interviews with practitioners from 17 companies that apply cse we asked practitioners how they capture and utilize user feedback in this paper, we detail the practitioners' answers by posing three research questions to improve continuous user feedback capture and utilization with respect to requirements engineering, we derived five recommendations: first, internal sources should be approached, as they provide a rich source of user feedback; second, existing tool support should be adapted and extended to automate user feedback processing; third, a concept of reference points should be established to relate user feedback to requirements; fourth, the utilization of user feedback for requirements validation should be increased; and last, the interaction with user feedback should be enabled and supported by increasing developer user communication we conclude that a continuous user understanding activity can improve requirements engineering by contributing to both the completeness and correctness of requirements \" [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR194607",
            "template_id": "R186491",
            "paper_id": "R194607",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "extracting and analyzing context information in user support conversations on twitter while many apps include built in options to report bugs or request features, users still provide an increasing amount of feedback via social media, like twitter compared to traditional issue trackers, the reporting process in social media is unstructured and the feedback often lacks basic context information, such as the app version or the device concerned when experiencing the issue to make this feedback actionable to developers, support teams engage in recurring, effortful conversations with app users to clarify missing context items this paper introduces a simple approach that accurately extracts basic context information from unstructured, informal user feedback on mobile apps, including the platform, device, app version, and system version evaluated against a truthset of 3014 tweets from official twitter support accounts of the 3 popular apps netflix, snapchat, and spotify, our approach achieved precisions from 81% to 99% and recalls from 86% to 98% for the different context item types combined with a chatbot that automatically requests missing context items from reporting users, our approach aims at auto populating issue trackers with structured bug reports",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] extracting and analyzing context information in user support conversations on twitter while many apps include built in options to report bugs or request features, users still provide an increasing amount of feedback via social media, like twitter compared to traditional issue trackers, the reporting process in social media is unstructured and the feedback often lacks basic context information, such as the app version or the device concerned when experiencing the issue to make this feedback actionable to developers, support teams engage in recurring, effortful conversations with app users to clarify missing context items this paper introduces a simple approach that accurately extracts basic context information from unstructured, informal user feedback on mobile apps, including the platform, device, app version, and system version evaluated against a truthset of 3014 tweets from official twitter support accounts of the 3 popular apps netflix, snapchat, and spotify, our approach achieved precisions from 81% to 99% and recalls from 86% to 98% for the different context item types combined with a chatbot that automatically requests missing context items from reporting users, our approach aims at auto populating issue trackers with structured bug reports [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR194686",
            "template_id": "R186491",
            "paper_id": "R194686",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "the manager perspective on requirements impact on automotive systems development speed context: historically, automotive manufacturers have adopted rigid requirements engineering processes, which allowed them to meet safety critical requirements while integrating thousands of physical and software components into a highly complex and differentiated product nowadays, needs of improving development speed are pushing companies in this domain towards new ways of developing software objectives: we aim at obtaining a manager perspective on how the goal to increase development speed impacts how software intense automotive systems are developed and their requirements managed methods: we used a qualitative multiple case study, based on 20 semi structured interviews, at two automotive manufacturers our sampling strategy focuses on manager roles, complemented with technical specialists results: we found that both a requirements style dominated by safety concerns, and decomposition of requirements over many levels of abstraction impact development speed negatively furthermore, the use of requirements as part of legal contracts with suppliers hiders fast collaboration suggestions for potential improvements include domain specific tooling, model based requirements, test automation, and a combination of lightweight pre development requirements engineering with precise specifications post development conclusions: we offer an empirical account of expectations and needs for new requirements engineering approaches in the automotive domain, necessary to coordinate hundreds of collaborating organizations developing software intensive and potentially safety critical systems",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] the manager perspective on requirements impact on automotive systems development speed context: historically, automotive manufacturers have adopted rigid requirements engineering processes, which allowed them to meet safety critical requirements while integrating thousands of physical and software components into a highly complex and differentiated product nowadays, needs of improving development speed are pushing companies in this domain towards new ways of developing software objectives: we aim at obtaining a manager perspective on how the goal to increase development speed impacts how software intense automotive systems are developed and their requirements managed methods: we used a qualitative multiple case study, based on 20 semi structured interviews, at two automotive manufacturers our sampling strategy focuses on manager roles, complemented with technical specialists results: we found that both a requirements style dominated by safety concerns, and decomposition of requirements over many levels of abstraction impact development speed negatively furthermore, the use of requirements as part of legal contracts with suppliers hiders fast collaboration suggestions for potential improvements include domain specific tooling, model based requirements, test automation, and a combination of lightweight pre development requirements engineering with precise specifications post development conclusions: we offer an empirical account of expectations and needs for new requirements engineering approaches in the automotive domain, necessary to coordinate hundreds of collaborating organizations developing software intensive and potentially safety critical systems [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR194698",
            "template_id": "R186491",
            "paper_id": "R194698",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "a qualitative study on using guidegen to keep requirements and acceptance tests aligned software requirements constantly change, thus impacting all other artifacts of an evolving system in order to keep the system in a consistent state, changes in requirements should be documented and applied accordingly to all affected artifacts, including acceptance tests in practice, however, changes in requirements are not always documented nor applied to the affected acceptance tests this is mostly due to poor communication, lack of time or work overload, and eventually leads to project delays, unintended costs and unsatisfied customers guidegen is a tool supported approach for keeping requirements and acceptance tests aligned when a requirement is changed, guidegen automatically generates guidance in natural language on how to modify impacted acceptance tests and communicates this information to the concerned parties in this paper, we evaluate guidegen in terms of its perceived usefulness for practitioners and its applicability to real software projects the evaluation was conducted via interviews with 23 industrial practitioners from ten companies based in europe the results indicate that guidegen is a useful approach that facilitates requirements change management and the communication of changes between requirements and test engineers the participants also identified potential for improvement, in particular for using guidegen in large projects",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] a qualitative study on using guidegen to keep requirements and acceptance tests aligned software requirements constantly change, thus impacting all other artifacts of an evolving system in order to keep the system in a consistent state, changes in requirements should be documented and applied accordingly to all affected artifacts, including acceptance tests in practice, however, changes in requirements are not always documented nor applied to the affected acceptance tests this is mostly due to poor communication, lack of time or work overload, and eventually leads to project delays, unintended costs and unsatisfied customers guidegen is a tool supported approach for keeping requirements and acceptance tests aligned when a requirement is changed, guidegen automatically generates guidance in natural language on how to modify impacted acceptance tests and communicates this information to the concerned parties in this paper, we evaluate guidegen in terms of its perceived usefulness for practitioners and its applicability to real software projects the evaluation was conducted via interviews with 23 industrial practitioners from ten companies based in europe the results indicate that guidegen is a useful approach that facilitates requirements change management and the communication of changes between requirements and test engineers the participants also identified potential for improvement, in particular for using guidegen in large projects [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR194875",
            "template_id": "R186491",
            "paper_id": "R194875",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "a metamodeling approach to support the engineering of modeling method requirements the notion of \"modeling method requirements\" refers to a category typically neglected by re taxonomies and frameworks i e , those requirements that motivate the realization of (conceptual) modeling methods and tools they can be considered domain specific, in the sense that all modeling methods provide a knowledge schema for some selected application domain (narrow or broad) besides this inherent domain specific nature, we are investigating how the characteristics of modeling methods inform the re perspective, and how in turn re can support the engineering of such artifacts thus, the work at hand aims to raise awareness about modeling method requirements in the re community the core contribution is the cochaco (concept characteristic connector) method for the representation and management of such requirements, as well as for streamlining with subsequent engineering phases cochaco is itself a modeling method i e , it achieves its goals through diagrammatic modeling means for which a supporting tool was prototyped and evolved the proposal originates in required support for the initial phase of the agile modeling method engineering (amme) methodology, which was successfully applied in developing a variety of project specific modeling tools from this accumulated experience, awareness of \"modeling method requirements\" emerged and informed the design decisions of cochaco",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] a metamodeling approach to support the engineering of modeling method requirements the notion of \"modeling method requirements\" refers to a category typically neglected by re taxonomies and frameworks i e , those requirements that motivate the realization of (conceptual) modeling methods and tools they can be considered domain specific, in the sense that all modeling methods provide a knowledge schema for some selected application domain (narrow or broad) besides this inherent domain specific nature, we are investigating how the characteristics of modeling methods inform the re perspective, and how in turn re can support the engineering of such artifacts thus, the work at hand aims to raise awareness about modeling method requirements in the re community the core contribution is the cochaco (concept characteristic connector) method for the representation and management of such requirements, as well as for streamlining with subsequent engineering phases cochaco is itself a modeling method i e , it achieves its goals through diagrammatic modeling means for which a supporting tool was prototyped and evolved the proposal originates in required support for the initial phase of the agile modeling method engineering (amme) methodology, which was successfully applied in developing a variety of project specific modeling tools from this accumulated experience, awareness of \"modeling method requirements\" emerged and informed the design decisions of cochaco [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR194884",
            "template_id": "R186491",
            "paper_id": "R194884",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "extraction of system states from natural language requirements in recent years, simulations have proven to be an important means to verify the behavior of complex software systems the different states of a system are monitored in the simulations and are compared against the requirements specification so far, system states in natural language requirements cannot be automatically linked to signals from the simulation however, the manual mapping between requirements and simulation is a time consuming task named entity recognition is a sub task from the field of automated information retrieval and is used to classify parts of natural language texts into categories in this paper, we use a self trained named entity recognition model with bidirectional lstms and cnns to extract states from requirements specifications we present an almost entirely automated approach and an iterative semi automated approach to train our model the automated and iterative approach are compared and discussed with respect to the usual manual extraction we show that the manual extraction of states in 2,000 requirements takes nine hours our automated approach achieves an f1 score of 0 51 with 15 minutes of manual work and the iterative approach achieves an f1 score of 0 62 with 100 minutes of work",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] extraction of system states from natural language requirements in recent years, simulations have proven to be an important means to verify the behavior of complex software systems the different states of a system are monitored in the simulations and are compared against the requirements specification so far, system states in natural language requirements cannot be automatically linked to signals from the simulation however, the manual mapping between requirements and simulation is a time consuming task named entity recognition is a sub task from the field of automated information retrieval and is used to classify parts of natural language texts into categories in this paper, we use a self trained named entity recognition model with bidirectional lstms and cnns to extract states from requirements specifications we present an almost entirely automated approach and an iterative semi automated approach to train our model the automated and iterative approach are compared and discussed with respect to the usual manual extraction we show that the manual extraction of states in 2,000 requirements takes nine hours our automated approach achieves an f1 score of 0 51 with 15 minutes of manual work and the iterative approach achieves an f1 score of 0 62 with 100 minutes of work [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR194954",
            "template_id": "R186491",
            "paper_id": "R194954",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "vetting automatically generated trace links: what information is useful to human analysts? \"automated traceability has been investigated for over a decade with promising results however, a human analyst is needed to vet the generated trace links to ensure their quality the process of vetting trace links is not trivial and while previous studies have analyzed the performance of the human analyst, they have not focused on the analyst's information needs the aim of this study is to investigate what context information the human analyst needs we used design science research, in which we conducted interviews with ten practitioners in the traceability area to understand the information needed by human analysts we then compared the information collected from the interviews with existing literature we created a prototype tool that presents this information to the human analyst to further understand the role of context information, we conducted a controlled experiment with 33 participants our interviews reveal that human analysts need information from three different sources: 1) from the artifacts connected by the link, 2) from the traceability information model, and 3) from the tracing algorithm the experiment results show that the content of the connected artifacts is more useful to the analyst than the contextual information of the artifacts \"",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] vetting automatically generated trace links: what information is useful to human analysts? \"automated traceability has been investigated for over a decade with promising results however, a human analyst is needed to vet the generated trace links to ensure their quality the process of vetting trace links is not trivial and while previous studies have analyzed the performance of the human analyst, they have not focused on the analyst's information needs the aim of this study is to investigate what context information the human analyst needs we used design science research, in which we conducted interviews with ten practitioners in the traceability area to understand the information needed by human analysts we then compared the information collected from the interviews with existing literature we created a prototype tool that presents this information to the human analyst to further understand the role of context information, we conducted a controlled experiment with 33 participants our interviews reveal that human analysts need information from three different sources: 1) from the artifacts connected by the link, 2) from the traceability information model, and 3) from the tracing algorithm the experiment results show that the content of the connected artifacts is more useful to the analyst than the contextual information of the artifacts \" [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR194974",
            "template_id": "R186491",
            "paper_id": "R194974",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "modeling user concerns in the app store: a case study on the rise and fall of yik yak \"mobile application (app) stores have lowered the barriers to app market entry, leading to an accelerated and unprecedented pace of mobile software production to survive in such a highly competitive and vibrant market, release engineering decisions should be driven by a systematic analysis of the complex interplay between the user, system, and market components of the mobile app ecosystem to demonstrate the feasibility and value of such analysis, in this paper, we present a case study on the rise and fall of yik yak, one of the most popular social networking apps at its peak in particular, we identify and analyze the design decisions that led to the downfall of yik yak and track rival apps' attempts to take advantage of this failure we further perform a systematic in depth analysis to identify the main user concerns in the domain of anonymous social networking apps and model their relations to the core features of the domain such a model can be utilized by app developers to devise sustainable release engineering strategies that can address urgent user concerns and maintain market viability \"",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] modeling user concerns in the app store: a case study on the rise and fall of yik yak \"mobile application (app) stores have lowered the barriers to app market entry, leading to an accelerated and unprecedented pace of mobile software production to survive in such a highly competitive and vibrant market, release engineering decisions should be driven by a systematic analysis of the complex interplay between the user, system, and market components of the mobile app ecosystem to demonstrate the feasibility and value of such analysis, in this paper, we present a case study on the rise and fall of yik yak, one of the most popular social networking apps at its peak in particular, we identify and analyze the design decisions that led to the downfall of yik yak and track rival apps' attempts to take advantage of this failure we further perform a systematic in depth analysis to identify the main user concerns in the domain of anonymous social networking apps and model their relations to the core features of the domain such a model can be utilized by app developers to devise sustainable release engineering strategies that can address urgent user concerns and maintain market viability \" [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR195005",
            "template_id": "R186491",
            "paper_id": "R195005",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "catalog of invisibility requirements for ubicomp and iot applications a new set of non functional requirements (nfrs) have appeared with the advent of ubiquitous computing (ubicomp) and more recently internet of things (iot) invisibility is one of these nfrs that means the ability to hide technology from users although invisibility is long seen as an essential characteristic for achieving the goals of ubicomp, it has not been cataloged regarding its subcharacteristics and solutions, making its design and requirements specification in such applications a challenging task considering the softgoal interdependency graph (sig), which is a well known format to catalog nfrs, this work aims at capturing subcharacteristics and solutions for invisibility and cataloging them in a sig since there is no systematic approach on how to build sigs, we also propose to systematize the definition of invisibility sig using the following well defined research methods: snowballing, database search, grounded theory and questionnaires as a result, we got an invisibility sig composed of two main subcharacteristics, twelve sub subcharacteristics, ten general solutions and fifty six specific solutions this organized body of knowledge is useful for supporting software engineers to specify requirements and practical solutions for ubicomp and iot applications furthermore, the proposed methodology used to capture and catalog requirements in a sig can be reused for other nfrs",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] catalog of invisibility requirements for ubicomp and iot applications a new set of non functional requirements (nfrs) have appeared with the advent of ubiquitous computing (ubicomp) and more recently internet of things (iot) invisibility is one of these nfrs that means the ability to hide technology from users although invisibility is long seen as an essential characteristic for achieving the goals of ubicomp, it has not been cataloged regarding its subcharacteristics and solutions, making its design and requirements specification in such applications a challenging task considering the softgoal interdependency graph (sig), which is a well known format to catalog nfrs, this work aims at capturing subcharacteristics and solutions for invisibility and cataloging them in a sig since there is no systematic approach on how to build sigs, we also propose to systematize the definition of invisibility sig using the following well defined research methods: snowballing, database search, grounded theory and questionnaires as a result, we got an invisibility sig composed of two main subcharacteristics, twelve sub subcharacteristics, ten general solutions and fifty six specific solutions this organized body of knowledge is useful for supporting software engineers to specify requirements and practical solutions for ubicomp and iot applications furthermore, the proposed methodology used to capture and catalog requirements in a sig can be reused for other nfrs [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR195023",
            "template_id": "R186491",
            "paper_id": "R195023",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "re and society a perspective on re in times of smart cities and smart rural areas our requirements engineering (re) community has known for decades that the success or failure of re methods heavily depends on the context in which they are applied thus, many experiences have been gained and shared in the community that reflect which re methods are suitable for a specific context, such as embedded systems development (e g , automotive or military domain) or information systems development (e g , banking or flight control domain) nowadays, in times of smart cities and their counterpart smart rural areas, where newly introduced it systems have a strong effect on our society, a new and challenging context arises for re, which opens up new research questions as a contribution to this situation and to foster discussions in our community about whether our re methods are appropriate in this new \"social context\", this perspective paper reflects on the state of the art and on our own experiences in applying re in the context of smart rural areas these results might also pertain in the context of smart cities that pose similar challenges to re in addition, we present a framework comprising both an initial classification of social contexts, particularly their end users, and a classification for re methods example usage scenarios illustrate how this framework helps to reflect on the suitability of our re methods, and, if necessary, provides the basis for adapting them or creating new ones finally, we outline a roadmap with research questions and related activities with which we want to encourage our community to perform the proposed research activities in order to enrich our body of experiences and adapt our methods to this highly relevant context",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] re and society a perspective on re in times of smart cities and smart rural areas our requirements engineering (re) community has known for decades that the success or failure of re methods heavily depends on the context in which they are applied thus, many experiences have been gained and shared in the community that reflect which re methods are suitable for a specific context, such as embedded systems development (e g , automotive or military domain) or information systems development (e g , banking or flight control domain) nowadays, in times of smart cities and their counterpart smart rural areas, where newly introduced it systems have a strong effect on our society, a new and challenging context arises for re, which opens up new research questions as a contribution to this situation and to foster discussions in our community about whether our re methods are appropriate in this new \"social context\", this perspective paper reflects on the state of the art and on our own experiences in applying re in the context of smart rural areas these results might also pertain in the context of smart cities that pose similar challenges to re in addition, we present a framework comprising both an initial classification of social contexts, particularly their end users, and a classification for re methods example usage scenarios illustrate how this framework helps to reflect on the suitability of our re methods, and, if necessary, provides the basis for adapting them or creating new ones finally, we outline a roadmap with research questions and related activities with which we want to encourage our community to perform the proposed research activities in order to enrich our body of experiences and adapt our methods to this highly relevant context [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR195113",
            "template_id": "R186491",
            "paper_id": "R195113",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "automated extraction of semantic legal metadata using natural language processing [context] semantic legal metadata provides information that helps with understanding and interpreting the meaning of legal provisions such metadata is important for the systematic analysis of legal requirements [objectives] our work is motivated by two observations: (1) the existing requirements engineering (re) literature does not provide a harmonized view on the semantic metadata types that are useful for legal requirements analysis (2) automated support for the extraction of semantic legal metadata is scarce, and further does not exploit the full potential of natural language processing (nlp) our objective is to take steps toward addressing these limitations [methods] we review and reconcile the semantic legal metadata types proposed in re subsequently, we conduct a qualitative study aimed at investigating how the identified metadata types can be extracted automatically [results and conclusions] we propose (1) a harmonized conceptual model for the semantic metadata types pertinent to legal requirements analysis, and (2) automated extraction rules for these metadata types based on nlp we evaluate the extraction rules through a case study our results indicate that the rules generate metadata annotations with high accuracy",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] automated extraction of semantic legal metadata using natural language processing [context] semantic legal metadata provides information that helps with understanding and interpreting the meaning of legal provisions such metadata is important for the systematic analysis of legal requirements [objectives] our work is motivated by two observations: (1) the existing requirements engineering (re) literature does not provide a harmonized view on the semantic metadata types that are useful for legal requirements analysis (2) automated support for the extraction of semantic legal metadata is scarce, and further does not exploit the full potential of natural language processing (nlp) our objective is to take steps toward addressing these limitations [methods] we review and reconcile the semantic legal metadata types proposed in re subsequently, we conduct a qualitative study aimed at investigating how the identified metadata types can be extracted automatically [results and conclusions] we propose (1) a harmonized conceptual model for the semantic metadata types pertinent to legal requirements analysis, and (2) automated extraction rules for these metadata types based on nlp we evaluate the extraction rules through a case study our results indicate that the rules generate metadata annotations with high accuracy [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR195124",
            "template_id": "R186491",
            "paper_id": "R195124",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "the grace period has ended: an approach to operationalize gdpr requirements \"the general data protection regulation (gdpr) aims to protect personal data of eu residents and can impose severe sanctions for non compliance organizations are currently implementing various measures to ensure their software systems fulfill gdpr obligations such as identifying a legal basis for data processing or enforcing data anonymization however, as regulations are formulated vaguely, it is difficult for practitioners to extract and operationalize legal requirements from the gdpr this paper aims to help organizations understand the data protection obligations imposed by the gdpr and identify measures to ensure compliance to achieve this goal, we propose guideme, a 6 step systematic approach that supports elicitation of solution requirements that link gdpr data protection obligations with the privacy controls that fulfill these obligations and that should be implemented in an organization's software system we illustrate and evaluate our approach using an example of a university information system our results demonstrate that the solution requirements elicited using our approach are aligned with the recommendations of privacy experts and are expressed correctly \"",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] the grace period has ended: an approach to operationalize gdpr requirements \"the general data protection regulation (gdpr) aims to protect personal data of eu residents and can impose severe sanctions for non compliance organizations are currently implementing various measures to ensure their software systems fulfill gdpr obligations such as identifying a legal basis for data processing or enforcing data anonymization however, as regulations are formulated vaguely, it is difficult for practitioners to extract and operationalize legal requirements from the gdpr this paper aims to help organizations understand the data protection obligations imposed by the gdpr and identify measures to ensure compliance to achieve this goal, we propose guideme, a 6 step systematic approach that supports elicitation of solution requirements that link gdpr data protection obligations with the privacy controls that fulfill these obligations and that should be implemented in an organization's software system we illustrate and evaluate our approach using an example of a university information system our results demonstrate that the solution requirements elicited using our approach are aligned with the recommendations of privacy experts and are expressed correctly \" [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR195144",
            "template_id": "R186491",
            "paper_id": "R195144",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "learning from mistakes: an empirical study of elicitation interviews performed by novices [context] interviews are the most widely used elicitation technique in requirements engineering however, conducting effective requirements elicitation interviews is challenging, due to the combination of technical and soft skills that requirements analysts often acquire after a long period of professional practice empirical evidence about training the novices on conducting effective requirements elicitation interviews is scarce [objectives] we present a list of most common mistakes that novices make in requirements elicitation interviews the objective is to assist the educators in teaching interviewing skills to student analysts [re search method] we conducted an empirical study involving role playing and authentic assessment with 110 students, teamed up in 28 groups, to conduct interviews with a customer one re searcher made observation notes during the interview while two researchers reviewed the recordings we qualitatively analyzed the data to identify the themes and classify the mistakes [results and conclusion] we identified 34 unique mistakes classified into 7 high level themes we also give examples of the mistakes made by the novices in each theme, to assist the educationists and trainers our research design is a novel combination of well known pedagogical approaches described in sufficient details to make it re peatable for future requirements engineering education and training research",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] learning from mistakes: an empirical study of elicitation interviews performed by novices [context] interviews are the most widely used elicitation technique in requirements engineering however, conducting effective requirements elicitation interviews is challenging, due to the combination of technical and soft skills that requirements analysts often acquire after a long period of professional practice empirical evidence about training the novices on conducting effective requirements elicitation interviews is scarce [objectives] we present a list of most common mistakes that novices make in requirements elicitation interviews the objective is to assist the educators in teaching interviewing skills to student analysts [re search method] we conducted an empirical study involving role playing and authentic assessment with 110 students, teamed up in 28 groups, to conduct interviews with a customer one re searcher made observation notes during the interview while two researchers reviewed the recordings we qualitatively analyzed the data to identify the themes and classify the mistakes [results and conclusion] we identified 34 unique mistakes classified into 7 high level themes we also give examples of the mistakes made by the novices in each theme, to assist the educationists and trainers our research design is a novel combination of well known pedagogical approaches described in sufficient details to make it re peatable for future requirements engineering education and training research [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR195173",
            "template_id": "R186491",
            "paper_id": "R195173",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "towards ubiquitous re: a perspective on requirements engineering in the era of digital transformation we are now living in the era of digital transformation: innovative and digital business models are transforming the global business world and society however, the authors of this paper have perceived barriers that prevent requirements engineers from contributing properly to the development of the software systems that underpin the digital transformation we also realized that breaking down each of these barriers would contribute to requirements engineering (re) becoming ubiquitous in certain dimensions: re everywhere, with everyone, for everything, automated, accepting openness, and cross domain in this paper, we analyze each dimension of ubiquity in the scope of the interaction between requirements engineers and end users in particular, we point out the transformation that is required to break down each barrier, present the perspective of the scientific community and our own practical perspective, and discuss our vision on how to achieve this dimension of ubiquity our goal is to raise the interest of the research community in providing approaches to address the barriers and move towards ubiquitous re",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] towards ubiquitous re: a perspective on requirements engineering in the era of digital transformation we are now living in the era of digital transformation: innovative and digital business models are transforming the global business world and society however, the authors of this paper have perceived barriers that prevent requirements engineers from contributing properly to the development of the software systems that underpin the digital transformation we also realized that breaking down each of these barriers would contribute to requirements engineering (re) becoming ubiquitous in certain dimensions: re everywhere, with everyone, for everything, automated, accepting openness, and cross domain in this paper, we analyze each dimension of ubiquity in the scope of the interaction between requirements engineers and end users in particular, we point out the transformation that is required to break down each barrier, present the perspective of the scientific community and our own practical perspective, and discuss our vision on how to achieve this dimension of ubiquity our goal is to raise the interest of the research community in providing approaches to address the barriers and move towards ubiquitous re [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR195201",
            "template_id": "R186491",
            "paper_id": "R195201",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "an experimental comparison of two navigation techniques for requirements modeling tools \"in requirements engineering, many modeling tasks require viewing different parts of a model concurrently however, traditional zoom+scroll navigation uses a single focus zoom, i e , at a given point in time, a user can zoom in on a single spot in the model only therefore, new focus+context navigation techniques have been proposed that allow multiple foci at the same time in this paper, we report on an experiment with students where we compare the participants' performance when using a requirements modeling tool with traditional zoom+scroll navigation vs one with so called flexiview navigation which is a focus+context technique with multiple foci the participants had to perform typical modeling tasks such as searching, editing, and traversing a model all tasks were performed on medium sized tablets with a tool for manipulating so called imitgraphs imitgraphs are enriched node and edge diagrams that can mimic various diagram types such as class, activity, or goal decomposition diagrams we found that navigation with flexiview outperformed zoom+scroll navigation with respect to task completion time, number of mistakes, cognitive load, and user satisfaction \"",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] an experimental comparison of two navigation techniques for requirements modeling tools \"in requirements engineering, many modeling tasks require viewing different parts of a model concurrently however, traditional zoom+scroll navigation uses a single focus zoom, i e , at a given point in time, a user can zoom in on a single spot in the model only therefore, new focus+context navigation techniques have been proposed that allow multiple foci at the same time in this paper, we report on an experiment with students where we compare the participants' performance when using a requirements modeling tool with traditional zoom+scroll navigation vs one with so called flexiview navigation which is a focus+context technique with multiple foci the participants had to perform typical modeling tasks such as searching, editing, and traversing a model all tasks were performed on medium sized tablets with a tool for manipulating so called imitgraphs imitgraphs are enriched node and edge diagrams that can mimic various diagram types such as class, activity, or goal decomposition diagrams we found that navigation with flexiview outperformed zoom+scroll navigation with respect to task completion time, number of mistakes, cognitive load, and user satisfaction \" [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR195216",
            "template_id": "R186491",
            "paper_id": "R195216",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "morse: reducing the feature interaction explosion problem using subject matter knowledge as abstract requirements the feature interaction problem appears in many different kinds of complex systems, especially systems whose elements are created or maintained by separate entities for example, a modern automobile that incorporates electronic systems produced by different suppliers cross cutting concerns, such as safety and security, require a comprehensive analysis of the possible interactions however, there is a combinatorial explosion in the number of feature combinations to be considered our work approaches the feature interaction problem from a novel point of view: we seek to use the abstract subject matter knowledge of domain experts to deduce why some features will not interact, rather than trying to discover or resolve the interactions in this paper, we present a method that can automatically reduce the required number of combinations and situations that have to be evaluated or resolved for feature interactions our tool, called morse, rules out feature combinations that cannot have interactions based on traceable deductions from relatively simple abstract requirements that capture relevant subject matter knowledge our method is useful as a means of focusing attention on particular situations where more detailed functional requirements may be needed to avoid unacceptable risk arising from unintended interactions between features relatively simple abstract requirements that capture relevant subject matter knowledge our method is useful as a means of focusing attention on particular situations where more detailed functional requirements may be needed to avoid unacceptable risk arising from unintended interactions between features",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] morse: reducing the feature interaction explosion problem using subject matter knowledge as abstract requirements the feature interaction problem appears in many different kinds of complex systems, especially systems whose elements are created or maintained by separate entities for example, a modern automobile that incorporates electronic systems produced by different suppliers cross cutting concerns, such as safety and security, require a comprehensive analysis of the possible interactions however, there is a combinatorial explosion in the number of feature combinations to be considered our work approaches the feature interaction problem from a novel point of view: we seek to use the abstract subject matter knowledge of domain experts to deduce why some features will not interact, rather than trying to discover or resolve the interactions in this paper, we present a method that can automatically reduce the required number of combinations and situations that have to be evaluated or resolved for feature interactions our tool, called morse, rules out feature combinations that cannot have interactions based on traceable deductions from relatively simple abstract requirements that capture relevant subject matter knowledge our method is useful as a means of focusing attention on particular situations where more detailed functional requirements may be needed to avoid unacceptable risk arising from unintended interactions between features relatively simple abstract requirements that capture relevant subject matter knowledge our method is useful as a means of focusing attention on particular situations where more detailed functional requirements may be needed to avoid unacceptable risk arising from unintended interactions between features [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR195224",
            "template_id": "R186491",
            "paper_id": "R195224",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "understanding challenging situations in agile quality requirements engineering and their solution strategies: insights from a case study in the last few years, agile development methods are getting increasingly popular in large scale distributed contexts despite this popularity, empirical studies have reported several challenges that large scale distributed agile projects face regarding the implementation of quality requirements however, there is little known about the mechanisms behind those challenges and the practices currently used by agile practitioners to adequately assure the implementation of quality requirements in distributed context to look deeper into this, we performed a qualitative multi case study in six different organizations in the netherlands our multi case study included seventeen semi structured open ended in depth interviews with agile practitioners of different background and expertise the analysis of the collected data re sulted in identifying eleven mechanisms that could be associated with the previously published list of challenges moreover, the analysis uncovered nine practices used by agile practitioners as solutions to the challenges, in order to ensure the implementation of quality requirements last, we have mapped the identified mechanisms and practices to the previously identified challenges to get insight into the possible cause and mitigation of those challenges",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] understanding challenging situations in agile quality requirements engineering and their solution strategies: insights from a case study in the last few years, agile development methods are getting increasingly popular in large scale distributed contexts despite this popularity, empirical studies have reported several challenges that large scale distributed agile projects face regarding the implementation of quality requirements however, there is little known about the mechanisms behind those challenges and the practices currently used by agile practitioners to adequately assure the implementation of quality requirements in distributed context to look deeper into this, we performed a qualitative multi case study in six different organizations in the netherlands our multi case study included seventeen semi structured open ended in depth interviews with agile practitioners of different background and expertise the analysis of the collected data re sulted in identifying eleven mechanisms that could be associated with the previously published list of challenges moreover, the analysis uncovered nine practices used by agile practitioners as solutions to the challenges, in order to ensure the implementation of quality requirements last, we have mapped the identified mechanisms and practices to the previously identified challenges to get insight into the possible cause and mitigation of those challenges [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR195495",
            "template_id": "R186491",
            "paper_id": "R195495",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "safe: a simple approach for feature extraction from app descriptions and app reviews a main advantage of app stores is that they aggregate important information created by both developers and users in the app store product pages, developers usually describe and maintain the features of their apps in the app reviews, users comment these features recent studies focused on mining app features either as described by developers or as reviewed by users however, extracting and matching the features from the app descriptions and the reviews is essential to bear the app store advantages, e g allowing analysts to identify which app features are actually being reviewed and which are not in this paper, we propose safe, a novel uniform approach to extract app features from the single app pages, the single reviews and to match them we manually build 18 part of speech patterns and 5 sentence patterns that are frequently used in text referring to app features we then apply these patterns with several text pre and post processing steps a major advantage of our approach is that it does not require large training and configuration data to evaluate its accuracy, we manually extracted the features mentioned in the pages and reviews of 10 apps the extraction precision and recall outperformed two state of the art approaches for well maintained app pages such as for google drive our approach has a precision of 87% and on average 56% for 10 evaluated apps safe also matches 87% of the features extracted from user reviews to those extracted from the app descriptions",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] safe: a simple approach for feature extraction from app descriptions and app reviews a main advantage of app stores is that they aggregate important information created by both developers and users in the app store product pages, developers usually describe and maintain the features of their apps in the app reviews, users comment these features recent studies focused on mining app features either as described by developers or as reviewed by users however, extracting and matching the features from the app descriptions and the reviews is essential to bear the app store advantages, e g allowing analysts to identify which app features are actually being reviewed and which are not in this paper, we propose safe, a novel uniform approach to extract app features from the single app pages, the single reviews and to match them we manually build 18 part of speech patterns and 5 sentence patterns that are frequently used in text referring to app features we then apply these patterns with several text pre and post processing steps a major advantage of our approach is that it does not require large training and configuration data to evaluate its accuracy, we manually extracted the features mentioned in the pages and reviews of 10 apps the extraction precision and recall outperformed two state of the art approaches for well maintained app pages such as for google drive our approach has a precision of 87% and on average 56% for 10 evaluated apps safe also matches 87% of the features extracted from user reviews to those extracted from the app descriptions [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR195551",
            "template_id": "R186491",
            "paper_id": "R195551",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "feedback gathering from an industrial point of view feedback communication channels allow end users to express their needs, which can be considered in software development and evolution although feedback gathering and analysis have been identified as an important topic and several researchers have started their investigation, information is scarce on how software companies currently elicit end user feedback in this study, we explore the experiences of software companies with respect to feedback gathering the results of a case study and online survey indicate two sides of the same coin: on the one hand, most software companies are aware of the relevance of end user feedback for software evolution and provide feedback channels, which allow end users to communicate their needs and problems on the other hand, the quantity and quality of the feedback received varies we conclude that software companies still do not fully exploit the potential of end user feedback for software development and evolution",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] feedback gathering from an industrial point of view feedback communication channels allow end users to express their needs, which can be considered in software development and evolution although feedback gathering and analysis have been identified as an important topic and several researchers have started their investigation, information is scarce on how software companies currently elicit end user feedback in this study, we explore the experiences of software companies with respect to feedback gathering the results of a case study and online survey indicate two sides of the same coin: on the one hand, most software companies are aware of the relevance of end user feedback for software evolution and provide feedback channels, which allow end users to communicate their needs and problems on the other hand, the quantity and quality of the feedback received varies we conclude that software companies still do not fully exploit the potential of end user feedback for software development and evolution [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR195587",
            "template_id": "R186491",
            "paper_id": "R195587",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "what questions do requirements engineers ask? requirements engineering (re) is comprised of various tasks related to discovering, documenting, and maintaining different kinds of requirements to accomplish these tasks, a requirements engineer or business analyst needs to retrieve and combine information from multiple sources such as use case models, interview scripts, and business rules however, collecting and analyzing all the required data can be tedious and the resulting data is often incomplete with inadequate trace links analyzing real world queries can shed light on the questions requirements professionals would like to ask and the artifacts needed to support such questions we therefore conducted an online survey with requirements professionals in the it industry our analysis included 29 survey responses and a total of 159 natural language queries using open coding and grounded theory, we analyzed and grouped these queries into 9 different query purposes and 54 sub purposes, and also identified frequently used artifacts the results from the survey could help project level planners identify important questions, proactively instrument their environments with supporting tools, and strategically collect data that is needed to answer the queries of interest to their project",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] what questions do requirements engineers ask? requirements engineering (re) is comprised of various tasks related to discovering, documenting, and maintaining different kinds of requirements to accomplish these tasks, a requirements engineer or business analyst needs to retrieve and combine information from multiple sources such as use case models, interview scripts, and business rules however, collecting and analyzing all the required data can be tedious and the resulting data is often incomplete with inadequate trace links analyzing real world queries can shed light on the questions requirements professionals would like to ask and the artifacts needed to support such questions we therefore conducted an online survey with requirements professionals in the it industry our analysis included 29 survey responses and a total of 159 natural language queries using open coding and grounded theory, we analyzed and grouped these queries into 9 different query purposes and 54 sub purposes, and also identified frequently used artifacts the results from the survey could help project level planners identify important questions, proactively instrument their environments with supporting tools, and strategically collect data that is needed to answer the queries of interest to their project [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR195664",
            "template_id": "R186491",
            "paper_id": "R195664",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "the trouble with security requirements \"manifold approaches to security requirements engineering have been proposed, yet there is no consensus how to elicit, analyze, or express security needs this perspective paper systematizes the problem space of security requirements engineering security needs result from the interplay of three dimensions: threats, security goals, and system design elementary statements can be made in each dimension, but such one dimensional requirements remain partial and insufficient to understand security needs, one has to analyze their interaction distinct analysis tasks arise for each pair of dimensions and are supported by different techniques: risk analysis, as in coras, between threats and security goals; security design, as exemplified by the framework of haley et al , between goals and design; and security design analysis, such as microsoft's threat modeling technique with data flow diagrams and stride, between design and threats all three perspectives are necessary to develop secure systems security requirements engineering must iterate through them, because threats determine the relevance of security goals, security design seeks ways to fulfill them, and design choices themselves influence threats and security goals \"",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] the trouble with security requirements \"manifold approaches to security requirements engineering have been proposed, yet there is no consensus how to elicit, analyze, or express security needs this perspective paper systematizes the problem space of security requirements engineering security needs result from the interplay of three dimensions: threats, security goals, and system design elementary statements can be made in each dimension, but such one dimensional requirements remain partial and insufficient to understand security needs, one has to analyze their interaction distinct analysis tasks arise for each pair of dimensions and are supported by different techniques: risk analysis, as in coras, between threats and security goals; security design, as exemplified by the framework of haley et al , between goals and design; and security design analysis, such as microsoft's threat modeling technique with data flow diagrams and stride, between design and threats all three perspectives are necessary to develop secure systems security requirements engineering must iterate through them, because threats determine the relevance of security goals, security design seeks ways to fulfill them, and design choices themselves influence threats and security goals \" [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR195692",
            "template_id": "R186491",
            "paper_id": "R195692",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "\u201cshort\u201der reasoning about larger requirements models when requirements engineering(re) models are unreasonably complex, they cannot support efficient decision making short is a tool to simplify that reasoning by exploiting the \"key\" decisions within re models these \"keys\" have the property that once values are assigned to them, it is very fast to reason over the remaining decisions using these \"keys\", reasoning about re models can be greatly shortened by focusing stakeholder discussion on just these key decisions this paper evaluates the short tool on eight complex re models we find that the number of keys are typically only 12% of all decisions since they are so few in number, keys can be used to reason faster about models for example, using keys, we can optimize over those models (to achieve the most goals at least cost) two to three orders of magnitude faster than standard methods better yet, finding those keys is not difficult: short runs in low order polynomial time and terminates in a few minutes for the largest models",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] \u201cshort\u201der reasoning about larger requirements models when requirements engineering(re) models are unreasonably complex, they cannot support efficient decision making short is a tool to simplify that reasoning by exploiting the \"key\" decisions within re models these \"keys\" have the property that once values are assigned to them, it is very fast to reason over the remaining decisions using these \"keys\", reasoning about re models can be greatly shortened by focusing stakeholder discussion on just these key decisions this paper evaluates the short tool on eight complex re models we find that the number of keys are typically only 12% of all decisions since they are so few in number, keys can be used to reason faster about models for example, using keys, we can optimize over those models (to achieve the most goals at least cost) two to three orders of magnitude faster than standard methods better yet, finding those keys is not difficult: short runs in low order polynomial time and terminates in a few minutes for the largest models [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR195713",
            "template_id": "R186491",
            "paper_id": "R195713",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "modeling and reasoning with changing intentions: an experiment \"existing modeling approaches in requirements engineering assume that stakeholder goals are static: once set, they remain the same throughout the lifecycle of the project of course, such goals, like anything else, may change over time in earlier work, we introduced evolving intentions: an approach that allows stakeholders to specify how evaluations of goal model elements change over time simulation over evolving intentions enables stakeholders to ask a variety of 'what if' questions, and evaluate possible evolutions of a goal model growingleaf is a web based tool that implements both the modeling and analysis components of this approach in this paper, we investigate the effectiveness and usability of evolving intentions, simulation over evolving intentions, and growingleaf we report on a between subjects experiment we conducted with fifteen graduate students familiar with requirements engineering using qualitative, quantitative, and timing data, we show that evolving intentions were intuitive, that simulation over evolving intentions increased the subjects' understanding and produced meaningful results, and that growingleaf was found to be effective and usable \"",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] modeling and reasoning with changing intentions: an experiment \"existing modeling approaches in requirements engineering assume that stakeholder goals are static: once set, they remain the same throughout the lifecycle of the project of course, such goals, like anything else, may change over time in earlier work, we introduced evolving intentions: an approach that allows stakeholders to specify how evaluations of goal model elements change over time simulation over evolving intentions enables stakeholders to ask a variety of 'what if' questions, and evaluate possible evolutions of a goal model growingleaf is a web based tool that implements both the modeling and analysis components of this approach in this paper, we investigate the effectiveness and usability of evolving intentions, simulation over evolving intentions, and growingleaf we report on a between subjects experiment we conducted with fifteen graduate students familiar with requirements engineering using qualitative, quantitative, and timing data, we show that evolving intentions were intuitive, that simulation over evolving intentions increased the subjects' understanding and produced meaningful results, and that growingleaf was found to be effective and usable \" [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR195743",
            "template_id": "R186491",
            "paper_id": "R195743",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "new frontiers for requirements engineering requirements engineering (re) has grown from its humble beginnings to embrace a wide variety of techniques, drawn from many disciplines, and the diversity of tasks currently performed under the label of re has grown beyond that encom passed by software development we briefly review how re has evolved and observe that re is now a collection of best practices for pragmatic, outcome focused critical thinking \u2013 applicable to any domain we discuss an alternative perspective on, and de scription of, the discipline of re and advocate for the evolution of re toward a discipline that supports the application of re prac tice to any domain we call upon re practitioners to proactively engage in alternative domains and call upon researchers that adopt practices from other domains to actively engage with their inspiring domains for both, we ask that they report upon their experience so that we can continue to expand re frontiers",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] new frontiers for requirements engineering requirements engineering (re) has grown from its humble beginnings to embrace a wide variety of techniques, drawn from many disciplines, and the diversity of tasks currently performed under the label of re has grown beyond that encom passed by software development we briefly review how re has evolved and observe that re is now a collection of best practices for pragmatic, outcome focused critical thinking \u2013 applicable to any domain we discuss an alternative perspective on, and de scription of, the discipline of re and advocate for the evolution of re toward a discipline that supports the application of re prac tice to any domain we call upon re practitioners to proactively engage in alternative domains and call upon researchers that adopt practices from other domains to actively engage with their inspiring domains for both, we ask that they report upon their experience so that we can continue to expand re frontiers [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR195789",
            "template_id": "R186491",
            "paper_id": "R195789",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "improving the identification of hedonic quality in user requirements \u2014 a controlled experiment context and motivation systematically engineering a good user experience (ux) into a computer based system under development demands that the user requirements of the system reflect all needs, including emotional, of all stakeholders user requirements address two different types of qualities: pragmatic qualities (pqs), that address system functionality and usability, and hedonic qualities (hqs) that address the stakeholder\\'s psychological well being studies show that users tend to describe such satisfying uxes mainly with pqs, and that some users seem to believe that they are describing a hq when they are actually describing a pq question/problem the problem is to see if classification of any user requirement as pq related or hq related is difficult, and if so, why principal ideas/results we conducted a controlled experiment in which twelve requirements engineering and ux professionals, hereinafter called \"classifiers\" classified each of 105 user requirements as pq related or hq related the experiment shows that neither (1) a classifier\\'s involvement in the project from which the requirements came nor (2) the classifier\\'s use of a detailed model of the qualities in addition to the standard definitions of \"pq\" and \"hq\" has a positive effect on the consistency of the classifier\\'s classification with that of others contribution the experiment revealed that classification of user requirements is a lot harder than initially assumed",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] improving the identification of hedonic quality in user requirements \u2014 a controlled experiment context and motivation systematically engineering a good user experience (ux) into a computer based system under development demands that the user requirements of the system reflect all needs, including emotional, of all stakeholders user requirements address two different types of qualities: pragmatic qualities (pqs), that address system functionality and usability, and hedonic qualities (hqs) that address the stakeholder\\'s psychological well being studies show that users tend to describe such satisfying uxes mainly with pqs, and that some users seem to believe that they are describing a hq when they are actually describing a pq question/problem the problem is to see if classification of any user requirement as pq related or hq related is difficult, and if so, why principal ideas/results we conducted a controlled experiment in which twelve requirements engineering and ux professionals, hereinafter called \"classifiers\" classified each of 105 user requirements as pq related or hq related the experiment shows that neither (1) a classifier\\'s involvement in the project from which the requirements came nor (2) the classifier\\'s use of a detailed model of the qualities in addition to the standard definitions of \"pq\" and \"hq\" has a positive effect on the consistency of the classifier\\'s classification with that of others contribution the experiment revealed that classification of user requirements is a lot harder than initially assumed [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR195806",
            "template_id": "R186491",
            "paper_id": "R195806",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "usability insights for requirements engineering tools: a user study with practitioners in aeronautics requirements engineering plays a crucial role in coordinating the different stakeholders needed for safe aeronautics systems engineering we conducted a qualitative study, using interviews and mockups, with fifteen industrial practitioners from four aeronautics companies, in order to investigate what tasks are actually performed by requirements engineers and how current tools support these tasks we found that re specific tools constrain engineers to a rigid workflow, which is conflicting with the adaptive exploration of the problem engineers often start by using general purpose tools to foster exploration and collaborative work with suppliers, at the expense of traceability when engineers shift to requirements refinement and verification, they must use re specific tools to grant traceability then, the lack of tool usability yields significant time loss and dissatisfaction based on scenarios of observed re practices and walkthrough, we formulate usability insights for re specific tools in order to conciliate flexibility and traceability throughout the re process",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] usability insights for requirements engineering tools: a user study with practitioners in aeronautics requirements engineering plays a crucial role in coordinating the different stakeholders needed for safe aeronautics systems engineering we conducted a qualitative study, using interviews and mockups, with fifteen industrial practitioners from four aeronautics companies, in order to investigate what tasks are actually performed by requirements engineers and how current tools support these tasks we found that re specific tools constrain engineers to a rigid workflow, which is conflicting with the adaptive exploration of the problem engineers often start by using general purpose tools to foster exploration and collaborative work with suppliers, at the expense of traceability when engineers shift to requirements refinement and verification, they must use re specific tools to grant traceability then, the lack of tool usability yields significant time loss and dissatisfaction based on scenarios of observed re practices and walkthrough, we formulate usability insights for re specific tools in order to conciliate flexibility and traceability throughout the re process [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR195849",
            "template_id": "R186491",
            "paper_id": "R195849",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "detecting vague words &amp; phrases in requirements documents in a multilingual environment vagueness in software requirements documents can lead to several maintenance problems, especially when the customer and development team do not share the same language currently, companies rely on human translators to maintain communication and limit vagueness by translating the requirement documents by hand in this paper, we describe two approaches that automatically identify vagueness in requirements documents in a multilingual environment we perform two studies for calibration purposes under strict industrial limitations, and describe the tool that we ultimately deploy in the first study, six participants, two native portuguese speakers and four native spanish speakers, evaluated both approaches then, we conducted a field study to test the performance of the best approach in real world environments at two companies we describe several lessons learned for research and industrial deployment",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] detecting vague words &amp; phrases in requirements documents in a multilingual environment vagueness in software requirements documents can lead to several maintenance problems, especially when the customer and development team do not share the same language currently, companies rely on human translators to maintain communication and limit vagueness by translating the requirement documents by hand in this paper, we describe two approaches that automatically identify vagueness in requirements documents in a multilingual environment we perform two studies for calibration purposes under strict industrial limitations, and describe the tool that we ultimately deploy in the first study, six participants, two native portuguese speakers and four native spanish speakers, evaluated both approaches then, we conducted a field study to test the performance of the best approach in real world environments at two companies we describe several lessons learned for research and industrial deployment [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR195866",
            "template_id": "R186491",
            "paper_id": "R195866",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "a case study on a specification approach using activity diagrams in requirements documents \"rising complexity of systems has long been a major challenge in requirements engineering this manifests in more extensive and harder to understand requirements documents at the daimler ag, an approach is applied that combines the use of activity diagrams with natural language specifications to specify system functions the approach starts with an activity diagram that is created to get an early overview the contained information is then transferred to a textual requirements document, where details are added and the behavior is refined while the approach aims to reduce efforts needed to understand a system's behavior, the application of the approach itself causes new challenges on its own by examining existing specifications at daimler, we identified nine categories of inconsistencies and deviations between activity diagrams and their textual representations in a case study, we examined one system in detail to assess how often these occur in a follow up survey, we presented instances of the categories to different stakeholders of the system and let them asses the categories regarding their severity our analysis indicates that a coexistence of textual and graphical representations of models without proper tool support results in inconsistencies and deviations that may cause severe maintenance costs or even provoke faults in subsequent development steps \"",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] a case study on a specification approach using activity diagrams in requirements documents \"rising complexity of systems has long been a major challenge in requirements engineering this manifests in more extensive and harder to understand requirements documents at the daimler ag, an approach is applied that combines the use of activity diagrams with natural language specifications to specify system functions the approach starts with an activity diagram that is created to get an early overview the contained information is then transferred to a textual requirements document, where details are added and the behavior is refined while the approach aims to reduce efforts needed to understand a system's behavior, the application of the approach itself causes new challenges on its own by examining existing specifications at daimler, we identified nine categories of inconsistencies and deviations between activity diagrams and their textual representations in a case study, we examined one system in detail to assess how often these occur in a follow up survey, we presented instances of the categories to different stakeholders of the system and let them asses the categories regarding their severity our analysis indicates that a coexistence of textual and graphical representations of models without proper tool support results in inconsistencies and deviations that may cause severe maintenance costs or even provoke faults in subsequent development steps \" [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR195982",
            "template_id": "R186491",
            "paper_id": "R195982",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "a formalization method to process structured natural language to logic expressions to detect redundant specification and test statements automotive systems are constantly increasing in complexity and size beside the increase of requirements specifications and related test specification due to new systems and higher system interaction, we observe an increase of redundant specifications as the predominant specification language (both for requirements and test cases) is still natural text, it is not easy to detect these redundancies in principle, to detect these redundancies, each statement has to be compared to all others this proves to be difficult because of number and informal expression of statements in this paper we propose a solution to the problem of detecting redundant specification and test statements described in structured natural language we propose a formalization process for requirements specification and test statements, allowing us to detect redundant statements and thus reduce the efforts for specification and validation specification pattern systems and linear temporal logic provide the base for our process we did evaluate the method in the context of mercedes benz passenger car development the results show that for the investigated sample set of test statements, we could detect about 30% of test steps as redundant this indicates the savings potential of our approach",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] a formalization method to process structured natural language to logic expressions to detect redundant specification and test statements automotive systems are constantly increasing in complexity and size beside the increase of requirements specifications and related test specification due to new systems and higher system interaction, we observe an increase of redundant specifications as the predominant specification language (both for requirements and test cases) is still natural text, it is not easy to detect these redundancies in principle, to detect these redundancies, each statement has to be compared to all others this proves to be difficult because of number and informal expression of statements in this paper we propose a solution to the problem of detecting redundant specification and test statements described in structured natural language we propose a formalization process for requirements specification and test statements, allowing us to detect redundant statements and thus reduce the efforts for specification and validation specification pattern systems and linear temporal logic provide the base for our process we did evaluate the method in the context of mercedes benz passenger car development the results show that for the investigated sample set of test statements, we could detect about 30% of test steps as redundant this indicates the savings potential of our approach [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR195986",
            "template_id": "R186491",
            "paper_id": "R195986",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "do words make a difference? an empirical study on the impact of taxonomies on the classification of requirements \"requirements taxonomies help to classify and channel the requirements in a project a very simple taxonomy is the distinction between functional and non functional requirements furthermore, a taxonomy helps to decide if a statement is a requirement at all or just something else (e g , 'information') the quality of a taxonomy is important as we do not want to put a statement in the wrong category in this paper, we argue that we need to take cognitive psychology into account in this task of requirements classification cognitive psychology focuses on the abilities and limitations of the human mind we present a controlled experiment and a replication in which we compare three requirements taxonomies the participants had to evaluate a set of requirements based on the given taxonomies the results of these experiments show that there are differences between the taxonomies: interestingly, the question whether a statement is identified as a requirement or not depends on the taxonomy these experiments present initial results, we assume that these results are related to phenomena of cognitive psychology we conclude that the wording should be carefully taken into account in the definition of the categories of a high quality requirements taxonomy \"",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] do words make a difference? an empirical study on the impact of taxonomies on the classification of requirements \"requirements taxonomies help to classify and channel the requirements in a project a very simple taxonomy is the distinction between functional and non functional requirements furthermore, a taxonomy helps to decide if a statement is a requirement at all or just something else (e g , 'information') the quality of a taxonomy is important as we do not want to put a statement in the wrong category in this paper, we argue that we need to take cognitive psychology into account in this task of requirements classification cognitive psychology focuses on the abilities and limitations of the human mind we present a controlled experiment and a replication in which we compare three requirements taxonomies the participants had to evaluate a set of requirements based on the given taxonomies the results of these experiments show that there are differences between the taxonomies: interestingly, the question whether a statement is identified as a requirement or not depends on the taxonomy these experiments present initial results, we assume that these results are related to phenomena of cognitive psychology we conclude that the wording should be carefully taken into account in the definition of the categories of a high quality requirements taxonomy \" [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR195995",
            "template_id": "R186491",
            "paper_id": "R195995",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "requirements capture and analysis in assert(tm) capturing high level requirements in a human readable but formal representation suitable for analysis is an important goal for ge to that end we have augmented an existing controlled english modeling language with a new controlled english requirements capture language to create the requirements capture frontend of the assert(tm) tool suite requirements captured in assert can be analyzed for a number of possible shortcomings, both individually and collectively once a set of requirements has reached a satisfactory level of completeness, consistency, etc , it can then be further used to generate test cases and test procedures this paper will focus on the requirements capture and analysis functions of assert and will illustrate its capabilities with a sample problem previously used as a challenge problem for requirements specification",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] requirements capture and analysis in assert(tm) capturing high level requirements in a human readable but formal representation suitable for analysis is an important goal for ge to that end we have augmented an existing controlled english modeling language with a new controlled english requirements capture language to create the requirements capture frontend of the assert(tm) tool suite requirements captured in assert can be analyzed for a number of possible shortcomings, both individually and collectively once a set of requirements has reached a satisfactory level of completeness, consistency, etc , it can then be further used to generate test cases and test procedures this paper will focus on the requirements capture and analysis functions of assert and will illustrate its capabilities with a sample problem previously used as a challenge problem for requirements specification [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR195998",
            "template_id": "R186491",
            "paper_id": "R195998",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "mining associations between quality concerns and functional requirements the cost and effort of developing software systems in a new technical area can be extensive an organization must perform a domain analysis to discover competing products, analyze their architectures and features, and ultimately discover and specify product requirements however, delivering high quality products, depends not only on gaining an understanding of functional requirements, but also of qualities such as performance, reliability, security, and usability discovering such concerns early in the requirements process drives architectural design decisions this paper extends our prior work on mining functional requirements from large collections of domain documents, by proposing and evaluating a new technique for discovering and specifying quality concerns related to specific functional components we evaluate our approach against three domains of positive train control, electronic health records, and medical infusion pumps, and show that it significantly outperforms a basic information retrieval approach finally we classified the forms of retrieved information, discussed the utility of different types, and conducted a small study with an experienced engineer to investigate the quality of requirements produced using our approach",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] mining associations between quality concerns and functional requirements the cost and effort of developing software systems in a new technical area can be extensive an organization must perform a domain analysis to discover competing products, analyze their architectures and features, and ultimately discover and specify product requirements however, delivering high quality products, depends not only on gaining an understanding of functional requirements, but also of qualities such as performance, reliability, security, and usability discovering such concerns early in the requirements process drives architectural design decisions this paper extends our prior work on mining functional requirements from large collections of domain documents, by proposing and evaluating a new technique for discovering and specifying quality concerns related to specific functional components we evaluate our approach against three domains of positive train control, electronic health records, and medical infusion pumps, and show that it significantly outperforms a basic information retrieval approach finally we classified the forms of retrieved information, discussed the utility of different types, and conducted a small study with an experienced engineer to investigate the quality of requirements produced using our approach [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR196033",
            "template_id": "R186491",
            "paper_id": "R196033",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "behind points and levels \u2014 the influence of gamification algorithms on requirements prioritization \"prioritizing requirements is a crucial ingredient of successful requirements engineering (re) the popular prioritization techniques assume that stakeholders are known and can be mandated to contribute to the prioritization process this prerequisite no longer holds for many of today's systems where significant stakeholders (end users, in particular) are outside organizational reach: they are neither known nor can they be identified among the members of the involved organizations classic techniques for involving these stakeholders such as polls or questionnaires are neither interactive nor collaborative, which is detrimental for prioritization social media enable collaborative prioritization, but fall short in motivating stakeholders outside organizational reach to participate voluntarily in this light, we are developing the garuso platform, which combines social media with gamification for motivating stakeholders while first approaches to employing gamification in re are promising, research is still in its infancy especially, little is known about the influence of the gamification algorithms controlling single game elements on the stakeholders' activities in this paper we report on a field experiment in which we investigated this influence with garuso we found statistically significant differences between different algorithms controlling single game elements on the contributions of stakeholders to the prioritization of requirements \"",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] behind points and levels \u2014 the influence of gamification algorithms on requirements prioritization \"prioritizing requirements is a crucial ingredient of successful requirements engineering (re) the popular prioritization techniques assume that stakeholders are known and can be mandated to contribute to the prioritization process this prerequisite no longer holds for many of today's systems where significant stakeholders (end users, in particular) are outside organizational reach: they are neither known nor can they be identified among the members of the involved organizations classic techniques for involving these stakeholders such as polls or questionnaires are neither interactive nor collaborative, which is detrimental for prioritization social media enable collaborative prioritization, but fall short in motivating stakeholders outside organizational reach to participate voluntarily in this light, we are developing the garuso platform, which combines social media with gamification for motivating stakeholders while first approaches to employing gamification in re are promising, research is still in its infancy especially, little is known about the influence of the gamification algorithms controlling single game elements on the stakeholders' activities in this paper we report on a field experiment in which we investigated this influence with garuso we found statistically significant differences between different algorithms controlling single game elements on the contributions of stakeholders to the prioritization of requirements \" [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR196065",
            "template_id": "R186491",
            "paper_id": "R196065",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "piggybacking on an autonomous hauler: business models enabling a system of systems approach to mapping an underground mine \"with ever increasing productivity targets in mining operations, there is a growing interest in mining automation in future mines, remote controlled and autonomous haulers will operate underground guided by lidar (light detection and ranging) sensors we envision reusing lidar measurements to maintain accurate mine maps that would contribute to both safety and productivity extrapolating from a pilot project on reliable wireless communication in boliden's kankberg mine, we propose establishing a system of systems (sos) with lidar equipped haulers and existing mapping solutions as constituent systems sos requirements engineering inevitably adds a political layer, as independent actors are stakeholders both on the system and sos levels we present four sos scenarios representing different business models, discussing how development and operations could be distributed among boliden and external stakeholders, e g , the vehicle suppliers, the hauling company, and the developers of the mapping software based on eight key variation points, we compare the four scenarios from both technical and business perspectives finally, we validate our findings in a seminar with participants from the relevant stakeholders we conclude that to determine which scenario is the most promising for boliden, trade offs regarding control, costs, risks, and innovation must be carefully evaluated \"",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] piggybacking on an autonomous hauler: business models enabling a system of systems approach to mapping an underground mine \"with ever increasing productivity targets in mining operations, there is a growing interest in mining automation in future mines, remote controlled and autonomous haulers will operate underground guided by lidar (light detection and ranging) sensors we envision reusing lidar measurements to maintain accurate mine maps that would contribute to both safety and productivity extrapolating from a pilot project on reliable wireless communication in boliden's kankberg mine, we propose establishing a system of systems (sos) with lidar equipped haulers and existing mapping solutions as constituent systems sos requirements engineering inevitably adds a political layer, as independent actors are stakeholders both on the system and sos levels we present four sos scenarios representing different business models, discussing how development and operations could be distributed among boliden and external stakeholders, e g , the vehicle suppliers, the hauling company, and the developers of the mapping software based on eight key variation points, we compare the four scenarios from both technical and business perspectives finally, we validate our findings in a seminar with participants from the relevant stakeholders we conclude that to determine which scenario is the most promising for boliden, trade offs regarding control, costs, risks, and innovation must be carefully evaluated \" [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR196070",
            "template_id": "R186491",
            "paper_id": "R196070",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "what do support analysts know about their customers? on the study and prediction of support ticket escalations in large software organizations \"understanding and keeping the customer happy is a central tenet of requirements engineering strategies to gather, analyze, and negotiate requirements are complemented by efforts to manage customer input after products have been deployed for the latter, support tickets are key in allowing customers to submit their issues, bug reports, and feature requests whenever insufficient attention is given to support issues, however, their escalation to management is time consuming and expensive, especially for large organizations managing hundreds of customers and thousands of support tickets our work provides a step towards simplifying the job of support analysts and managers, particularly in predicting the risk of escalating support tickets in a field study at our large industrial partner, ibm, we used a design science methodology to characterize the support process and data available to ibm analysts in managing escalations through iterative cycles of design and evaluation, we translated our understanding of support analysts' expert knowledge of their customers into features of a support ticket model to be implemented into a machine learning model to predict support ticket escalations we trained and evaluated our machine learning model on over 2 5 million support tickets and 10,000 escalations, obtaining a recall of 79 9% and an 80 8% reduction in the workload for support analysts looking to identify support tickets at risk of escalation further on site evaluations, through a prototype tool we developed to implement our machine learning techniques in practice, showed more efficient weekly support ticket management meetings the features we developed in the support ticket model are designed to serve as a starting place for organizations interested in implementing our model to predict support ticket escalations, and for future researchers to build on to advance research in escalation prediction \"",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] what do support analysts know about their customers? on the study and prediction of support ticket escalations in large software organizations \"understanding and keeping the customer happy is a central tenet of requirements engineering strategies to gather, analyze, and negotiate requirements are complemented by efforts to manage customer input after products have been deployed for the latter, support tickets are key in allowing customers to submit their issues, bug reports, and feature requests whenever insufficient attention is given to support issues, however, their escalation to management is time consuming and expensive, especially for large organizations managing hundreds of customers and thousands of support tickets our work provides a step towards simplifying the job of support analysts and managers, particularly in predicting the risk of escalating support tickets in a field study at our large industrial partner, ibm, we used a design science methodology to characterize the support process and data available to ibm analysts in managing escalations through iterative cycles of design and evaluation, we translated our understanding of support analysts' expert knowledge of their customers into features of a support ticket model to be implemented into a machine learning model to predict support ticket escalations we trained and evaluated our machine learning model on over 2 5 million support tickets and 10,000 escalations, obtaining a recall of 79 9% and an 80 8% reduction in the workload for support analysts looking to identify support tickets at risk of escalation further on site evaluations, through a prototype tool we developed to implement our machine learning techniques in practice, showed more efficient weekly support ticket management meetings the features we developed in the support ticket model are designed to serve as a starting place for organizations interested in implementing our model to predict support ticket escalations, and for future researchers to build on to advance research in escalation prediction \" [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR197609",
            "template_id": "R186491",
            "paper_id": "R197609",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "requirements engineering challenges in large scale agile system development motivated by their success in software development, companies implement agile methods and their practices increasingly for software intense, large products, such as cars, telecommunication infrastructure, and embedded systems such systems are usually subject to safety and regulative concerns as well as different development cycles of hardware and software consequently, requirements engineering involves upfront and detailed analysis, which can be at odds with agile (software) development in this paper, we present results from a multiple case study with two car manufacturers, a telecommunications company, and a technology company that are on the journey to introduce organization wide continuous integration and continuous delivery to customers based on 20 qualitative interviews, 5 focus groups, and 2 cross company workshops, we discuss possible scopes of agile methods within system development, the consequences this has on the role of requirements, and the challenges that arise from the interplay of requirements engineering and agile methods in large scale system development these relate in particular to communicating and managing knowledge about a) customer value and b) the system under development we conclude that better alignment of a holistic requirements model with agile development practices promises rich gains in development speed, flexibility, and overall quality of software and systems",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] requirements engineering challenges in large scale agile system development motivated by their success in software development, companies implement agile methods and their practices increasingly for software intense, large products, such as cars, telecommunication infrastructure, and embedded systems such systems are usually subject to safety and regulative concerns as well as different development cycles of hardware and software consequently, requirements engineering involves upfront and detailed analysis, which can be at odds with agile (software) development in this paper, we present results from a multiple case study with two car manufacturers, a telecommunications company, and a technology company that are on the journey to introduce organization wide continuous integration and continuous delivery to customers based on 20 qualitative interviews, 5 focus groups, and 2 cross company workshops, we discuss possible scopes of agile methods within system development, the consequences this has on the role of requirements, and the challenges that arise from the interplay of requirements engineering and agile methods in large scale system development these relate in particular to communicating and managing knowledge about a) customer value and b) the system under development we conclude that better alignment of a holistic requirements model with agile development practices promises rich gains in development speed, flexibility, and overall quality of software and systems [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR198663",
            "template_id": "R186491",
            "paper_id": "R198663",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "requirements engineering visualization: a systematic literature review requirements engineering (re) is a decision centric activity which is highly data intensive the results of this process are known to have key impact on the results of the project as known from the experience in other fields and disciplines, visualization can potentially provide more insights into data, information and knowledge studied while research in the area of information visualization and its application to software engineering has rapidly increased over the last decade, there is only a limited amount of studies addressing the usage and impact of visualization techniques for re activities in this paper, we report on the results of a systematic literature review (slr) related to re visualization extending the established slr process by the usage of grounded theory for the encoding of papers, we synthesize 18 usage patterns even though there are punctual applications, there is a clear deficit on a holistic perspective across the different re activities as another conclusion, we derive the clear need for more research on visualization support in particular for tackling requirements uncertainty, requirements verification, and modeling, as well as non functional requirements (nfrs)",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] requirements engineering visualization: a systematic literature review requirements engineering (re) is a decision centric activity which is highly data intensive the results of this process are known to have key impact on the results of the project as known from the experience in other fields and disciplines, visualization can potentially provide more insights into data, information and knowledge studied while research in the area of information visualization and its application to software engineering has rapidly increased over the last decade, there is only a limited amount of studies addressing the usage and impact of visualization techniques for re activities in this paper, we report on the results of a systematic literature review (slr) related to re visualization extending the established slr process by the usage of grounded theory for the encoding of papers, we synthesize 18 usage patterns even though there are punctual applications, there is a clear deficit on a holistic perspective across the different re activities as another conclusion, we derive the clear need for more research on visualization support in particular for tackling requirements uncertainty, requirements verification, and modeling, as well as non functional requirements (nfrs) [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR198673",
            "template_id": "R186491",
            "paper_id": "R198673",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "a serious game for eliciting social engineering security requirements social engineering is the acquisition of information about computer systems by methods that deeply include nontechnical means while technical security of most critical systems is high, the systems remain vulnerable to attacks from social engineers social engineering is a technique that: (i) does not require any (advanced) technical tools, (ii) can be used by anyone, (iii) is cheap traditional security requirements elicitation approaches often focus on vulnerabilities in network or software systems few approaches even consider the exploitation of humans via social engineering and none of them elicits personal behaviours of individual employees while the amount of social engineering attacks and the damage they cause rise every year, the security awareness of these attacks and their consideration during requirements elicitation remains negligible we propose to use a card game to elicit these requirements, which all employees of a company can play to understand the threat and document security requirements the game considers the individual context of a company and presents underlying principles of human behaviour that social engineers exploit, as well as concrete attack patterns we evaluated our approach with several groups of researchers, it administrators, and professionals from industry",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] a serious game for eliciting social engineering security requirements social engineering is the acquisition of information about computer systems by methods that deeply include nontechnical means while technical security of most critical systems is high, the systems remain vulnerable to attacks from social engineers social engineering is a technique that: (i) does not require any (advanced) technical tools, (ii) can be used by anyone, (iii) is cheap traditional security requirements elicitation approaches often focus on vulnerabilities in network or software systems few approaches even consider the exploitation of humans via social engineering and none of them elicits personal behaviours of individual employees while the amount of social engineering attacks and the damage they cause rise every year, the security awareness of these attacks and their consideration during requirements elicitation remains negligible we propose to use a card game to elicit these requirements, which all employees of a company can play to understand the threat and document security requirements the game considers the individual context of a company and presents underlying principles of human behaviour that social engineers exploit, as well as concrete attack patterns we evaluated our approach with several groups of researchers, it administrators, and professionals from industry [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR198724",
            "template_id": "R186491",
            "paper_id": "R198724",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "ambiguity cues in requirements elicitation interviews customer analyst interviews are considered among the most effective means to perform requirements elicitation however, during these interviews, ambiguity can hamper communication between customer and requirements analyst ambiguity is particularly dangerous in those cases in which the analyst misunderstands some linguistic expression of the customer, with out being aware of the misunderstanding on the other hand, if the analyst is able to detect ambiguous situations, this has been shown to help him/her in disclosing tacit knowledge indeed, the occurrence of an ambiguity might reveal the presence of unexpressed, system relevant knowledge that needs to be elicited therefore, for the requirements elicitation interview to succeed, it is important for the analyst not to overlook ambiguities to support the ambiguity awareness of the requirements analyst, this paper aims to provide a set of cues that can be identified in the linguistic expressions of the customer, and that typically lead to ambiguity to this end, we performed 34 customer analyst interviews, and we isolated the speech fragments that caused the ambiguity based on the analysis of these fragments, and leveraging the previous literature on ambiguity in written requirements, we identified a set of cues that can be used by requirements analysts as a reference handbook to detect ambiguities",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] ambiguity cues in requirements elicitation interviews customer analyst interviews are considered among the most effective means to perform requirements elicitation however, during these interviews, ambiguity can hamper communication between customer and requirements analyst ambiguity is particularly dangerous in those cases in which the analyst misunderstands some linguistic expression of the customer, with out being aware of the misunderstanding on the other hand, if the analyst is able to detect ambiguous situations, this has been shown to help him/her in disclosing tacit knowledge indeed, the occurrence of an ambiguity might reveal the presence of unexpressed, system relevant knowledge that needs to be elicited therefore, for the requirements elicitation interview to succeed, it is important for the analyst not to overlook ambiguities to support the ambiguity awareness of the requirements analyst, this paper aims to provide a set of cues that can be identified in the linguistic expressions of the customer, and that typically lead to ambiguity to this end, we performed 34 customer analyst interviews, and we isolated the speech fragments that caused the ambiguity based on the analysis of these fragments, and leveraging the previous literature on ambiguity in written requirements, we identified a set of cues that can be used by requirements analysts as a reference handbook to detect ambiguities [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR198917",
            "template_id": "R186491",
            "paper_id": "R198917",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "mining requirements knowledge from collections of domain documents \"when organizations enter domains that are entirely new to them, they need to invest significant time and effort to acquire domain knowledge this typically involves searching through a broad set of domain documents, retrieving relevant ones, and analyzing the textual content in order to discover and specify pertinent requirements depending on the nature of the domain and the availability of documentation, this task can be extremely time consuming and may require non trivial human effort furthermore, the task must often be performed repeatedly throughout early phases of the project in this paper we first explore the effort needed to manually build a high level domain model capturing the functional components we then present mark (mining requirements knowledge), which identifies and retrieves the documents containing descriptions of functional components in the domain model domain analysts can use this information to to specify requirements we introduce and evaluate an algorithm which ranks domain documents according to their relevance to a component and then highlights sections of text which are likely to contain requirements related information we describe our process within the context of the positive train control (ptc) domain with a repository of of 523 documents, representing 852mb of data we empirically evaluate the mark relevance algorithm and its ability to retrieve relevant requirements knowledge for requirements related to ptc's on board unit \"",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] mining requirements knowledge from collections of domain documents \"when organizations enter domains that are entirely new to them, they need to invest significant time and effort to acquire domain knowledge this typically involves searching through a broad set of domain documents, retrieving relevant ones, and analyzing the textual content in order to discover and specify pertinent requirements depending on the nature of the domain and the availability of documentation, this task can be extremely time consuming and may require non trivial human effort furthermore, the task must often be performed repeatedly throughout early phases of the project in this paper we first explore the effort needed to manually build a high level domain model capturing the functional components we then present mark (mining requirements knowledge), which identifies and retrieves the documents containing descriptions of functional components in the domain model domain analysts can use this information to to specify requirements we introduce and evaluate an algorithm which ranks domain documents according to their relevance to a component and then highlights sections of text which are likely to contain requirements related information we describe our process within the context of the positive train control (ptc) domain with a repository of of 523 documents, representing 852mb of data we empirically evaluate the mark relevance algorithm and its ability to retrieve relevant requirements knowledge for requirements related to ptc's on board unit \" [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR198944",
            "template_id": "R186491",
            "paper_id": "R198944",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "acquiring creative requirements from the crowd: understanding the influences of personality and creative potential in crowd re \"as a creative discipline, requirements engineering (re), lends importance to understanding the associated human factors crowd re, the approach of acquiring requirements from members of the public the so called crowd emphasizes human factors further we investigate how human personality and creative potential influence a requirement acquisition task these factors are of specific importance to crowd re because (1) crowd workers are generally not trained in re, and (2) a key motivation in engaging them is to benefit from their creativity we propose a sequential crowd re process, where workers in one stage review requirements from the previous stage and produce additional requirements to reduce potential information overload in this process, we propose strategies for selecting requirements from one stage to expose to workers in later stages we conducted a study on amazon mechanical turk tasking 300 workers with creating requirements via the above sequential process (in the domain of smart home applications for concreteness) and tasking an additional 300 workers to rate the creativity (novelty and usefulness) of those requirements our findings offer insights on how to carry out crowd re effectively first, we find that a crowd worker's (1) creative potential, and personality traits of openness and conscientiousness have significant positive influence on the novelty of the worker's ideas, and (2) personality traits of agreeableness and conscientiousness have significant positive influence, but extraversion has significant negative influence on the usefulness of the worker's ideas second, we find that exposing a worker to ideas from previous workers cognitively stimulates the worker to produce creative ideas third, we identify effective strategies based on personality traits and creative potential for selecting a few requirements from a pool of previous requirements to stimulate a worker \"",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] acquiring creative requirements from the crowd: understanding the influences of personality and creative potential in crowd re \"as a creative discipline, requirements engineering (re), lends importance to understanding the associated human factors crowd re, the approach of acquiring requirements from members of the public the so called crowd emphasizes human factors further we investigate how human personality and creative potential influence a requirement acquisition task these factors are of specific importance to crowd re because (1) crowd workers are generally not trained in re, and (2) a key motivation in engaging them is to benefit from their creativity we propose a sequential crowd re process, where workers in one stage review requirements from the previous stage and produce additional requirements to reduce potential information overload in this process, we propose strategies for selecting requirements from one stage to expose to workers in later stages we conducted a study on amazon mechanical turk tasking 300 workers with creating requirements via the above sequential process (in the domain of smart home applications for concreteness) and tasking an additional 300 workers to rate the creativity (novelty and usefulness) of those requirements our findings offer insights on how to carry out crowd re effectively first, we find that a crowd worker's (1) creative potential, and personality traits of openness and conscientiousness have significant positive influence on the novelty of the worker's ideas, and (2) personality traits of agreeableness and conscientiousness have significant positive influence, but extraversion has significant negative influence on the usefulness of the worker's ideas second, we find that exposing a worker to ideas from previous workers cognitively stimulates the worker to produce creative ideas third, we identify effective strategies based on personality traits and creative potential for selecting a few requirements from a pool of previous requirements to stimulate a worker \" [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR198994",
            "template_id": "R186491",
            "paper_id": "R198994",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "collaborative traceability management: challenges and opportunities traceability and trace link management are important for various reasons, including managing knowledge about a complex software system, monitoring the progress of its development, and proving that it is developed in accordance to regulations however, it is difficult to maintain and use trace links in real world projects where artifacts undergo constant change and multiple stakeholders are involved in this paper, we extend the current body of knowledge on traceability management by regarding its collaborative aspects in an industrial setting based on 15 industrial cases and semi structured interviews with 24 practitioners, we identify challenges involved in collaborative traceability management, and how traceability management can be used to enable collaboration our findings show that main challenges are boundaries between organizations and tools, a lack of common goals and responsibilities, and the difficulty of collaboratively maintaining trace links we also identify traceability as an important facilitator for communication and knowledge management across these boundaries",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] collaborative traceability management: challenges and opportunities traceability and trace link management are important for various reasons, including managing knowledge about a complex software system, monitoring the progress of its development, and proving that it is developed in accordance to regulations however, it is difficult to maintain and use trace links in real world projects where artifacts undergo constant change and multiple stakeholders are involved in this paper, we extend the current body of knowledge on traceability management by regarding its collaborative aspects in an industrial setting based on 15 industrial cases and semi structured interviews with 24 practitioners, we identify challenges involved in collaborative traceability management, and how traceability management can be used to enable collaboration our findings show that main challenges are boundaries between organizations and tools, a lack of common goals and responsibilities, and the difficulty of collaboratively maintaining trace links we also identify traceability as an important facilitator for communication and knowledge management across these boundaries [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR199028",
            "template_id": "R186491",
            "paper_id": "R199028",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "an information theoretic approach for extracting and tracing non functional requirements \"non functional requirements (nfrs) are high level quality constraints that a software system should exhibit detecting such constraints early in the process is critical for the stability of software architectural design however, due to their pervasive nature, and the lack of robust modeling and documentation techniques, nfrs are often overlooked during the requirements elicitation phase realizing such constraints at later stages of the development process often leads to architecture erosion and poor traceability motivated by these observations, we propose an unsupervised, computationally efficient, and scalable approach for extracting and tracing nfrs in software systems based on main assumptions of the cluster hypothesis and information theory, the proposed approach exploits the semantic knowledge embedded in the textual content of requirements specifications to discover, classify, and trace high level software quality constraints imposed by the system's functional features three experimental systems are used to conduct the experimental analysis in this paper results show that the proposed approach can discover software nfrs with an average accuracy of 73%, enabling these nfrs to be traced to their implementations with accuracy levels adequate for practical applications \"",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] an information theoretic approach for extracting and tracing non functional requirements \"non functional requirements (nfrs) are high level quality constraints that a software system should exhibit detecting such constraints early in the process is critical for the stability of software architectural design however, due to their pervasive nature, and the lack of robust modeling and documentation techniques, nfrs are often overlooked during the requirements elicitation phase realizing such constraints at later stages of the development process often leads to architecture erosion and poor traceability motivated by these observations, we propose an unsupervised, computationally efficient, and scalable approach for extracting and tracing nfrs in software systems based on main assumptions of the cluster hypothesis and information theory, the proposed approach exploits the semantic knowledge embedded in the textual content of requirements specifications to discover, classify, and trace high level software quality constraints imposed by the system's functional features three experimental systems are used to conduct the experimental analysis in this paper results show that the proposed approach can discover software nfrs with an average accuracy of 73%, enabling these nfrs to be traced to their implementations with accuracy levels adequate for practical applications \" [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR199039",
            "template_id": "R186491",
            "paper_id": "R199039",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "from requirements elicitation to variability analysis using repertory grid: a cognitive approach \"the growing complexity and dynamics of the execution environment have been major motivation for designing self adaptive systems although significant work can be found in the field of formalizing or modeling the requirements of adaptive system, not enough attention has been paid towards the requirements elicitation techniques for the same it is still an open challenge to elicit the users' requirements in the light of various contexts and introduce the required flexibility in the system's behavior at an early phase of requirements engineering we explore the idea of using a cognitive technique, repertory grid, to acquire the knowledge of various stakeholders along multiple dimensions of problem space and design space we aim at discovering the scope of variations in the features of the system by capturing the intentional and technical variability in the problem space and design space respectively a stepwise methodology for finding the right set of features in the changing context has also been provided in this work we evaluate the proposed idea by a preliminary case study using smart home system domain \"",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] from requirements elicitation to variability analysis using repertory grid: a cognitive approach \"the growing complexity and dynamics of the execution environment have been major motivation for designing self adaptive systems although significant work can be found in the field of formalizing or modeling the requirements of adaptive system, not enough attention has been paid towards the requirements elicitation techniques for the same it is still an open challenge to elicit the users' requirements in the light of various contexts and introduce the required flexibility in the system's behavior at an early phase of requirements engineering we explore the idea of using a cognitive technique, repertory grid, to acquire the knowledge of various stakeholders along multiple dimensions of problem space and design space we aim at discovering the scope of variations in the features of the system by capturing the intentional and technical variability in the problem space and design space respectively a stepwise methodology for finding the right set of features in the changing context has also been provided in this work we evaluate the proposed idea by a preliminary case study using smart home system domain \" [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR199149",
            "template_id": "R186491",
            "paper_id": "R199149",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "what you ask is what you get: understanding architecturally significant functional requirements \"software architects are responsible for designing an architectural solution that satisfies the functional and non functional requirements of the system to the fullest extent possible however, the details they need to make informed architectural decisions are often missing from the requirements specification an earlier study we conducted indicated that architects intuitively recognize architecturally significant requirements in a project, and often seek out relevant stakeholders in order to ask probing questions (pqs) that help them acquire the information they need this paper presents results from a qualitative interview study aimed at identifying architecturally significant functional requirements' categories from various business domains, exploring relevant pqs for each category, and then grouping pqs by type using interview data from 14 software architects in three countries, we identified 15 categories of architecturally significant functional requirements and 6 types of pqs we found that the domain knowledge of the architect and her experience influence the choice of pqs significantly a preliminary quantitative evaluation of the results against real life software requirements specification documents indicated that software specifications in our sample largely do not contain the crucial architectural differentiators that may impact architectural choices and that pqs are a necessary mechanism to unearth them further, our findings provide the initial list of pqs which could be used to prompt business analysts to elicit architecturally significant functional requirements that the architects need \"",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] what you ask is what you get: understanding architecturally significant functional requirements \"software architects are responsible for designing an architectural solution that satisfies the functional and non functional requirements of the system to the fullest extent possible however, the details they need to make informed architectural decisions are often missing from the requirements specification an earlier study we conducted indicated that architects intuitively recognize architecturally significant requirements in a project, and often seek out relevant stakeholders in order to ask probing questions (pqs) that help them acquire the information they need this paper presents results from a qualitative interview study aimed at identifying architecturally significant functional requirements' categories from various business domains, exploring relevant pqs for each category, and then grouping pqs by type using interview data from 14 software architects in three countries, we identified 15 categories of architecturally significant functional requirements and 6 types of pqs we found that the domain knowledge of the architect and her experience influence the choice of pqs significantly a preliminary quantitative evaluation of the results against real life software requirements specification documents indicated that software specifications in our sample largely do not contain the crucial architectural differentiators that may impact architectural choices and that pqs are a necessary mechanism to unearth them further, our findings provide the initial list of pqs which could be used to prompt business analysts to elicit architecturally significant functional requirements that the architects need \" [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR199158",
            "template_id": "R186491",
            "paper_id": "R199158",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "a requirements monitoring model for systems of systems many software systems today can be characterized as systems of systems (sos) comprising interrelated and heterogeneous systems developed by diverse teams over many years due to their scale, complexity, and heterogeneity engineers face significant challenges when determining the compliance of sos with their requirements requirements monitoring approaches are a viable solution for checking system properties at runtime however, existing approaches do not adequately consider the characteristics of sos: different types of requirements exist at different levels and across different systems; requirements are maintained by different stakeholders; and systems are implemented using diverse technologies this paper describes a three dimensional requirements monitoring model (rmm) for sos providing the following contributions: (i) our approach allows modeling the monitoring scopes of requirements with respect to the sos architecture; (ii) it employs event models to abstract from different technologies and systems to be monitored; and (iii) it supports instantiating the rmm at runtime depending on the actual sos configuration to evaluate the feasibility of our approach we created a rmm for a real world sos from the automation software domain we evaluated the model by instantiating it using an existing monitoring framework and a simulator running parts of this sos the results indicate that the model is sufficiently expressive to support monitoring sos requirements of a directed sos it further facilitates diagnosis by discovering violations of requirements across different levels and systems in realistic monitoring scenarios",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] a requirements monitoring model for systems of systems many software systems today can be characterized as systems of systems (sos) comprising interrelated and heterogeneous systems developed by diverse teams over many years due to their scale, complexity, and heterogeneity engineers face significant challenges when determining the compliance of sos with their requirements requirements monitoring approaches are a viable solution for checking system properties at runtime however, existing approaches do not adequately consider the characteristics of sos: different types of requirements exist at different levels and across different systems; requirements are maintained by different stakeholders; and systems are implemented using diverse technologies this paper describes a three dimensional requirements monitoring model (rmm) for sos providing the following contributions: (i) our approach allows modeling the monitoring scopes of requirements with respect to the sos architecture; (ii) it employs event models to abstract from different technologies and systems to be monitored; and (iii) it supports instantiating the rmm at runtime depending on the actual sos configuration to evaluate the feasibility of our approach we created a rmm for a real world sos from the automation software domain we evaluated the model by instantiating it using an existing monitoring framework and a simulator running parts of this sos the results indicate that the model is sufficiently expressive to support monitoring sos requirements of a directed sos it further facilitates diagnosis by discovering violations of requirements across different levels and systems in realistic monitoring scenarios [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR199162",
            "template_id": "R186491",
            "paper_id": "R199162",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "handling knowledge uncertainty in risk based requirements engineering \"requirements engineers are faced with multiple sources of uncertainty in particular, the extent to which the identified software requirements and environment assumptions are adequate and sufficiently complete is uncertain; the extent to which they will be satisfied in the system to be is uncertain; and the extent to which obstacles to their satisfaction will occur is uncertain the resolution of such domain level uncertainty requires estimations of the likelihood that those different types of situations may or may not occur however, the extent to which the resulting estimates are accurate is uncertain as well this meta level uncertainty limits current risk based methods for requirements engineering the paper introduces a quantitative approach for managing it an earlier formal framework for probabilistic goals and obstacles is extended to explicitly cope with uncertainties about estimates of likelihoods of fine grained obstacles to goal satisfaction such estimates are elicited from multiple sources and combined in order to reduce their uncertainty margins the combined estimates and their uncertainties are up propagated through obstacle refinement trees and then through the system's goal model two metrics are introduced for measuring problematic uncertainties when applied to the probability distributions obtained by up propagation to the top level goals, the metrics allow critical leaf obstacles with most problematic uncertainty margins to be highlighted the proposed approach is evaluated on excerpts from a real ambulance dispatching system \"",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] handling knowledge uncertainty in risk based requirements engineering \"requirements engineers are faced with multiple sources of uncertainty in particular, the extent to which the identified software requirements and environment assumptions are adequate and sufficiently complete is uncertain; the extent to which they will be satisfied in the system to be is uncertain; and the extent to which obstacles to their satisfaction will occur is uncertain the resolution of such domain level uncertainty requires estimations of the likelihood that those different types of situations may or may not occur however, the extent to which the resulting estimates are accurate is uncertain as well this meta level uncertainty limits current risk based methods for requirements engineering the paper introduces a quantitative approach for managing it an earlier formal framework for probabilistic goals and obstacles is extended to explicitly cope with uncertainties about estimates of likelihoods of fine grained obstacles to goal satisfaction such estimates are elicited from multiple sources and combined in order to reduce their uncertainty margins the combined estimates and their uncertainties are up propagated through obstacle refinement trees and then through the system's goal model two metrics are introduced for measuring problematic uncertainties when applied to the probability distributions obtained by up propagation to the top level goals, the metrics allow critical leaf obstacles with most problematic uncertainty margins to be highlighted the proposed approach is evaluated on excerpts from a real ambulance dispatching system \" [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R186491xR200098",
            "template_id": "R186491",
            "paper_id": "R200098",
            "premise": "research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer",
            "hypothesis": "detecting repurposing and over collection in multi party privacy requirements specifications mobile and web applications increasingly leverage service oriented architectures in which developers integrate third party services into end user applications this includes identity management, mapping and navigation, cloud storage, and advertising services, among others while service reuse reduces development time, it introduces new privacy and security risks due to data repurposing and over collection as data is shared among multiple parties who lack transparency into third party data practices to address this challenge, we propose new techniques based on description logic (dl) for modeling multiparty data flow requirements and verifying the purpose specification and collection and use limitation principles, which are prominent privacy properties found in international standards and guidelines we evaluate our techniques in an empirical case study that examines the data practices of the waze mobile application and three of their service providers: facebook login, amazon web services (a cloud storage provider), and flurry com (a popular mobile analytics and advertising platform) the study results include detected conflicts and violations of the principles as well as two patterns for balancing privacy and data use flexibility in requirements specifications analysis of automation reasoning over the dl models show that reasoning over complex compositions of multi party systems is feasible within exponential asymptotic timeframes proportional to the policy size, the number of expressed data, and orthogonal to the number of conflicts found",
            "sequence": "[CLS] research practices in re contribution data analysis research question threat to validity data collection method research paradigm research question answer [SEP] detecting repurposing and over collection in multi party privacy requirements specifications mobile and web applications increasingly leverage service oriented architectures in which developers integrate third party services into end user applications this includes identity management, mapping and navigation, cloud storage, and advertising services, among others while service reuse reduces development time, it introduces new privacy and security risks due to data repurposing and over collection as data is shared among multiple parties who lack transparency into third party data practices to address this challenge, we propose new techniques based on description logic (dl) for modeling multiparty data flow requirements and verifying the purpose specification and collection and use limitation principles, which are prominent privacy properties found in international standards and guidelines we evaluate our techniques in an empirical case study that examines the data practices of the waze mobile application and three of their service providers: facebook login, amazon web services (a cloud storage provider), and flurry com (a popular mobile analytics and advertising platform) the study results include detected conflicts and violations of the principles as well as two patterns for balancing privacy and data use flexibility in requirements specifications analysis of automation reasoning over the dl models show that reasoning over complex compositions of multi party systems is feasible within exponential asymptotic timeframes proportional to the policy size, the number of expressed data, and orthogonal to the number of conflicts found [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R187648xR195925",
            "template_id": "R187648",
            "paper_id": "R195925",
            "premise": "oco for control research problem theoretical guarantees has application in data driven approach has constraints type of considered system measurement noise process noise",
            "hypothesis": "model free nonlinear feedback optimization feedback optimization is a control paradigm that enables physical systems to autonomously reach efficient operating points its central idea is to interconnect optimization iterations in closed loop with the physical plant since iterative gradient based methods are extensively used to achieve optimality, feedback optimization controllers typically require the knowledge of the steady state sensitivity of the plant, which may not be easily accessible in some applications in contrast, in this paper we develop a model free feedback controller for efficient steady state operation of general dynamical systems the proposed design consists in updating control inputs via gradient estimates constructed from evaluations of the nonconvex objective at the current input and at the measured output we study the dynamic interconnection of the proposed iterative controller with a stable nonlinear discrete time plant for this setup, we characterize the optimality and the stability of the closed loop behavior as functions of the problem dimension, the number of iterations, and the rate of convergence of the physical plant to handle general constraints that affect multiple inputs, we enhance the controller with frank wolfe type updates",
            "sequence": "[CLS] oco for control research problem theoretical guarantees has application in data driven approach has constraints type of considered system measurement noise process noise [SEP] model free nonlinear feedback optimization feedback optimization is a control paradigm that enables physical systems to autonomously reach efficient operating points its central idea is to interconnect optimization iterations in closed loop with the physical plant since iterative gradient based methods are extensively used to achieve optimality, feedback optimization controllers typically require the knowledge of the steady state sensitivity of the plant, which may not be easily accessible in some applications in contrast, in this paper we develop a model free feedback controller for efficient steady state operation of general dynamical systems the proposed design consists in updating control inputs via gradient estimates constructed from evaluations of the nonconvex objective at the current input and at the measured output we study the dynamic interconnection of the proposed iterative controller with a stable nonlinear discrete time plant for this setup, we characterize the optimality and the stability of the closed loop behavior as functions of the problem dimension, the number of iterations, and the rate of convergence of the physical plant to handle general constraints that affect multiple inputs, we enhance the controller with frank wolfe type updates [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R109",
                "label": "Control Theory"
            },
            "results": [
                {
                    "template_id": "R187648"
                }
            ]
        },
        {
            "instance_id": "R194212xR194747",
            "template_id": "R194212",
            "paper_id": "R194747",
            "premise": "ontology construction method methodology tool has uri data source research problem evaluation process name people involved serialization language",
            "hypothesis": "a health consumer ontology of fast food information a variety of severe health issues can be attributed to poor nutrition and poor eating behaviors research has explored the impact of nutritional knowledge on an individual\u2019s inclination to purchase and consume certain foods this paper introduces the ontology of fast food facts, a knowledge base that models consumer nutritional data from major fast food establishments this artifact serves as an aggregate knowledge base to centralize nutritional information for consumers as a semantically linked data source, the ontology of fast food facts could engender methods and tools to further the research and impact the health consumers\u2019 diet and behavior, which is a factor in many severe health outcomes we describe the initial development of this ontology and future directions we plan with this knowledge base",
            "sequence": "[CLS] ontology construction method methodology tool has uri data source research problem evaluation process name people involved serialization language [SEP] a health consumer ontology of fast food information a variety of severe health issues can be attributed to poor nutrition and poor eating behaviors research has explored the impact of nutritional knowledge on an individual\u2019s inclination to purchase and consume certain foods this paper introduces the ontology of fast food facts, a knowledge base that models consumer nutritional data from major fast food establishments this artifact serves as an aggregate knowledge base to centralize nutritional information for consumers as a semantically linked data source, the ontology of fast food facts could engender methods and tools to further the research and impact the health consumers\u2019 diet and behavior, which is a factor in many severe health outcomes we describe the initial development of this ontology and future directions we plan with this knowledge base [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R141823",
                "label": "Semantic Web"
            },
            "results": [
                {
                    "template_id": "R138077"
                },
                {
                    "template_id": "R194212"
                }
            ]
        },
        {
            "instance_id": "R198658xR189677",
            "template_id": "R198658",
            "paper_id": "R189677",
            "premise": "nacre mechanics template has method has result research problem has material",
            "hypothesis": "multifunctional nanoclay hybrids of high toughness, thermal, and barrier performances to address brittleness of nanoclay hybrids of high inorganic content, ductile polymers (polyethylene oxide and hydroxyethyl cellulose) and montmorillonite (mtm) have been assembled into hybrid films using a water based filtration process nacre mimetic layered films resulted and were characterized by fe sem and xrd mechanical properties at ambient condition were studied by tensile test, while performance at elevated temperature and moisture conditions were evaluated by tga, dynamic vapor sorption, and dynamic thermomechanical and hygromechanical analyses antiflammability and barrier properties against oxygen and water vapor were also investigated despite their high mtm content in the 60 85 wt % range, the hybrids exhibit remarkable ductility and a storage modulus above 2 gpa even in severe conditions (300\u00b0c or 94% rh) moreover, they present fire shielding property and are amongst the best oxygen and water vapor barrier hybrids reported in the literature this study thus demonstrates nanostructure property advantages for synergistic effects in hybrids combining inexpensive, available, and environmentally benign constituents",
            "sequence": "[CLS] nacre mechanics template has method has result research problem has material [SEP] multifunctional nanoclay hybrids of high toughness, thermal, and barrier performances to address brittleness of nanoclay hybrids of high inorganic content, ductile polymers (polyethylene oxide and hydroxyethyl cellulose) and montmorillonite (mtm) have been assembled into hybrid films using a water based filtration process nacre mimetic layered films resulted and were characterized by fe sem and xrd mechanical properties at ambient condition were studied by tensile test, while performance at elevated temperature and moisture conditions were evaluated by tga, dynamic vapor sorption, and dynamic thermomechanical and hygromechanical analyses antiflammability and barrier properties against oxygen and water vapor were also investigated despite their high mtm content in the 60 85 wt % range, the hybrids exhibit remarkable ductility and a storage modulus above 2 gpa even in severe conditions (300\u00b0c or 94% rh) moreover, they present fire shielding property and are amongst the best oxygen and water vapor barrier hybrids reported in the literature this study thus demonstrates nanostructure property advantages for synergistic effects in hybrids combining inexpensive, available, and environmentally benign constituents [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R254",
                "label": "Materials Science and Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R40006xR44731",
            "template_id": "R40006",
            "paper_id": "R44731",
            "premise": "basic reproduction number estimate time period basic reproduction number location",
            "hypothesis": "transmission interval estimates suggest pre symptomatic spread of covid 19 abstract background as the covid 19 epidemic is spreading, incoming data allows us to quantify values of key variables that determine the transmission and the effort required to control the epidemic we determine the incubation period and serial interval distribution for transmission clusters in singapore and in tianjin we infer the basic reproduction number and identify the extent of pre symptomatic transmission methods we collected outbreak information from singapore and tianjin, china, reported from jan 19 feb 26 and jan 21 feb 27, respectively we estimated incubation periods and serial intervals in both populations results the mean incubation period was 7 1 (6 13, 8 25) days for singapore and 9 (7 92, 10 2) days for tianjin both datasets had shorter incubation periods for earlier occurring cases the mean serial interval was 4 56 (2 69, 6 42) days for singapore and 4 22 (3 43, 5 01) for tianjin we inferred that early in the outbreaks, infection was transmitted on average 2 55 and 2 89 days before symptom onset (singapore, tianjin) the estimated basic reproduction number for singapore was 1 97 (1 45, 2 48) secondary cases per infective; for tianjin it was 1 87 (1 65, 2 09) secondary cases per infective conclusions estimated serial intervals are shorter than incubation periods in both singapore and tianjin, suggesting that pre symptomatic transmission is occurring shorter serial intervals lead to lower estimates of r0, which suggest that half of all secondary infections should be prevented to control spread",
            "sequence": "[CLS] basic reproduction number estimate time period basic reproduction number location [SEP] transmission interval estimates suggest pre symptomatic spread of covid 19 abstract background as the covid 19 epidemic is spreading, incoming data allows us to quantify values of key variables that determine the transmission and the effort required to control the epidemic we determine the incubation period and serial interval distribution for transmission clusters in singapore and in tianjin we infer the basic reproduction number and identify the extent of pre symptomatic transmission methods we collected outbreak information from singapore and tianjin, china, reported from jan 19 feb 26 and jan 21 feb 27, respectively we estimated incubation periods and serial intervals in both populations results the mean incubation period was 7 1 (6 13, 8 25) days for singapore and 9 (7 92, 10 2) days for tianjin both datasets had shorter incubation periods for earlier occurring cases the mean serial interval was 4 56 (2 69, 6 42) days for singapore and 4 22 (3 43, 5 01) for tianjin we inferred that early in the outbreaks, infection was transmitted on average 2 55 and 2 89 days before symptom onset (singapore, tianjin) the estimated basic reproduction number for singapore was 1 97 (1 45, 2 48) secondary cases per infective; for tianjin it was 1 87 (1 65, 2 09) secondary cases per infective conclusions estimated serial intervals are shorter than incubation periods in both singapore and tianjin, suggesting that pre symptomatic transmission is occurring shorter serial intervals lead to lower estimates of r0, which suggest that half of all secondary infections should be prevented to control spread [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R52190xR45108",
            "template_id": "R52190",
            "paper_id": "R45108",
            "premise": "transient absorption spectroscopy has participating device has participating person has gaseous sample input has solution sample input has solid sample input has radiant energy input has output",
            "hypothesis": "identification of reactive species in photoexcited nanocrystalline tio2 films by wide wavelength range (400\u22122500 nm) transient absorption spectroscopy reactive species, holes, and electrons in photoexcited nanocrystalline tio2 films were studied by transient absorption spectroscopy in the wavelength range from 400 to 2500 nm the electron spectrum was obtained through a hole scavenging reaction under steady state light irradiation the spectrum can be analyzed by a superposition of the free electron and trapped electron spectra by subtracting the electron spectrum from the transient absorption spectrum, the spectrum of trapped holes was obtained as a result, three reactive speciestrapped holes and free and trapped electronswere identified in the transient absorption spectrum the reactivity of these species was evaluated through transient absorption spectroscopy in the presence of hole and electron scavenger molecules the spectra indicate that trapped holes and electrons are localized at the surface of the particles and free electrons are distributed in the bulk",
            "sequence": "[CLS] transient absorption spectroscopy has participating device has participating person has gaseous sample input has solution sample input has solid sample input has radiant energy input has output [SEP] identification of reactive species in photoexcited nanocrystalline tio2 films by wide wavelength range (400\u22122500 nm) transient absorption spectroscopy reactive species, holes, and electrons in photoexcited nanocrystalline tio2 films were studied by transient absorption spectroscopy in the wavelength range from 400 to 2500 nm the electron spectrum was obtained through a hole scavenging reaction under steady state light irradiation the spectrum can be analyzed by a superposition of the free electron and trapped electron spectra by subtracting the electron spectrum from the transient absorption spectrum, the spectrum of trapped holes was obtained as a result, three reactive speciestrapped holes and free and trapped electronswere identified in the transient absorption spectrum the reactivity of these species was evaluated through transient absorption spectroscopy in the presence of hole and electron scavenger molecules the spectra indicate that trapped holes and electrons are localized at the surface of the particles and free electrons are distributed in the bulk [SEP]",
            "target": "entailment",
            "research_field": {
                "id": "R122",
                "label": "Chemistry"
            },
            "results": [
                {
                    "template_id": "R52190"
                }
            ]
        }
    ],
    "contradictions": [],
    "neutrals": [
        {
            "instance_id": "R5289",
            "template_id": null,
            "paper_id": "R5289",
            "premise": null,
            "hypothesis": "controlling an autonomous agent using internal value based action selection in this paper we describe an approach of controlling an autonomous robot by means of a hierarchical control structure, with a learning action selection since damasio\\'s \"descartes\\' error\" in 1994 the number of approaches to action selection that use internal values, derived from psychological models of emotions or drives has increased significantly the approach realises a learning action selection mechanism in a hierarchy of sensory and actuatory layers the sensory values yield the internal states, as a basis for action selection in addition they are used to calculate the reinforcement signal that trains the action selection",
            "sequence": "[CLS] None [SEP] controlling an autonomous agent using internal value based action selection in this paper we describe an approach of controlling an autonomous robot by means of a hierarchical control structure, with a learning action selection since damasio\\'s \"descartes\\' error\" in 1994 the number of approaches to action selection that use internal values, derived from psychological models of emotions or drives has increased significantly the approach realises a learning action selection mechanism in a hierarchy of sensory and actuatory layers the sensory values yield the internal states, as a basis for action selection in addition they are used to calculate the reinforcement signal that trains the action selection [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R133",
                "label": "Artificial Intelligence"
            },
            "results": [
                {
                    "template_id": "R138077"
                },
                {
                    "template_id": "R182248"
                },
                {
                    "template_id": "R194212"
                }
            ]
        },
        {
            "instance_id": "R6286",
            "template_id": null,
            "paper_id": "R6286",
            "premise": null,
            "hypothesis": "template based question answering as an increasing amount of rdf data is published as linked data, intuitive ways of accessing this data become more and more important question answering approaches have been proposed as a good compromise between intuitiveness and expressivity most question answering systems translate questions into triples which are matched against the rdf data to retrieve an answer, typically relying on some similarity metric however, in many cases, triples do not represent a faithful representation of the semantic structure of the natural language question, with the result that more expressive queries can not be answered to circumvent this problem, we present a novel approach that relies on a parse of the question to produce a sparql template that directly mirrors the internal structure of the question this template is then instantiated using statistical entity identification and predicate detection we show that this approach is competitive and discuss cases of questions that can be answered with our approach but not with competing approaches",
            "sequence": "[CLS] None [SEP] template based question answering as an increasing amount of rdf data is published as linked data, intuitive ways of accessing this data become more and more important question answering approaches have been proposed as a good compromise between intuitiveness and expressivity most question answering systems translate questions into triples which are matched against the rdf data to retrieve an answer, typically relying on some similarity metric however, in many cases, triples do not represent a faithful representation of the semantic structure of the natural language question, with the result that more expressive queries can not be answered to circumvent this problem, we present a novel approach that relies on a parse of the question to produce a sparql template that directly mirrors the internal structure of the question this template is then instantiated using statistical entity identification and predicate detection we show that this approach is competitive and discuss cases of questions that can be answered with our approach but not with competing approaches [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R133",
                "label": "Artificial Intelligence"
            },
            "results": [
                {
                    "template_id": "R138077"
                },
                {
                    "template_id": "R182248"
                },
                {
                    "template_id": "R194212"
                }
            ]
        },
        {
            "instance_id": "R6294",
            "template_id": null,
            "paper_id": "R6294",
            "premise": null,
            "hypothesis": "natural language question answering over rdf rdf question/answering (q/a) allows users to ask questions in natural languages over a knowledge base represented by rdf to answer a national language question, the existing work takes a two stage approach: question understanding and query evaluation their focus is on question understanding to deal with the disambiguation of the natural language phrases the most common technique is the joint disambiguation, which has the exponential search space in this paper, we propose a systematic framework to answer natural language questions over rdf repository (rdf q/a) from a graph data driven perspective we propose a semantic query graph to model the query intention in the natural language question in a structural way, based on which, rdf q/a is reduced to subgraph matching problem more importantly, we resolve the ambiguity of natural language questions at the time when matches of query are found the cost of disambiguation is saved if there are no matching found we compare our method with some state of the art rdf q/a systems in the benchmark dataset extensive experiments confirm that our method not only improves the precision but also speeds up query performance greatly",
            "sequence": "[CLS] None [SEP] natural language question answering over rdf rdf question/answering (q/a) allows users to ask questions in natural languages over a knowledge base represented by rdf to answer a national language question, the existing work takes a two stage approach: question understanding and query evaluation their focus is on question understanding to deal with the disambiguation of the natural language phrases the most common technique is the joint disambiguation, which has the exponential search space in this paper, we propose a systematic framework to answer natural language questions over rdf repository (rdf q/a) from a graph data driven perspective we propose a semantic query graph to model the query intention in the natural language question in a structural way, based on which, rdf q/a is reduced to subgraph matching problem more importantly, we resolve the ambiguity of natural language questions at the time when matches of query are found the cost of disambiguation is saved if there are no matching found we compare our method with some state of the art rdf q/a systems in the benchmark dataset extensive experiments confirm that our method not only improves the precision but also speeds up query performance greatly [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R133",
                "label": "Artificial Intelligence"
            },
            "results": [
                {
                    "template_id": "R138077"
                },
                {
                    "template_id": "R182248"
                },
                {
                    "template_id": "R194212"
                }
            ]
        },
        {
            "instance_id": "R6300",
            "template_id": null,
            "paper_id": "R6300",
            "premise": null,
            "hypothesis": "question answering over biomedical linked data with grammatical framework the blending of linked data with ontologies leverages the access to data gfmed introduces grammars for a controlled natural language targeted towards biomedical linked data and the corresponding controlled sparql language the grammars are described in grammatical framework and introduce linguistic and sparql phrases mostly about drugs, diseases and relationships between them the semantic and linguistic chunks correspond to description logic constructors problems and solutions for querying biomedical linked data with romanian, besides english, are also considered in the context of gf",
            "sequence": "[CLS] None [SEP] question answering over biomedical linked data with grammatical framework the blending of linked data with ontologies leverages the access to data gfmed introduces grammars for a controlled natural language targeted towards biomedical linked data and the corresponding controlled sparql language the grammars are described in grammatical framework and introduce linguistic and sparql phrases mostly about drugs, diseases and relationships between them the semantic and linguistic chunks correspond to description logic constructors problems and solutions for querying biomedical linked data with romanian, besides english, are also considered in the context of gf [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R133",
                "label": "Artificial Intelligence"
            },
            "results": [
                {
                    "template_id": "R138077"
                },
                {
                    "template_id": "R182248"
                },
                {
                    "template_id": "R194212"
                }
            ]
        },
        {
            "instance_id": "R6313",
            "template_id": null,
            "paper_id": "R6313",
            "premise": null,
            "hypothesis": "natural language queries over heterogeneous linked data graphs the demand to access large amounts of heterogeneous structured data is emerging as a trend for many users and applications however, the effort involved in querying heterogeneous and distributed third party databases can create major barriers for data consumers at the core of this problem is the semantic gap between the way users express their information needs and the representation of the data this work aims to provide a natural language interface and an associated semantic index to support an increased level of vocabulary independency for queries over linked data/semantic web datasets, using a distributional compositional semantics approach distributional semantics focuses on the automatic construction of a semantic model based on the statistical distribution of co occurring words in large scale texts the proposed query model targets the following features: (i) a principled semantic approximation approach with low adaptation effort (independent from manually created resources such as ontologies, thesauri or dictionaries), (ii) comprehensive semantic matching supported by the inclusion of large volumes of distributional (unstructured) commonsense knowledge into the semantic approximation process and (iii) expressive natural language queries the approach is evaluated using natural language queries on an open domain dataset and achieved avg recall=0 81, mean avg precision=0 62 and mean reciprocal rank=0 49",
            "sequence": "[CLS] None [SEP] natural language queries over heterogeneous linked data graphs the demand to access large amounts of heterogeneous structured data is emerging as a trend for many users and applications however, the effort involved in querying heterogeneous and distributed third party databases can create major barriers for data consumers at the core of this problem is the semantic gap between the way users express their information needs and the representation of the data this work aims to provide a natural language interface and an associated semantic index to support an increased level of vocabulary independency for queries over linked data/semantic web datasets, using a distributional compositional semantics approach distributional semantics focuses on the automatic construction of a semantic model based on the statistical distribution of co occurring words in large scale texts the proposed query model targets the following features: (i) a principled semantic approximation approach with low adaptation effort (independent from manually created resources such as ontologies, thesauri or dictionaries), (ii) comprehensive semantic matching supported by the inclusion of large volumes of distributional (unstructured) commonsense knowledge into the semantic approximation process and (iii) expressive natural language queries the approach is evaluated using natural language queries on an open domain dataset and achieved avg recall=0 81, mean avg precision=0 62 and mean reciprocal rank=0 49 [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R133",
                "label": "Artificial Intelligence"
            },
            "results": [
                {
                    "template_id": "R138077"
                },
                {
                    "template_id": "R182248"
                },
                {
                    "template_id": "R194212"
                }
            ]
        },
        {
            "instance_id": "R110733",
            "template_id": null,
            "paper_id": "R110733",
            "premise": null,
            "hypothesis": "extractive summarization of meeting recordings several approaches to automatic speech summarization are discussed below, using the icsi meetings corpus we contrast feature based approaches using prosodic and lexical features with maximal marginal relevance and latent semantic analysis approaches to summarization while the latter two techniques are borrowed directly from the field of text summarization, feature based approaches using prosodic information are able to utilize characteristics unique to speech data we also investigate how the summarization results might deteriorate when carried out on asr output as opposed to manual transcripts all of the summaries are of an extractive variety, and are compared using the software rouge",
            "sequence": "[CLS] None [SEP] extractive summarization of meeting recordings several approaches to automatic speech summarization are discussed below, using the icsi meetings corpus we contrast feature based approaches using prosodic and lexical features with maximal marginal relevance and latent semantic analysis approaches to summarization while the latter two techniques are borrowed directly from the field of text summarization, feature based approaches using prosodic information are able to utilize characteristics unique to speech data we also investigate how the summarization results might deteriorate when carried out on asr output as opposed to manual transcripts all of the summaries are of an extractive variety, and are compared using the software rouge [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R322",
                "label": "Computational Linguistics"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R12208",
            "template_id": null,
            "paper_id": "R12208",
            "premise": null,
            "hypothesis": "bert: pre training of deep bidirectional transformers for language understanding we introduce a new language representation model called bert, which stands for bidirectional encoder representations from transformers unlike recent language representation models (peters et al , 2018a; radford et al , 2018), bert is designed to pre train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers as a result, the pre trained bert model can be fine tuned with just one additional output layer to create state of the art models for a wide range of tasks, such as question answering and language inference, without substantial task specific architecture modifications bert is conceptually simple and empirically powerful it obtains new state of the art results on eleven natural language processing tasks, including pushing the glue score to 80 5 (7 7 point absolute improvement), multinli accuracy to 86 7% (4 6% absolute improvement), squad v1 1 question answering test f1 to 93 2 (1 5 point absolute improvement) and squad v2 0 test f1 to 83 1 (5 1 point absolute improvement)",
            "sequence": "[CLS] None [SEP] bert: pre training of deep bidirectional transformers for language understanding we introduce a new language representation model called bert, which stands for bidirectional encoder representations from transformers unlike recent language representation models (peters et al , 2018a; radford et al , 2018), bert is designed to pre train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers as a result, the pre trained bert model can be fine tuned with just one additional output layer to create state of the art models for a wide range of tasks, such as question answering and language inference, without substantial task specific architecture modifications bert is conceptually simple and empirically powerful it obtains new state of the art results on eleven natural language processing tasks, including pushing the glue score to 80 5 (7 7 point absolute improvement), multinli accuracy to 86 7% (4 6% absolute improvement), squad v1 1 question answering test f1 to 93 2 (1 5 point absolute improvement) and squad v2 0 test f1 to 83 1 (5 1 point absolute improvement) [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R172408",
            "template_id": null,
            "paper_id": "R172408",
            "premise": null,
            "hypothesis": "a survey on recent advances in named entity recognition from deep learning models named entity recognition (ner) is a key component in nlp systems for question answering, information retrieval, relation extraction, etc ner systems have been studied and developed widely for decades, but accurate systems using deep neural networks (nn) have only been introduced in the last few years we present a comprehensive survey of deep neural network architectures for ner, and contrast them with previous approaches to ner based on feature engineering and other supervised or semi supervised learning algorithms our results highlight the improvements achieved by neural networks, and show how incorporating some of the lessons learned from past work on feature based ner systems can yield further improvements",
            "sequence": "[CLS] None [SEP] a survey on recent advances in named entity recognition from deep learning models named entity recognition (ner) is a key component in nlp systems for question answering, information retrieval, relation extraction, etc ner systems have been studied and developed widely for decades, but accurate systems using deep neural networks (nn) have only been introduced in the last few years we present a comprehensive survey of deep neural network architectures for ner, and contrast them with previous approaches to ner based on feature engineering and other supervised or semi supervised learning algorithms our results highlight the improvements achieved by neural networks, and show how incorporating some of the lessons learned from past work on feature based ner systems can yield further improvements [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R172432",
            "template_id": null,
            "paper_id": "R172432",
            "premise": null,
            "hypothesis": "a unified architecture for natural language processing: deep neural networks with multitask learning we describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part of speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model the entire network is trained jointly on all these tasks using weight sharing, an instance of multitask learning all the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi supervised learning for the shared tasks we show how both multitask learning and semi supervised learning improve the generalization of the shared tasks, resulting in state of the art performance",
            "sequence": "[CLS] None [SEP] a unified architecture for natural language processing: deep neural networks with multitask learning we describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part of speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model the entire network is trained jointly on all these tasks using weight sharing, an instance of multitask learning all the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi supervised learning for the shared tasks we show how both multitask learning and semi supervised learning improve the generalization of the shared tasks, resulting in state of the art performance [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R172500",
            "template_id": null,
            "paper_id": "R172500",
            "premise": null,
            "hypothesis": "natural language processing (almost) from scratch we propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part of speech tagging, chunking, named entity recognition, and semantic role labeling this versatility is achieved by trying to avoid task specific engineering and therefore disregarding a lot of prior knowledge instead of exploiting man made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data this work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements",
            "sequence": "[CLS] None [SEP] natural language processing (almost) from scratch we propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part of speech tagging, chunking, named entity recognition, and semantic role labeling this versatility is achieved by trying to avoid task specific engineering and therefore disregarding a lot of prior knowledge instead of exploiting man made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data this work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R172520",
            "template_id": null,
            "paper_id": "R172520",
            "premise": null,
            "hypothesis": "farewell freebase: migrating the simplequestions dataset to dbpedia question answering over knowledge graphs is an important problem of interest both commercially and academically there is substantial interest in the class of natural language questions that can be answered via the lookup of a single fact, driven by the availability of the popular simplequestions dataset the problem with this dataset, however, is that answer triples are provided from freebase, which has been defunct for several years as a result, it is difficult to build \u201creal world\u201d question answering systems that are operationally deployable furthermore, a defunct knowledge graph means that much of the infrastructure for querying, browsing, and manipulating triples no longer exists to address this problem, we present simpledbpediaqa, a new benchmark dataset for simple question answering over knowledge graphs that was created by mapping simplequestions entities and predicates from freebase to dbpedia although this mapping is conceptually straightforward, there are a number of nuances that make the task non trivial, owing to the different conceptual organizations of the two knowledge graphs to lay the foundation for future research using this dataset, we leverage recent work to provide simple yet strong baselines with and without neural networks",
            "sequence": "[CLS] None [SEP] farewell freebase: migrating the simplequestions dataset to dbpedia question answering over knowledge graphs is an important problem of interest both commercially and academically there is substantial interest in the class of natural language questions that can be answered via the lookup of a single fact, driven by the availability of the popular simplequestions dataset the problem with this dataset, however, is that answer triples are provided from freebase, which has been defunct for several years as a result, it is difficult to build \u201creal world\u201d question answering systems that are operationally deployable furthermore, a defunct knowledge graph means that much of the infrastructure for querying, browsing, and manipulating triples no longer exists to address this problem, we present simpledbpediaqa, a new benchmark dataset for simple question answering over knowledge graphs that was created by mapping simplequestions entities and predicates from freebase to dbpedia although this mapping is conceptually straightforward, there are a number of nuances that make the task non trivial, owing to the different conceptual organizations of the two knowledge graphs to lay the foundation for future research using this dataset, we leverage recent work to provide simple yet strong baselines with and without neural networks [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R172664",
            "template_id": null,
            "paper_id": "R172664",
            "premise": null,
            "hypothesis": "end to end sequence labeling via bi directional lstm cnns crf state of the art sequence labeling systems traditionally require large amounts of task specific knowledge in the form of hand crafted features and data pre processing in this paper, we introduce a novel neutral network architecture that benefits from both word and character level representations automatically, by using combination of bidirectional lstm, cnn and crf our system is truly end to end, requiring no feature engineering or data pre processing, thus making it applicable to a wide range of sequence labeling tasks we evaluate our system on two data sets for two sequence labeling tasks penn treebank wsj corpus for part of speech (pos) tagging and conll 2003 corpus for named entity recognition (ner) we obtain state of the art performance on both the two data 97 55\\\\% accuracy for pos tagging and 91 21\\\\% f1 for ner",
            "sequence": "[CLS] None [SEP] end to end sequence labeling via bi directional lstm cnns crf state of the art sequence labeling systems traditionally require large amounts of task specific knowledge in the form of hand crafted features and data pre processing in this paper, we introduce a novel neutral network architecture that benefits from both word and character level representations automatically, by using combination of bidirectional lstm, cnn and crf our system is truly end to end, requiring no feature engineering or data pre processing, thus making it applicable to a wide range of sequence labeling tasks we evaluate our system on two data sets for two sequence labeling tasks penn treebank wsj corpus for part of speech (pos) tagging and conll 2003 corpus for named entity recognition (ner) we obtain state of the art performance on both the two data 97 55\\\\% accuracy for pos tagging and 91 21\\\\% f1 for ner [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R172672",
            "template_id": null,
            "paper_id": "R172672",
            "premise": null,
            "hypothesis": "named entity recognition with bidirectional lstm cnns named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance in this paper, we present a novel neural network architecture that automatically detects word and character level features using a hybrid bidirectional lstm and cnn architecture, eliminating the need for most feature engineering we also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the conll 2003 dataset and surpasses the previously reported state of the art performance on the ontonotes 5 0 dataset by 2 13 f1 points by using two lexicons constructed from publicly available sources, we establish new state of the art performance with an f1 score of 91 62 on conll 2003 and 86 28 on ontonotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information",
            "sequence": "[CLS] None [SEP] named entity recognition with bidirectional lstm cnns named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance in this paper, we present a novel neural network architecture that automatically detects word and character level features using a hybrid bidirectional lstm and cnn architecture, eliminating the need for most feature engineering we also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the conll 2003 dataset and surpasses the previously reported state of the art performance on the ontonotes 5 0 dataset by 2 13 f1 points by using two lexicons constructed from publicly available sources, we establish new state of the art performance with an f1 score of 91 62 on conll 2003 and 86 28 on ontonotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R175469",
            "template_id": null,
            "paper_id": "R175469",
            "premise": null,
            "hypothesis": "extracting a knowledge base of mechanisms from covid 19 papers the covid 19 pandemic has spawned a diverse body of scientific literature that is challenging to navigate, stimulating interest in automated tools to help find useful knowledge we pursue the construction of a knowledge base (kb) of mechanisms\u2014a fundamental concept across the sciences, which encompasses activities, functions and causal relations, ranging from cellular processes to economic impacts we extract this information from the natural language of scientific papers by developing a broad, unified schema that strikes a balance between relevance and breadth we annotate a dataset of mechanisms with our schema and train a model to extract mechanism relations from papers our experiments demonstrate the utility of our kb in supporting interdisciplinary scientific search over covid 19 literature, outperforming the prominent pubmed search in a study with clinical experts our search engine, dataset and code are publicly available",
            "sequence": "[CLS] None [SEP] extracting a knowledge base of mechanisms from covid 19 papers the covid 19 pandemic has spawned a diverse body of scientific literature that is challenging to navigate, stimulating interest in automated tools to help find useful knowledge we pursue the construction of a knowledge base (kb) of mechanisms\u2014a fundamental concept across the sciences, which encompasses activities, functions and causal relations, ranging from cellular processes to economic impacts we extract this information from the natural language of scientific papers by developing a broad, unified schema that strikes a balance between relevance and breadth we annotate a dataset of mechanisms with our schema and train a model to extract mechanism relations from papers our experiments demonstrate the utility of our kb in supporting interdisciplinary scientific search over covid 19 literature, outperforming the prominent pubmed search in a study with clinical experts our search engine, dataset and code are publicly available [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R182418",
            "template_id": null,
            "paper_id": "R182418",
            "premise": null,
            "hypothesis": "specter: document level representation learning using citation informed transformers representation learning is a critical ingredient for natural language processing systems recent transformer language models like bert learn powerful textual representations, but these models are targeted towards token and sentence level training objectives and do not leverage information on inter document relatedness, which limits their document level representation power for applications on scientific documents, such as classification and recommendation, accurate embeddings of documents are a necessity we propose specter, a new method to generate document level embedding of scientific papers based on pretraining a transformer language model on a powerful signal of document level relatedness: the citation graph unlike existing pretrained language models, specter can be easily applied to downstream applications without task specific fine tuning additionally, to encourage further research on document level models, we introduce scidocs, a new evaluation benchmark consisting of seven document level tasks ranging from citation prediction, to document classification and recommendation we show that specter outperforms a variety of competitive baselines on the benchmark",
            "sequence": "[CLS] None [SEP] specter: document level representation learning using citation informed transformers representation learning is a critical ingredient for natural language processing systems recent transformer language models like bert learn powerful textual representations, but these models are targeted towards token and sentence level training objectives and do not leverage information on inter document relatedness, which limits their document level representation power for applications on scientific documents, such as classification and recommendation, accurate embeddings of documents are a necessity we propose specter, a new method to generate document level embedding of scientific papers based on pretraining a transformer language model on a powerful signal of document level relatedness: the citation graph unlike existing pretrained language models, specter can be easily applied to downstream applications without task specific fine tuning additionally, to encourage further research on document level models, we introduce scidocs, a new evaluation benchmark consisting of seven document level tasks ranging from citation prediction, to document classification and recommendation we show that specter outperforms a variety of competitive baselines on the benchmark [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R184238",
            "template_id": null,
            "paper_id": "R184238",
            "premise": null,
            "hypothesis": "a survey on recent advances in named entity recognition from deep learning models named entity recognition (ner) is a key component in nlp systems for question answering, information retrieval, relation extraction, etc ner systems have been studied and developed widely for decades, but accurate systems using deep neural networks (nn) have only been introduced in the last few years we present a comprehensive survey of deep neural network architectures for ner, and contrast them with previous approaches to ner based on feature engineering and other supervised or semi supervised learning algorithms our results highlight the improvements achieved by neural networks, and show how incorporating some of the lessons learned from past work on feature based ner systems can yield further improvements",
            "sequence": "[CLS] None [SEP] a survey on recent advances in named entity recognition from deep learning models named entity recognition (ner) is a key component in nlp systems for question answering, information retrieval, relation extraction, etc ner systems have been studied and developed widely for decades, but accurate systems using deep neural networks (nn) have only been introduced in the last few years we present a comprehensive survey of deep neural network architectures for ner, and contrast them with previous approaches to ner based on feature engineering and other supervised or semi supervised learning algorithms our results highlight the improvements achieved by neural networks, and show how incorporating some of the lessons learned from past work on feature based ner systems can yield further improvements [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R187500",
            "template_id": null,
            "paper_id": "R187500",
            "premise": null,
            "hypothesis": "incorporating non local information into information extraction systems by gibbs sampling most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use we show how to solve this dilemma with gibbs sampling, a simple monte carlo method used to perform approximate inference in factored probabilistic models by using simulated annealing in place of viterbi decoding in sequence models such as hmms, cmms, and crfs, it is possible to incorporate non local structure while preserving tractable inference we use this technique to augment an existing crf based information extraction system with long distance dependency models, enforcing label consistency and extraction template consistency constraints this technique results in an error reduction of up to 9% over state of the art systems on two established information extraction tasks",
            "sequence": "[CLS] None [SEP] incorporating non local information into information extraction systems by gibbs sampling most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use we show how to solve this dilemma with gibbs sampling, a simple monte carlo method used to perform approximate inference in factored probabilistic models by using simulated annealing in place of viterbi decoding in sequence models such as hmms, cmms, and crfs, it is possible to incorporate non local structure while preserving tractable inference we use this technique to augment an existing crf based information extraction system with long distance dependency models, enforcing label consistency and extraction template consistency constraints this technique results in an error reduction of up to 9% over state of the art systems on two established information extraction tasks [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R189373",
            "template_id": null,
            "paper_id": "R189373",
            "premise": null,
            "hypothesis": "falcon 2 0: an entity and relation linking tool over wikidata the natural language processing (nlp) community has significantly contributed to the solutions for entity and relation recognition from a natural language text, and possibly linking them to proper matches in knowledge graphs (kgs) considering wikidata as the background kg, there are still limited tools to link knowledge within the text to wikidata in this paper, we present falcon 2 0, the first joint entity and relation linking tool over wikidata it receives a short natural language text in the english language and outputs a ranked list of entities and relations annotated with the proper candidates in wikidata the candidates are represented by their internationalized resource identifier (iri) in wikidata falcon 2 0 resorts to the english language model for the recognition task (e g , n gram tiling and n gram splitting), and then an optimization approach for the linking task we have empirically studied the performance of falcon 2 0 on wikidata and concluded that it outperforms all the existing baselines falcon 2 0 is open source and can be reused by the community; all the required instructions of falcon 2 0 are well documented at our github repository (https://github com/sdm tib/falcon2 0) we also demonstrate an online api, which can be run without any technical expertise falcon 2 0 and its background knowledge bases are available as resources at https://labs tib eu/falcon/falcon2/",
            "sequence": "[CLS] None [SEP] falcon 2 0: an entity and relation linking tool over wikidata the natural language processing (nlp) community has significantly contributed to the solutions for entity and relation recognition from a natural language text, and possibly linking them to proper matches in knowledge graphs (kgs) considering wikidata as the background kg, there are still limited tools to link knowledge within the text to wikidata in this paper, we present falcon 2 0, the first joint entity and relation linking tool over wikidata it receives a short natural language text in the english language and outputs a ranked list of entities and relations annotated with the proper candidates in wikidata the candidates are represented by their internationalized resource identifier (iri) in wikidata falcon 2 0 resorts to the english language model for the recognition task (e g , n gram tiling and n gram splitting), and then an optimization approach for the linking task we have empirically studied the performance of falcon 2 0 on wikidata and concluded that it outperforms all the existing baselines falcon 2 0 is open source and can be reused by the community; all the required instructions of falcon 2 0 are well documented at our github repository (https://github com/sdm tib/falcon2 0) we also demonstrate an online api, which can be run without any technical expertise falcon 2 0 and its background knowledge bases are available as resources at https://labs tib eu/falcon/falcon2/ [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R189391",
            "template_id": null,
            "paper_id": "R189391",
            "premise": null,
            "hypothesis": "deep reinforcement learning for mention ranking coreference models coreference resolution systems are typically trained with heuristic loss functions that require careful tuning in this paper we instead apply reinforcement learning to directly optimize a neural mention ranking model for coreference evaluation metrics we experiment with two approaches: the reinforce policy gradient algorithm and a reward rescaled max margin objective we find the latter to be more effective, resulting in significant improvements over the current state of the art on the english and chinese portions of the conll 2012 shared task",
            "sequence": "[CLS] None [SEP] deep reinforcement learning for mention ranking coreference models coreference resolution systems are typically trained with heuristic loss functions that require careful tuning in this paper we instead apply reinforcement learning to directly optimize a neural mention ranking model for coreference evaluation metrics we experiment with two approaches: the reinforce policy gradient algorithm and a reward rescaled max margin objective we find the latter to be more effective, resulting in significant improvements over the current state of the art on the english and chinese portions of the conll 2012 shared task [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R189450",
            "template_id": null,
            "paper_id": "R189450",
            "premise": null,
            "hypothesis": "fng ie: an improved graph based method for keyword extraction from scholarly big data keyword extraction is essential in determining influenced keywords from huge documents as the research repositories are becoming massive in volume day by day the research community is drowning in data and starving for information the keywords are the words that describe the theme of the whole document in a precise way by consisting of just a few words furthermore, many state of the art approaches are available for keyword extraction from a huge collection of documents and are classified into three types, the statistical approaches, machine learning, and graph based methods the machine learning approaches require a large training dataset that needs to be developed manually by domain experts, which sometimes is difficult to produce while determining influenced keywords however, this research focused on enhancing state of the art graph based methods to extract keywords when the training dataset is unavailable this research first converted the handcrafted dataset, collected from impact factor journals into n grams combinations, ranging from unigram to pentagram and also enhanced traditional graph based approaches the experiment was conducted on a handcrafted dataset, and all methods were applied on it domain experts performed the user study to evaluate the results the results were observed from every method and were evaluated with the user study using precision, recall and f measure as evaluation matrices the results showed that the proposed method (fng ie) performed well and scored near the machine learning approaches score",
            "sequence": "[CLS] None [SEP] fng ie: an improved graph based method for keyword extraction from scholarly big data keyword extraction is essential in determining influenced keywords from huge documents as the research repositories are becoming massive in volume day by day the research community is drowning in data and starving for information the keywords are the words that describe the theme of the whole document in a precise way by consisting of just a few words furthermore, many state of the art approaches are available for keyword extraction from a huge collection of documents and are classified into three types, the statistical approaches, machine learning, and graph based methods the machine learning approaches require a large training dataset that needs to be developed manually by domain experts, which sometimes is difficult to produce while determining influenced keywords however, this research focused on enhancing state of the art graph based methods to extract keywords when the training dataset is unavailable this research first converted the handcrafted dataset, collected from impact factor journals into n grams combinations, ranging from unigram to pentagram and also enhanced traditional graph based approaches the experiment was conducted on a handcrafted dataset, and all methods were applied on it domain experts performed the user study to evaluate the results the results were observed from every method and were evaluated with the user study using precision, recall and f measure as evaluation matrices the results showed that the proposed method (fng ie) performed well and scored near the machine learning approaches score [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R189458",
            "template_id": null,
            "paper_id": "R189458",
            "premise": null,
            "hypothesis": "axcell: automatic extraction of results from machine learning papers tracking progress in machine learning has become increasingly difficult with the recent explosion in the number of papers in this paper, we present axcell, an automatic machine learning pipeline for extracting results from papers axcell uses several novel components, including a table segmentation subtask, to learn relevant structural knowledge that aids extraction when compared with existing methods, our approach significantly improves the state of the art for results extraction we also release a structured, annotated dataset for training models for results extraction, and a dataset for evaluating the performance of models on this task lastly, we show the viability of our approach enables it to be used for semi automated results extraction in production, suggesting our improvements make this task practically viable for the first time code is available on github",
            "sequence": "[CLS] None [SEP] axcell: automatic extraction of results from machine learning papers tracking progress in machine learning has become increasingly difficult with the recent explosion in the number of papers in this paper, we present axcell, an automatic machine learning pipeline for extracting results from papers axcell uses several novel components, including a table segmentation subtask, to learn relevant structural knowledge that aids extraction when compared with existing methods, our approach significantly improves the state of the art for results extraction we also release a structured, annotated dataset for training models for results extraction, and a dataset for evaluating the performance of models on this task lastly, we show the viability of our approach enables it to be used for semi automated results extraction in production, suggesting our improvements make this task practically viable for the first time code is available on github [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R191261",
            "template_id": null,
            "paper_id": "R191261",
            "premise": null,
            "hypothesis": "linkbert: pretraining language models with document links language model (lm) pretraining captures various knowledge from text corpora, helping downstream tasks however, existing methods such as bert model a single document, and do not capture dependencies or knowledge that span across documents in this work, we propose linkbert, an lm pretraining method that leverages links between documents, e g , hyperlinks given a text corpus, we view it as a graph of documents and create lm inputs by placing linked documents in the same context we then pretrain the lm with two joint self supervised objectives: masked language modeling and our new proposal, document relation prediction we show that linkbert outperforms bert on various downstream tasks across two domains: the general domain (pretrained on wikipedia with hyperlinks) and biomedical domain (pretrained on pubmed with citation links) linkbert is especially effective for multi hop reasoning and few shot qa (+5% absolute improvement on hotpotqa and triviaqa), and our biomedical linkbert sets new states of the art on various bionlp tasks (+7% on bioasq and usmle) we release our pretrained models, linkbert and biolinkbert, as well as code and data",
            "sequence": "[CLS] None [SEP] linkbert: pretraining language models with document links language model (lm) pretraining captures various knowledge from text corpora, helping downstream tasks however, existing methods such as bert model a single document, and do not capture dependencies or knowledge that span across documents in this work, we propose linkbert, an lm pretraining method that leverages links between documents, e g , hyperlinks given a text corpus, we view it as a graph of documents and create lm inputs by placing linked documents in the same context we then pretrain the lm with two joint self supervised objectives: masked language modeling and our new proposal, document relation prediction we show that linkbert outperforms bert on various downstream tasks across two domains: the general domain (pretrained on wikipedia with hyperlinks) and biomedical domain (pretrained on pubmed with citation links) linkbert is especially effective for multi hop reasoning and few shot qa (+5% absolute improvement on hotpotqa and triviaqa), and our biomedical linkbert sets new states of the art on various bionlp tasks (+7% on bioasq and usmle) we release our pretrained models, linkbert and biolinkbert, as well as code and data [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R193649",
            "template_id": null,
            "paper_id": "R193649",
            "premise": null,
            "hypothesis": "a study of neural machine translation from chinese to urdu &lt;div&gt;machine translation (mt) is used for giving a translation from a source language to a target language machine translation simply translates text or speech from one language to another language, but this process is not sufficient to give the perfect translation of a text due to the requirement of identification of whole expressions and their direct counterparts neural machine translation (nmt) is one of the most standard machine translation methods, which has made great progress in the recent years especially in non universal languages however, local language translation software for other foreign languages is limited and needs improving in this paper, the chinese language is translated to the urdu language with the help of open neural machine translation (opennmt) in deep learning firstly, a chinese&lt;/div&gt;&lt;div&gt;to urdu language sentences datasets were established and supported with seven million sentences after that, these datasets were trained by using the open neural machine translation (opennmt) method at the final stage, the translation was compared to the desired translation with the help of the bleu score method &lt;/div&gt;",
            "sequence": "[CLS] None [SEP] a study of neural machine translation from chinese to urdu &lt;div&gt;machine translation (mt) is used for giving a translation from a source language to a target language machine translation simply translates text or speech from one language to another language, but this process is not sufficient to give the perfect translation of a text due to the requirement of identification of whole expressions and their direct counterparts neural machine translation (nmt) is one of the most standard machine translation methods, which has made great progress in the recent years especially in non universal languages however, local language translation software for other foreign languages is limited and needs improving in this paper, the chinese language is translated to the urdu language with the help of open neural machine translation (opennmt) in deep learning firstly, a chinese&lt;/div&gt;&lt;div&gt;to urdu language sentences datasets were established and supported with seven million sentences after that, these datasets were trained by using the open neural machine translation (opennmt) method at the final stage, the translation was compared to the desired translation with the help of the bleu score method &lt;/div&gt; [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R193655",
            "template_id": null,
            "paper_id": "R193655",
            "premise": null,
            "hypothesis": "research on chinese urdu machine translation based on deep learning \" urdu is pakistan 's national language however, chinese expertise is very negligible in pakistan and the asian nations yet fewer research has been undertaken in the area of computer translation on chinese to urdu in order to solve the above problems, we designed of an electronic dictionary for chinese urdu, and studied the sentence level machine translation technology which is based on deep learning the design of an electronic dictionary chinese urdu machine translation system we collected and constructed an electronic dictionary containing 24000 entries from chinese to urdu for sentence we used english as an intermediate language, and based on the existing parallel corpus of chinese to english and english to urdu, we constructed a bilingual parallel corpus containing 66000 sentences from chinese to urdu the corpus has trained by using two nmt models (lstm,transformer model) and the above two translation model were compared to the desired translation, with the help of bilingual valuation understudy (bleu) score \\xa0 on nmt, the lstm model is gain of 0 067 to 0 41 in bleu score while on transformer model, there is gain of 0 077 to 0 52 in bleu which is better than from lstm model score furthermore, we compared the proposed model with google and microsoft translation \"",
            "sequence": "[CLS] None [SEP] research on chinese urdu machine translation based on deep learning \" urdu is pakistan 's national language however, chinese expertise is very negligible in pakistan and the asian nations yet fewer research has been undertaken in the area of computer translation on chinese to urdu in order to solve the above problems, we designed of an electronic dictionary for chinese urdu, and studied the sentence level machine translation technology which is based on deep learning the design of an electronic dictionary chinese urdu machine translation system we collected and constructed an electronic dictionary containing 24000 entries from chinese to urdu for sentence we used english as an intermediate language, and based on the existing parallel corpus of chinese to english and english to urdu, we constructed a bilingual parallel corpus containing 66000 sentences from chinese to urdu the corpus has trained by using two nmt models (lstm,transformer model) and the above two translation model were compared to the desired translation, with the help of bilingual valuation understudy (bleu) score \\xa0 on nmt, the lstm model is gain of 0 067 to 0 41 in bleu score while on transformer model, there is gain of 0 077 to 0 52 in bleu which is better than from lstm model score furthermore, we compared the proposed model with google and microsoft translation \" [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R193658",
            "template_id": null,
            "paper_id": "R193658",
            "premise": null,
            "hypothesis": "a seq to seq machine translation from urdu to chinese machine translation (mt) is a subtype of computational linguistics that uses to implement the translation between different natural languages (nl) simply word to word exchanging on machine translation is not enough to give desire result neural machine translation is one of the standard methods of machine learning which make a huge improvement in recent time especially in local and some national languages however these languages translation are not enough and need to focus on it in this research we translate urdu to chinese language with the help of neural machine translation (nmt) in deep learning methods first we build a monolingual corpus of urdu and chinese languages, after that we train our model using neural machine translation (nmt) and then compare the data test result to accurate translation with the help of bleu score method",
            "sequence": "[CLS] None [SEP] a seq to seq machine translation from urdu to chinese machine translation (mt) is a subtype of computational linguistics that uses to implement the translation between different natural languages (nl) simply word to word exchanging on machine translation is not enough to give desire result neural machine translation is one of the standard methods of machine learning which make a huge improvement in recent time especially in local and some national languages however these languages translation are not enough and need to focus on it in this research we translate urdu to chinese language with the help of neural machine translation (nmt) in deep learning methods first we build a monolingual corpus of urdu and chinese languages, after that we train our model using neural machine translation (nmt) and then compare the data test result to accurate translation with the help of bleu score method [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R203383",
            "template_id": null,
            "paper_id": "R203383",
            "premise": null,
            "hypothesis": "tdmsci: a specialized corpus for scientific literature entity tagging of tasks datasets and metrics tasks, datasets and evaluation metrics are important concepts for understanding experimental scientific papers however, previous work on information extraction for scientific literature mainly focuses on the abstracts only, and does not treat datasets as a separate type of entity (zadeh and schumann, 2016; luan et al , 2018) in this paper, we present a new corpus that contains domain expert annotations for task (t), dataset (d), metric (m) entities 2,000 sentences extracted from nlp papers we report experiment results on tdm extraction using a simple data augmentation strategy and apply our tagger to around 30,000 nlp papers from the acl anthology the corpus is made publicly available to the community for fostering research on scientific publication summarization (erera et al , 2019) and knowledge discovery",
            "sequence": "[CLS] None [SEP] tdmsci: a specialized corpus for scientific literature entity tagging of tasks datasets and metrics tasks, datasets and evaluation metrics are important concepts for understanding experimental scientific papers however, previous work on information extraction for scientific literature mainly focuses on the abstracts only, and does not treat datasets as a separate type of entity (zadeh and schumann, 2016; luan et al , 2018) in this paper, we present a new corpus that contains domain expert annotations for task (t), dataset (d), metric (m) entities 2,000 sentences extracted from nlp papers we report experiment results on tdm extraction using a simple data augmentation strategy and apply our tagger to around 30,000 nlp papers from the acl anthology the corpus is made publicly available to the community for fostering research on scientific publication summarization (erera et al , 2019) and knowledge discovery [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R141003",
            "template_id": null,
            "paper_id": "R141003",
            "premise": null,
            "hypothesis": "semeval 2021 task 5: toxic spans detection the toxic spans detection task of semeval 2021 required participants to predict the spans of toxic posts that were responsible for the toxic label of the posts the task could be addressed as supervised sequence labeling, using training data with gold toxic spans provided by the organisers it could also be treated as rationale extraction, using classifiers trained on potentially larger external datasets of posts manually annotated as toxic or not, without toxic span annotations for the supervised sequence labeling approach and evaluation purposes, posts previously labeled as toxic were crowd annotated for toxic spans participants submitted their predicted spans for a held out test set and were scored using character based f1 this overview summarises the work of the 36 teams that provided system descriptions",
            "sequence": "[CLS] None [SEP] semeval 2021 task 5: toxic spans detection the toxic spans detection task of semeval 2021 required participants to predict the spans of toxic posts that were responsible for the toxic label of the posts the task could be addressed as supervised sequence labeling, using training data with gold toxic spans provided by the organisers it could also be treated as rationale extraction, using classifiers trained on potentially larger external datasets of posts manually annotated as toxic or not, without toxic span annotations for the supervised sequence labeling approach and evaluation purposes, posts previously labeled as toxic were crowd annotated for toxic spans participants submitted their predicted spans for a held out test set and were scored using character based f1 this overview summarises the work of the 36 teams that provided system descriptions [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R141010",
            "template_id": null,
            "paper_id": "R141010",
            "premise": null,
            "hypothesis": "unimplicit shared task report: detecting clarification requirements in instructional text this paper describes the data, task setup, and results of the shared task at the first workshop on understanding implicit and underspecified language (unimplicit) the task requires computational models to predict whether a sentence contains aspects of meaning that are contextually unspecified and thus require clarification two teams participated and the best scoring system achieved an accuracy of 68%",
            "sequence": "[CLS] None [SEP] unimplicit shared task report: detecting clarification requirements in instructional text this paper describes the data, task setup, and results of the shared task at the first workshop on understanding implicit and underspecified language (unimplicit) the task requires computational models to predict whether a sentence contains aspects of meaning that are contextually unspecified and thus require clarification two teams participated and the best scoring system achieved an accuracy of 68% [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R141018",
            "template_id": null,
            "paper_id": "R141018",
            "premise": null,
            "hypothesis": "rdoc task at bionlp ost 2019 bionlp open shared tasks (bionlp ost) is an international competition organized to facilitate development and sharing of computational tasks of biomedical text mining and solutions to them for bionlp ost 2019, we introduced a new mental health informatics task called \u201crdoc task\u201d, which is composed of two subtasks: information retrieval and sentence extraction through national institutes of mental health\u2019s research domain criteria framework five and four teams around the world participated in the two tasks, respectively according to the performance on the two tasks, we observe that there is room for improvement for text mining on brain research and mental illness",
            "sequence": "[CLS] None [SEP] rdoc task at bionlp ost 2019 bionlp open shared tasks (bionlp ost) is an international competition organized to facilitate development and sharing of computational tasks of biomedical text mining and solutions to them for bionlp ost 2019, we introduced a new mental health informatics task called \u201crdoc task\u201d, which is composed of two subtasks: information retrieval and sentence extraction through national institutes of mental health\u2019s research domain criteria framework five and four teams around the world participated in the two tasks, respectively according to the performance on the two tasks, we observe that there is room for improvement for text mining on brain research and mental illness [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R141066",
            "template_id": null,
            "paper_id": "R141066",
            "premise": null,
            "hypothesis": "clpsych 2018 shared task: predicting current and future psychological health from childhood essays we describe the shared task for the clpsych 2018 workshop, which focused on predicting current and future psychological health from an essay authored in childhood language based predictions of a person\u2019s current health have the potential to supplement traditional psychological assessment such as questionnaires, improving intake risk measurement and monitoring predictions of future psychological health can aid with both early detection and the development of preventative care research into the mental health trajectory of people, beginning from their childhood, has thus far been an area of little work within the nlp community this shared task represents one of the first attempts to evaluate the use of early language to predict future health; this has the potential to support a wide variety of clinical health care tasks, from early assessment of lifetime risk for mental health problems, to optimal timing for targeted interventions aimed at both prevention and treatment",
            "sequence": "[CLS] None [SEP] clpsych 2018 shared task: predicting current and future psychological health from childhood essays we describe the shared task for the clpsych 2018 workshop, which focused on predicting current and future psychological health from an essay authored in childhood language based predictions of a person\u2019s current health have the potential to supplement traditional psychological assessment such as questionnaires, improving intake risk measurement and monitoring predictions of future psychological health can aid with both early detection and the development of preventative care research into the mental health trajectory of people, beginning from their childhood, has thus far been an area of little work within the nlp community this shared task represents one of the first attempts to evaluate the use of early language to predict future health; this has the potential to support a wide variety of clinical health care tasks, from early assessment of lifetime risk for mental health problems, to optimal timing for targeted interventions aimed at both prevention and treatment [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R141070",
            "template_id": null,
            "paper_id": "R141070",
            "premise": null,
            "hypothesis": "named entity recognition on code switched data: overview of the calcs 2018 shared task in the third shared task of the computational approaches to linguistic code switching (calcs) workshop, we focus on named entity recognition (ner) on code switched social media data we divide the shared task into two competitions based on the english spanish (eng spa) and modern standard arabic egyptian (msa egy) language pairs we use twitter data and 9 entity types to establish a new dataset for code switched ner benchmarks in addition to the cs phenomenon, the diversity of the entities and the social media challenges make the task considerably hard to process as a result, the best scores of the competitions are 63 76% and 71 61% for eng spa and msa egy, respectively we present the scores of 9 participants and discuss the most common challenges among submissions",
            "sequence": "[CLS] None [SEP] named entity recognition on code switched data: overview of the calcs 2018 shared task in the third shared task of the computational approaches to linguistic code switching (calcs) workshop, we focus on named entity recognition (ner) on code switched social media data we divide the shared task into two competitions based on the english spanish (eng spa) and modern standard arabic egyptian (msa egy) language pairs we use twitter data and 9 entity types to establish a new dataset for code switched ner benchmarks in addition to the cs phenomenon, the diversity of the entities and the social media challenges make the task considerably hard to process as a result, the best scores of the competitions are 63 76% and 71 61% for eng spa and msa egy, respectively we present the scores of 9 participants and discuss the most common challenges among submissions [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R141092",
            "template_id": null,
            "paper_id": "R141092",
            "premise": null,
            "hypothesis": "dstc7 task 1: noetic end to end response selection goal oriented dialogue in complex domains is an extremely challenging problem and there are relatively few datasets this task provided two new resources that presented different challenges: one was focused but small, while the other was large but diverse we also considered several new variations on the next utterance selection problem: (1) increasing the number of candidates, (2) including paraphrases, and (3) not including a correct option in the candidate set twenty teams participated, developing a range of neural network models, including some that successfully incorporated external data to boost performance both datasets have been publicly released, enabling future work to build on these results, working towards robust goal oriented dialogue systems",
            "sequence": "[CLS] None [SEP] dstc7 task 1: noetic end to end response selection goal oriented dialogue in complex domains is an extremely challenging problem and there are relatively few datasets this task provided two new resources that presented different challenges: one was focused but small, while the other was large but diverse we also considered several new variations on the next utterance selection problem: (1) increasing the number of candidates, (2) including paraphrases, and (3) not including a correct option in the candidate set twenty teams participated, developing a range of neural network models, including some that successfully incorporated external data to boost performance both datasets have been publicly released, enabling future work to build on these results, working towards robust goal oriented dialogue systems [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R142108",
            "template_id": null,
            "paper_id": "R142108",
            "premise": null,
            "hypothesis": "semeval 2013 task 7: the joint student response analysis and 8th recognizing textual entailment challenge we present the results of the joint student response analysis and 8th recognizing textual entailment challenge, aiming to bring together researchers in educational nlp technology and textual entailment the task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment thus, we offered to the community a 5 way student response labeling task, as well as 3 way and 2way rte style tasks on educational data in addition, a partial entailment task was piloted we present and compare results from 9 participating teams, and discuss future directions",
            "sequence": "[CLS] None [SEP] semeval 2013 task 7: the joint student response analysis and 8th recognizing textual entailment challenge we present the results of the joint student response analysis and 8th recognizing textual entailment challenge, aiming to bring together researchers in educational nlp technology and textual entailment the task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment thus, we offered to the community a 5 way student response labeling task, as well as 3 way and 2way rte style tasks on educational data in addition, a partial entailment task was piloted we present and compare results from 9 participating teams, and discuss future directions [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R145757",
            "template_id": null,
            "paper_id": "R145757",
            "premise": null,
            "hypothesis": "semeval 2018 task 7: semantic relation extraction and classification in scientific papers this paper describes the first task on semantic relation extraction and classification in scientific paper abstracts at semeval 2018 the challenge focuses on domain specific semantic relations and includes three different subtasks the subtasks were designed so as to compare and quantify the effect of different pre processing steps on the relation classification results we expect the task to be relevant for a broad range of researchers working on extracting specialized knowledge from domain corpora, for example but not limited to scientific or bio medical information extraction the task attracted a total of 32 participants, with 158 submissions across different scenarios",
            "sequence": "[CLS] None [SEP] semeval 2018 task 7: semantic relation extraction and classification in scientific papers this paper describes the first task on semantic relation extraction and classification in scientific paper abstracts at semeval 2018 the challenge focuses on domain specific semantic relations and includes three different subtasks the subtasks were designed so as to compare and quantify the effect of different pre processing steps on the relation classification results we expect the task to be relevant for a broad range of researchers working on extracting specialized knowledge from domain corpora, for example but not limited to scientific or bio medical information extraction the task attracted a total of 32 participants, with 158 submissions across different scenarios [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R108821",
            "template_id": null,
            "paper_id": "R108821",
            "premise": null,
            "hypothesis": "a broadband plasma radiation detector with spatial resolution based on the optical scanning of the fluorescence of a phosphor a detector which converts the line\u2010integrated plasma radiation profile to visible light within a selected spectral range, by means of a film of sodium salicylate, is presented the phosphor fluorescent emission is spatially scanned by a rapidly vibrating mirror and detected by a filtered photomultiplier, allowing one to measure the time evolution of plasma radiation profiles in real time a detailed description of the detector, calibration method, and its performances in the tj\u2010i tokamak are included",
            "sequence": "[CLS] None [SEP] a broadband plasma radiation detector with spatial resolution based on the optical scanning of the fluorescence of a phosphor a detector which converts the line\u2010integrated plasma radiation profile to visible light within a selected spectral range, by means of a film of sodium salicylate, is presented the phosphor fluorescent emission is spatially scanned by a rapidly vibrating mirror and detected by a filtered photomultiplier, allowing one to measure the time evolution of plasma radiation profiles in real time a detailed description of the detector, calibration method, and its performances in the tj\u2010i tokamak are included [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R175",
                "label": "Atomic, Molecular and Optical Physics"
            },
            "results": [
                {
                    "template_id": "R184022"
                }
            ]
        },
        {
            "instance_id": "R108823",
            "template_id": null,
            "paper_id": "R108823",
            "premise": null,
            "hypothesis": "absolute intensities of the vacuum ultraviolet spectra in a metal etch plasma processing discharge in this paper we report absolute intensities of vacuum ultraviolet and near ultraviolet emission lines (4 8 ev to 18 ev ) for aluminum etching discharges in an inductively coupled plasma reactor we report line intensities as a function of wafer type, pressure, gas mixture and rf excitation level iri a standard aluminum etching mixture containing c12 and bc13 almost all the light emitted at energies exceeding 8 8 ev was due to neutral atomic chlorine optical trapping of the wv radiation in the discharge complicates calculations of vuv fluxes to the wafer however, we see total photon fluxes to the wailer at energies above 8 8 ev on the order of 4 x 1014 photons/cm2sec with anon reactive wafer and 0 7 x 10 `4 photons/cm2sec with a reactive wtier the maj ority of the radiation observed was between 8 9 and 9 3 ev at these energies, the photons have enough energy to create electron hole pairs in si02, but may penetrate up to a micron into the si02 before being absorbed relevance of these measurements to vacuum w photon induced darnage of si02 during etching is discussed",
            "sequence": "[CLS] None [SEP] absolute intensities of the vacuum ultraviolet spectra in a metal etch plasma processing discharge in this paper we report absolute intensities of vacuum ultraviolet and near ultraviolet emission lines (4 8 ev to 18 ev ) for aluminum etching discharges in an inductively coupled plasma reactor we report line intensities as a function of wafer type, pressure, gas mixture and rf excitation level iri a standard aluminum etching mixture containing c12 and bc13 almost all the light emitted at energies exceeding 8 8 ev was due to neutral atomic chlorine optical trapping of the wv radiation in the discharge complicates calculations of vuv fluxes to the wafer however, we see total photon fluxes to the wailer at energies above 8 8 ev on the order of 4 x 1014 photons/cm2sec with anon reactive wafer and 0 7 x 10 `4 photons/cm2sec with a reactive wtier the maj ority of the radiation observed was between 8 9 and 9 3 ev at these energies, the photons have enough energy to create electron hole pairs in si02, but may penetrate up to a micron into the si02 before being absorbed relevance of these measurements to vacuum w photon induced darnage of si02 during etching is discussed [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R175",
                "label": "Atomic, Molecular and Optical Physics"
            },
            "results": [
                {
                    "template_id": "R184022"
                }
            ]
        },
        {
            "instance_id": "R108934",
            "template_id": null,
            "paper_id": "R108934",
            "premise": null,
            "hypothesis": "detector system with high time resolution for the continuous measurement of spectra in the vacuum ultraviolet wavelength range a new detector system with high time resolution (1 ms) has been developed and applied for the continuous measurement of spectra in the vacuum ultraviolet (vuv) and extreme ultraviolet (euv) wavelength region at the fusion plasma experiment torus experiment for technology oriented research (textor) the system consists of an open multichannel plate (mcp) detector with subsequent first generation (gen i) light amplifier and a camera head which is based on a linear photodiode array with 1024 elements (pixels) the camera head provides the output signals of the individual pixels sequentially as an analog voltage with a full spectra rate of 1000 per second, which are measured using a pc based data acquisition system three vacuum spectrometers operating in the vuv/euv region (10\u2013130 nm) have been equipped with the new system and a successful campaign of measurements from about 4000 discharges at textor has been performed spectra are recorded with a usable linear dynamic range of 10 bit and a wavelength resolution corresponding to a width of 3\u20134 pixels a new detector system with high time resolution (1 ms) has been developed and applied for the continuous measurement of spectra in the vacuum ultraviolet (vuv) and extreme ultraviolet (euv) wavelength region at the fusion plasma experiment torus experiment for technology oriented research (textor) the system consists of an open multichannel plate (mcp) detector with subsequent first generation (gen i) light amplifier and a camera head which is based on a linear photodiode array with 1024 elements (pixels) the camera head provides the output signals of the individual pixels sequentially as an analog voltage with a full spectra rate of 1000 per second, which are measured using a pc based data acquisition system three vacuum spectrometers operating in the vuv/euv region (10\u2013130 nm) have been equipped with the new system and a successful campaign of measurements from about 4000 discharges at textor has been performed spectra are recorded with a usable linear dynamic range of 10 bit and a wavelength resolu",
            "sequence": "[CLS] None [SEP] detector system with high time resolution for the continuous measurement of spectra in the vacuum ultraviolet wavelength range a new detector system with high time resolution (1 ms) has been developed and applied for the continuous measurement of spectra in the vacuum ultraviolet (vuv) and extreme ultraviolet (euv) wavelength region at the fusion plasma experiment torus experiment for technology oriented research (textor) the system consists of an open multichannel plate (mcp) detector with subsequent first generation (gen i) light amplifier and a camera head which is based on a linear photodiode array with 1024 elements (pixels) the camera head provides the output signals of the individual pixels sequentially as an analog voltage with a full spectra rate of 1000 per second, which are measured using a pc based data acquisition system three vacuum spectrometers operating in the vuv/euv region (10\u2013130 nm) have been equipped with the new system and a successful campaign of measurements from about 4000 discharges at textor has been performed spectra are recorded with a usable linear dynamic range of 10 bit and a wavelength resolution corresponding to a width of 3\u20134 pixels a new detector system with high time resolution (1 ms) has been developed and applied for the continuous measurement of spectra in the vacuum ultraviolet (vuv) and extreme ultraviolet (euv) wavelength region at the fusion plasma experiment torus experiment for technology oriented research (textor) the system consists of an open multichannel plate (mcp) detector with subsequent first generation (gen i) light amplifier and a camera head which is based on a linear photodiode array with 1024 elements (pixels) the camera head provides the output signals of the individual pixels sequentially as an analog voltage with a full spectra rate of 1000 per second, which are measured using a pc based data acquisition system three vacuum spectrometers operating in the vuv/euv region (10\u2013130 nm) have been equipped with the new system and a successful campaign of measurements from about 4000 discharges at textor has been performed spectra are recorded with a usable linear dynamic range of 10 bit and a wavelength resolu [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R175",
                "label": "Atomic, Molecular and Optical Physics"
            },
            "results": [
                {
                    "template_id": "R184022"
                }
            ]
        },
        {
            "instance_id": "R108936",
            "template_id": null,
            "paper_id": "R108936",
            "premise": null,
            "hypothesis": "absolute vacuum ultraviolet flux in inductively coupled plasmas and chemical modifications of 193 nm photoresist vacuum ultraviolet (vuv) photons in plasma processing systems are known to alter surface chemistry and may damage gate dielectrics and photoresist we characterize absolute vuv fluxes to surfaces exposed in an inductively coupled argon plasma, 1\u201350 mtorr, 25\u2013400 w, using a calibrated vuv spectrometer we also demonstrate an alternative method to estimate vuv fluence in an inductively coupled plasma (icp) reactor using a chemical dosimeter type monitor we illustrate the technique with argon icp and xenon lamp exposure experiments, comparing direct vuv measurements with measured chemical changes in 193 nm photoresist covered si wafers following vuv exposure",
            "sequence": "[CLS] None [SEP] absolute vacuum ultraviolet flux in inductively coupled plasmas and chemical modifications of 193 nm photoresist vacuum ultraviolet (vuv) photons in plasma processing systems are known to alter surface chemistry and may damage gate dielectrics and photoresist we characterize absolute vuv fluxes to surfaces exposed in an inductively coupled argon plasma, 1\u201350 mtorr, 25\u2013400 w, using a calibrated vuv spectrometer we also demonstrate an alternative method to estimate vuv fluence in an inductively coupled plasma (icp) reactor using a chemical dosimeter type monitor we illustrate the technique with argon icp and xenon lamp exposure experiments, comparing direct vuv measurements with measured chemical changes in 193 nm photoresist covered si wafers following vuv exposure [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R175",
                "label": "Atomic, Molecular and Optical Physics"
            },
            "results": [
                {
                    "template_id": "R184022"
                }
            ]
        },
        {
            "instance_id": "R108938",
            "template_id": null,
            "paper_id": "R108938",
            "premise": null,
            "hypothesis": "prediction of uv spectra and uv radiation damage in actual plasma etching processes using on wafer monitoring technique uv radiation during plasma processing affects the surface of materials nevertheless, the interaction of uv photons with surface is not clearly understood because of the difficulty in monitoring photons during plasma processing for this purpose, we have previously proposed an on wafer monitoring technique for uv photons for this study, using the combination of this on wafer monitoring technique and a neural network, we established a relationship between the data obtained from the on wafer monitoring technique and uv spectra also, we obtained absolute intensities of uv radiation by calibrating arbitrary units of uv intensity with a 126 nm excimer lamp as a result, uv spectra and their absolute intensities could be predicted with the on wafer monitoring furthermore, we developed a prediction system with the on wafer monitoring technique to simulate uv radiation damage in dielectric films during plasma etching uv induced damage in sioc films was predicted in this study our prediction results of damage",
            "sequence": "[CLS] None [SEP] prediction of uv spectra and uv radiation damage in actual plasma etching processes using on wafer monitoring technique uv radiation during plasma processing affects the surface of materials nevertheless, the interaction of uv photons with surface is not clearly understood because of the difficulty in monitoring photons during plasma processing for this purpose, we have previously proposed an on wafer monitoring technique for uv photons for this study, using the combination of this on wafer monitoring technique and a neural network, we established a relationship between the data obtained from the on wafer monitoring technique and uv spectra also, we obtained absolute intensities of uv radiation by calibrating arbitrary units of uv intensity with a 126 nm excimer lamp as a result, uv spectra and their absolute intensities could be predicted with the on wafer monitoring furthermore, we developed a prediction system with the on wafer monitoring technique to simulate uv radiation damage in dielectric films during plasma etching uv induced damage in sioc films was predicted in this study our prediction results of damage [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R175",
                "label": "Atomic, Molecular and Optical Physics"
            },
            "results": [
                {
                    "template_id": "R184022"
                }
            ]
        },
        {
            "instance_id": "R108940",
            "template_id": null,
            "paper_id": "R108940",
            "premise": null,
            "hypothesis": "vacuum ultraviolet emission from microwave ar h2 plasmas vacuum ultraviolet emission from ar h2 wave driven microwave (2 45\\u2009ghz) plasmas operating at low pressures (0 1\u20131\\u2009mbar) has been investigated the emitted spectra show the presence of the ar resonance lines at 104 8 and 106 7\\u2009nm and of the lyman \u03b1,\u03b2 atomic lines at 121 6\\u2009nm and 102 6\\u2009nm, respectively the increase of the hydrogen amount in the mixture results in an abrupt increase of the werner and lyman molecular bands intensity the lyman \u03b2 intensity shows little changes in the range of 5%\u201330% of hydrogen in the mixture while the lyman \u03b1 intensity tends to decrease as the percentage of hydrogen increases",
            "sequence": "[CLS] None [SEP] vacuum ultraviolet emission from microwave ar h2 plasmas vacuum ultraviolet emission from ar h2 wave driven microwave (2 45\\u2009ghz) plasmas operating at low pressures (0 1\u20131\\u2009mbar) has been investigated the emitted spectra show the presence of the ar resonance lines at 104 8 and 106 7\\u2009nm and of the lyman \u03b1,\u03b2 atomic lines at 121 6\\u2009nm and 102 6\\u2009nm, respectively the increase of the hydrogen amount in the mixture results in an abrupt increase of the werner and lyman molecular bands intensity the lyman \u03b2 intensity shows little changes in the range of 5%\u201330% of hydrogen in the mixture while the lyman \u03b1 intensity tends to decrease as the percentage of hydrogen increases [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R175",
                "label": "Atomic, Molecular and Optical Physics"
            },
            "results": [
                {
                    "template_id": "R184022"
                }
            ]
        },
        {
            "instance_id": "R108942",
            "template_id": null,
            "paper_id": "R108942",
            "premise": null,
            "hypothesis": "comparison of surface vacuum ultraviolet emissions with resonance level number densities i argon plasmas vacuum ultraviolet (vuv) photons emitted from excited atomic states are ubiquitous in material processing plasmas the highly energetic photons can induce surface damage by driving surface reactions, disordering surface regions, and affecting bonds in the bulk material in argon plasmas, the vuv emissions are due to the decay of the 1s4 and 1s2 principal resonance levels with emission wavelengths of 104 8 and 106 7\\u2009nm, respectively the authors have measured the number densities of atoms in the two resonance levels using both white light optical absorption spectroscopy and radiation trapping induced changes in the 3p54p\u21923p54s branching fractions measured via visible/near infrared optical emission spectroscopy in an argon inductively coupled plasma as a function of both pressure and power an emission model that takes into account radiation trapping was used to calculate the vuv emission rate the model results were compared to experimental measurements made with a national institute of standards and techn",
            "sequence": "[CLS] None [SEP] comparison of surface vacuum ultraviolet emissions with resonance level number densities i argon plasmas vacuum ultraviolet (vuv) photons emitted from excited atomic states are ubiquitous in material processing plasmas the highly energetic photons can induce surface damage by driving surface reactions, disordering surface regions, and affecting bonds in the bulk material in argon plasmas, the vuv emissions are due to the decay of the 1s4 and 1s2 principal resonance levels with emission wavelengths of 104 8 and 106 7\\u2009nm, respectively the authors have measured the number densities of atoms in the two resonance levels using both white light optical absorption spectroscopy and radiation trapping induced changes in the 3p54p\u21923p54s branching fractions measured via visible/near infrared optical emission spectroscopy in an argon inductively coupled plasma as a function of both pressure and power an emission model that takes into account radiation trapping was used to calculate the vuv emission rate the model results were compared to experimental measurements made with a national institute of standards and techn [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R175",
                "label": "Atomic, Molecular and Optical Physics"
            },
            "results": [
                {
                    "template_id": "R184022"
                }
            ]
        },
        {
            "instance_id": "R108944",
            "template_id": null,
            "paper_id": "R108944",
            "premise": null,
            "hypothesis": "comparison of surface vacuum ultraviolet emissions with resonance level number densities ii rare gas plasmas and ar molecular gas mixtures vacuum ultraviolet (vuv) emissions from excited plasma species can play a variety of roles in processing plasmas, including damaging the surface properties of materials used in semiconductor processing depending on their wavelength, vuv photons can easily transmit thin upper dielectric layers and affect the electrical characteristics of the devices despite their importance, measuring vuv fluxes is complicated by the fact that few materials transmit at vuv wavelengths, and both detectors and windows are easily damaged by plasma exposure the authors have previously reported on measuring vuv fluxes in pure argon plasmas by monitoring the concentrations of ar(3p54s) resonance atoms that produce the vuv emissions using noninvasive optical emission spectroscopy in the visible/near infrared wavelength range [boffard et al , j vac sci technol , a 32, 021304 (2014)] here, the authors extend this technique to other rare gases (ne, kr, and xe) and argon molecular gas plasmas (ar/h2, ar/o2, and ar/n2) results",
            "sequence": "[CLS] None [SEP] comparison of surface vacuum ultraviolet emissions with resonance level number densities ii rare gas plasmas and ar molecular gas mixtures vacuum ultraviolet (vuv) emissions from excited plasma species can play a variety of roles in processing plasmas, including damaging the surface properties of materials used in semiconductor processing depending on their wavelength, vuv photons can easily transmit thin upper dielectric layers and affect the electrical characteristics of the devices despite their importance, measuring vuv fluxes is complicated by the fact that few materials transmit at vuv wavelengths, and both detectors and windows are easily damaged by plasma exposure the authors have previously reported on measuring vuv fluxes in pure argon plasmas by monitoring the concentrations of ar(3p54s) resonance atoms that produce the vuv emissions using noninvasive optical emission spectroscopy in the visible/near infrared wavelength range [boffard et al , j vac sci technol , a 32, 021304 (2014)] here, the authors extend this technique to other rare gases (ne, kr, and xe) and argon molecular gas plasmas (ar/h2, ar/o2, and ar/n2) results [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R175",
                "label": "Atomic, Molecular and Optical Physics"
            },
            "results": [
                {
                    "template_id": "R184022"
                }
            ]
        },
        {
            "instance_id": "R108946",
            "template_id": null,
            "paper_id": "R108946",
            "premise": null,
            "hypothesis": "quantification of the vuv radiation in low pressure hydrogen and nitrogen plasmas hydrogen and nitrogen containing discharges emit intense radiation in a broad wavelength region in the vuv the measured radiant power of individual molecular transitions and atomic lines between 117\\u2009nm and 280\\u2009nm are compared to those obtained in the visible spectral range and moreover to the rf power supplied to the icp discharge in hydrogen plasmas driven at 540\\u2009w of rf power up to 110\\u2009w are radiated in the vuv, whereas less than 2\\u2009w is emitted in the vis in nitrogen plasmas the power level of about 25\\u2009w is emitted both in the vuv and in the vis in hydrogen\u2013nitrogen mixtures, the nh radiation increases the vuv amount the analysis of molecular and atomic hydrogen emission supported by a collisional radiative model allowed determining plasma parameters and particle densities and thus particle fluxes a comparison of the fluxes showed that the photon fluxes determined from the measured emission are similar to the ion fluxes, whereas the atomic hydrogen fluxes are by far dominant photon fluxes up to 5\\u2009\\u2009\u00d7\\u2009\\u20091020 m\u22122 s\u22121 are obtained, demonstrating that the vuv radiation should not be neglected in surface modifications processes, whereas the radiant power converted to vuv photons is to be considered in power balances varying the admixture of nitrogen to hydrogen offers a possibility to tune photon fluxes in the respective wavelength intervals",
            "sequence": "[CLS] None [SEP] quantification of the vuv radiation in low pressure hydrogen and nitrogen plasmas hydrogen and nitrogen containing discharges emit intense radiation in a broad wavelength region in the vuv the measured radiant power of individual molecular transitions and atomic lines between 117\\u2009nm and 280\\u2009nm are compared to those obtained in the visible spectral range and moreover to the rf power supplied to the icp discharge in hydrogen plasmas driven at 540\\u2009w of rf power up to 110\\u2009w are radiated in the vuv, whereas less than 2\\u2009w is emitted in the vis in nitrogen plasmas the power level of about 25\\u2009w is emitted both in the vuv and in the vis in hydrogen\u2013nitrogen mixtures, the nh radiation increases the vuv amount the analysis of molecular and atomic hydrogen emission supported by a collisional radiative model allowed determining plasma parameters and particle densities and thus particle fluxes a comparison of the fluxes showed that the photon fluxes determined from the measured emission are similar to the ion fluxes, whereas the atomic hydrogen fluxes are by far dominant photon fluxes up to 5\\u2009\\u2009\u00d7\\u2009\\u20091020 m\u22122 s\u22121 are obtained, demonstrating that the vuv radiation should not be neglected in surface modifications processes, whereas the radiant power converted to vuv photons is to be considered in power balances varying the admixture of nitrogen to hydrogen offers a possibility to tune photon fluxes in the respective wavelength intervals [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R175",
                "label": "Atomic, Molecular and Optical Physics"
            },
            "results": [
                {
                    "template_id": "R184022"
                }
            ]
        },
        {
            "instance_id": "R108948",
            "template_id": null,
            "paper_id": "R108948",
            "premise": null,
            "hypothesis": "a microwave plasma source for vuv atmospheric photochemistry microwave plasma discharges working at low pressure are nowadays a well developed technique mainly used to provide radiations at different wavelengths the aim of this work is to show that those discharges are an efficient windowless vuv photon source for planetary atmospheric photochemistry experiments to do this, we use a surfatron type discharge with a neon gas flow in the mbar pressure range coupled to a photochemical reactor working in the vuv range allows to focus on nitrogen dominated atmospheres ({\\\\lambda}<100nm) the experimental setup makes sure that no other energy sources (electrons, metastable atoms) than the vuv photons interact with the reactive medium neon owns two resonance lines at 73 6 and 74 3 nm which behave differently regarding the pressure or power conditions in parallel, the vuv photon flux emitted at 73 6 nm has been experimentally estimated in different conditions of pressure and power and varies in a large range between 2x1013 this http url 2 and 4x1014 this http url 2 which is comparable to a vuv synchrotron photon flux our first case study is the atmosphere of titan and its n2 ch4 atmosphere with this vuv source, the production of hcn and c2n2, two major titan compounds, is detected, ensuring the suitability of the source for atmospheric photochemistry experiments",
            "sequence": "[CLS] None [SEP] a microwave plasma source for vuv atmospheric photochemistry microwave plasma discharges working at low pressure are nowadays a well developed technique mainly used to provide radiations at different wavelengths the aim of this work is to show that those discharges are an efficient windowless vuv photon source for planetary atmospheric photochemistry experiments to do this, we use a surfatron type discharge with a neon gas flow in the mbar pressure range coupled to a photochemical reactor working in the vuv range allows to focus on nitrogen dominated atmospheres ({\\\\lambda}<100nm) the experimental setup makes sure that no other energy sources (electrons, metastable atoms) than the vuv photons interact with the reactive medium neon owns two resonance lines at 73 6 and 74 3 nm which behave differently regarding the pressure or power conditions in parallel, the vuv photon flux emitted at 73 6 nm has been experimentally estimated in different conditions of pressure and power and varies in a large range between 2x1013 this http url 2 and 4x1014 this http url 2 which is comparable to a vuv synchrotron photon flux our first case study is the atmosphere of titan and its n2 ch4 atmosphere with this vuv source, the production of hcn and c2n2, two major titan compounds, is detected, ensuring the suitability of the source for atmospheric photochemistry experiments [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R175",
                "label": "Atomic, Molecular and Optical Physics"
            },
            "results": [
                {
                    "template_id": "R184022"
                }
            ]
        },
        {
            "instance_id": "R108950",
            "template_id": null,
            "paper_id": "R108950",
            "premise": null,
            "hypothesis": "in situ measurement of vuv/uv radiation from low pressure microwave produced plasma in ar/o2 gas mixtures ultraviolet (uv) and vacuum ultraviolet (vuv) spectral irradiance is determined in low pressure microwave produced plasma, which is regularly used for polymer surface treatment the re emitted fluorescence in the uv/vis spectral range from a sodium salicylate layer is measured this fluorescence is related to vuv/uv radiation in different spectral bands based on cut off filters the background produced by direct emitted radiation in the fluorescence spectral region is quantified using a specific background filter, thus enabling the use of the whole fluorescence spectral range a novel procedure is applied to determine the absolute value of the vuv/uv irradiance on a substrate for that, an independent measurement of the absolute spectral emissivity of the plasma in the uv is performed the measured irradiances on a substrate from a 25 pa ar/o2 produced plasma are in the range of 1015\u20131016 (photon s\u22121cm\u22122) these values include the contribution from impurities present in the discharge",
            "sequence": "[CLS] None [SEP] in situ measurement of vuv/uv radiation from low pressure microwave produced plasma in ar/o2 gas mixtures ultraviolet (uv) and vacuum ultraviolet (vuv) spectral irradiance is determined in low pressure microwave produced plasma, which is regularly used for polymer surface treatment the re emitted fluorescence in the uv/vis spectral range from a sodium salicylate layer is measured this fluorescence is related to vuv/uv radiation in different spectral bands based on cut off filters the background produced by direct emitted radiation in the fluorescence spectral region is quantified using a specific background filter, thus enabling the use of the whole fluorescence spectral range a novel procedure is applied to determine the absolute value of the vuv/uv irradiance on a substrate for that, an independent measurement of the absolute spectral emissivity of the plasma in the uv is performed the measured irradiances on a substrate from a 25 pa ar/o2 produced plasma are in the range of 1015\u20131016 (photon s\u22121cm\u22122) these values include the contribution from impurities present in the discharge [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R175",
                "label": "Atomic, Molecular and Optical Physics"
            },
            "results": [
                {
                    "template_id": "R184022"
                }
            ]
        },
        {
            "instance_id": "R108952",
            "template_id": null,
            "paper_id": "R108952",
            "premise": null,
            "hypothesis": "vacuum ultraviolet radiation emitted by microwave driven argon plasmas \"vacuum ultraviolet (vuv) radiation emitted by microwave driven argon plasmas has been investigated at low pressure conditions (0 36 mbar) a classical surface wave sustained discharge at 2 45\\u2009ghz has been used as plasma source vuv radiation has been detected by emission spectroscopy in the 30\u2013125\\u2009nm spectral range the spectrum exhibits atomic and ionic argon emissions with the most intense spectral lines corresponding to the atomic resonance lines, at 104 8\\u2009nm and 106 7\\u2009nm, and to the ion lines, at 92 0\\u2009nm and 93 2\\u2009nm emissions at lower wavelengths were also detected, including lines with no information concerning level transitions in the well known nist database (e g , the atomic line at 89 4\\u2009nm) the dependence of the lines' intensity on the microwave power delivered to the launcher was investigated the electron density was estimated to be around 1012\\u2009cm\u22123 using the stark broadening of the hydrogen h\u03b2 line at 486 1\\u2009nm the main population and loss mechanisms considered in the model for the excited a \"",
            "sequence": "[CLS] None [SEP] vacuum ultraviolet radiation emitted by microwave driven argon plasmas \"vacuum ultraviolet (vuv) radiation emitted by microwave driven argon plasmas has been investigated at low pressure conditions (0 36 mbar) a classical surface wave sustained discharge at 2 45\\u2009ghz has been used as plasma source vuv radiation has been detected by emission spectroscopy in the 30\u2013125\\u2009nm spectral range the spectrum exhibits atomic and ionic argon emissions with the most intense spectral lines corresponding to the atomic resonance lines, at 104 8\\u2009nm and 106 7\\u2009nm, and to the ion lines, at 92 0\\u2009nm and 93 2\\u2009nm emissions at lower wavelengths were also detected, including lines with no information concerning level transitions in the well known nist database (e g , the atomic line at 89 4\\u2009nm) the dependence of the lines' intensity on the microwave power delivered to the launcher was investigated the electron density was estimated to be around 1012\\u2009cm\u22123 using the stark broadening of the hydrogen h\u03b2 line at 486 1\\u2009nm the main population and loss mechanisms considered in the model for the excited a \" [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R175",
                "label": "Atomic, Molecular and Optical Physics"
            },
            "results": [
                {
                    "template_id": "R184022"
                }
            ]
        },
        {
            "instance_id": "R108954",
            "template_id": null,
            "paper_id": "R108954",
            "premise": null,
            "hypothesis": "ultraviolet/vacuum ultraviolet emission from a high power magnetron sputtering plasma with an aluminum target we report the in situ measurement of the ultraviolet/vacuum ultraviolet (uv/vuv) emission from a plasma produced by high power impulse magnetron sputtering with aluminum target, using argon as background gas the uv/vuv detection system is based upon the quantification of the re emitted fluorescence from a sodium salicylate layer that is placed in a housing inside the vacuum chamber, at 11\\u2009cm from the center of the cathode the detector is equipped with filters that allow for differentiating various spectral regions, and with a front collimating tube that provides a spatial resolution\\u2009\\u2009\u2248\\u2009\\u20090 5\\u2009cm using various views of the plasma, the measured absolutely calibrated photon rates enable to calculate emissivities and irradiances based on a model of the ionization region we present results that demonstrate that al+ ions are responsible for most of the vuv irradiance we also discuss the photoelectric emission due to irradiances on the target produced by high energy photons from resonance lines of ar+",
            "sequence": "[CLS] None [SEP] ultraviolet/vacuum ultraviolet emission from a high power magnetron sputtering plasma with an aluminum target we report the in situ measurement of the ultraviolet/vacuum ultraviolet (uv/vuv) emission from a plasma produced by high power impulse magnetron sputtering with aluminum target, using argon as background gas the uv/vuv detection system is based upon the quantification of the re emitted fluorescence from a sodium salicylate layer that is placed in a housing inside the vacuum chamber, at 11\\u2009cm from the center of the cathode the detector is equipped with filters that allow for differentiating various spectral regions, and with a front collimating tube that provides a spatial resolution\\u2009\\u2009\u2248\\u2009\\u20090 5\\u2009cm using various views of the plasma, the measured absolutely calibrated photon rates enable to calculate emissivities and irradiances based on a model of the ionization region we present results that demonstrate that al+ ions are responsible for most of the vuv irradiance we also discuss the photoelectric emission due to irradiances on the target produced by high energy photons from resonance lines of ar+ [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R175",
                "label": "Atomic, Molecular and Optical Physics"
            },
            "results": [
                {
                    "template_id": "R184022"
                }
            ]
        },
        {
            "instance_id": "R108956",
            "template_id": null,
            "paper_id": "R108956",
            "premise": null,
            "hypothesis": "vuv radiation flux from argon dc magnetron plasma vacuum ultraviolet (vuv) flux of argon plasma radiation in a dc magnetron discharge with a plane circular titanium cathode is measured it is found that the intensity of vuv radiation, mainly indicated by the resonance lines of argon atoms at 104 8 and 106 7 nm and ions at 92 and 93 2 nm, is proportional to the discharge current and decreases with pressure following the results of the measurements, a numerical model of resonance radiation transport is developed to determine the vuv flux to the substrate placed near the sputtering cathode where direct measurements are impossible due to the fast contamination of the detector by sputtered atoms in the case of a substrate located 10 cm opposite the cathode surface, the upper limit of estimated vuv flux is of the order of 1015 photons\\u2009cm\u22122\\u2009s\u22121 at a coating deposition rate of 1 5 nm\\u2009s\u22121 for 2 and 12 mtorr gas pressures based on the measurements, the damage to a porous low k dielectric by vuv radiation during the deposition of barrier layers in the dc magnetron discharge is first estimated",
            "sequence": "[CLS] None [SEP] vuv radiation flux from argon dc magnetron plasma vacuum ultraviolet (vuv) flux of argon plasma radiation in a dc magnetron discharge with a plane circular titanium cathode is measured it is found that the intensity of vuv radiation, mainly indicated by the resonance lines of argon atoms at 104 8 and 106 7 nm and ions at 92 and 93 2 nm, is proportional to the discharge current and decreases with pressure following the results of the measurements, a numerical model of resonance radiation transport is developed to determine the vuv flux to the substrate placed near the sputtering cathode where direct measurements are impossible due to the fast contamination of the detector by sputtered atoms in the case of a substrate located 10 cm opposite the cathode surface, the upper limit of estimated vuv flux is of the order of 1015 photons\\u2009cm\u22122\\u2009s\u22121 at a coating deposition rate of 1 5 nm\\u2009s\u22121 for 2 and 12 mtorr gas pressures based on the measurements, the damage to a porous low k dielectric by vuv radiation during the deposition of barrier layers in the dc magnetron discharge is first estimated [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R175",
                "label": "Atomic, Molecular and Optical Physics"
            },
            "results": [
                {
                    "template_id": "R184022"
                }
            ]
        },
        {
            "instance_id": "R189876",
            "template_id": null,
            "paper_id": "R189876",
            "premise": null,
            "hypothesis": "precision wavelength determination of 2^1p 1 1^1s 0 and 2^3p 1 1^1s 0 transitions in helium like sulfur ions transitions from the 21p1 and 23p1 state to the ground state 11s0 in helium like sulphur ions have been measured with an accuracy of 4 \u00d7 10 5 energy calibration is described in detail and two reference wavelengths have been reevaluated substantial line blending was observed, due to long lived spectator electrons the two transition energies were corrected for doppler shift and compared with most refined theoretical calculations, including terms of order \u03b14z6 in the breit operator and terms of order \u03b15z6 in the quantum electrodynamical corrections the experimental contributions to the ground state qed shifts agree within its error (\u223c 15%) with the theoretical values",
            "sequence": "[CLS] None [SEP] precision wavelength determination of 2^1p 1 1^1s 0 and 2^3p 1 1^1s 0 transitions in helium like sulfur ions transitions from the 21p1 and 23p1 state to the ground state 11s0 in helium like sulphur ions have been measured with an accuracy of 4 \u00d7 10 5 energy calibration is described in detail and two reference wavelengths have been reevaluated substantial line blending was observed, due to long lived spectator electrons the two transition energies were corrected for doppler shift and compared with most refined theoretical calculations, including terms of order \u03b14z6 in the breit operator and terms of order \u03b15z6 in the quantum electrodynamical corrections the experimental contributions to the ground state qed shifts agree within its error (\u223c 15%) with the theoretical values [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R114010",
                "label": "Atomic Physics"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R189910",
            "template_id": null,
            "paper_id": "R189910",
            "premise": null,
            "hypothesis": "spectroscopy of hydrogenlike and heliumlike argon the x ray transitions ($n=2\\\\ensuremath{\\\\rightarrow}n=1$) emitted by fast hydrogenlike and heliumlike argon ions have been studied the absolute energy of the lyman $\\\\ensuremath{\\\\alpha}$ lines of hydrogenlike ions has been measured and the value of the ($1s$) lamb shift of argon evaluated for the first time the $^{3}p {1,2}$ and $^{1}p {1}$ transitions of heliumlike argon have also been studied at very high precision and their energies compared to multiconfiguration dirac fock calculations the energies of lyman $\\\\ensuremath{\\\\alpha}$ lines are of 3323 2\\\\ifmmode\\\\pm\\\\else\\\\textpm\\\\fi{}0 5 ev ($\\\\mathrm{ly}{\\\\ensuremath{\\\\alpha}} {1}$) and 3318 1\\\\ifmmode\\\\pm\\\\else\\\\textpm\\\\fi{}0 5 ev ($\\\\mathrm{ly}{\\\\ensuremath{\\\\alpha}} {2}$), and those of $n=2\\\\ensuremath{\\\\rightarrow}n=1$ transitions for heliumlike argon are of 3123 6\\\\ifmmode\\\\pm\\\\else\\\\textpm\\\\fi{}0 25 ev ($^{3}p {1}$), 3126 4\\\\ifmmode\\\\pm\\\\else\\\\textpm\\\\fi{}0 4 ev ($^{3}p {2}$), and 3139 6\\\\ifmmode\\\\pm\\\\else\\\\textpm\\\\fi{}0 25 ev ($^{1}p {1}$)",
            "sequence": "[CLS] None [SEP] spectroscopy of hydrogenlike and heliumlike argon the x ray transitions ($n=2\\\\ensuremath{\\\\rightarrow}n=1$) emitted by fast hydrogenlike and heliumlike argon ions have been studied the absolute energy of the lyman $\\\\ensuremath{\\\\alpha}$ lines of hydrogenlike ions has been measured and the value of the ($1s$) lamb shift of argon evaluated for the first time the $^{3}p {1,2}$ and $^{1}p {1}$ transitions of heliumlike argon have also been studied at very high precision and their energies compared to multiconfiguration dirac fock calculations the energies of lyman $\\\\ensuremath{\\\\alpha}$ lines are of 3323 2\\\\ifmmode\\\\pm\\\\else\\\\textpm\\\\fi{}0 5 ev ($\\\\mathrm{ly}{\\\\ensuremath{\\\\alpha}} {1}$) and 3318 1\\\\ifmmode\\\\pm\\\\else\\\\textpm\\\\fi{}0 5 ev ($\\\\mathrm{ly}{\\\\ensuremath{\\\\alpha}} {2}$), and those of $n=2\\\\ensuremath{\\\\rightarrow}n=1$ transitions for heliumlike argon are of 3123 6\\\\ifmmode\\\\pm\\\\else\\\\textpm\\\\fi{}0 25 ev ($^{3}p {1}$), 3126 4\\\\ifmmode\\\\pm\\\\else\\\\textpm\\\\fi{}0 4 ev ($^{3}p {2}$), and 3139 6\\\\ifmmode\\\\pm\\\\else\\\\textpm\\\\fi{}0 25 ev ($^{1}p {1}$) [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R114010",
                "label": "Atomic Physics"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R190068",
            "template_id": null,
            "paper_id": "R190068",
            "premise": null,
            "hypothesis": "precision x ray wavelength measurements in helium like argon recoil ions \"the authors report precise wavelength measurements of the 1s2 1s0 1s2p3p1,2,1p1 transitions in ar16+ produced by collisions of 5 9 mev amu 1 u66+ ions with an argon gas target by use of this 'recoil source', the precision is not limited by doppler shifts while the influence of spectator electrons is minimised by observation of their relative importance as a function of gas pressure the accuracy obtained is at the 12 p p m level dominated by the x ray calibration standard the measurement is thus sensitive to quantum electrodynamic (qed) and electron correlation effects \"",
            "sequence": "[CLS] None [SEP] precision x ray wavelength measurements in helium like argon recoil ions \"the authors report precise wavelength measurements of the 1s2 1s0 1s2p3p1,2,1p1 transitions in ar16+ produced by collisions of 5 9 mev amu 1 u66+ ions with an argon gas target by use of this 'recoil source', the precision is not limited by doppler shifts while the influence of spectator electrons is minimised by observation of their relative importance as a function of gas pressure the accuracy obtained is at the 12 p p m level dominated by the x ray calibration standard the measurement is thus sensitive to quantum electrodynamic (qed) and electron correlation effects \" [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R114010",
                "label": "Atomic Physics"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R190165",
            "template_id": null,
            "paper_id": "R190165",
            "premise": null,
            "hypothesis": "precision measurement of the k \u03b1 transitions in heliumlike ge^30+ a measurement of the 1{ital s}2{ital p} {sup 1}{ital p}{sub 1}{r arrow}1{ital s}{sup 2} {ital s}{sub 0} resonance transition in heliumlike germanium (ge{sup 30+}) has been made on the lawrence livermore national laboratory electron beam ion trap to a precision of 21 ppm the result is compared with theoretical values and confirms a trend previously seen in the differences between experiment and theory for this transition as a function of {ital z} results for the 1{ital s}2{ital p} {sup 3}{ital p}{sub 1}{r arrow}1{ital s}{sup 2} {ital s}{sub 0} and 1{ital s}2{ital p} {sup 3}{ital p}{sub 2}{r arrow}1{ital s}{sup 2} {ital s}{sub 0} intercombination lines and for the 1{ital s}2{ital s} {sup 3}{ital s}{sub 1}{r arrow}1{ital s}{sup 2} {sup 1}{ital s}{sub 0} forbidden line are also presented and show similar differences with theoretical predictions",
            "sequence": "[CLS] None [SEP] precision measurement of the k \u03b1 transitions in heliumlike ge^30+ a measurement of the 1{ital s}2{ital p} {sup 1}{ital p}{sub 1}{r arrow}1{ital s}{sup 2} {ital s}{sub 0} resonance transition in heliumlike germanium (ge{sup 30+}) has been made on the lawrence livermore national laboratory electron beam ion trap to a precision of 21 ppm the result is compared with theoretical values and confirms a trend previously seen in the differences between experiment and theory for this transition as a function of {ital z} results for the 1{ital s}2{ital p} {sup 3}{ital p}{sub 1}{r arrow}1{ital s}{sup 2} {ital s}{sub 0} and 1{ital s}2{ital p} {sup 3}{ital p}{sub 2}{r arrow}1{ital s}{sup 2} {ital s}{sub 0} intercombination lines and for the 1{ital s}2{ital s} {sup 3}{ital s}{sub 1}{r arrow}1{ital s}{sup 2} {sup 1}{ital s}{sub 0} forbidden line are also presented and show similar differences with theoretical predictions [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R114010",
                "label": "Atomic Physics"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R190520",
            "template_id": null,
            "paper_id": "R190520",
            "premise": null,
            "hypothesis": "high precision spectroscopic studies of few electron ions in this report some experiments are described in which the lyman cap alpha lines of hydrogen like and helium like argon and iron ions have been measured the principle of all these experiments was to study with a crystal spectrometer the x rays emitted in flight by the ions, at 90/sup 0/ with respect to the direction of the beam 5 references, 14 figures, 9 tables",
            "sequence": "[CLS] None [SEP] high precision spectroscopic studies of few electron ions in this report some experiments are described in which the lyman cap alpha lines of hydrogen like and helium like argon and iron ions have been measured the principle of all these experiments was to study with a crystal spectrometer the x rays emitted in flight by the ions, at 90/sup 0/ with respect to the direction of the beam 5 references, 14 figures, 9 tables [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R114010",
                "label": "Atomic Physics"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R190553",
            "template_id": null,
            "paper_id": "R190553",
            "premise": null,
            "hypothesis": "high precision spectroscopic study of heliumlike iron the x ray spectrum emitted by high velocity heliumlike iron ions has been studied with a crystal spectrometer the absolute energies of the n = 2 > n = 1 lines have been measured with a precision of 40 ppm a very good agreement has been found between our experimental values and a very accurate multiconfiguration dirac fock calculation the precision of the measurement and the calculation of the energies are such that for the first time the magnetic correlation energy (spin spin) as well as the screening of quantum electrodynamic effects can now be appreciated the contamination of the considered lines by the so called dielectronic recombination satellites has been studied in great detail by varying the nature and the thickness of the targets",
            "sequence": "[CLS] None [SEP] high precision spectroscopic study of heliumlike iron the x ray spectrum emitted by high velocity heliumlike iron ions has been studied with a crystal spectrometer the absolute energies of the n = 2 > n = 1 lines have been measured with a precision of 40 ppm a very good agreement has been found between our experimental values and a very accurate multiconfiguration dirac fock calculation the precision of the measurement and the calculation of the energies are such that for the first time the magnetic correlation energy (spin spin) as well as the screening of quantum electrodynamic effects can now be appreciated the contamination of the considered lines by the so called dielectronic recombination satellites has been studied in great detail by varying the nature and the thickness of the targets [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R114010",
                "label": "Atomic Physics"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R138527",
            "template_id": null,
            "paper_id": "R138527",
            "premise": null,
            "hypothesis": "sensitivity analysis applied to a variational data assimilation of a simulated pollution transport problem: sensitivity analysis in variational data assimilation problem understanding the impact of the changes in pollutant emission from a foreign region onto a target region is a key factor for taking appropriate mitigating actions this requires a sensitivity analysis of a response function (defined on the target region) with respect to the source(s) of pollutant(s) the basic and straightforward approach to sensitivity analysis consists of multiple simulations of the pollution transport model with variations of the parameters that define the source of the pollutant a more systematic approach uses the adjoint of the pollution transport model derived from applying the principle of variations both approaches assume that the transport velocity and the initial distribution of the pollutant are known however, when observations of both the velocity and concentration fields are available, the transport velocity and the initial distribution of the pollutant are given by the solution of a data assimilation problem as a consequence, the sensitivity analysis should be carried out on the optimality system of the data assimilation problem, and not on the direct model alone this leads to a sensitivity analysis that involves the second\u2010order adjoint model, which is presented in the present work it is especially shown theoretically and with numerical experiments that the sensitivity on the optimality system includes important terms that are ignored by the sensitivity on the direct model the latter shows only the direct effects of the variation of the source on the response function while the first shows the indirect effects in addition to the direct effects copyright \u00a9 2016 john wiley & sons, ltd",
            "sequence": "[CLS] None [SEP] sensitivity analysis applied to a variational data assimilation of a simulated pollution transport problem: sensitivity analysis in variational data assimilation problem understanding the impact of the changes in pollutant emission from a foreign region onto a target region is a key factor for taking appropriate mitigating actions this requires a sensitivity analysis of a response function (defined on the target region) with respect to the source(s) of pollutant(s) the basic and straightforward approach to sensitivity analysis consists of multiple simulations of the pollution transport model with variations of the parameters that define the source of the pollutant a more systematic approach uses the adjoint of the pollution transport model derived from applying the principle of variations both approaches assume that the transport velocity and the initial distribution of the pollutant are known however, when observations of both the velocity and concentration fields are available, the transport velocity and the initial distribution of the pollutant are given by the solution of a data assimilation problem as a consequence, the sensitivity analysis should be carried out on the optimality system of the data assimilation problem, and not on the direct model alone this leads to a sensitivity analysis that involves the second\u2010order adjoint model, which is presented in the present work it is especially shown theoretically and with numerical experiments that the sensitivity on the optimality system includes important terms that are ignored by the sensitivity on the direct model the latter shows only the direct effects of the variation of the source on the response function while the first shows the indirect effects in addition to the direct effects copyright \u00a9 2016 john wiley & sons, ltd [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R109",
                "label": "Control Theory"
            },
            "results": [
                {
                    "template_id": "R187648"
                }
            ]
        },
        {
            "instance_id": "R187658",
            "template_id": null,
            "paper_id": "R187658",
            "premise": null,
            "hypothesis": "an online convex optimization algorithm for controlling linear systems with state and input constraints \"this paper studies the problem of controlling linear dynamical systems subject to point wise in time constraints we present an algorithm similar to online gradient descent, that can handle time varying and a priori unknown convex cost functions while restraining the system states and inputs to polytopic constraint sets analysis of the algorithm's performance, measured by dynamic regret, reveals that sub linear regret is achieved if the variation of the cost functions is sublinear in time finally, we present an example to illustrate implementation details as well as the algorithm's performance and show that the proposed algorithm ensures constraint satisfaction \"",
            "sequence": "[CLS] None [SEP] an online convex optimization algorithm for controlling linear systems with state and input constraints \"this paper studies the problem of controlling linear dynamical systems subject to point wise in time constraints we present an algorithm similar to online gradient descent, that can handle time varying and a priori unknown convex cost functions while restraining the system states and inputs to polytopic constraint sets analysis of the algorithm's performance, measured by dynamic regret, reveals that sub linear regret is achieved if the variation of the cost functions is sublinear in time finally, we present an example to illustrate implementation details as well as the algorithm's performance and show that the proposed algorithm ensures constraint satisfaction \" [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R109",
                "label": "Control Theory"
            },
            "results": [
                {
                    "template_id": "R187648"
                }
            ]
        },
        {
            "instance_id": "R187671",
            "template_id": null,
            "paper_id": "R187671",
            "premise": null,
            "hypothesis": "data driven online convex optimization for control of dynamical systems we propose a data driven online convex optimization algorithm for controlling dynamical systems in particular, the control scheme makes use of an initially measured input output trajectory and behavioral systems theory which enable it to handle unknown discrete time linear time invariant systems as well as a priori unknown time varying cost functions further, only output feedback instead of full state measurements is required for the proposed approach analysis of the closed loop\u2019s performance reveals that the algorithm achieves sublinear regret if the variation of the cost functions is sublinear the effectiveness of the proposed algorithm, even in the case of noisy measurements, is illustrated by a simulation example",
            "sequence": "[CLS] None [SEP] data driven online convex optimization for control of dynamical systems we propose a data driven online convex optimization algorithm for controlling dynamical systems in particular, the control scheme makes use of an initially measured input output trajectory and behavioral systems theory which enable it to handle unknown discrete time linear time invariant systems as well as a priori unknown time varying cost functions further, only output feedback instead of full state measurements is required for the proposed approach analysis of the closed loop\u2019s performance reveals that the algorithm achieves sublinear regret if the variation of the cost functions is sublinear the effectiveness of the proposed algorithm, even in the case of noisy measurements, is illustrated by a simulation example [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R109",
                "label": "Control Theory"
            },
            "results": [
                {
                    "template_id": "R187648"
                }
            ]
        },
        {
            "instance_id": "R187843",
            "template_id": null,
            "paper_id": "R187843",
            "premise": null,
            "hypothesis": "online optimization as a feedback controller: stability and tracking this paper develops and analyzes feedback based online optimization methods to regulate the output of a linear time invariant (lti) dynamical system to the optimal solution of a time varying convex optimization problem the design of the algorithm is based on continuous time primal dual dynamics, properly modified to incorporate feedback from the lti dynamical system, applied to a proximal augmented lagrangian function the resultant closed loop algorithm tracks the solution of the time varying optimization problem without requiring knowledge of (time varying) disturbances in the dynamical system the analysis leverages integral quadratic constraints to provide linear matrix inequality (lmi) conditions that guarantee global exponential stability and bounded tracking error analytical results show that under a sufficient time scale separation between the dynamics of the lti dynamical system and the algorithm, the lmi conditions can be always satisfied this paper further proposes a modified algorithm that can track an approximate solution trajectory of the constrained optimization problem under less restrictive assumptions as an illustrative example, the proposed algorithms are showcased for power transmission systems, to compress the time scales between secondary and tertiary control, and allow to simultaneously power rebalancing and tracking of the dc optimal power flow points",
            "sequence": "[CLS] None [SEP] online optimization as a feedback controller: stability and tracking this paper develops and analyzes feedback based online optimization methods to regulate the output of a linear time invariant (lti) dynamical system to the optimal solution of a time varying convex optimization problem the design of the algorithm is based on continuous time primal dual dynamics, properly modified to incorporate feedback from the lti dynamical system, applied to a proximal augmented lagrangian function the resultant closed loop algorithm tracks the solution of the time varying optimization problem without requiring knowledge of (time varying) disturbances in the dynamical system the analysis leverages integral quadratic constraints to provide linear matrix inequality (lmi) conditions that guarantee global exponential stability and bounded tracking error analytical results show that under a sufficient time scale separation between the dynamics of the lti dynamical system and the algorithm, the lmi conditions can be always satisfied this paper further proposes a modified algorithm that can track an approximate solution trajectory of the constrained optimization problem under less restrictive assumptions as an illustrative example, the proposed algorithms are showcased for power transmission systems, to compress the time scales between secondary and tertiary control, and allow to simultaneously power rebalancing and tracking of the dc optimal power flow points [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R109",
                "label": "Control Theory"
            },
            "results": [
                {
                    "template_id": "R187648"
                }
            ]
        },
        {
            "instance_id": "R38662",
            "template_id": null,
            "paper_id": "R38662",
            "premise": null,
            "hypothesis": "physical chemistry of the groups iva (ti, zr), va (v, nb, ta) and rare earth elements in steel \"vanadium, niobium and tantalum that belong to the grouj) va in the j)eriodic table have strong affinity lor nitrogen and carbon in steel besides, these elements differ from aluminium, and titanium and zirconium in the group iva in deoxidizing power, and are not oxidized remarkably in liquid steel these, therefore, are vely interesting as allqying elemen/s in steelmaking process on the other hand, the rare earths are very' important elements as a good scavenger 0 1 oxygen and sulphur in liqu id steelfrom their chemical activities in this paper, (1) chemical reactions between each 0 1 the iva elements, the va elements, the rare earths and aluminium and each 0 1 oxygen, nitrogen, carbon and sulphur, respectively, both in liquid and solid steels, and (2) roles and influences 0 1 these elements on steelmaking process and on the quality and property ~r steel, are discussed from a viewpoint 0 1 physical chemistly\u00b7\"",
            "sequence": "[CLS] None [SEP] physical chemistry of the groups iva (ti, zr), va (v, nb, ta) and rare earth elements in steel \"vanadium, niobium and tantalum that belong to the grouj) va in the j)eriodic table have strong affinity lor nitrogen and carbon in steel besides, these elements differ from aluminium, and titanium and zirconium in the group iva in deoxidizing power, and are not oxidized remarkably in liquid steel these, therefore, are vely interesting as allqying elemen/s in steelmaking process on the other hand, the rare earths are very' important elements as a good scavenger 0 1 oxygen and sulphur in liqu id steelfrom their chemical activities in this paper, (1) chemical reactions between each 0 1 the iva elements, the va elements, the rare earths and aluminium and each 0 1 oxygen, nitrogen, carbon and sulphur, respectively, both in liquid and solid steels, and (2) roles and influences 0 1 these elements on steelmaking process and on the quality and property ~r steel, are discussed from a viewpoint 0 1 physical chemistly\u00b7\" [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R254",
                "label": "Materials Science and Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R38668",
            "template_id": null,
            "paper_id": "R38668",
            "premise": null,
            "hypothesis": "effect of distribution of tin precipitate particles on the austenite grain size of low carbon low alloy steels i t is wcl l known that the toughness of steel is improved by the a ustenite grain r efinement there a re many ways to obtain a fin e grain size a typical example is to use fine aln or nb(cn) precipitate 1 ,2) gladman3 ) h as developed a genera l theory on the mechanism controlling the austenite grain size in which he correlated the critical stage of grain coarsening with the size a nd the vo lume frac tion of precipitates there are many expel\"imenta l evidences concerning the austenite grain refin ement using carbides and nitrid es george and ira ni ) determined grain coarsening temperature of th e ti bearing steels as about i 200\u00b0c, which is higher by loo\u00b0c tha n th at of the cases of other precipitate particles sim il a r experimental resu lts on ti containing steels were obtained by bashford and george 5 ) i t has been suggested from these works that the size and the volume fraction of tin a re the major factors which control the austenite grain coarsening at high tempel\"atu res , but no observation has been offered the present work is to study the dissolution , coalescence a nd precipita tion of tin in the austenite temperature range with special reference to the austenite grain size a simple model is proposed on the m echa nism of controlling the austenite g rain size by precipitate pa rti cles",
            "sequence": "[CLS] None [SEP] effect of distribution of tin precipitate particles on the austenite grain size of low carbon low alloy steels i t is wcl l known that the toughness of steel is improved by the a ustenite grain r efinement there a re many ways to obtain a fin e grain size a typical example is to use fine aln or nb(cn) precipitate 1 ,2) gladman3 ) h as developed a genera l theory on the mechanism controlling the austenite grain size in which he correlated the critical stage of grain coarsening with the size a nd the vo lume frac tion of precipitates there are many expel\"imenta l evidences concerning the austenite grain refin ement using carbides and nitrid es george and ira ni ) determined grain coarsening temperature of th e ti bearing steels as about i 200\u00b0c, which is higher by loo\u00b0c tha n th at of the cases of other precipitate particles sim il a r experimental resu lts on ti containing steels were obtained by bashford and george 5 ) i t has been suggested from these works that the size and the volume fraction of tin a re the major factors which control the austenite grain coarsening at high tempel\"atu res , but no observation has been offered the present work is to study the dissolution , coalescence a nd precipita tion of tin in the austenite temperature range with special reference to the austenite grain size a simple model is proposed on the m echa nism of controlling the austenite g rain size by precipitate pa rti cles [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R254",
                "label": "Materials Science and Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R189410",
            "template_id": null,
            "paper_id": "R189410",
            "premise": null,
            "hypothesis": "graphene oxide papers modified by divalent ions\u2014enhancing mechanical properties <i>via</i> chemical cross linking significant enhancement in mechanical stiffness (10 200%) and fracture strength (approximately 50%) of graphene oxide paper, a novel paperlike material made from individual graphene oxide sheets, can be achieved upon modification with a small amount (less than 1 wt %) of mg(2+) and ca(2+) these results can be readily rationalized in terms of the chemical interactions between the functional groups of the graphene oxide sheets and the divalent metals ions while oxygen functional groups on the basal planes of the sheets and the carboxylate groups on the edges can both bond to mg(2+) and ca(2+), the main contribution to mechanical enhancement of the paper comes from the latter",
            "sequence": "[CLS] None [SEP] graphene oxide papers modified by divalent ions\u2014enhancing mechanical properties <i>via</i> chemical cross linking significant enhancement in mechanical stiffness (10 200%) and fracture strength (approximately 50%) of graphene oxide paper, a novel paperlike material made from individual graphene oxide sheets, can be achieved upon modification with a small amount (less than 1 wt %) of mg(2+) and ca(2+) these results can be readily rationalized in terms of the chemical interactions between the functional groups of the graphene oxide sheets and the divalent metals ions while oxygen functional groups on the basal planes of the sheets and the carboxylate groups on the edges can both bond to mg(2+) and ca(2+), the main contribution to mechanical enhancement of the paper comes from the latter [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R254",
                "label": "Materials Science and Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R189415",
            "template_id": null,
            "paper_id": "R189415",
            "premise": null,
            "hypothesis": "byssal threads inspired ionic cross linked narce like graphene oxide paper with superior mechanical strength \"artificial nacre like graphene oxide paper has sparked great excitement in the scientific community for its unique properties the preparation of a bioinspired high strength nanocomposite paper via a simple vacuum assisted assembly technique from graphene oxide (go), tannic acid (ta) and fe3+ ions is reported in this article the fabricated papers were characterized by x ray diffraction (xrd), scanning electron microscopy (sem), thermogravimetric analysis (tga), fourier transformed infrared (ftir) spectroscopy, x ray photoelectron spectroscopy (xps) and dynamic mechanical analysis (dma) we show that fe3+ ions only induce limited improvement in the mechanical properties of the graphene oxide paper, while the efficient cross linking of neighboring sheets by fe3+\u2013ta complex network can significantly improve the fracture strength and young's modulus of graphene oxide paper by 150% and 521%, respectively, with an optimal content of 5 7 wt% fe3+ with general surface binding affinity, ta molecules can be adsorbed to go sheets and provide binding sites for fe3+ the fe3+\u2013ta coordinated compound serves as the \u201cmortar\u201d to stick the go \u201cbricks\u201d together the mechanical properties of our paper can be simply varied by controlling the cross linking condition the obtained nacre like ultrastrong go papers could find potential in energy and sustainability applications \"",
            "sequence": "[CLS] None [SEP] byssal threads inspired ionic cross linked narce like graphene oxide paper with superior mechanical strength \"artificial nacre like graphene oxide paper has sparked great excitement in the scientific community for its unique properties the preparation of a bioinspired high strength nanocomposite paper via a simple vacuum assisted assembly technique from graphene oxide (go), tannic acid (ta) and fe3+ ions is reported in this article the fabricated papers were characterized by x ray diffraction (xrd), scanning electron microscopy (sem), thermogravimetric analysis (tga), fourier transformed infrared (ftir) spectroscopy, x ray photoelectron spectroscopy (xps) and dynamic mechanical analysis (dma) we show that fe3+ ions only induce limited improvement in the mechanical properties of the graphene oxide paper, while the efficient cross linking of neighboring sheets by fe3+\u2013ta complex network can significantly improve the fracture strength and young's modulus of graphene oxide paper by 150% and 521%, respectively, with an optimal content of 5 7 wt% fe3+ with general surface binding affinity, ta molecules can be adsorbed to go sheets and provide binding sites for fe3+ the fe3+\u2013ta coordinated compound serves as the \u201cmortar\u201d to stick the go \u201cbricks\u201d together the mechanical properties of our paper can be simply varied by controlling the cross linking condition the obtained nacre like ultrastrong go papers could find potential in energy and sustainability applications \" [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R254",
                "label": "Materials Science and Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R189421",
            "template_id": null,
            "paper_id": "R189421",
            "premise": null,
            "hypothesis": "bio inspired borate cross linking in ultra stiff graphene oxide thin films adjacent graphene oxide nanosheets in a thin film structure have been covalently cross linked in a fashion similar to the cell walls of higher order plants the resulting ultra stiff structure exhibits a maximum storage modulus of 127 gpa that can be tuned by varying borate concentration",
            "sequence": "[CLS] None [SEP] bio inspired borate cross linking in ultra stiff graphene oxide thin films adjacent graphene oxide nanosheets in a thin film structure have been covalently cross linked in a fashion similar to the cell walls of higher order plants the resulting ultra stiff structure exhibits a maximum storage modulus of 127 gpa that can be tuned by varying borate concentration [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R254",
                "label": "Materials Science and Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R189423",
            "template_id": null,
            "paper_id": "R189423",
            "premise": null,
            "hypothesis": "graphene oxide sheets chemically cross linked by polyallylamine we report that a homogeneous aqueous colloidal suspension of chemically cross linked graphene oxide sheets was generated by addition of polyallylamine to an aqueous suspension of graphene oxide sheets followed by sonication of the mixture this is the first example for producing a homogeneous colloidal suspension of cross linked graphene sheets as a demonstration of the utility of such a colloidal suspension, \u201cpaper\u201d material samples made by simple filtration from such a suspension of cross linked graphene oxide sheets showed excellent mechanical stiffness and strength",
            "sequence": "[CLS] None [SEP] graphene oxide sheets chemically cross linked by polyallylamine we report that a homogeneous aqueous colloidal suspension of chemically cross linked graphene oxide sheets was generated by addition of polyallylamine to an aqueous suspension of graphene oxide sheets followed by sonication of the mixture this is the first example for producing a homogeneous colloidal suspension of cross linked graphene sheets as a demonstration of the utility of such a colloidal suspension, \u201cpaper\u201d material samples made by simple filtration from such a suspension of cross linked graphene oxide sheets showed excellent mechanical stiffness and strength [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R254",
                "label": "Materials Science and Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R189426",
            "template_id": null,
            "paper_id": "R189426",
            "premise": null,
            "hypothesis": "high nanofiller content graphene oxide polymer nanocomposites via vacuum assisted self assembly highly ordered, homogeneous polymer nanocomposites of layered graphene oxide are prepared using a vacuum\u2010assisted self\u2010assembly (vasa) technique in vasa, all components (nanofiller and polymer) are pre\u2010mixed prior to assembly under a flow, making it compatible with either hydrophilic poly(vinyl alcohol) (pva) or hydrophobic poly(methyl methacrylate) (pmma) for the preparation of composites with over 50 wt% filler this process is complimentary to layer\u2010by\u2010layer assembly, where the assembling components are required to interact strongly (e g , via coulombic attraction) the nanosheets within the vasa\u2010assembled composites exhibit a high degree of order with tunable intersheet spacing, depending on the polymer content graphene oxide\u2013pva nanocomposites, prepared from water, exhibit greatly improved modulus values in comparison to films of either pure pva or pure graphene oxide modulus values for graphene oxide\u2013pmma nanocomposites, prepared from dimethylformamide, are intermediate to those of the pure components the differences in structure, modulus, and strength can be attributed to the gallery composition, specifically the hydrogen bonding ability of the intercalating species",
            "sequence": "[CLS] None [SEP] high nanofiller content graphene oxide polymer nanocomposites via vacuum assisted self assembly highly ordered, homogeneous polymer nanocomposites of layered graphene oxide are prepared using a vacuum\u2010assisted self\u2010assembly (vasa) technique in vasa, all components (nanofiller and polymer) are pre\u2010mixed prior to assembly under a flow, making it compatible with either hydrophilic poly(vinyl alcohol) (pva) or hydrophobic poly(methyl methacrylate) (pmma) for the preparation of composites with over 50 wt% filler this process is complimentary to layer\u2010by\u2010layer assembly, where the assembling components are required to interact strongly (e g , via coulombic attraction) the nanosheets within the vasa\u2010assembled composites exhibit a high degree of order with tunable intersheet spacing, depending on the polymer content graphene oxide\u2013pva nanocomposites, prepared from water, exhibit greatly improved modulus values in comparison to films of either pure pva or pure graphene oxide modulus values for graphene oxide\u2013pmma nanocomposites, prepared from dimethylformamide, are intermediate to those of the pure components the differences in structure, modulus, and strength can be attributed to the gallery composition, specifically the hydrogen bonding ability of the intercalating species [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R254",
                "label": "Materials Science and Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R189429",
            "template_id": null,
            "paper_id": "R189429",
            "premise": null,
            "hypothesis": "the effect of interlayer adhesion on the mechanical behaviors of macroscopic graphene oxide papers \"high mechanical performances of macroscopic graphene oxide (go) papers are attracting great interest owing to their merits of lightweight and multiple functionalities however, the loading role of individual nanosheets and its effect on the mechanical properties of the macroscopic go papers are not yet well understood herein, we effectively tailored the interlayer adhesions of the go papers by introducing small molecules, that is, glutaraldehyde (ga) and water molecules, into the gallery regions with the help of in situ raman spectroscopy, we compared the varied load reinforcing roles of nanosheets, and further predicted the young's moduli of the go papers systematic mechanical tests have proven that the enhancement of the tensile modulus and strength of the ga treated go paper arose from the improved load bearing capability of the nanosheets on the basis of raman and macroscopic mechanical tests, the influences of interlayer adhesions on the fracture mechanisms of the strained go papers were inferred \"",
            "sequence": "[CLS] None [SEP] the effect of interlayer adhesion on the mechanical behaviors of macroscopic graphene oxide papers \"high mechanical performances of macroscopic graphene oxide (go) papers are attracting great interest owing to their merits of lightweight and multiple functionalities however, the loading role of individual nanosheets and its effect on the mechanical properties of the macroscopic go papers are not yet well understood herein, we effectively tailored the interlayer adhesions of the go papers by introducing small molecules, that is, glutaraldehyde (ga) and water molecules, into the gallery regions with the help of in situ raman spectroscopy, we compared the varied load reinforcing roles of nanosheets, and further predicted the young's moduli of the go papers systematic mechanical tests have proven that the enhancement of the tensile modulus and strength of the ga treated go paper arose from the improved load bearing capability of the nanosheets on the basis of raman and macroscopic mechanical tests, the influences of interlayer adhesions on the fracture mechanisms of the strained go papers were inferred \" [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R254",
                "label": "Materials Science and Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R189554",
            "template_id": null,
            "paper_id": "R189554",
            "premise": null,
            "hypothesis": "a constrained assembly strategy for high strength natural nanoclay film developing high performance materials from existing natural materials is highly desired because of their environmental friendliness and low cost; two dimensional nanoclay exfoliated from layered silicate minerals is a good building block to construct multilayered macroscopic assemblies for achieving high mechanical and functional properties nevertheless, the efforts have been frustrated by insufficient inter nanosheet stress transfer and nanosheet misalignment caused by capillary force during solution based spontaneous assembly, degrading the mechanical strength of clay based materials herein, a constrained assembly strategy that is implemented by in plane stretching a robust water containing nanoclay network with hydrogen and ionic bonding is developed to adjust the 2d topography of nanosheets within multilayered nanoclay film in plane stretching overcomes capillary force during water removal and thus restrains nanosheet conformation transition from nearly flat to wrinkled, leading to a highly aligned multilayered nanostructure with synergistic hydrogen and ionic bonding it is proved that inter nanosheet hydrogen and ionic bonding and nanosheet conformation extension generate profound mechanical reinforcement the tensile strength and modulus of natural nanoclay film reach up to 429 0 mpa and 43 8 gpa and surpass the counterparts fabricated by normal spontaneous assembly additionally, improved heat insulation function and good nonflammability are shown for the natural nanoclay film and extend its potential for realistic uses",
            "sequence": "[CLS] None [SEP] a constrained assembly strategy for high strength natural nanoclay film developing high performance materials from existing natural materials is highly desired because of their environmental friendliness and low cost; two dimensional nanoclay exfoliated from layered silicate minerals is a good building block to construct multilayered macroscopic assemblies for achieving high mechanical and functional properties nevertheless, the efforts have been frustrated by insufficient inter nanosheet stress transfer and nanosheet misalignment caused by capillary force during solution based spontaneous assembly, degrading the mechanical strength of clay based materials herein, a constrained assembly strategy that is implemented by in plane stretching a robust water containing nanoclay network with hydrogen and ionic bonding is developed to adjust the 2d topography of nanosheets within multilayered nanoclay film in plane stretching overcomes capillary force during water removal and thus restrains nanosheet conformation transition from nearly flat to wrinkled, leading to a highly aligned multilayered nanostructure with synergistic hydrogen and ionic bonding it is proved that inter nanosheet hydrogen and ionic bonding and nanosheet conformation extension generate profound mechanical reinforcement the tensile strength and modulus of natural nanoclay film reach up to 429 0 mpa and 43 8 gpa and surpass the counterparts fabricated by normal spontaneous assembly additionally, improved heat insulation function and good nonflammability are shown for the natural nanoclay film and extend its potential for realistic uses [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R254",
                "label": "Materials Science and Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R189565",
            "template_id": null,
            "paper_id": "R189565",
            "premise": null,
            "hypothesis": "facile access to large scale, self assembled, nacre inspired, high performance materials with tunable nanoscale periodicities although advances have been reported to mimic the mechanically excellent structure of natural nacre, larger scale applications are still limited due to time and energy intensive preparation pathways herein, we demonstrate that simple high shear homogenization of dispersions containing biobased high molecular weight sodium carboxymethyl cellulose (700 kg/mol, cmc) and natural sodium montmorillonite (mtm), serving as the soft energy dissipating phase and reinforcing platelets, respectively, can be used to prepare large area and thick films with well aligned hard/soft nacre mimetic mesostructure during this process, core shell nanoplatelets with intrinsic hard/soft structure form, which then self assemble into a layered nanocomposite during water removal the nanoscale periodicities of the alternating hard/soft layers can be precisely tuned by changing the ratio of cmc to mtm, which allows studying the evolution of mechanical properties as a function of the lamellar nanoscale periodicity and fractions of hard to soft material remarkable mechanical stiffness (25 gpa) and strength (320 mpa) can be obtained placing these materials among the top end of nacre inspired materials reported so far mechanical homogenization also allows direct preparation of concentrated, yet homogeneous, gel like dispersions of high nanoclay content, suited to doctor blade large area and thick films with essentially the same properties as films cast from dilute dispersions in terms of functional properties, we report high transparency, shape persistent fire blocking and the ability to surface pattern via inkjet printing considering the simple, fully scalable, waterborne preparation pathway, and the use of nature based components, we foresee applications as ecofriendly, bioinspired materials to promote sustainable engineering materials and novel types of functional barrier coatings and substrates",
            "sequence": "[CLS] None [SEP] facile access to large scale, self assembled, nacre inspired, high performance materials with tunable nanoscale periodicities although advances have been reported to mimic the mechanically excellent structure of natural nacre, larger scale applications are still limited due to time and energy intensive preparation pathways herein, we demonstrate that simple high shear homogenization of dispersions containing biobased high molecular weight sodium carboxymethyl cellulose (700 kg/mol, cmc) and natural sodium montmorillonite (mtm), serving as the soft energy dissipating phase and reinforcing platelets, respectively, can be used to prepare large area and thick films with well aligned hard/soft nacre mimetic mesostructure during this process, core shell nanoplatelets with intrinsic hard/soft structure form, which then self assemble into a layered nanocomposite during water removal the nanoscale periodicities of the alternating hard/soft layers can be precisely tuned by changing the ratio of cmc to mtm, which allows studying the evolution of mechanical properties as a function of the lamellar nanoscale periodicity and fractions of hard to soft material remarkable mechanical stiffness (25 gpa) and strength (320 mpa) can be obtained placing these materials among the top end of nacre inspired materials reported so far mechanical homogenization also allows direct preparation of concentrated, yet homogeneous, gel like dispersions of high nanoclay content, suited to doctor blade large area and thick films with essentially the same properties as films cast from dilute dispersions in terms of functional properties, we report high transparency, shape persistent fire blocking and the ability to surface pattern via inkjet printing considering the simple, fully scalable, waterborne preparation pathway, and the use of nature based components, we foresee applications as ecofriendly, bioinspired materials to promote sustainable engineering materials and novel types of functional barrier coatings and substrates [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R254",
                "label": "Materials Science and Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R189567",
            "template_id": null,
            "paper_id": "R189567",
            "premise": null,
            "hypothesis": "synergistic toughening of bioinspired poly(vinyl alcohol)\u2013clay\u2013nanofibrillar cellulose artificial nacre inspired by the layered aragonite platelet/nanofibrillar chitin/protein ternary structure and integration of extraordinary strength and toughness of natural nacre, artificial nacre based on clay platelet/nanofibrillar cellulose/poly(vinyl alcohol) is constructed through an evaporation induced self assembly technique the synergistic toughening effect from clay platelets and nanofibrillar cellulose is successfully demonstrated the artificial nacre achieves an excellent balance of strength and toughness and a fatigue resistant property, superior to natural nacre and other conventional layered clay/polymer binary composites",
            "sequence": "[CLS] None [SEP] synergistic toughening of bioinspired poly(vinyl alcohol)\u2013clay\u2013nanofibrillar cellulose artificial nacre inspired by the layered aragonite platelet/nanofibrillar chitin/protein ternary structure and integration of extraordinary strength and toughness of natural nacre, artificial nacre based on clay platelet/nanofibrillar cellulose/poly(vinyl alcohol) is constructed through an evaporation induced self assembly technique the synergistic toughening effect from clay platelets and nanofibrillar cellulose is successfully demonstrated the artificial nacre achieves an excellent balance of strength and toughness and a fatigue resistant property, superior to natural nacre and other conventional layered clay/polymer binary composites [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R254",
                "label": "Materials Science and Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R189569",
            "template_id": null,
            "paper_id": "R189569",
            "premise": null,
            "hypothesis": "hierarchical nacre mimetics with synergistic mechanical properties by control of molecular interactions in self healing polymers designing the reversible interactions of biopolymers remains a grand challenge for an integral mimicry of mechanically superior biological composites yet, they are the key to synergistic combinations of stiffness and toughness by providing sacrificial bonds with hidden length scales to address this challenge, dynamic polymers were designed with low glass transition temperature t(g) and bonded by quadruple hydrogen bonding motifs, and subsequently assembled with high aspect ratio synthetic nanoclays to generate nacre mimetic films the high dynamics and self healing of the polymers render transparent films with a near perfectly aligned structure varying the polymer composition allows molecular control over the mechanical properties up to very stiff and very strong films (e\u224845\\u2005gpa, \u03c3(uts)\u2248270\\u2005mpa) stable crack propagation and multiple toughening mechanisms occur in situations of balanced dynamics, enabling synergistic combinations of stiffness and toughness excellent gas barrier properties complement the multifunctional property profile",
            "sequence": "[CLS] None [SEP] hierarchical nacre mimetics with synergistic mechanical properties by control of molecular interactions in self healing polymers designing the reversible interactions of biopolymers remains a grand challenge for an integral mimicry of mechanically superior biological composites yet, they are the key to synergistic combinations of stiffness and toughness by providing sacrificial bonds with hidden length scales to address this challenge, dynamic polymers were designed with low glass transition temperature t(g) and bonded by quadruple hydrogen bonding motifs, and subsequently assembled with high aspect ratio synthetic nanoclays to generate nacre mimetic films the high dynamics and self healing of the polymers render transparent films with a near perfectly aligned structure varying the polymer composition allows molecular control over the mechanical properties up to very stiff and very strong films (e\u224845\\u2005gpa, \u03c3(uts)\u2248270\\u2005mpa) stable crack propagation and multiple toughening mechanisms occur in situations of balanced dynamics, enabling synergistic combinations of stiffness and toughness excellent gas barrier properties complement the multifunctional property profile [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R254",
                "label": "Materials Science and Engineering"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R178358",
            "template_id": null,
            "paper_id": "R178358",
            "premise": null,
            "hypothesis": "large scale self assembly of smectic nanocomposite films by doctor blading versus spray coating: impact of crystal quality on barrier properties flexible transparent barrier films are required in various fields of application ranging from flexible, transparent food packaging to display encapsulation environmentally friendly, waterborne polymer\u2013clay nanocomposites would be preferred but fail to meet in particular requirements for ultra high water vapor barriers here we show that self assembly of nanocomposite films into one dimensional crystalline (smectic) polymer\u2013clay domains is a so far overlooked key factor capable of suppressing water vapor diffusivity despite appreciable swelling at elevated temperatures and relative humidity (r h ) moreover, barrier performance was shown to improve with quality of the crystalline order in this respect, spray coating is superior to doctor blading because it yields significantly better ordered structures for spray coated waterborne nanocomposite films (21 4 \u03bcm) ultra high barrier specifications are met at 23 \u00b0c and 50% r h with oxygen transmission rates (otr) < 0 0005 cm3 m\u20132 day\u20131 bar\u20131 and water vapor",
            "sequence": "[CLS] None [SEP] large scale self assembly of smectic nanocomposite films by doctor blading versus spray coating: impact of crystal quality on barrier properties flexible transparent barrier films are required in various fields of application ranging from flexible, transparent food packaging to display encapsulation environmentally friendly, waterborne polymer\u2013clay nanocomposites would be preferred but fail to meet in particular requirements for ultra high water vapor barriers here we show that self assembly of nanocomposite films into one dimensional crystalline (smectic) polymer\u2013clay domains is a so far overlooked key factor capable of suppressing water vapor diffusivity despite appreciable swelling at elevated temperatures and relative humidity (r h ) moreover, barrier performance was shown to improve with quality of the crystalline order in this respect, spray coating is superior to doctor blading because it yields significantly better ordered structures for spray coated waterborne nanocomposite films (21 4 \u03bcm) ultra high barrier specifications are met at 23 \u00b0c and 50% r h with oxygen transmission rates (otr) < 0 0005 cm3 m\u20132 day\u20131 bar\u20131 and water vapor [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R137665",
                "label": "Coating and Surface Technology"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R178362",
            "template_id": null,
            "paper_id": "R178362",
            "premise": null,
            "hypothesis": "clay based nanocomposite coating for flexible optoelectronics applying commercial polymers transparency, flexibility, and especially ultralow oxygen (otr) and water vapor (wvtr) transmission rates are the key issues to be addressed for packaging of flexible organic photovoltaics and organic light emitting diodes concomitant optimization of all essential features is still a big challenge here we present a thin (1 5 \u03bcm), highly transparent, and at the same time flexible nanocomposite coating with an exceptionally low otr and wvtr (1 0 \u00d7 10( 2) cm(3) m( 2) day( 1) bar( 1) and <0 05 g m( 2) day( 1) at 50% rh, respectively) a commercially available polyurethane (desmodur n 3600 and desmophen 670 ba, bayer materialscience ag) was filled with a delaminated synthetic layered silicate exhibiting huge aspect ratios of about 25,000 functional films were prepared by simple doctor blading a suspension of the matrix and the organophilized clay this preparation procedure is technically benign, is easy to scale up, and may readily be applied for encapsulation of sensitive flexible electronics",
            "sequence": "[CLS] None [SEP] clay based nanocomposite coating for flexible optoelectronics applying commercial polymers transparency, flexibility, and especially ultralow oxygen (otr) and water vapor (wvtr) transmission rates are the key issues to be addressed for packaging of flexible organic photovoltaics and organic light emitting diodes concomitant optimization of all essential features is still a big challenge here we present a thin (1 5 \u03bcm), highly transparent, and at the same time flexible nanocomposite coating with an exceptionally low otr and wvtr (1 0 \u00d7 10( 2) cm(3) m( 2) day( 1) bar( 1) and <0 05 g m( 2) day( 1) at 50% rh, respectively) a commercially available polyurethane (desmodur n 3600 and desmophen 670 ba, bayer materialscience ag) was filled with a delaminated synthetic layered silicate exhibiting huge aspect ratios of about 25,000 functional films were prepared by simple doctor blading a suspension of the matrix and the organophilized clay this preparation procedure is technically benign, is easy to scale up, and may readily be applied for encapsulation of sensitive flexible electronics [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R137665",
                "label": "Coating and Surface Technology"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R12223",
            "template_id": null,
            "paper_id": "R12223",
            "premise": null,
            "hypothesis": "modelling the epidemic trend of the 2019 novel coronavirus outbreak in china we present a timely evaluation of the chinese 2019 ncov epidemic in its initial phase, where 2019 ncov demonstrates comparable transmissibility but lower fatality rates than sars and mers a quick diagnosis that leads to case isolation and integrated interventions will have a major impact on its future trend nevertheless, as china is facing its spring festival travel rush and the epidemic has spread beyond its borders, further investigation on its potential spatiotemporal transmission pattern and novel intervention strategies are warranted",
            "sequence": "[CLS] None [SEP] modelling the epidemic trend of the 2019 novel coronavirus outbreak in china we present a timely evaluation of the chinese 2019 ncov epidemic in its initial phase, where 2019 ncov demonstrates comparable transmissibility but lower fatality rates than sars and mers a quick diagnosis that leads to case isolation and integrated interventions will have a major impact on its future trend nevertheless, as china is facing its spring festival travel rush and the epidemic has spread beyond its borders, further investigation on its potential spatiotemporal transmission pattern and novel intervention strategies are warranted [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R12226",
            "template_id": null,
            "paper_id": "R12226",
            "premise": null,
            "hypothesis": "time varying transmission dynamics of novel coronavirus pneumonia in china abstract rationale several studies have estimated basic production number of novel coronavirus pneumonia (ncp) however, the time varying transmission dynamics of ncp during the outbreak remain unclear objectives we aimed to estimate the basic and time varying transmission dynamics of ncp across china, and compared them with sars methods data on ncp cases by february 7, 2020 were collected from epidemiological investigations or official websites data on severe acute respiratory syndrome (sars) cases in guangdong province, beijing and hong kong during 2002 2003 were also obtained we estimated the doubling time, basic reproduction number ( r 0 ) and time varying reproduction number ( r t ) of ncp and sars measurements and main results as of february 7, 2020, 34,598 ncp cases were identified in china, and daily confirmed cases decreased after february 4 the doubling time of ncp nationwide was 2 4 days which was shorter than that of sars in guangdong (14 3 days), hong kong (5 7 days) and beijing (12 4 days) the r 0 of ncp cases nationwide and in wuhan were 4 5 and 4 4 respectively, which were higher than r 0 of sars in guangdong ( r 0 =2 3), hongkong ( r 0 =2 3), and beijing ( r 0 =2 6) the r t for ncp continuously decreased especially after january 16 nationwide and in wuhan the r 0 for secondary ncp cases in guangdong was 0 6, and the r t values were less than 1 during the epidemic conclusions ncp may have a higher transmissibility than sars, and the efforts of containing the outbreak are effective however, the efforts are needed to persist in for reducing time varying reproduction number below one at a glance commentary scientific knowledge on the subject since december 29, 2019, pneumonia infection with 2019 ncov, now named as novel coronavirus pneumonia (ncp), occurred in wuhan, hubei province, china the disease has rapidly spread from wuhan to other areas as a novel virus, the time varying transmission dynamics of ncp remain unclear, and it is also important to compare it with sars what this study adds to the field we compared the transmission dynamics of ncp with sars, and found that ncp has a higher transmissibility than sars time varying production number indicates that rigorous control measures taken by governments are effective across china, and persistent efforts are needed to be taken for reducing instantaneous reproduction number below one",
            "sequence": "[CLS] None [SEP] time varying transmission dynamics of novel coronavirus pneumonia in china abstract rationale several studies have estimated basic production number of novel coronavirus pneumonia (ncp) however, the time varying transmission dynamics of ncp during the outbreak remain unclear objectives we aimed to estimate the basic and time varying transmission dynamics of ncp across china, and compared them with sars methods data on ncp cases by february 7, 2020 were collected from epidemiological investigations or official websites data on severe acute respiratory syndrome (sars) cases in guangdong province, beijing and hong kong during 2002 2003 were also obtained we estimated the doubling time, basic reproduction number ( r 0 ) and time varying reproduction number ( r t ) of ncp and sars measurements and main results as of february 7, 2020, 34,598 ncp cases were identified in china, and daily confirmed cases decreased after february 4 the doubling time of ncp nationwide was 2 4 days which was shorter than that of sars in guangdong (14 3 days), hong kong (5 7 days) and beijing (12 4 days) the r 0 of ncp cases nationwide and in wuhan were 4 5 and 4 4 respectively, which were higher than r 0 of sars in guangdong ( r 0 =2 3), hongkong ( r 0 =2 3), and beijing ( r 0 =2 6) the r t for ncp continuously decreased especially after january 16 nationwide and in wuhan the r 0 for secondary ncp cases in guangdong was 0 6, and the r t values were less than 1 during the epidemic conclusions ncp may have a higher transmissibility than sars, and the efforts of containing the outbreak are effective however, the efforts are needed to persist in for reducing time varying reproduction number below one at a glance commentary scientific knowledge on the subject since december 29, 2019, pneumonia infection with 2019 ncov, now named as novel coronavirus pneumonia (ncp), occurred in wuhan, hubei province, china the disease has rapidly spread from wuhan to other areas as a novel virus, the time varying transmission dynamics of ncp remain unclear, and it is also important to compare it with sars what this study adds to the field we compared the transmission dynamics of ncp with sars, and found that ncp has a higher transmissibility than sars time varying production number indicates that rigorous control measures taken by governments are effective across china, and persistent efforts are needed to be taken for reducing instantaneous reproduction number below one [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R12231",
            "template_id": null,
            "paper_id": "R12231",
            "premise": null,
            "hypothesis": "novel coronavirus 2019 ncov: early estimation of epidemiological parameters and epidemic predictions abstract since first identified, the epidemic scale of the recently emerged novel coronavirus (2019 ncov) in wuhan, china, has increased rapidly, with cases arising across china and other countries and regions using a transmission model, we estimate a basic reproductive number of 3 11 (95%ci, 2 39\u20134 13); 58\u201376% of transmissions must be prevented to stop increasing; wuhan case ascertainment of 5 0% (3 6\u20137 4); 21022 (11090\u201333490) total infections in wuhan 1 to 22 january changes to previous version case data updated to include 22 jan 2020; we did not use cases reported after this period as cases were reported at the province level hereafter, and large scale control interventions were initiated on 23 jan 2020; improved likelihood function, better accounting for first 41 confirmed cases, and now using all infections (rather than just cases detected) in wuhan for prediction of infection in international travellers; improved characterization of uncertainty in parameters, and calculation of epidemic trajectory confidence intervals using a more statistically rigorous method; extended range of latent period in sensitivity analysis to reflect reports of up to 6 day incubation period in household clusters; removed travel restriction analysis, as different modelling approaches (e g stochastic transmission, rather than deterministic transmission) are more appropriate to such analyses",
            "sequence": "[CLS] None [SEP] novel coronavirus 2019 ncov: early estimation of epidemiological parameters and epidemic predictions abstract since first identified, the epidemic scale of the recently emerged novel coronavirus (2019 ncov) in wuhan, china, has increased rapidly, with cases arising across china and other countries and regions using a transmission model, we estimate a basic reproductive number of 3 11 (95%ci, 2 39\u20134 13); 58\u201376% of transmissions must be prevented to stop increasing; wuhan case ascertainment of 5 0% (3 6\u20137 4); 21022 (11090\u201333490) total infections in wuhan 1 to 22 january changes to previous version case data updated to include 22 jan 2020; we did not use cases reported after this period as cases were reported at the province level hereafter, and large scale control interventions were initiated on 23 jan 2020; improved likelihood function, better accounting for first 41 confirmed cases, and now using all infections (rather than just cases detected) in wuhan for prediction of infection in international travellers; improved characterization of uncertainty in parameters, and calculation of epidemic trajectory confidence intervals using a more statistically rigorous method; extended range of latent period in sensitivity analysis to reflect reports of up to 6 day incubation period in household clusters; removed travel restriction analysis, as different modelling approaches (e g stochastic transmission, rather than deterministic transmission) are more appropriate to such analyses [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R12233",
            "template_id": null,
            "paper_id": "R12233",
            "premise": null,
            "hypothesis": "early transmissibility assessment of a novel coronavirus in wuhan between december 1, 2019 and january 26, 2020, nearly 3000 cases of respiratory illness caused by a novel coronavirus originating in wuhan, china have been reported in this short analysis, we combine publicly available cumulative case data from the ongoing outbreak with phenomenological modeling methods to conduct an early transmissibility assessment our model suggests that the basic reproduction number associated with the outbreak (at time of writing) may range from 2 0 to 3 1 though these estimates are preliminary and subject to change, they are consistent with previous findings regarding the transmissibility of the related sars coronavirus and indicate the possibility of epidemic potential",
            "sequence": "[CLS] None [SEP] early transmissibility assessment of a novel coronavirus in wuhan between december 1, 2019 and january 26, 2020, nearly 3000 cases of respiratory illness caused by a novel coronavirus originating in wuhan, china have been reported in this short analysis, we combine publicly available cumulative case data from the ongoing outbreak with phenomenological modeling methods to conduct an early transmissibility assessment our model suggests that the basic reproduction number associated with the outbreak (at time of writing) may range from 2 0 to 3 1 though these estimates are preliminary and subject to change, they are consistent with previous findings regarding the transmissibility of the related sars coronavirus and indicate the possibility of epidemic potential [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R12235",
            "template_id": null,
            "paper_id": "R12235",
            "premise": null,
            "hypothesis": "estimating the effective reproduction number of the 2019 ncov in china abstract we estimate the effective reproduction number for 2019 ncov based on the daily reported cases from china cdc the results indicate that 2019 ncov has a higher effective reproduction number than sars with a comparable fatality rate article summary line this modeling study indicates that 2019 ncov has a higher effective reproduction number than sars with a comparable fatality rate",
            "sequence": "[CLS] None [SEP] estimating the effective reproduction number of the 2019 ncov in china abstract we estimate the effective reproduction number for 2019 ncov based on the daily reported cases from china cdc the results indicate that 2019 ncov has a higher effective reproduction number than sars with a comparable fatality rate article summary line this modeling study indicates that 2019 ncov has a higher effective reproduction number than sars with a comparable fatality rate [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R12237",
            "template_id": null,
            "paper_id": "R12237",
            "premise": null,
            "hypothesis": "preliminary estimation of the basic reproduction number of novel coronavirus (2019 ncov) in china, from 2019 to 2020: a data driven analysis in the early phase of the outbreak abstract backgrounds an ongoing outbreak of a novel coronavirus (2019 ncov) pneumonia hit a major city of china, wuhan, december 2019 and subsequently reached other provinces/regions of china and countries we present estimates of the basic reproduction number, r 0 , of 2019 ncov in the early phase of the outbreak methods accounting for the impact of the variations in disease reporting rate, we modelled the epidemic curve of 2019 ncov cases time series, in mainland china from january 10 to january 24, 2020, through the exponential growth with the estimated intrinsic growth rate ( \u03b3 ), we estimated r 0 by using the serial intervals (si) of two other well known coronavirus diseases, mers and sars, as approximations for the true unknown si findings the early outbreak data largely follows the exponential growth we estimated that the mean r 0 ranges from 2 24 (95%ci: 1 96 2 55) to 3 58 (95%ci: 2 89 4 39) associated with 8 fold to 2 fold increase in the reporting rate we demonstrated that changes in reporting rate substantially affect estimates of r 0 conclusion the mean estimate of r 0 for the 2019 ncov ranges from 2 24 to 3 58, and significantly larger than 1 our findings indicate the potential of 2019 ncov to cause outbreaks",
            "sequence": "[CLS] None [SEP] preliminary estimation of the basic reproduction number of novel coronavirus (2019 ncov) in china, from 2019 to 2020: a data driven analysis in the early phase of the outbreak abstract backgrounds an ongoing outbreak of a novel coronavirus (2019 ncov) pneumonia hit a major city of china, wuhan, december 2019 and subsequently reached other provinces/regions of china and countries we present estimates of the basic reproduction number, r 0 , of 2019 ncov in the early phase of the outbreak methods accounting for the impact of the variations in disease reporting rate, we modelled the epidemic curve of 2019 ncov cases time series, in mainland china from january 10 to january 24, 2020, through the exponential growth with the estimated intrinsic growth rate ( \u03b3 ), we estimated r 0 by using the serial intervals (si) of two other well known coronavirus diseases, mers and sars, as approximations for the true unknown si findings the early outbreak data largely follows the exponential growth we estimated that the mean r 0 ranges from 2 24 (95%ci: 1 96 2 55) to 3 58 (95%ci: 2 89 4 39) associated with 8 fold to 2 fold increase in the reporting rate we demonstrated that changes in reporting rate substantially affect estimates of r 0 conclusion the mean estimate of r 0 for the 2019 ncov ranges from 2 24 to 3 58, and significantly larger than 1 our findings indicate the potential of 2019 ncov to cause outbreaks [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R12243",
            "template_id": null,
            "paper_id": "R12243",
            "premise": null,
            "hypothesis": "pattern of early human to human transmission of wuhan 2019 ncov abstract on december 31, 2019, the world health organization was notified about a cluster of pneumonia of unknown aetiology in the city of wuhan, china chinese authorities later identified a new coronavirus (2019 ncov) as the causative agent of the outbreak as of january 23, 2020, 655 cases have been confirmed in china and several other countries understanding the transmission characteristics and the potential for sustained human to human transmission of 2019 ncov is critically important for coordinating current screening and containment strategies, and determining whether the outbreak constitutes a public health emergency of international concern (pheic) we performed stochastic simulations of early outbreak trajectories that are consistent with the epidemiological findings to date we found the basic reproduction number, r 0 , to be around 2 2 (90% high density interval 1 4\u20143 8), indicating the potential for sustained human to human transmission transmission characteristics appear to be of a similar magnitude to severe acute respiratory syndrome related coronavirus (sars cov) and the 1918 pandemic influenza these findings underline the importance of heightened screening, surveillance and control efforts, particularly at airports and other travel hubs, in order to prevent further international spread of 2019 ncov",
            "sequence": "[CLS] None [SEP] pattern of early human to human transmission of wuhan 2019 ncov abstract on december 31, 2019, the world health organization was notified about a cluster of pneumonia of unknown aetiology in the city of wuhan, china chinese authorities later identified a new coronavirus (2019 ncov) as the causative agent of the outbreak as of january 23, 2020, 655 cases have been confirmed in china and several other countries understanding the transmission characteristics and the potential for sustained human to human transmission of 2019 ncov is critically important for coordinating current screening and containment strategies, and determining whether the outbreak constitutes a public health emergency of international concern (pheic) we performed stochastic simulations of early outbreak trajectories that are consistent with the epidemiological findings to date we found the basic reproduction number, r 0 , to be around 2 2 (90% high density interval 1 4\u20143 8), indicating the potential for sustained human to human transmission transmission characteristics appear to be of a similar magnitude to severe acute respiratory syndrome related coronavirus (sars cov) and the 1918 pandemic influenza these findings underline the importance of heightened screening, surveillance and control efforts, particularly at airports and other travel hubs, in order to prevent further international spread of 2019 ncov [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R12245",
            "template_id": null,
            "paper_id": "R12245",
            "premise": null,
            "hypothesis": "estimation of the transmission risk of 2019 ncov and its implication for public health interventions english abstract: background: since the emergence of the first pneumonia cases in wuhan, china, the novel coronavirus (2019 ncov) infection has been quickly spreading out to other provinces and neighbouring countries estimation of the basic reproduction number by means of mathematical modelling can be helpful for determining the potential and severity of an outbreak, and providing critical information for identifying the type of disease interventions and intensity \\r\\n\\r\\nmethods: a deterministic compartmental model was devised based on the clinical progression of the disease, epidemiological status of the individuals, and the intervention measures \\r\\n\\r\\nfindings: the estimation results based on likelihood and model analysis reveal that the control reproduction number may be as high as 6 47 (95% ci 5 71 7 23) sensitivity analyses reveal that interventions, such as intensive contact tracing followed by quarantine and isolation, can effectively reduce the control reproduction number and transmission risk, with the effect of travel restriction of wuhan on 2019 ncov infection in beijing being almost equivalent to increasing quarantine by 100 thousand baseline value \\r\\n\\r\\ninterpretation: it is essential to assess how the expensive, resource intensive measures implemented by the chinese authorities can contribute to the prevention and control of the 2019 ncov infection, and how long should be maintained under the most restrictive measures, the outbreak is expected to peak within two weeks (since january 23rd 2020) with significant low peak value with travel restriction (no imported exposed individuals to beijing), the number of infected individuals in 7 days will decrease by 91 14% in beijing, compared with the scenario of no travel restriction \\r\\n\\r\\nmandarin abstract: \u80cc\u666f\uff1a\u81ea\u4ece\u4e2d\u56fd\u6b66\u6c49\u51fa\u73b0\u7b2c\u4e00\u4f8b\u80ba\u708e\u75c5\u4f8b\u4ee5\u6765\uff0c\u65b0\u578b\u51a0\u72b6\u75c5\u6bd2\uff082019 ncov\uff09\u611f\u67d3\u5df2\u8fc5\u901f\u4f20\u64ad\u5230\u5176\u4ed6\u7701\u4efd\u548c\u5468\u8fb9\u56fd\u5bb6\u3002\u901a\u8fc7\u6570\u5b66\u6a21\u578b\u4f30\u8ba1\u57fa\u672c\u518d\u751f\u6570\uff0c\u6709\u52a9\u4e8e\u786e\u5b9a\u75ab\u60c5\u7206\u53d1\u7684\u53ef\u80fd\u6027\u548c\u4e25\u91cd\u6027\uff0c\u5e76\u4e3a\u786e\u5b9a\u75be\u75c5\u5e72\u9884\u7c7b\u578b\u548c\u5f3a\u5ea6\u63d0\u4f9b\u5173\u952e\u4fe1\u606f\u3002\\r\\n\\r\\n\u65b9\u6cd5\uff1a\u6839\u636e\u75be\u75c5\u7684\u4e34\u5e8a\u8fdb\u5c55\uff0c\u4e2a\u4f53\u7684\u6d41\u884c\u75c5\u5b66\u72b6\u51b5\u548c\u5e72\u9884\u63aa\u65bd\uff0c\u8bbe\u8ba1\u786e\u5b9a\u6027\u7684\u4ed3\u5ba4\u6a21\u578b\u3002\\r\\n\\r\\n\u7ed3\u679c\uff1a\u57fa\u4e8e\u4f3c\u7136\u51fd\u6570\u548c\u6a21\u578b\u5206\u6790\u7684\u4f30\u8ba1\u7ed3\u679c\u8868\u660e\uff0c\u63a7\u5236\u518d\u751f\u6570\u53ef\u80fd\u9ad8\u8fbe6 47\uff0895\uff05ci 5 71 7 23\uff09\u3002\u654f\u611f\u6027\u5206\u6790\u663e\u793a\uff0c\u5bc6\u96c6\u63a5\u89e6\u8ffd\u8e2a\u548c\u9694\u79bb\u7b49\u5e72\u9884\u63aa\u65bd\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u63a7\u5236\u518d\u751f\u6570\u548c\u4f20\u64ad\u98ce\u9669\uff0c\u6b66\u6c49\u5c01\u57ce\u63aa\u65bd\u5bf9\u5317\u4eac2019 ncov\u611f\u67d3\u7684\u5f71\u54cd\u51e0\u4e4e\u7b49\u540c\u4e8e\u589e\u52a0\u9694\u79bb\u63aa\u65bd10\u4e07\u7684\u57fa\u7ebf\u503c\u3002\\r\\n\\r\\n\u89e3\u91ca\uff1a\u5fc5\u987b\u8bc4\u4f30\u4e2d\u56fd\u5f53\u5c40\u5b9e\u65bd\u7684\u6602\u8d35\uff0c\u8d44\u6e90\u5bc6\u96c6\u578b\u63aa\u65bd\u5982\u4f55\u6709\u52a9\u4e8e\u9884\u9632\u548c\u63a7\u52362019 ncov\u611f\u67d3\uff0c\u4ee5\u53ca\u5e94\u7ef4\u6301\u591a\u957f\u65f6\u95f4\u3002\u5728\u6700\u4e25\u683c\u7684\u63aa\u65bd\u4e0b\uff0c\u9884\u8ba1\u75ab\u60c5\u5c06\u5728\u4e24\u5468\u5185\uff08\u81ea2020\u5e741\u670823\u65e5\u8d77\uff09\u8fbe\u5230\u5cf0\u503c\uff0c\u5cf0\u503c\u8f83\u4f4e\u3002\u4e0e\u6ca1\u6709\u51fa\u884c\u9650\u5236\u7684\u60c5\u51b5\u76f8\u6bd4\uff0c\u6709\u4e86\u51fa\u884c\u9650\u5236\uff08\u5373\u6ca1\u6709\u8f93\u5165\u7684\u6f5c\u4f0f\u7c7b\u4e2a\u4f53\u8fdb\u5165\u5317\u4eac\uff09\uff0c\u5317\u4eac\u76847\u5929\u611f\u67d3\u8005\u6570\u91cf\u5c06\u51cf\u5c1191 14\uff05\u3002",
            "sequence": "[CLS] None [SEP] estimation of the transmission risk of 2019 ncov and its implication for public health interventions english abstract: background: since the emergence of the first pneumonia cases in wuhan, china, the novel coronavirus (2019 ncov) infection has been quickly spreading out to other provinces and neighbouring countries estimation of the basic reproduction number by means of mathematical modelling can be helpful for determining the potential and severity of an outbreak, and providing critical information for identifying the type of disease interventions and intensity \\r\\n\\r\\nmethods: a deterministic compartmental model was devised based on the clinical progression of the disease, epidemiological status of the individuals, and the intervention measures \\r\\n\\r\\nfindings: the estimation results based on likelihood and model analysis reveal that the control reproduction number may be as high as 6 47 (95% ci 5 71 7 23) sensitivity analyses reveal that interventions, such as intensive contact tracing followed by quarantine and isolation, can effectively reduce the control reproduction number and transmission risk, with the effect of travel restriction of wuhan on 2019 ncov infection in beijing being almost equivalent to increasing quarantine by 100 thousand baseline value \\r\\n\\r\\ninterpretation: it is essential to assess how the expensive, resource intensive measures implemented by the chinese authorities can contribute to the prevention and control of the 2019 ncov infection, and how long should be maintained under the most restrictive measures, the outbreak is expected to peak within two weeks (since january 23rd 2020) with significant low peak value with travel restriction (no imported exposed individuals to beijing), the number of infected individuals in 7 days will decrease by 91 14% in beijing, compared with the scenario of no travel restriction \\r\\n\\r\\nmandarin abstract: \u80cc\u666f\uff1a\u81ea\u4ece\u4e2d\u56fd\u6b66\u6c49\u51fa\u73b0\u7b2c\u4e00\u4f8b\u80ba\u708e\u75c5\u4f8b\u4ee5\u6765\uff0c\u65b0\u578b\u51a0\u72b6\u75c5\u6bd2\uff082019 ncov\uff09\u611f\u67d3\u5df2\u8fc5\u901f\u4f20\u64ad\u5230\u5176\u4ed6\u7701\u4efd\u548c\u5468\u8fb9\u56fd\u5bb6\u3002\u901a\u8fc7\u6570\u5b66\u6a21\u578b\u4f30\u8ba1\u57fa\u672c\u518d\u751f\u6570\uff0c\u6709\u52a9\u4e8e\u786e\u5b9a\u75ab\u60c5\u7206\u53d1\u7684\u53ef\u80fd\u6027\u548c\u4e25\u91cd\u6027\uff0c\u5e76\u4e3a\u786e\u5b9a\u75be\u75c5\u5e72\u9884\u7c7b\u578b\u548c\u5f3a\u5ea6\u63d0\u4f9b\u5173\u952e\u4fe1\u606f\u3002\\r\\n\\r\\n\u65b9\u6cd5\uff1a\u6839\u636e\u75be\u75c5\u7684\u4e34\u5e8a\u8fdb\u5c55\uff0c\u4e2a\u4f53\u7684\u6d41\u884c\u75c5\u5b66\u72b6\u51b5\u548c\u5e72\u9884\u63aa\u65bd\uff0c\u8bbe\u8ba1\u786e\u5b9a\u6027\u7684\u4ed3\u5ba4\u6a21\u578b\u3002\\r\\n\\r\\n\u7ed3\u679c\uff1a\u57fa\u4e8e\u4f3c\u7136\u51fd\u6570\u548c\u6a21\u578b\u5206\u6790\u7684\u4f30\u8ba1\u7ed3\u679c\u8868\u660e\uff0c\u63a7\u5236\u518d\u751f\u6570\u53ef\u80fd\u9ad8\u8fbe6 47\uff0895\uff05ci 5 71 7 23\uff09\u3002\u654f\u611f\u6027\u5206\u6790\u663e\u793a\uff0c\u5bc6\u96c6\u63a5\u89e6\u8ffd\u8e2a\u548c\u9694\u79bb\u7b49\u5e72\u9884\u63aa\u65bd\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u63a7\u5236\u518d\u751f\u6570\u548c\u4f20\u64ad\u98ce\u9669\uff0c\u6b66\u6c49\u5c01\u57ce\u63aa\u65bd\u5bf9\u5317\u4eac2019 ncov\u611f\u67d3\u7684\u5f71\u54cd\u51e0\u4e4e\u7b49\u540c\u4e8e\u589e\u52a0\u9694\u79bb\u63aa\u65bd10\u4e07\u7684\u57fa\u7ebf\u503c\u3002\\r\\n\\r\\n\u89e3\u91ca\uff1a\u5fc5\u987b\u8bc4\u4f30\u4e2d\u56fd\u5f53\u5c40\u5b9e\u65bd\u7684\u6602\u8d35\uff0c\u8d44\u6e90\u5bc6\u96c6\u578b\u63aa\u65bd\u5982\u4f55\u6709\u52a9\u4e8e\u9884\u9632\u548c\u63a7\u52362019 ncov\u611f\u67d3\uff0c\u4ee5\u53ca\u5e94\u7ef4\u6301\u591a\u957f\u65f6\u95f4\u3002\u5728\u6700\u4e25\u683c\u7684\u63aa\u65bd\u4e0b\uff0c\u9884\u8ba1\u75ab\u60c5\u5c06\u5728\u4e24\u5468\u5185\uff08\u81ea2020\u5e741\u670823\u65e5\u8d77\uff09\u8fbe\u5230\u5cf0\u503c\uff0c\u5cf0\u503c\u8f83\u4f4e\u3002\u4e0e\u6ca1\u6709\u51fa\u884c\u9650\u5236\u7684\u60c5\u51b5\u76f8\u6bd4\uff0c\u6709\u4e86\u51fa\u884c\u9650\u5236\uff08\u5373\u6ca1\u6709\u8f93\u5165\u7684\u6f5c\u4f0f\u7c7b\u4e2a\u4f53\u8fdb\u5165\u5317\u4eac\uff09\uff0c\u5317\u4eac\u76847\u5929\u611f\u67d3\u8005\u6570\u91cf\u5c06\u51cf\u5c1191 14\uff05\u3002 [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R12247",
            "template_id": null,
            "paper_id": "R12247",
            "premise": null,
            "hypothesis": "early transmission dynamics in wuhan, china, of novel coronavirus infected pneumonia abstract background the initial cases of novel coronavirus (2019 ncov)\u2013infected pneumonia (ncip) occurred in wuhan, hubei province, china, in december 2019 and january 2020 we analyzed data on the first 425 confirmed cases in wuhan to determine the epidemiologic characteristics of ncip methods we collected information on demographic characteristics, exposure history, and illness timelines of laboratory confirmed cases of ncip that had been reported by january 22, 2020 we described characteristics of the cases and estimated the key epidemiologic time delay distributions in the early period of exponential growth, we estimated the epidemic doubling time and the basic reproductive number results among the first 425 patients with confirmed ncip, the median age was 59 years and 56% were male the majority of cases (55%) with onset before january 1, 2020, were linked to the huanan seafood wholesale market, as compared with 8 6% of the subsequent cases the mean incubation period was 5 2 days (95% confidence interval [ci], 4 1 to 7 0), with the 95th percentile of the distribution at 12 5 days in its early stages, the epidemic doubled in size every 7 4 days with a mean serial interval of 7 5 days (95% ci, 5 3 to 19), the basic reproductive number was estimated to be 2 2 (95% ci, 1 4 to 3 9) conclusions on the basis of this information, there is evidence that human to human transmission has occurred among close contacts since the middle of december 2019 considerable efforts to reduce transmission will be required to control outbreaks if similar dynamics apply elsewhere measures to prevent or reduce transmission should be implemented in populations at risk (funded by the ministry of science and technology of china and others )",
            "sequence": "[CLS] None [SEP] early transmission dynamics in wuhan, china, of novel coronavirus infected pneumonia abstract background the initial cases of novel coronavirus (2019 ncov)\u2013infected pneumonia (ncip) occurred in wuhan, hubei province, china, in december 2019 and january 2020 we analyzed data on the first 425 confirmed cases in wuhan to determine the epidemiologic characteristics of ncip methods we collected information on demographic characteristics, exposure history, and illness timelines of laboratory confirmed cases of ncip that had been reported by january 22, 2020 we described characteristics of the cases and estimated the key epidemiologic time delay distributions in the early period of exponential growth, we estimated the epidemic doubling time and the basic reproductive number results among the first 425 patients with confirmed ncip, the median age was 59 years and 56% were male the majority of cases (55%) with onset before january 1, 2020, were linked to the huanan seafood wholesale market, as compared with 8 6% of the subsequent cases the mean incubation period was 5 2 days (95% confidence interval [ci], 4 1 to 7 0), with the 95th percentile of the distribution at 12 5 days in its early stages, the epidemic doubled in size every 7 4 days with a mean serial interval of 7 5 days (95% ci, 5 3 to 19), the basic reproductive number was estimated to be 2 2 (95% ci, 1 4 to 3 9) conclusions on the basis of this information, there is evidence that human to human transmission has occurred among close contacts since the middle of december 2019 considerable efforts to reduce transmission will be required to control outbreaks if similar dynamics apply elsewhere measures to prevent or reduce transmission should be implemented in populations at risk (funded by the ministry of science and technology of china and others ) [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R36106",
            "template_id": null,
            "paper_id": "R36106",
            "premise": null,
            "hypothesis": "characterizing the transmission and identifying the control strategy for covid 19 through epidemiological modeling abstract the outbreak of the novel coronavirus disease, covid 19, originating from wuhan, china in early december, has infected more than 70,000 people in china and other countries and has caused more than 2,000 deaths as the disease continues to spread, the biomedical society urgently began identifying effective approaches to prevent further outbreaks through rigorous epidemiological analysis, we characterized the fast transmission of covid 19 with a basic reproductive number 5 6 and proved a sole zoonotic source to originate in wuhan no changes in transmission have been noted across generations by evaluating different control strategies through predictive modeling and monte carlo simulations, a comprehensive quarantine in hospitals and quarantine stations has been found to be the most effective approach government action to immediately enforce this quarantine is highly recommended",
            "sequence": "[CLS] None [SEP] characterizing the transmission and identifying the control strategy for covid 19 through epidemiological modeling abstract the outbreak of the novel coronavirus disease, covid 19, originating from wuhan, china in early december, has infected more than 70,000 people in china and other countries and has caused more than 2,000 deaths as the disease continues to spread, the biomedical society urgently began identifying effective approaches to prevent further outbreaks through rigorous epidemiological analysis, we characterized the fast transmission of covid 19 with a basic reproductive number 5 6 and proved a sole zoonotic source to originate in wuhan no changes in transmission have been noted across generations by evaluating different control strategies through predictive modeling and monte carlo simulations, a comprehensive quarantine in hospitals and quarantine stations has been found to be the most effective approach government action to immediately enforce this quarantine is highly recommended [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R36109",
            "template_id": null,
            "paper_id": "R36109",
            "premise": null,
            "hypothesis": "transmission interval estimates suggest pre symptomatic spread of covid 19 abstract background as the covid 19 epidemic is spreading, incoming data allows us to quantify values of key variables that determine the transmission and the effort required to control the epidemic we determine the incubation period and serial interval distribution for transmission clusters in singapore and in tianjin we infer the basic reproduction number and identify the extent of pre symptomatic transmission methods we collected outbreak information from singapore and tianjin, china, reported from jan 19 feb 26 and jan 21 feb 27, respectively we estimated incubation periods and serial intervals in both populations results the mean incubation period was 7 1 (6 13, 8 25) days for singapore and 9 (7 92, 10 2) days for tianjin both datasets had shorter incubation periods for earlier occurring cases the mean serial interval was 4 56 (2 69, 6 42) days for singapore and 4 22 (3 43, 5 01) for tianjin we inferred that early in the outbreaks, infection was transmitted on average 2 55 and 2 89 days before symptom onset (singapore, tianjin) the estimated basic reproduction number for singapore was 1 97 (1 45, 2 48) secondary cases per infective; for tianjin it was 1 87 (1 65, 2 09) secondary cases per infective conclusions estimated serial intervals are shorter than incubation periods in both singapore and tianjin, suggesting that pre symptomatic transmission is occurring shorter serial intervals lead to lower estimates of r0, which suggest that half of all secondary infections should be prevented to control spread",
            "sequence": "[CLS] None [SEP] transmission interval estimates suggest pre symptomatic spread of covid 19 abstract background as the covid 19 epidemic is spreading, incoming data allows us to quantify values of key variables that determine the transmission and the effort required to control the epidemic we determine the incubation period and serial interval distribution for transmission clusters in singapore and in tianjin we infer the basic reproduction number and identify the extent of pre symptomatic transmission methods we collected outbreak information from singapore and tianjin, china, reported from jan 19 feb 26 and jan 21 feb 27, respectively we estimated incubation periods and serial intervals in both populations results the mean incubation period was 7 1 (6 13, 8 25) days for singapore and 9 (7 92, 10 2) days for tianjin both datasets had shorter incubation periods for earlier occurring cases the mean serial interval was 4 56 (2 69, 6 42) days for singapore and 4 22 (3 43, 5 01) for tianjin we inferred that early in the outbreaks, infection was transmitted on average 2 55 and 2 89 days before symptom onset (singapore, tianjin) the estimated basic reproduction number for singapore was 1 97 (1 45, 2 48) secondary cases per infective; for tianjin it was 1 87 (1 65, 2 09) secondary cases per infective conclusions estimated serial intervals are shorter than incubation periods in both singapore and tianjin, suggesting that pre symptomatic transmission is occurring shorter serial intervals lead to lower estimates of r0, which suggest that half of all secondary infections should be prevented to control spread [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R36114",
            "template_id": null,
            "paper_id": "R36114",
            "premise": null,
            "hypothesis": "estimation of the epidemic properties of the 2019 novel coronavirus: a mathematical modeling study abstract background the 2019 novel coronavirus (covid 19) emerged in wuhan, china in december 2019 and has been spreading rapidly in china decisions about its pandemic threat and the appropriate level of public health response depend heavily on estimates of its basic reproduction number and assessments of interventions conducted in the early stages of the epidemic methods we conducted a mathematical modeling study using five independent methods to assess the basic reproduction number (r0) of covid 19, using data on confirmed cases obtained from the china national health commission for the period 10 th january \u2013 8 th february we analyzed the data for the period before the closure of wuhan city (10 th january \u2013 23 rd january) and the post closure period (23 rd january \u2013 8 th february) and for the whole period, to assess both the epidemic risk of the virus and the effectiveness of the closure of wuhan city on spread of covid 19 findings before the closure of wuhan city the basic reproduction number of covid 19 was 4 38 (95% ci: 3 63 \u2013 5 13), dropping to 3 41 (95% ci: 3 16 \u2013 3 65) after the closure of wuhan city over the entire epidemic period covid 19 had a basic reproduction number of 3 39 (95% ci: 3 09 \u2013 3 70), indicating it has a very high transmissibility interpretation covid 19 is a highly transmissible virus with a very high risk of epidemic outbreak once it emerges in metropolitan areas the closure of wuhan city was effective in reducing the severity of the epidemic, but even after closure of the city and the subsequent expansion of that closure to other parts of hubei the virus remained extremely infectious emergency planners in other cities should consider this high infectiousness when considering responses to this virus funding national natural science foundation of china, china medical board, national science and technology major project of china",
            "sequence": "[CLS] None [SEP] estimation of the epidemic properties of the 2019 novel coronavirus: a mathematical modeling study abstract background the 2019 novel coronavirus (covid 19) emerged in wuhan, china in december 2019 and has been spreading rapidly in china decisions about its pandemic threat and the appropriate level of public health response depend heavily on estimates of its basic reproduction number and assessments of interventions conducted in the early stages of the epidemic methods we conducted a mathematical modeling study using five independent methods to assess the basic reproduction number (r0) of covid 19, using data on confirmed cases obtained from the china national health commission for the period 10 th january \u2013 8 th february we analyzed the data for the period before the closure of wuhan city (10 th january \u2013 23 rd january) and the post closure period (23 rd january \u2013 8 th february) and for the whole period, to assess both the epidemic risk of the virus and the effectiveness of the closure of wuhan city on spread of covid 19 findings before the closure of wuhan city the basic reproduction number of covid 19 was 4 38 (95% ci: 3 63 \u2013 5 13), dropping to 3 41 (95% ci: 3 16 \u2013 3 65) after the closure of wuhan city over the entire epidemic period covid 19 had a basic reproduction number of 3 39 (95% ci: 3 09 \u2013 3 70), indicating it has a very high transmissibility interpretation covid 19 is a highly transmissible virus with a very high risk of epidemic outbreak once it emerges in metropolitan areas the closure of wuhan city was effective in reducing the severity of the epidemic, but even after closure of the city and the subsequent expansion of that closure to other parts of hubei the virus remained extremely infectious emergency planners in other cities should consider this high infectiousness when considering responses to this virus funding national natural science foundation of china, china medical board, national science and technology major project of china [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R36118",
            "template_id": null,
            "paper_id": "R36118",
            "premise": null,
            "hypothesis": "the novel coronavirus, 2019 ncov, is highly contagious and more infectious than initially estimated abstract the novel coronavirus (2019 ncov) is a recently emerged human pathogen that has spread widely since january 2020 initially, the basic reproductive number, r 0 , was estimated to be 2 2 to 2 7 here we provide a new estimate of this quantity we collected extensive individual case reports and estimated key epidemiology parameters, including the incubation period integrating these estimates and high resolution real time human travel and infection data with mathematical models, we estimated that the number of infected individuals during early epidemic double every 2 4 days, and the r 0 value is likely to be between 4 7 and 6 6 we further show that quarantine and contact tracing of symptomatic individuals alone may not be effective and early, strong control measures are needed to stop transmission of the virus one sentence summary by collecting and analyzing spatiotemporal data, we estimated the transmission potential for 2019 ncov",
            "sequence": "[CLS] None [SEP] the novel coronavirus, 2019 ncov, is highly contagious and more infectious than initially estimated abstract the novel coronavirus (2019 ncov) is a recently emerged human pathogen that has spread widely since january 2020 initially, the basic reproductive number, r 0 , was estimated to be 2 2 to 2 7 here we provide a new estimate of this quantity we collected extensive individual case reports and estimated key epidemiology parameters, including the incubation period integrating these estimates and high resolution real time human travel and infection data with mathematical models, we estimated that the number of infected individuals during early epidemic double every 2 4 days, and the r 0 value is likely to be between 4 7 and 6 6 we further show that quarantine and contact tracing of symptomatic individuals alone may not be effective and early, strong control measures are needed to stop transmission of the virus one sentence summary by collecting and analyzing spatiotemporal data, we estimated the transmission potential for 2019 ncov [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R36128",
            "template_id": null,
            "paper_id": "R36128",
            "premise": null,
            "hypothesis": "risk estimation and prediction by modeling the transmission of the novel coronavirus (covid 19) in mainland china excluding hubei province abstract background in december 2019, an outbreak of coronavirus disease (covid 19) was identified in wuhan, china and, later on, detected in other parts of china our aim is to evaluate the effectiveness of the evolution of interventions and self protection measures, estimate the risk of partial lifting control measures and predict the epidemic trend of the virus in mainland china excluding hubei province based on the published data and a novel mathematical model methods a novel covid 19 transmission dynamic model incorporating the intervention measures implemented in china is proposed covid 19 daily data of mainland china excluding hubei province, including the cumulative confirmed cases, the cumulative deaths, newly confirmed cases and the cumulative recovered cases for the period january 20th march 3rd, 2020, were archived from the national health commission of china (nhcc) we parameterize the model by using the markov chain monte carlo (mcmc) method and estimate the control reproduction number r c , as well as the effective daily reproduction ratio r e ( t ), of the disease transmission in mainland china excluding hubei province results the estimation outcomes indicate that r c is 3 36 (95% ci 3 20 3 64) and r e ( t ) has dropped below 1 since january 31st, 2020, which implies that the containment strategies implemented by the chinese government in mainland china excluding hubei province are indeed effective and magnificently suppressed covid 19 transmission moreover, our results show that relieving personal protection too early may lead to the spread of disease for a longer time and more people would be infected, and may even cause epidemic or outbreak again by calculating the effective reproduction ratio, we prove that the contact rate should be kept at least less than 30% of the normal level by april, 2020 conclusions to ensure the epidemic ending rapidly, it is necessary to maintain the current integrated restrict interventions and self protection measures, including travel restriction, quarantine of entry, contact tracing followed by quarantine and isolation and reduction of contact, like wearing masks, etc people should be fully aware of the real time epidemic situation and keep sufficient personal protection until april if all the above conditions are met, the outbreak is expected to be ended by april in mainland china apart from hubei province",
            "sequence": "[CLS] None [SEP] risk estimation and prediction by modeling the transmission of the novel coronavirus (covid 19) in mainland china excluding hubei province abstract background in december 2019, an outbreak of coronavirus disease (covid 19) was identified in wuhan, china and, later on, detected in other parts of china our aim is to evaluate the effectiveness of the evolution of interventions and self protection measures, estimate the risk of partial lifting control measures and predict the epidemic trend of the virus in mainland china excluding hubei province based on the published data and a novel mathematical model methods a novel covid 19 transmission dynamic model incorporating the intervention measures implemented in china is proposed covid 19 daily data of mainland china excluding hubei province, including the cumulative confirmed cases, the cumulative deaths, newly confirmed cases and the cumulative recovered cases for the period january 20th march 3rd, 2020, were archived from the national health commission of china (nhcc) we parameterize the model by using the markov chain monte carlo (mcmc) method and estimate the control reproduction number r c , as well as the effective daily reproduction ratio r e ( t ), of the disease transmission in mainland china excluding hubei province results the estimation outcomes indicate that r c is 3 36 (95% ci 3 20 3 64) and r e ( t ) has dropped below 1 since january 31st, 2020, which implies that the containment strategies implemented by the chinese government in mainland china excluding hubei province are indeed effective and magnificently suppressed covid 19 transmission moreover, our results show that relieving personal protection too early may lead to the spread of disease for a longer time and more people would be infected, and may even cause epidemic or outbreak again by calculating the effective reproduction ratio, we prove that the contact rate should be kept at least less than 30% of the normal level by april, 2020 conclusions to ensure the epidemic ending rapidly, it is necessary to maintain the current integrated restrict interventions and self protection measures, including travel restriction, quarantine of entry, contact tracing followed by quarantine and isolation and reduction of contact, like wearing masks, etc people should be fully aware of the real time epidemic situation and keep sufficient personal protection until april if all the above conditions are met, the outbreak is expected to be ended by april in mainland china apart from hubei province [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R36130",
            "template_id": null,
            "paper_id": "R36130",
            "premise": null,
            "hypothesis": "assessing the plausibility of subcritical transmission of 2019 ncov in the united states abstract rapid assessment of the transmission potential of an emerging or reemerging pathogen is a cornerstone of public health response a simple approach is shown for using the number of disease introductions and secondary cases to determine whether the upper bound of the reproduction number exceeds the critical value of one",
            "sequence": "[CLS] None [SEP] assessing the plausibility of subcritical transmission of 2019 ncov in the united states abstract rapid assessment of the transmission potential of an emerging or reemerging pathogen is a cornerstone of public health response a simple approach is shown for using the number of disease introductions and secondary cases to determine whether the upper bound of the reproduction number exceeds the critical value of one [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R36132",
            "template_id": null,
            "paper_id": "R36132",
            "premise": null,
            "hypothesis": "lessons drawn from china and south korea for managing covid 19 epidemic: insights from a comparative modeling study abstract we conducted a comparative study of covid 19 epidemic in three different settings: mainland china, the guangdong province of china and south korea, by formulating two disease transmission dynamics models incorporating epidemic characteristics and setting specific interventions, and fitting the models to multi source data to identify initial and effective reproduction numbers and evaluate effectiveness of interventions we estimated the initial basic reproduction number for south korea, the guangdong province and mainland china as 2 6 (95% confidence interval (ci): (2 5, 2 7)), 3 0 (95%ci: (2 6, 3 3)) and 3 8 (95%ci: (3 5,4 2)), respectively, given a serial interval with mean of 5 days with standard deviation of 3 days we found that the effective reproduction number for the guangdong province and mainland china has fallen below the threshold 1 since february 8 th and 18 th respectively, while the effective reproduction number for south korea remains high, suggesting that the interventions implemented need to be enhanced in order to halt further infections we also project the epidemic trend in south korea under different scenarios where a portion or the entirety of the integrated package of interventions in china is used we show that a coherent and integrated approach with stringent public health interventions is the key to the success of containing the epidemic in china and specially its provinces outside its epicenter, and we show that this approach can also be effective to mitigate the burden of the covid 19 epidemic in south korea the experience of outbreak control in mainland china should be a guiding reference for the rest of the world including south korea",
            "sequence": "[CLS] None [SEP] lessons drawn from china and south korea for managing covid 19 epidemic: insights from a comparative modeling study abstract we conducted a comparative study of covid 19 epidemic in three different settings: mainland china, the guangdong province of china and south korea, by formulating two disease transmission dynamics models incorporating epidemic characteristics and setting specific interventions, and fitting the models to multi source data to identify initial and effective reproduction numbers and evaluate effectiveness of interventions we estimated the initial basic reproduction number for south korea, the guangdong province and mainland china as 2 6 (95% confidence interval (ci): (2 5, 2 7)), 3 0 (95%ci: (2 6, 3 3)) and 3 8 (95%ci: (3 5,4 2)), respectively, given a serial interval with mean of 5 days with standard deviation of 3 days we found that the effective reproduction number for the guangdong province and mainland china has fallen below the threshold 1 since february 8 th and 18 th respectively, while the effective reproduction number for south korea remains high, suggesting that the interventions implemented need to be enhanced in order to halt further infections we also project the epidemic trend in south korea under different scenarios where a portion or the entirety of the integrated package of interventions in china is used we show that a coherent and integrated approach with stringent public health interventions is the key to the success of containing the epidemic in china and specially its provinces outside its epicenter, and we show that this approach can also be effective to mitigate the burden of the covid 19 epidemic in south korea the experience of outbreak control in mainland china should be a guiding reference for the rest of the world including south korea [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R36138",
            "template_id": null,
            "paper_id": "R36138",
            "premise": null,
            "hypothesis": "estimating the generation interval for covid 19 based on symptom onset data abstract background estimating key infectious disease parameters from the covid 19 outbreak is quintessential for modelling studies and guiding intervention strategies whereas different estimates for the incubation period distribution and the serial interval distribution have been reported, estimates of the generation interval for covid 19 have not been provided methods we used outbreak data from clusters in singapore and tianjin, china to estimate the generation interval from symptom onset data while acknowledging uncertainty about the incubation period distribution and the underlying transmission network from those estimates we obtained the proportions pre symptomatic transmission and reproduction numbers results the mean generation interval was 5 20 (95%ci 3 78 6 78) days for singapore and 3 95 (95%ci 3 01 4 91) days for tianjin, china when relying on a previously reported incubation period with mean 5 2 and sd 2 8 days the proportion of pre symptomatic transmission was 48% (95%ci 32 67%) for singapore and 62% (95%ci 50 76%) for tianjin, china estimates of the reproduction number based on the generation interval distribution were slightly higher than those based on the serial interval distribution conclusions estimating generation and serial interval distributions from outbreak data requires careful investigation of the underlying transmission network detailed contact tracing information is essential for correctly estimating these quantities",
            "sequence": "[CLS] None [SEP] estimating the generation interval for covid 19 based on symptom onset data abstract background estimating key infectious disease parameters from the covid 19 outbreak is quintessential for modelling studies and guiding intervention strategies whereas different estimates for the incubation period distribution and the serial interval distribution have been reported, estimates of the generation interval for covid 19 have not been provided methods we used outbreak data from clusters in singapore and tianjin, china to estimate the generation interval from symptom onset data while acknowledging uncertainty about the incubation period distribution and the underlying transmission network from those estimates we obtained the proportions pre symptomatic transmission and reproduction numbers results the mean generation interval was 5 20 (95%ci 3 78 6 78) days for singapore and 3 95 (95%ci 3 01 4 91) days for tianjin, china when relying on a previously reported incubation period with mean 5 2 and sd 2 8 days the proportion of pre symptomatic transmission was 48% (95%ci 32 67%) for singapore and 62% (95%ci 50 76%) for tianjin, china estimates of the reproduction number based on the generation interval distribution were slightly higher than those based on the serial interval distribution conclusions estimating generation and serial interval distributions from outbreak data requires careful investigation of the underlying transmission network detailed contact tracing information is essential for correctly estimating these quantities [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R36143",
            "template_id": null,
            "paper_id": "R36143",
            "premise": null,
            "hypothesis": "a cybernetics based dynamic infection model for analyzing sars cov 2 infection stability and predicting uncontrollable risks abstract since december 2019, covid 19 has raged in wuhan and subsequently all over china and the world we propose a cybernetics based dynamic infection model (cdim) to the dynamic infection process with a probability distributed incubation delay and feedback principle reproductive trends and the stability of the sars cov 2 infection in a city can then be analyzed, and the uncontrollable risks can be forecasted before they really happen the infection mechanism of a city is depicted using the philosophy of cybernetics and approaches of the control engineering distinguished with other epidemiological models, such as sir, seir, etc , that compute the theoretical number of infected people in a closed population, cdim considers the immigration and emigration population as system inputs, and administrative and medical resources as dynamic control variables the epidemic regulation can be simulated in the model to support the decision making for containing the outbreak city case studies are demonstrated for verification and validation",
            "sequence": "[CLS] None [SEP] a cybernetics based dynamic infection model for analyzing sars cov 2 infection stability and predicting uncontrollable risks abstract since december 2019, covid 19 has raged in wuhan and subsequently all over china and the world we propose a cybernetics based dynamic infection model (cdim) to the dynamic infection process with a probability distributed incubation delay and feedback principle reproductive trends and the stability of the sars cov 2 infection in a city can then be analyzed, and the uncontrollable risks can be forecasted before they really happen the infection mechanism of a city is depicted using the philosophy of cybernetics and approaches of the control engineering distinguished with other epidemiological models, such as sir, seir, etc , that compute the theoretical number of infected people in a closed population, cdim considers the immigration and emigration population as system inputs, and administrative and medical resources as dynamic control variables the epidemic regulation can be simulated in the model to support the decision making for containing the outbreak city case studies are demonstrated for verification and validation [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R36146",
            "template_id": null,
            "paper_id": "R36146",
            "premise": null,
            "hypothesis": "covid 19 outbreak in algeria: a mathematical model to predict the incidence abstract introduction since december 29, 2019 a pandemic of new novel coronavirus infected pneumonia named covid 19 has started from wuhan, china, has led to 254 996 confirmed cases until midday march 20, 2020 sporadic cases have been imported worldwide, in algeria, the first case reported on february 25, 2020 was imported from italy, and then the epidemic has spread to other parts of the country very quickly with 139 confirmed cases until march 21, 2020 methods it is crucial to estimate the cases number growth in the early stages of the outbreak, to this end, we have implemented the alg covid 19 model which allows to predict the incidence and the reproduction number r0 in the coming months in order to help decision makers the alg covis 19 model initial equation 1, estimates the cumulative cases at t prediction time using two parameters: the reproduction number r0 and the serial interval si results we found r0=2 55 based on actual incidence at the first 25 days, using the serial interval si= 4,4 and the prediction time t=26 the herd immunity hi estimated is hi=61% also, the covid 19 incidence predicted with the alg covid 19 model fits closely the actual incidence during the first 26 days of the epidemic in algeria fig 1 a which allows us to use it according to alg covid 19 model, the number of cases will exceed 5000 on the 42 th day (april 7 th ) and it will double to 10000 on 46th day of the epidemic (april 11 th ), thus, exponential phase will begin (table 1; fig 1 b) and increases continuously until reaching \u00e0 herd immunity of 61% unless serious preventive measures are considered discussion this model is valid only when the majority of the population is vulnerable to covid 19 infection, however, it can be updated to fit the new parameters values",
            "sequence": "[CLS] None [SEP] covid 19 outbreak in algeria: a mathematical model to predict the incidence abstract introduction since december 29, 2019 a pandemic of new novel coronavirus infected pneumonia named covid 19 has started from wuhan, china, has led to 254 996 confirmed cases until midday march 20, 2020 sporadic cases have been imported worldwide, in algeria, the first case reported on february 25, 2020 was imported from italy, and then the epidemic has spread to other parts of the country very quickly with 139 confirmed cases until march 21, 2020 methods it is crucial to estimate the cases number growth in the early stages of the outbreak, to this end, we have implemented the alg covid 19 model which allows to predict the incidence and the reproduction number r0 in the coming months in order to help decision makers the alg covis 19 model initial equation 1, estimates the cumulative cases at t prediction time using two parameters: the reproduction number r0 and the serial interval si results we found r0=2 55 based on actual incidence at the first 25 days, using the serial interval si= 4,4 and the prediction time t=26 the herd immunity hi estimated is hi=61% also, the covid 19 incidence predicted with the alg covid 19 model fits closely the actual incidence during the first 26 days of the epidemic in algeria fig 1 a which allows us to use it according to alg covid 19 model, the number of cases will exceed 5000 on the 42 th day (april 7 th ) and it will double to 10000 on 46th day of the epidemic (april 11 th ), thus, exponential phase will begin (table 1; fig 1 b) and increases continuously until reaching \u00e0 herd immunity of 61% unless serious preventive measures are considered discussion this model is valid only when the majority of the population is vulnerable to covid 19 infection, however, it can be updated to fit the new parameters values [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "results": [
                {
                    "template_id": null
                }
            ]
        },
        {
            "instance_id": "R41120",
            "template_id": null,
            "paper_id": "R41120",
            "premise": null,
            "hypothesis": "improving purity and process volume during direct electrolytic reduction of solid sio 2 in molten cacl 2 for the production of solar grade silicon the direct electrolytic reduction of solid sio2 is investigated in molten cacl2 at 1123\\u2005k to produce solar grade silicon the target concentrations of impurities for the primary si are calculated from the acceptable concentrations of impurities in solar grade silicon (sog si) and the segregation coefficients for the impurity elements the concentrations of most metal impurities are significantly decreased below their target concentrations by using a quartz vessel and new types of sio2 contacting electrodes the electrolytic reduction rate is increased by improving an electron pathway from the lead material to the sio2, which demonstrates that the characteristics of the electric contact are important factors affecting the reduction rate pellet and basket type electrodes are tested to improve the process volume for powdery and granular sio2 based on the purity of the si product after melting, refining, and solidifying, the potential of the technology is discussed",
            "sequence": "[CLS] None [SEP] improving purity and process volume during direct electrolytic reduction of solid sio 2 in molten cacl 2 for the production of solar grade silicon the direct electrolytic reduction of solid sio2 is investigated in molten cacl2 at 1123\\u2005k to produce solar grade silicon the target concentrations of impurities for the primary si are calculated from the acceptable concentrations of impurities in solar grade silicon (sog si) and the segregation coefficients for the impurity elements the concentrations of most metal impurities are significantly decreased below their target concentrations by using a quartz vessel and new types of sio2 contacting electrodes the electrolytic reduction rate is increased by improving an electron pathway from the lead material to the sio2, which demonstrates that the characteristics of the electric contact are important factors affecting the reduction rate pellet and basket type electrodes are tested to improve the process volume for powdery and granular sio2 based on the purity of the si product after melting, refining, and solidifying, the potential of the technology is discussed [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R122",
                "label": "Chemistry"
            },
            "results": [
                {
                    "template_id": "R52190"
                }
            ]
        },
        {
            "instance_id": "R41122",
            "template_id": null,
            "paper_id": "R41122",
            "premise": null,
            "hypothesis": "direct electrolytic reduction of solid silicon dioxide in molten licl kcl cacl at 773 k we investigated electrolytic reduction of solid sio 2 by a contacting electrode method in molten licl kcl cacl 2 at 773 k the results of cyclic voltammetry indicated that reduction of sio 2 occurs at potential more negative than 0 85 v (vs ca 2 + , li + /ca li) samples were prepared by potentiostatic electrolysis for 2 h at 0 25, 0 50, 0 70, and 1 00 v energy dispersive x ray analysis and raman spectra clarified that the reduction products at 0 50 and 0 70 v are composed of amorphous si and microcrystalline si scanning electron microscope (sem) observations revealed that the morphology of the produced si is spongelike with a particle size smaller than 50 nm the mechanism of si formation was discussed by comparing the sem observations and the raman spectra of the si samples prepared at 773 and 1123 k the reduction mechanism of the direct electrolytic reduction of sio 2 at lower temperature was also discussed",
            "sequence": "[CLS] None [SEP] direct electrolytic reduction of solid silicon dioxide in molten licl kcl cacl at 773 k we investigated electrolytic reduction of solid sio 2 by a contacting electrode method in molten licl kcl cacl 2 at 773 k the results of cyclic voltammetry indicated that reduction of sio 2 occurs at potential more negative than 0 85 v (vs ca 2 + , li + /ca li) samples were prepared by potentiostatic electrolysis for 2 h at 0 25, 0 50, 0 70, and 1 00 v energy dispersive x ray analysis and raman spectra clarified that the reduction products at 0 50 and 0 70 v are composed of amorphous si and microcrystalline si scanning electron microscope (sem) observations revealed that the morphology of the produced si is spongelike with a particle size smaller than 50 nm the mechanism of si formation was discussed by comparing the sem observations and the raman spectra of the si samples prepared at 773 and 1123 k the reduction mechanism of the direct electrolytic reduction of sio 2 at lower temperature was also discussed [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R122",
                "label": "Chemistry"
            },
            "results": [
                {
                    "template_id": "R52190"
                }
            ]
        },
        {
            "instance_id": "R41128",
            "template_id": null,
            "paper_id": "R41128",
            "premise": null,
            "hypothesis": "verification and implications of the dissolution electrodeposition process during the electro reduction of solid silica in molten cacl 2 with the verification of the existence of the dissolution electrodeposition mechanism during the electro reduction of solid silica in molten cacl2, the present study not only provides direct scientific support for the controllable electrolytic extraction of nanostructured silicon in molten salts but it also opens an avenue to a continuous silicon extraction process via the electro deposition of dissolved silicates in molten cacl2 in addition, the present study increases the general understanding of the versatile material extraction route via the electro deoxidization process of solid oxides in molten salts, which also provokes reconsiderations on the electrochemistry of insulating compounds",
            "sequence": "[CLS] None [SEP] verification and implications of the dissolution electrodeposition process during the electro reduction of solid silica in molten cacl 2 with the verification of the existence of the dissolution electrodeposition mechanism during the electro reduction of solid silica in molten cacl2, the present study not only provides direct scientific support for the controllable electrolytic extraction of nanostructured silicon in molten salts but it also opens an avenue to a continuous silicon extraction process via the electro deposition of dissolved silicates in molten cacl2 in addition, the present study increases the general understanding of the versatile material extraction route via the electro deoxidization process of solid oxides in molten salts, which also provokes reconsiderations on the electrochemistry of insulating compounds [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R122",
                "label": "Chemistry"
            },
            "results": [
                {
                    "template_id": "R52190"
                }
            ]
        },
        {
            "instance_id": "R41132",
            "template_id": null,
            "paper_id": "R41132",
            "premise": null,
            "hypothesis": "the use of silicon wafer barriers in the electrochemical reduction of solid silica to form silicon in molten salts nowadays, silicon is the most critical element in solar cells and/or solar chips silicon having 98 to 99% si as being metallurgical grade, requires further refinement/purification processes such as zone refining [1,2] and/or siemens process [3] to upgrade it for solar applications a promising method, based on straightforward electrochemical reduction of oxides by ffc cambridge process [4], was adopted to form silicon from porous sio2 pellets in molten cacl2 and cacl2 nacl salt mixture [5] it was reported that silicon powder contaminated by iron and nickel emanated from stainless steel cathode, consequently disqualified the product from solar applications sio2 pellets sintered at 1300oc for 4 hours, were placed in between pure silicon wafer plates to defeat the contamination problem encouraging results indicated a reliable alternative method of direct solar grade silicon production for expanding solar energy field",
            "sequence": "[CLS] None [SEP] the use of silicon wafer barriers in the electrochemical reduction of solid silica to form silicon in molten salts nowadays, silicon is the most critical element in solar cells and/or solar chips silicon having 98 to 99% si as being metallurgical grade, requires further refinement/purification processes such as zone refining [1,2] and/or siemens process [3] to upgrade it for solar applications a promising method, based on straightforward electrochemical reduction of oxides by ffc cambridge process [4], was adopted to form silicon from porous sio2 pellets in molten cacl2 and cacl2 nacl salt mixture [5] it was reported that silicon powder contaminated by iron and nickel emanated from stainless steel cathode, consequently disqualified the product from solar applications sio2 pellets sintered at 1300oc for 4 hours, were placed in between pure silicon wafer plates to defeat the contamination problem encouraging results indicated a reliable alternative method of direct solar grade silicon production for expanding solar energy field [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R122",
                "label": "Chemistry"
            },
            "results": [
                {
                    "template_id": "R52190"
                }
            ]
        },
        {
            "instance_id": "R41134",
            "template_id": null,
            "paper_id": "R41134",
            "premise": null,
            "hypothesis": "oscillatory behavior in electrochemical deposition reaction of polycrystalline silicon thin films through reduction of silicon tetrachloride in a molten salt electrolyte a new electrochemical oscillation is found for reduction reaction of silicon tetrachloride on a partially immersed single crystal n si electrode in a lithium chloride potassium chloride eutectic melt electrolyte the reduction of sicl4, which is almost insoluble in the electrolyte, occurs mainly near the upper edge of an electrolyte meniscus on the electrode, and it is discussed that the oscillation is caused by a change in the height of the meniscus due to a change in the chemical structure (and hence the interfacial tension) of the electrode surface with progress of the silicon deposition reaction",
            "sequence": "[CLS] None [SEP] oscillatory behavior in electrochemical deposition reaction of polycrystalline silicon thin films through reduction of silicon tetrachloride in a molten salt electrolyte a new electrochemical oscillation is found for reduction reaction of silicon tetrachloride on a partially immersed single crystal n si electrode in a lithium chloride potassium chloride eutectic melt electrolyte the reduction of sicl4, which is almost insoluble in the electrolyte, occurs mainly near the upper edge of an electrolyte meniscus on the electrode, and it is discussed that the oscillation is caused by a change in the height of the meniscus due to a change in the chemical structure (and hence the interfacial tension) of the electrode surface with progress of the silicon deposition reaction [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R122",
                "label": "Chemistry"
            },
            "results": [
                {
                    "template_id": "R52190"
                }
            ]
        },
        {
            "instance_id": "R41136",
            "template_id": null,
            "paper_id": "R41136",
            "premise": null,
            "hypothesis": "toward cost effective manufacturing of silicon solar cells: electrodeposition of high quality si films in a cacl 2 based molten salt electrodeposition of si films from a si containing electrolyte is a cost effective approach for the manufacturing of solar cells proposals relying on fluoride based molten salts have suffered from low product quality due to difficulties in impurity control here we demonstrate the successful electrodeposition of high quality si films from a cacl2 based molten salt soluble siiv o anions generated from solid sio2 are electrodeposited onto a graphite substrate to form a dense film of crystalline si impurities in the deposited si film are controlled at low concentrations (both b and p are less than 1\\u2005ppm) in the photoelectrochemical measurements, the film shows p type semiconductor character and large photocurrent a p n junction fabricated from the deposited si film exhibits clear photovoltaic effects this study represents the first step to the ultimate goal of developing a cost effective manufacturing process for si solar cells based on electrodeposition",
            "sequence": "[CLS] None [SEP] toward cost effective manufacturing of silicon solar cells: electrodeposition of high quality si films in a cacl 2 based molten salt electrodeposition of si films from a si containing electrolyte is a cost effective approach for the manufacturing of solar cells proposals relying on fluoride based molten salts have suffered from low product quality due to difficulties in impurity control here we demonstrate the successful electrodeposition of high quality si films from a cacl2 based molten salt soluble siiv o anions generated from solid sio2 are electrodeposited onto a graphite substrate to form a dense film of crystalline si impurities in the deposited si film are controlled at low concentrations (both b and p are less than 1\\u2005ppm) in the photoelectrochemical measurements, the film shows p type semiconductor character and large photocurrent a p n junction fabricated from the deposited si film exhibits clear photovoltaic effects this study represents the first step to the ultimate goal of developing a cost effective manufacturing process for si solar cells based on electrodeposition [SEP]",
            "target": "neutral",
            "research_field": {
                "id": "R122",
                "label": "Chemistry"
            },
            "results": [
                {
                    "template_id": "R52190"
                }
            ]
        }
    ],
    "metrics": {
        "accuracy": 0.9714285714285714,
        "recall": 0.42857142857142855,
        "precision": 0.42857142857142855,
        "f1": 0.42857142857142855,
        "research_fields": {
            "R141823": {
                "label": "Semantic Web",
                "tp": 1,
                "fp": 1,
                "n_instances": 2,
                "recall": 0.5,
                "precision": 0.5,
                "f1": 0.5
            },
            "R126": {
                "label": "Materials Chemistry",
                "tp": 1,
                "fp": 0,
                "n_instances": 1,
                "recall": 1.0,
                "precision": 1.0,
                "f1": 1.0
            },
            "R136127": {
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions",
                "tp": 1,
                "fp": 0,
                "n_instances": 1,
                "recall": 1.0,
                "precision": 1.0,
                "f1": 1.0
            },
            "R278": {
                "label": "Information Science",
                "tp": 1,
                "fp": 0,
                "n_instances": 1,
                "recall": 1.0,
                "precision": 1.0,
                "f1": 1.0
            },
            "R137654": {
                "label": "Mechanical Process Engineering",
                "tp": 1,
                "fp": 0,
                "n_instances": 1,
                "recall": 1.0,
                "precision": 1.0,
                "f1": 1.0
            },
            "R129": {
                "label": "Organic Chemistry",
                "tp": 1,
                "fp": 0,
                "n_instances": 1,
                "recall": 1.0,
                "precision": 1.0,
                "f1": 1.0
            },
            "R130": {
                "label": "Physical Chemistry",
                "tp": 1,
                "fp": 0,
                "n_instances": 1,
                "recall": 1.0,
                "precision": 1.0,
                "f1": 1.0
            },
            "R137681": {
                "label": "Information Systems, Process and Knowledge Management",
                "tp": 1,
                "fp": 0,
                "n_instances": 1,
                "recall": 1.0,
                "precision": 1.0,
                "f1": 1.0
            },
            "R131": {
                "label": "Polymer Chemistry",
                "tp": 1,
                "fp": 0,
                "n_instances": 1,
                "recall": 1.0,
                "precision": 1.0,
                "f1": 1.0
            },
            "R114008": {
                "label": "Applied Physics",
                "tp": 1,
                "fp": 0,
                "n_instances": 1,
                "recall": 1.0,
                "precision": 1.0,
                "f1": 1.0
            },
            "R11": {
                "label": "Science",
                "tp": 0,
                "fp": 1,
                "n_instances": 1,
                "recall": 0.0,
                "precision": 0.0,
                "f1": 0.0
            },
            "R145261": {
                "label": "Natural Language Processing",
                "tp": 28,
                "fp": 1,
                "n_instances": 29,
                "recall": 0.9655172413793104,
                "precision": 0.9655172413793104,
                "f1": 0.9655172413793104
            },
            "R175": {
                "label": "Atomic, Molecular and Optical Physics",
                "tp": 1,
                "fp": 14,
                "n_instances": 15,
                "recall": 0.06666666666666667,
                "precision": 0.06666666666666667,
                "f1": 0.06666666666666667
            },
            "R140": {
                "label": "Software Engineering",
                "tp": 0,
                "fp": 74,
                "n_instances": 74,
                "recall": 0.0,
                "precision": 0.0,
                "f1": 0.0
            },
            "R109": {
                "label": "Control Theory",
                "tp": 1,
                "fp": 4,
                "n_instances": 5,
                "recall": 0.2,
                "precision": 0.2,
                "f1": 0.20000000000000004
            },
            "R254": {
                "label": "Materials Science and Engineering",
                "tp": 12,
                "fp": 1,
                "n_instances": 13,
                "recall": 0.9230769230769231,
                "precision": 0.9230769230769231,
                "f1": 0.9230769230769231
            },
            "R57": {
                "label": "Virology",
                "tp": 19,
                "fp": 1,
                "n_instances": 20,
                "recall": 0.95,
                "precision": 0.95,
                "f1": 0.9500000000000001
            },
            "R122": {
                "label": "Chemistry",
                "tp": 1,
                "fp": 6,
                "n_instances": 7,
                "recall": 0.14285714285714285,
                "precision": 0.14285714285714285,
                "f1": 0.14285714285714285
            },
            "R133": {
                "label": "Artificial Intelligence",
                "tp": 0,
                "fp": 5,
                "n_instances": 5,
                "recall": 0.0,
                "precision": 0.0,
                "f1": 0.0
            },
            "R322": {
                "label": "Computational Linguistics",
                "tp": 1,
                "fp": 0,
                "n_instances": 1,
                "recall": 1.0,
                "precision": 1.0,
                "f1": 1.0
            },
            "R114010": {
                "label": "Atomic Physics",
                "tp": 6,
                "fp": 0,
                "n_instances": 6,
                "recall": 1.0,
                "precision": 1.0,
                "f1": 1.0
            },
            "R137665": {
                "label": "Coating and Surface Technology",
                "tp": 2,
                "fp": 0,
                "n_instances": 2,
                "recall": 1.0,
                "precision": 1.0,
                "f1": 1.0
            }
        }
    }
}