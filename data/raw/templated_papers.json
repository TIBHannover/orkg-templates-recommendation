{
    "templates": [
        {
            "id": "R138077",
            "label": "Ontology learning from data sources",
            "research_fields": [
                {
                    "id": "R133",
                    "label": "Artificial Intelligence"
                },
                {
                    "id": "R141823",
                    "label": "Semantic Web"
                }
            ],
            "properties": [
                "has dataset",
                "Application Domain",
                "Input format",
                "Output format",
                "Accuracy",
                "F1-score",
                "Precision",
                "Knowledge source",
                "has Data Source",
                "Learning purpose",
                "Terms learning",
                "Axiom learning",
                "Properties learning",
                "Properties hierarchy learning",
                "Rule learning",
                "Class hierarchy learning",
                "Learning method",
                "Learning tool",
                "Evaluation metrics",
                "Related work",
                "Validation tool",
                "Training corpus",
                "Testing corpus",
                "Class learning",
                "Implemented technologies",
                "Relationships learning",
                "Instance learning",
                "Taxonomy learning",
                "Validation comment",
                "Assessment of acquired knowledge",
                "Recall",
                "F-measure"
            ],
            "papers": [
                {
                    "id": "R185271",
                    "label": "Multimedia ontology learning for automatic annotation and video browsing",
                    "doi": "10.1145/1460096.1460159",
                    "research_field": {
                        "id": "R141823",
                        "label": "Semantic Web"
                    },
                    "abstract": "in this work, we offer an approach to combine standard multimedia analysis techniques with knowledge drawn from conceptual metadata provided by domain experts of a specialized scholarly domain, to learn a domain-specific multimedia ontology from a set of annotated examples. a standard bayesian network learning algorithm that learns structure and parameters of a bayesian network is extended to include media observables in the learning. an expert group provides domain knowledge to construct a basic ontology of the domain as well as to annotate a set of training videos. these annotations help derive the associations between high-level semantic concepts of the domain and low-level mpeg-7 based features representing audio-visual content of the videos. we construct a more robust and refined version of this ontology by learning from this set of conceptually annotated videos. to encode this knowledge, we use mowl, a multimedia extension of web ontology language (owl) which is capable of describing domain concepts in terms of their media properties and of capturing the inherent uncertainties involved. we use the ontology specified knowledge for recognizing concepts relevant to a video to annotate fresh addition to the video database with relevant concepts in the ontology. these conceptual annotations are used to create hyperlinks in the video collection, to provide an effective video browsing interface to the user."
                },
                {
                    "id": "R185300",
                    "label": "Automatic Product Ontology Extraction from Textual Reviews",
                    "doi": "",
                    "research_field": {
                        "id": "R141823",
                        "label": "Semantic Web"
                    },
                    "abstract": "ontologies have proven beneficial in different settings that make use of textual reviews. however, manually constructing ontologies is a laborious and time-consuming process in need of automation. we propose a novel methodology for automatically extracting ontologies, in the form of meronomies, from product reviews, using a very limited amount of hand-annotated training data. we show that the ontologies generated by our method outperform hand-crafted ontologies (wordnet) and ontologies extracted by existing methods (text2onto and comet) in several, diverse settings. specifically, our generated ontologies outperform the others when evaluated by human annotators as well as on an existing q&a dataset from amazon. moreover, our method is better able to generalise, in capturing knowledge about unseen products. finally, we consider a real-world setting, showing that our method is better able to determine recommended products based on their reviews, in alternative to using amazon\u2019s standard score aggregations."
                },
                {
                    "id": "R185335",
                    "label": "Ontology Learning Process as a Bottom-up Strategy for Building Domain-specific Ontology from Legal Texts",
                    "doi": "10.5220/0006188004730480",
                    "research_field": {
                        "id": "R141823",
                        "label": "Semantic Web"
                    },
                    "abstract": "the objective of this paper is to present the role of ontology learning process in supporting an ontology engineer for creating and maintaining ontologies from textual resources. the knowledge structures that interest us are legal domain-specific ontologies. we will use these ontologies to build legal domain ontology for a lebanese legal knowledge based system. the domain application of this work is the lebanese criminal system. ontologies can be learnt from various sources, such as databases, structured and unstructured documents. here, the focus is on the acquisition of ontologies from unstructured text, provided as input. in this work, the ontology learning process represents a knowledge extraction phase using natural language processing techniques. the resulted ontology is considered as inexpressive ontology. there is a need to reengineer it in order to build a complete, correct and more expressive domain-specific ontology."
                },
                {
                    "id": "R185349",
                    "label": "The Ontology Extraction & Maintenance Framework Text-To-Onto",
                    "doi": "",
                    "research_field": {
                        "id": "R141823",
                        "label": "Semantic Web"
                    },
                    "abstract": "ontologies play an increasingly important role in knowledge management. one of the main problems associated with ontologies is that they need to be constructed and maintained. manual construction of larger ontologies is usually not feasible within companies because of the effort and costs required. therefore, a semi-automatic approach to ontology construction and maintenance is what everybody is wishing for. the paper presents a framework for semi-automatically learning ontologies from domainspecific texts by applying machine learning techniques. the text-to-onto framework integrates manual engineering facilities to follow a balanced cooperative modelling paradigm."
                },
                {
                    "id": "R180001",
                    "label": "A Deep Learning based Approach for Precise Video Tagging",
                    "doi": "10.1109/icet48972.2019.8994567",
                    "research_field": {
                        "id": "R141823",
                        "label": "Semantic Web"
                    },
                    "abstract": "with the increase in smart devices and abundance of video contents, efficient techniques for the indexing, analysis and retrieval of videos are becoming more and more desirable. improved indexing and automated analysis of millions of videos could be accomplished by getting videos tagged automatically. a lot of existing methods fail to precisely tag videos because of their lack of ability to capture the video context. the context in a video represents the interactions of objects in a scene and their overall meaning. in this work, we propose a novel approach that integrates the video scene ontology with cnn (convolutional neural network) for improved video tagging. our method captures the content of a video by extracting the information from individual key frames. the key frames are then fed to a cnn based deep learning model to train its parameters. the trained parameters are used to generate the most frequent tags. highly frequent tags are used to summarize the input video. the proposed technique is benchmarked on the most widely used dataset of video activities, namely, ucf-101. our method managed to achieve an overall accuracy of 99.8% with an f1- score of 96.2%."
                },
                {
                    "id": "R149916",
                    "label": "Image domain ontology fusion approach using multi-level inference mechanism",
                    "doi": "10.1109/iccsnt.2011.6182514",
                    "research_field": {
                        "id": "R141823",
                        "label": "Semantic Web"
                    },
                    "abstract": "one of the main challenges in content-based or semantic image retrieval is still to bridge the gap between low-level features and semantic information. in this paper, an approach is presented using integrated multi-level image features in ontology fusion construction by a fusion framework, which based on the latent semantic analysis. the proposed method promotes images ontology fusion efficiently and broadens the application fields of image ontology retrieval system. the relevant experiment shows that this method ameliorates the problem, such as too many redundant data and relations, in the traditional ontology system construction, as well as improves the performance of semantic images retrieval."
                },
                {
                    "id": "R149947",
                    "label": "Image based mammographie ontology learning",
                    "doi": "10.1109/sita.2016.7772310",
                    "research_field": {
                        "id": "R141823",
                        "label": "Semantic Web"
                    },
                    "abstract": "understanding the content of an image is one of the challenges in the image processing field. recently, the content based image retrieval (cbir) and especially semantic content based image retrieval (scbir) are the main goal of many research works. in medical field, understanding the content of an image is very helpful in the automatic decision making. in fact, analyzing the semantic information in an image support can assist the doctor to make the adequate diagnosis. this paper presents a new method for mammographic ontology learning from a set of mammographic images. the approach is based on four main modules: (1) the mammography segmentation, (2) the features extraction (3) the local ontology modeling and (4) the global ontology construction basing on merging the local ones. the first module allows detecting the pathological regions in the represented breast. the second module consists on extracting the most important features from the pathological zones. the third module allows modeling a local ontology by representing the pertinent entities (conceptual entities) as well as their correspondent features (shape, size, form, etc.) discovered in the previous step. the last module consists on merging the local ontologies extracted from a set of mammographies in order to obtain a global and exhaustive one. our approach attempts to fully describe the semantic content of mammographic images in order to perform the domain knowledge modeling."
                },
                {
                    "id": "R151328",
                    "label": "Neuro-Symbolic Architectures for Context Understanding",
                    "doi": "10.3233/SSW200016",
                    "research_field": {
                        "id": "R112125",
                        "label": "Machine Learning"
                    },
                    "abstract": "\"computational context understanding refers to an agent's ability to fuse disparate sources of information for decision-making and is, therefore, generally regarded as a prerequisite for sophisticated machine reasoning capabilities, such as in artificial intelligence (ai). data-driven and knowledge-driven methods are two classical techniques in the pursuit of such machine sense-making capability. however, while data-driven methods seek to model the statistical regularities of events by making observations in the real-world, they remain difficult to interpret and they lack mechanisms for naturally incorporating external knowledge. conversely, knowledge-driven methods, combine structured knowledge bases, perform symbolic reasoning based on axiomatic principles, and are more interpretable in their inferential processing; however, they often lack the ability to estimate the statistical salience of an inference. to combat these issues, we propose the use of hybrid ai methodology as a general framework for combining the strengths of both approaches. specifically, we inherit the concept of neuro-symbolism as a way of using knowledge-bases to guide the learning progress of deep neural networks. we further ground our discussion in two applications of neuro-symbolism and, in both cases, show that our systems maintain interpretability while achieving comparable performance, relative to the state-of-the-art.\""
                },
                {
                    "id": "R195342",
                    "label": "Constructing Cooking Ontology for Live Streams",
                    "doi": "10.5130/acis2018.ai",
                    "research_field": {
                        "id": "R141823",
                        "label": "Semantic Web"
                    },
                    "abstract": "we build a cooking domain knowledge by using an ontology schema that reflects natural language processing and enhances ontology instances with semantic query. our research helps audiences to better understand live streaming, especially when they just switch to a show. the practical contribution of our research is to use cooking ontology, so we may map clips of cooking live stream video and instructions of recipes. the architecture of our study presents three sections: ontology construction, ontology enhancement, and mapping cooking video to cooking ontology. also, our preliminary evaluations consist of three hierarchies\u2014nodes, ordered-pairs, and 3-tuples\u2014that we use to referee (1) ontology enhancement performance for our first experiment evaluation and (2) the accuracy ratio of mapping between video clips and cooking ontology for our second experiment evaluation. our results indicate that ontology enhancement is effective and heightens accuracy ratios on matching pairs with cooking ontology and video clips."
                }
            ]
        },
        {
            "id": "R146876",
            "label": "Organic solar cells",
            "research_fields": [
                {
                    "id": "R126",
                    "label": "Materials Chemistry"
                }
            ],
            "properties": [
                "Mobility",
                "Donor",
                "Acceptor",
                "LUMO",
                "HOMO",
                "Energy band gap",
                "Open circuit voltage, Voc",
                "Short-circuit current density, Jsc",
                "Fill factor, FF",
                "Power conversion efficiency"
            ],
            "papers": [
                {
                    "id": "R146888",
                    "label": "High-performance fullerene-free polymer solar cells with 6.31% efficiency",
                    "doi": "10.1039/c4ee03424d",
                    "research_field": {
                        "id": "R126",
                        "label": "Materials Chemistry"
                    },
                    "abstract": "a nonfullerene electron acceptor (ieic) based on indaceno[1,2- b :5,6- b \u2032]dithiophene and 2-(3-oxo-2,3-dihydroinden-1-ylidene)malononitrile was designed and synthesized, and fullerene-free polymer solar cells based on the ieic acceptor showed power conversion efficiencies of up to 6.31%."
                },
                {
                    "id": "R146907",
                    "label": "Non-fullerene polymer solar cells based on a selenophene-containing fused-ring acceptor with photovoltaic performance of 8.6%",
                    "doi": "10.1039/c6ee00315j",
                    "research_field": {
                        "id": "R126",
                        "label": "Materials Chemistry"
                    },
                    "abstract": "in this work, we present a non-fullerene electron acceptor bearing a fused five-heterocyclic ring containing selenium atoms, denoted as idse-t-ic, for fullerene-free polymer solar cells (pscs)."
                },
                {
                    "id": "R146918",
                    "label": "Design and Synthesis of a Low Bandgap Small Molecule Acceptor for Efficient Polymer Solar Cells",
                    "doi": "10.1002/adma.201602642",
                    "research_field": {
                        "id": "R126",
                        "label": "Materials Chemistry"
                    },
                    "abstract": "a novel non-fullerene acceptor, possessing a very low bandgap of 1.34 ev and a high-lying lowest unoccupied molecular orbital level of -3.95 ev, is designed and synthesized by introducing electron-donating alkoxy groups to the backbone of a conjugated small molecule. impressive power conversion efficiencies of 8.4% and 10.7% are obtained for fabricated single and tandem polymer solar cells."
                },
                {
                    "id": "R146924",
                    "label": "Nonfullerene Polymer Solar Cells Based on a Main-Chain Twisted Low-Bandgap Acceptor with Power Conversion Efficiency of 13.2%",
                    "doi": "10.1021/acsenergylett.8b00627",
                    "research_field": {
                        "id": "R126",
                        "label": "Materials Chemistry"
                    },
                    "abstract": "a new acceptor\u2013donor\u2013acceptor-structured nonfullerene acceptor, 2,2\u2032-((2z,2\u2032z)-(((4,4,9,9-tetrakis(4-hexylphenyl)-4,9-dihydro-s-indaceno[1,2-b:5,6-b\u2032]dithiophene-2,7-diyl)bis(4-((2-ethylhexyl)oxy)thiophene-4,3-diyl))bis(methanylylidene))bis(5,6-difluoro-3-oxo-2,3-dihydro-1h-indene-2,1-diylidene))dimalononitrile (i-ieico-4f), is designed and synthesized via main-chain substituting position modification of 2-(5,6-difluoro-3-oxo-2,3-dihydro-1h-indene-2,1-diylidene)dimalononitrile. unlike its planar analogue ieico-4f with strong absorption in the near-infrared region, i-ieico-4f exhibits a twisted main-chain configuration, resulting in 164 nm blue shifts and leading to complementary absorption with the wide-bandgap polymer (j52). a high solution molar extinction coefficient of 2.41 \u00d7 105 m\u20131 cm\u20131, and sufficiently high energy of charge-transfer excitons of 1.15 ev in a j52:i-ieico-4f blend were observed, in comparison with those of 2.26 \u00d7 105 m\u20131 cm\u20131 and 1.08 ev for ieico-4f. a power conversion efficiency of..."
                },
                {
                    "id": "R146985",
                    "label": "Exploiting Noncovalently Conformational Locking as a Design Strategy for High Performance Fused-Ring Electron Acceptor Used in Polymer Solar Cells",
                    "doi": "10.1021/jacs.7b00566",
                    "research_field": {
                        "id": "R126",
                        "label": "Materials Chemistry"
                    },
                    "abstract": "we have developed a kind of novel fused-ring small molecular acceptor, whose planar conformation can be locked by intramolecular noncovalent interaction. the formation of planar supramolecular fused-ring structure by conformation locking can effectively broaden its absorption spectrum, enhance the electron mobility, and reduce the nonradiative energy loss. polymer solar cells (pscs) based on this acceptor afforded a power conversion efficiency (pce) of 9.6%. in contrast, pscs based on similar acceptor, which cannot form a flat conformation, only gave a pce of 2.3%. such design strategy, which can make the synthesis of small molecular acceptor much easier, will be promising in developing a new acceptor for high efficiency polymer solar cells."
                },
                {
                    "id": "R146997",
                    "label": "Enhancing the Performance of Organic Solar Cells by Hierarchically Supramolecular Self-Assembly of Fused-Ring Electron Acceptors",
                    "doi": "10.1021/acs.chemmater.8b01319",
                    "research_field": {
                        "id": "R126",
                        "label": "Materials Chemistry"
                    },
                    "abstract": "three novel non-fullerene small molecular acceptors itoic, itoic-f, and itoic-2f were designed and synthesized with easy chemistry. the concept of supramolecular chemistry was successfully used in the molecular design, which includes noncovalently conformational locking (via intrasupramolecular interaction) to enhance the planarity of backbone and electrostatic interaction (intersupramolecular interaction) to enhance the \u03c0\u2013\u03c0 stacking of terminal groups. fluorination can further strengthen the intersupramolecular electrostatic interaction of terminal groups. as expected, the designed acceptors exhibited excellent device performance when blended with polymer donor pbdb-t. in comparison with the parent acceptor molecule dc-idt2t reported in the literature with a power conversion efficiency (pce) of 3.93%, itoic with a planar structure exhibited a pce of 8.87% and itoic-2f with a planar structure and enhanced electrostatic interaction showed a quite impressive pce of 12.17%. our result demonstrates the import..."
                },
                {
                    "id": "R147898",
                    "label": "Side-Chain Isomerization on an n-type Organic Semiconductor ITIC Acceptor Makes 11.77% High Efficiency Polymer Solar Cells",
                    "doi": "10.1021/jacs.6b09110",
                    "research_field": {
                        "id": "R126",
                        "label": "Materials Chemistry"
                    },
                    "abstract": "low bandgap n-type organic semiconductor (n-os) itic has attracted great attention for the application as an acceptor with medium bandgap p-type conjugated polymer as donor in nonfullerene polymer solar cells (pscs) because of its attractive photovoltaic performance. here we report a modification on the molecular structure of itic by side-chain isomerization with meta-alkyl-phenyl substitution, m-itic, to further improve its photovoltaic performance. in a comparison with its isomeric counterpart itic with para-alkyl-phenyl substitution, m-itic shows a higher film absorption coefficient, a larger crystalline coherence, and higher electron mobility. these inherent advantages of m-itic resulted in a higher power conversion efficiency (pce) of 11.77% for the nonfullerene pscs with m-itic as acceptor and a medium bandgap polymer j61 as donor, which is significantly improved over that (10.57%) of the corresponding devices with itic as acceptor. to the best of our knowledge, the pce of 11.77% is one of the highest values reported in the literature to date for nonfullerene pscs. more importantly, the m-itic-based device shows less thickness-dependent photovoltaic behavior than itic-based devices in the active-layer thickness range of 80-360 nm, which is beneficial for large area device fabrication. these results indicate that m-itic is a promising low bandgap n-os for the application as an acceptor in pscs, and the side-chain isomerization could be an easy and convenient way to further improve the photovoltaic performance of the donor and acceptor materials for high efficiency pscs."
                },
                {
                    "id": "R147918",
                    "label": "High-Performance Electron Acceptor with Thienyl Side Chains for Organic Photovoltaics",
                    "doi": "10.1021/jacs.6b02004",
                    "research_field": {
                        "id": "R126",
                        "label": "Materials Chemistry"
                    },
                    "abstract": "we develop an efficient fused-ring electron acceptor (itic-th) based on indacenodithieno[3,2-b]thiophene core and thienyl side-chains for organic solar cells (oscs). relative to its counterpart with phenyl side-chains (itic), itic-th shows lower energy levels (itic-th: homo = -5.66 ev, lumo = -3.93 ev; itic: homo = -5.48 ev, lumo = -3.83 ev) due to the \u03c3-inductive effect of thienyl side-chains, which can match with high-performance narrow-band-gap polymer donors and wide-band-gap polymer donors. itic-th has higher electron mobility (6.1 \u00d7 10(-4) cm(2) v(-1) s(-1)) than itic (2.6 \u00d7 10(-4) cm(2) v(-1) s(-1)) due to enhanced intermolecular interaction induced by sulfur-sulfur interaction. we fabricate oscs by blending itic-th acceptor with two different low-band-gap and wide-band-gap polymer donors. in one case, a power conversion efficiency of 9.6% was observed, which rivals some of the highest efficiencies for single junction oscs based on fullerene acceptors."
                },
                {
                    "id": "R147944",
                    "label": "A near-infrared non-fullerene electron acceptor for high performance polymer solar cells",
                    "doi": "10.1039/c7ee00844a",
                    "research_field": {
                        "id": "R126",
                        "label": "Materials Chemistry"
                    },
                    "abstract": "low-bandgap polymers/molecules are an interesting family of semiconductor materials, and have enabled many recent exciting breakthroughs in the field of organic electronics, especially for organic photovoltaics (opvs)."
                },
                {
                    "id": "R148204",
                    "label": "Halogenated conjugated molecules for ambipolar field-effect transistors and non-fullerene organic solar cells",
                    "doi": "10.1039/c7qm00025a",
                    "research_field": {
                        "id": "R126",
                        "label": "Materials Chemistry"
                    },
                    "abstract": "halogenated conjugated molecules containing f, cl, br and i with high crystallinity were developed to show high electron mobilities of 1.3 cm 2 v \u22121 s \u22121 in field-effect transistors and power conversion efficiencies above 9% in non-fullerene solar cells."
                },
                {
                    "id": "R148232",
                    "label": "Enhancing Performance of Nonfullerene Acceptors via Side\u2010Chain Conjugation Strategy",
                    "doi": "10.1002/adma.201702125",
                    "research_field": {
                        "id": "R126",
                        "label": "Materials Chemistry"
                    },
                    "abstract": "a side\u2010chain conjugation strategy in the design of nonfullerene electron acceptors is proposed, with the design and synthesis of a side\u2010chain\u2010conjugated acceptor (itic2) based on a 4,8\u2010bis(5\u2010(2\u2010ethylhexyl)thiophen\u20102\u2010yl)benzo[1,2\u2010b:4,5\u2010b\u2032]di(cyclopenta\u2010dithiophene) electron\u2010donating core and 1,1\u2010dicyanomethylene\u20103\u2010indanone electron\u2010withdrawing end groups. itic2 with the conjugated side chains exhibits an absorption peak at 714 nm, which redshifts 12 nm relative to itic1. the absorption extinction coefficient of itic2 is 2.7 \u00d7 105m\u22121 cm\u22121, higher than that of itic1 (1.5 \u00d7 105m\u22121 cm\u22121). itic2 exhibits slightly higher highest occupied molecular orbital (homo) (\u22125.43 ev) and lowest unoccupied molecular orbital (lumo) (\u22123.80 ev) energy levels relative to itic1 (homo: \u22125.48 ev; lumo: \u22123.84 ev), and higher electron mobility (1.3 \u00d7 10\u22123 cm2 v\u22121 s\u22121) than that of itic1 (9.6 \u00d7 10\u22124 cm2 v\u22121 s\u22121). the power conversion efficiency of itic2\u2010based organic solar cells is 11.0%, much higher than that of itic1\u2010based control devices (8.54%). our results demonstrate that side\u2010chain conjugation can tune energy levels, enhance absorption, and electron mobility, and finally enhance photovoltaic performance of nonfullerene acceptors."
                },
                {
                    "id": "R148246",
                    "label": "Design, synthesis, and structural characterization of the first dithienocyclopentacarbazole-based n-type organic semiconductor and its application in non-fullerene polymer solar cells",
                    "doi": "10.1039/c7ta01143a",
                    "research_field": {
                        "id": "R126",
                        "label": "Materials Chemistry"
                    },
                    "abstract": "a novel dithienocyclopentacarbazole-containing n-type organic semiconductor (dtcc\u2013ic) was designed and synthesized as the acceptor for non-fullerene solar cells."
                },
                {
                    "id": "R148537",
                    "label": "A Twisted Thieno[3,4-b\n]thiophene-Based Electron Acceptor Featuring a 14-\u03c0-Electron Indenoindene Core for High-Performance Organic Photovoltaics",
                    "doi": "10.1002/adma.201704510",
                    "research_field": {
                        "id": "R126",
                        "label": "Materials Chemistry"
                    },
                    "abstract": "with an indenoindene core, a new thieno[3,4\u2010b]thiophene\u2010based small\u2010molecule electron acceptor, 2,2\u2032\u2010((2z,2\u2032z)\u2010((6,6\u2032\u2010(5,5,10,10\u2010tetrakis(2\u2010ethylhexyl)\u20105,10\u2010dihydroindeno[2,1\u2010a]indene\u20102,7\u2010diyl)bis(2\u2010octylthieno[3,4\u2010b]thiophene\u20106,4\u2010diyl))bis(methanylylidene))bis(5,6\u2010difluoro\u20103\u2010oxo\u20102,3\u2010dihydro\u20101h\u2010indene\u20102,1\u2010diylidene))dimalononitrile (niti), is successfully designed and synthesized. compared with 12\u2010\u03c0\u2010electron fluorene, a carbon\u2010bridged biphenylene with an axial symmetry, indenoindene, a carbon\u2010bridged e\u2010stilbene with a centrosymmetry, shows elongated \u03c0\u2010conjugation with 14 \u03c0\u2010electrons and one more sp3 carbon bridge, which may increase the tunability of electronic structure and film morphology. despite its twisted molecular framework, niti shows a low optical bandgap of 1.49 ev in thin film and a high molar extinction coefficient of 1.90 \u00d7 105m\u22121 cm\u22121 in solution. by matching niti with a large\u2010bandgap polymer donor, an extraordinary power conversion efficiency of 12.74% is achieved, which is among the best performance so far reported for fullerene\u2010free organic photovoltaics and is inspiring for the design of new electron acceptors."
                },
                {
                    "id": "R148606",
                    "label": "Fused Hexacyclic Nonfullerene Acceptor with Strong Near\u2010Infrared Absorption for Semitransparent Organic Solar Cells with 9.77% Efficiency",
                    "doi": "10.1002/adma.201701308",
                    "research_field": {
                        "id": "R126",
                        "label": "Materials Chemistry"
                    },
                    "abstract": "a fused hexacyclic electron acceptor, ihic, based on strong electron\u2010donating group dithienocyclopentathieno[3,2\u2010b]thiophene flanked by strong electron\u2010withdrawing group 1,1\u2010dicyanomethylene\u20103\u2010indanone, is designed, synthesized, and applied in semitransparent organic solar cells (st\u2010oscs). ihic exhibits strong near\u2010infrared absorption with extinction coefficients of up to 1.6 \u00d7 105m\u22121 cm\u22121, a narrow optical bandgap of 1.38 ev, and a high electron mobility of 2.4 \u00d7 10\u22123 cm2 v\u22121 s\u22121. the st\u2010oscs based on blends of a narrow\u2010bandgap polymer donor ptb7\u2010th and narrow\u2010bandgap ihic acceptor exhibit a champion power conversion efficiency of 9.77% with an average visible transmittance of 36% and excellent device stability; this efficiency is much higher than any single\u2010junction and tandem st\u2010oscs reported in the literature."
                },
                {
                    "id": "R148630",
                    "label": "Naphthodithiophene\u2010Based Nonfullerene Acceptor for High\u2010Performance Organic Photovoltaics: Effect of Extended Conjugation",
                    "doi": "10.1002/adma.201704713",
                    "research_field": {
                        "id": "R126",
                        "label": "Materials Chemistry"
                    },
                    "abstract": "naphtho[1,2\u2010b:5,6\u2010b\u2032]dithiophene is extended to a fused octacyclic building block, which is end capped by strong electron\u2010withdrawing 2\u2010(5,6\u2010difluoro\u20103\u2010oxo\u20102,3\u2010dihydro\u20101h\u2010inden\u20101\u2010ylidene)malononitrile to yield a fused\u2010ring electron acceptor (ioic2) for organic solar cells (oscs). relative to naphthalene\u2010based ihic2, naphthodithiophene\u2010based ioic2 with a larger \u03c0\u2010conjugation and a stronger electron\u2010donating core shows a higher lowest unoccupied molecular orbital energy level (ioic2: \u22123.78 ev vs ihic2: \u22123.86 ev), broader absorption with a smaller optical bandgap (ioic2: 1.55 ev vs ihic2: 1.66 ev), and a higher electron mobility (ioic2: 1.0 \u00d7 10\u22123 cm2 v\u22121 s\u22121 vs ihic2: 5.0 \u00d7 10\u22124 cm2 v\u22121 s\u22121). thus, ioic2\u2010based oscs show higher values in open\u2010circuit voltage, short\u2010circuit current density, fill factor, and thereby much higher power conversion efficiency (pce) values than those of the ihic2\u2010based counterpart. in particular, as\u2010cast oscs based on ftaz: ioic2 yield pces of up to 11.2%, higher than that of the control devices based on ftaz: ihic2 (7.45%). furthermore, by using 0.2% 1,8\u2010diiodooctane as the processing additive, a pce of 12.3% is achieved from the ftaz:ioic2\u2010based devices, higher than that of the ftaz:ihic2\u2010based devices (7.31%). these results indicate that incorporating extended conjugation into the electron\u2010donating fused\u2010ring units in nonfullerene acceptors is a promising strategy for designing high\u2010performance electron acceptors."
                },
                {
                    "id": "R148652",
                    "label": "Dithieno[3,2-b\n:2\u2032,3\u2032-d\n]pyrrol Fused Nonfullerene Acceptors Enabling Over 13% Efficiency for Organic Solar Cells",
                    "doi": "10.1002/adma.201707150",
                    "research_field": {
                        "id": "R126",
                        "label": "Materials Chemistry"
                    },
                    "abstract": "a new electron\u2010rich central building block, 5,5,12,12\u2010tetrakis(4\u2010hexylphenyl)\u2010indacenobis\u2010(dithieno[3,2\u2010b:2\u2032,3\u2032\u2010d]pyrrol) (inp), and two derivative nonfullerene acceptors (inpic and inpic\u20104f) are designed and synthesized. the two molecules reveal broad (600\u2013900 nm) and strong absorption due to the satisfactory electron\u2010donating ability of inp. compared with its counterpart inpic, fluorinated nonfullerene acceptor inpic\u20104f exhibits a stronger near\u2010infrared absorption with a narrower optical bandgap of 1.39 ev, an improved crystallinity with higher electron mobility, and down\u2010shifted highest occupied molecular orbital and lowest unoccupied molecular orbital energy levels. organic solar cells (oscs) based on inpic\u20104f exhibit a high power conversion efficiency (pce) of 13.13% and a relatively low energy loss of 0.54 ev, which is among the highest efficiencies reported for binary oscs in the literature. the results demonstrate the great potential of the new inp as an electron\u2010donating building block for constructing high\u2010performance nonfullerene acceptors for oscs."
                },
                {
                    "id": "R148663",
                    "label": "Dithienopicenocarbazole-Based Acceptors for Efficient Organic Solar Cells with Optoelectronic Response Over 1000 nm and an Extremely Low Energy Loss",
                    "doi": "10.1021/jacs.7b13239",
                    "research_field": {
                        "id": "R126",
                        "label": "Materials Chemistry"
                    },
                    "abstract": "two cheliform non-fullerene acceptors, dtpc-ic and dtpc-dfic, based on a highly electron-rich core, dithienopicenocarbazole (dtpc), are synthesized, showing ultra-narrow bandgaps (as low as 1.21 ev). the two-dimensional nitrogen-containing conjugated dtpc possesses strong electron-donating capability, which induces intense intramolecular charge transfer and intermolecular \u03c0-\u03c0 stacking in derived acceptors. the solar cell based on dtpc-dfic and a spectrally complementary polymer donor, ptb7-th, showed a high power conversion efficiency of 10.21% and an extremely low energy loss of 0.45 ev, which is the lowest among reported efficient oscs."
                }
            ]
        },
        {
            "id": "R149061",
            "label": "Biodiversity inventories with DNA barcoding",
            "research_fields": [
                {
                    "id": "R136127",
                    "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                },
                {
                    "id": "R136191",
                    "label": "Ecology and Biodiversity of Plants and Ecosystems"
                }
            ],
            "properties": [
                "Biogeographical region",
                "Locus (genetics)",
                "higher number estimated species",
                "higher number estimated species (Method)",
                "No. of estimated species",
                "No. of estimated species (Method)",
                "lower number estimated species",
                "lower number estimated species (Method)",
                "Class (Taxonomy - biology)",
                "DNA sequencing technology",
                "Phylum (Biology)",
                "No. of potential undescribed species",
                "Study Location ",
                "Order (Taxonomy - biology)",
                "No. of samples (sequences)",
                "Studied taxonomic group (Biology)",
                "Number of identified species with current taxonomy"
            ],
            "papers": [
                {
                    "id": "R108960",
                    "label": "Use of species delimitation approaches to tackle the cryptic diversity of an assemblage of high Andean butterflies (Lepidoptera: Papilionoidea)",
                    "doi": "10.1139/gen-2020-0100",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "cryptic biological diversity has generated ambiguity in taxonomic and evolutionary studies. single-locus methods and other approaches for species delimitation are useful for addressing this challenge, enabling the practical processing of large numbers of samples for identification and inventory purposes. this study analyzed an assemblage of high andean butterflies using dna barcoding and compared the identifications based on the current morphological taxonomy with three methods of species delimitation (automatic barcode gap discovery, generalized mixed yule coalescent model, and poisson tree processes). sixteen potential cryptic species were recognized using these three methods, representing a net richness increase of 11.3% in the assemblage. a well-studied taxon of the genus vanessa, which has a wide geographical distribution, appeared with the potential cryptic species that had a higher genetic differentiation at the local level than at the continental level. the analyses were useful for identifying the potential cryptic species in pedaliodes and forsterinaria complexes, which also show differentiation along altitudinal and latitudinal gradients. this genetic assessment of an entire assemblage of high andean butterflies (papilionoidea) provides baseline information for future research in a region characterized by high rates of endemism and population isolation."
                },
                {
                    "id": "R108983",
                    "label": "Barcoding the butterflies of southern South America: Species delimitation efficacy, cryptic diversity and geographic patterns of divergence",
                    "doi": "10.1371/journal.pone.0186845",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "because the tropical regions of america harbor the highest concentration of butterfly species, its fauna has attracted considerable attention. much less is known about the butterflies of southern south america, particularly argentina, where over 1,200 species occur. to advance understanding of this fauna, we assembled a dna barcode reference library for 417 butterfly species of argentina, focusing on the atlantic forest, a biodiversity hotspot. we tested the efficacy of this library for specimen identification, used it to assess the frequency of cryptic species, and examined geographic patterns of genetic variation, making this study the first large-scale genetic assessment of the butterflies of southern south america. the average sequence divergence to the nearest neighbor (i.e. minimum interspecific distance) was 6.91%, ten times larger than the mean distance to the furthest conspecific (0.69%), with a clear barcode gap present in all but four of the species represented by two or more specimens. as a consequence, the dna barcode library was extremely effective in the discrimination of these species, allowing a correct identification in more than 95% of the cases. singletons (i.e. species represented by a single sequence) were also distinguishable in the gene trees since they all had unique dna barcodes, divergent from those of the closest non-conspecific. the clustering algorithms implemented recognized from 416 to 444 barcode clusters, suggesting that the actual diversity of butterflies in argentina is 3%\u20139% higher than currently recognized. furthermore, our survey added three new records of butterflies for the country (eurema agave, mithras hannelore, melanis hillapana). in summary, this study not only supported the utility of dna barcoding for the identification of the butterfly species of argentina, but also highlighted several cases of both deep intraspecific and shallow interspecific divergence that should be studied in more detail."
                },
                {
                    "id": "R109043",
                    "label": "A DNA barcode library for the butterflies of North America",
                    "doi": "10.7717/peerj.11157",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "although the butterflies of north america have received considerable taxonomic attention, overlooked species and instances of hybridization continue to be revealed. the present study assembles a dna barcode reference library for this fauna to identify groups whose patterns of sequence variation suggest the need for further taxonomic study. based on 14,626 records from 814 species, dna barcodes were obtained for 96% of the fauna. the maximum intraspecific distance averaged 1/4 the minimum distance to the nearest neighbor, producing a barcode gap in 76% of the species. most species (80%) were monophyletic, the others were para- or polyphyletic. although 15% of currently recognized species shared barcodes, the incidence of such taxa was far higher in regions exposed to pleistocene glaciations than in those that were ice-free. nearly 10% of species displayed high intraspecific variation (&gt;2.5%), suggesting the need for further investigation to assess potential cryptic diversity. aside from aiding the identification of all life stages of north american butterflies, the reference library has provided new perspectives on the incidence of both cryptic and potentially over-split species, setting the stage for future studies that can further explore the evolutionary dynamics of this group."
                },
                {
                    "id": "R136193",
                    "label": "Complete DNA barcode reference library for a country's butterfly fauna reveals high performance for temperate Europe",
                    "doi": "10.1098/rspb.2010.1089",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "dna barcoding aims to accelerate species identification and discovery, but performance tests have shown marked differences in identification success. as a consequence, there remains a great need for comprehensive studies which objectively test the method in groups with a solid taxonomic framework. this study focuses on the 180 species of butterflies in romania, accounting for about one third of the european butterfly fauna. this country includes five eco-regions, the highest of any in the european union, and is a good representative for temperate areas. morphology and dna barcodes of more than 1300 specimens were carefully studied and compared. our results indicate that 90 per cent of the species form barcode clusters allowing their reliable identification. the remaining cases involve nine closely related species pairs, some whose taxonomic status is controversial or that hybridize regularly. interestingly, dna barcoding was found to be the most effective identification tool, outperforming external morphology, and being slightly better than male genitalia. romania is now the first country to have a comprehensive dna barcode reference database for butterflies. similar barcoding efforts based on comprehensive sampling of specific geographical regions can act as functional modules that will foster the early application of dna barcoding while a global system is under development."
                },
                {
                    "id": "R138551",
                    "label": "Probing planetary biodiversity with DNA barcodes: The Noctuoidea of North America",
                    "doi": "10.1371/journal.pone.0178548",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "this study reports the assembly of a dna barcode reference library for species in the lepidopteran superfamily noctuoidea from canada and the usa. based on the analysis of 69,378 specimens, the library provides coverage for 97.3% of the noctuoid fauna (3565 of 3664 species). in addition to verifying the strong performance of dna barcodes in the discrimination of these species, the results indicate close congruence between the number of species analyzed (3565) and the number of sequence clusters (3816) recognized by the barcode index number (bin) system. distributional patterns across 12 north american ecoregions are examined for the 3251 species that have gps data while bin analysis is used to quantify overlap between the noctuoid faunas of north america and other zoogeographic regions. this analysis reveals that 90% of north american noctuoids are endemic and that just 7.5% and 1.8% of bins are shared with the neotropics and with the palearctic, respectively. one third (29) of the latter species are recent introductions and, as expected, they possess low intraspecific divergences."
                },
                {
                    "id": "R138562",
                    "label": "Fast Census of Moth Diversity in the Neotropics: A Comparison of Field-Assigned Morphospecies and DNA Barcoding in Tiger Moths",
                    "doi": "10.1371/journal.pone.0148423",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "the morphological species delimitations (i.e. morphospecies) have long been the best way to avoid the taxonomic impediment and compare insect taxa biodiversity in highly diverse tropical and subtropical regions. the development of dna barcoding, however, has shown great potential to replace (or at least complement) the morphospecies approach, with the advantage of relying on automated methods implemented in computer programs or even online rather than in often subjective morphological features. we sampled moths extensively for two years using light traps in a patch of the highly endangered atlantic forest of brazil to produce a nearly complete census of arctiines (noctuoidea: erebidae), whose species richness was compared using different morphological and molecular approaches (dna barcoding). a total of 1,075 barcode sequences of 286 morphospecies were analyzed. based on the clustering method barcode index number (bin) we found a taxonomic bias of approximately 30% in our initial morphological assessment. however, a morphological reassessment revealed that the correspondence between morphospecies and molecular operational taxonomic units (motus) can be up to 94% if differences in genitalia morphology are evaluated in individuals of different motus originated from the same morphospecies (putative cases of cryptic species), and by recording if individuals of different genders in different morphospecies merge together in the same motu (putative cases of sexual dimorphism). the results of two other clustering methods (i.e. automatic barcode gap discovery and 2% threshold) were very similar to those of the bin approach. using empirical data we have shown that dna barcoding performed substantially better than the morphospecies approach, based on superficial morphology, to delimit species of a highly diverse moth taxon, and thus should be used in species inventories."
                },
                {
                    "id": "R139497",
                    "label": "Congruence between morphology-based species and Barcode Index Numbers (BINs) in Neotropical Eumaeini (Lycaenidae)",
                    "doi": "10.7717/peerj.11843",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "\\n background \\n with about 1,000 species in the neotropics, the eumaeini (theclinae) are one of the most diverse butterfly tribes. correct morphology-based identifications are challenging in many genera due to relatively little interspecific differences in wing patterns. geographic infraspecific variation is sometimes more substantial than variation between species. in this paper we present a large dna barcode dataset of south american lycaenidae. we analyze how well dna barcode bins match morphologically delimited species. \\n \\n \\n methods \\n we compare morphology-based species identifications with the clustering of molecular operational taxonomic units (motus) delimitated by the resl algorithm in bold, which assigns barcode index numbers (bins). we examine intra- and interspecific divergences for genera represented by at least four morphospecies. we discuss the existence of local barcode gaps in a genus by genus analysis. we also note differences in the percentage of species with barcode gaps in groups of lowland and high mountain genera. \\n \\n \\n results \\n we identified 2,213 specimens and obtained 1,839 sequences of 512 species in 90 genera. overall, the mean intraspecific divergence value of co1 sequences was 1.20%, while the mean interspecific divergence between nearest congeneric neighbors was 4.89%, demonstrating the presence of a barcode gap. however, the gap seemed to disappear from the entire set when comparing the maximum intraspecific distance (8.40%) with the minimum interspecific distance (0.40%). clear barcode gaps are present in many genera but absent in others. from the set of specimens that yielded coi fragment lengths of at least 650 bp, 75% of the a priori morphology-based identifications were unambiguously assigned to a single barcode index number (bin). however, after a taxonomic a posteriori review, the percentage of matched identifications rose to 85%. bin splitting was observed for 17% of the species and bin sharing for 9%. we found that genera that contain primarily lowland species show higher percentages of local barcode gaps and congruence between bins and morphology than genera that contain exclusively high montane species. the divergence values to the nearest neighbors were significantly lower in high andean species while the intra-specific divergence values were significantly lower in the lowland species. these results raise questions regarding the causes of observed low inter and high intraspecific genetic variation. we discuss incomplete lineage sorting and hybridization as most likely causes of this phenomenon, as the montane species concerned are relatively young and hybridization is probable. the release of our data set represents an essential baseline for a reference library for biological assessment studies of butterflies in mega diverse countries using modern high-throughput technologies an highlights the necessity of taxonomic revisions for various genera combining both molecular and morphological data. \\n"
                },
                {
                    "id": "R139538",
                    "label": "High resolution DNA barcode library for European butterflies reveals continental patterns of mitochondrial genetic diversity",
                    "doi": "10.1038/s42003-021-01834-7",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "abstract the study of global biodiversity will greatly benefit from access to comprehensive dna barcode libraries at continental scale, but such datasets are still very rare. here, we assemble the first high-resolution reference library for european butterflies that provides 97% taxon coverage (459 species) and 22,306 coi sequences. we estimate that we captured 62% of the total haplotype diversity and show that most species possess a few very common haplotypes and many rare ones. specimens in the dataset have an average 95.3% probability of being correctly identified. mitochondrial diversity displayed elevated haplotype richness in southern european refugia, establishing the generality of this key biogeographic pattern for an entire taxonomic group. fifteen percent of the species are involved in barcode sharing, but two thirds of these cases may reflect the need for further taxonomic research. this dataset provides a unique resource for conservation and for studying evolutionary processes, cryptic species, phylogeography, and ecology."
                },
                {
                    "id": "R139546",
                    "label": "A DNA barcode reference library for Swiss butterflies and forester moths as a tool for species identification, systematics and conservation",
                    "doi": "10.1371/journal.pone.0208639",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "butterfly monitoring and red list programs in switzerland rely on a combination of observations and collection records to document changes in species distributions through time. while most butterflies can be identified using morphology, some taxa remain challenging, making it difficult to accurately map their distributions and develop appropriate conservation measures. in this paper, we explore the use of the dna barcode (a fragment of the mitochondrial gene coi) as a tool for the identification of swiss butterflies and forester moths (rhopalocera and zygaenidae). we present a national dna barcode reference library including 868 sequences representing 217 out of 224 resident species, or 96.9% of swiss fauna. dna barcodes were diagnostic for nearly 90% of swiss species. the remaining 10% represent cases of para- and polyphyly likely involving introgression or incomplete lineage sorting among closely related taxa. we demonstrate that integrative taxonomic methods incorporating a combination of morphological and genetic techniques result in a rate of species identification of over 96% in females and over 98% in males, higher than either morphology or dna barcodes alone. we explore the use of the dna barcode for exploring boundaries among taxa, understanding the geographical distribution of cryptic diversity and evaluating the status of purportedly endemic taxa. finally, we discuss how dna barcodes may be used to improve field practices and ultimately enhance conservation strategies."
                },
                {
                    "id": "R140187",
                    "label": "DNA Barcoding the Geometrid Fauna of Bavaria (Lepidoptera): Successes, Surprises, and Questions",
                    "doi": "10.1371/journal.pone.0017134",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "background the state of bavaria is involved in a research program that will lead to the construction of a dna barcode library for all animal species within its territorial boundaries. the present study provides a comprehensive dna barcode library for the geometridae, one of the most diverse of insect families. methodology/principal findings this study reports dna barcodes for 400 bavarian geometrid species, 98 per cent of the known fauna, and approximately one per cent of all bavarian animal species. although 98.5% of these species possess diagnostic barcode sequences in bavaria, records from neighbouring countries suggest that species-level resolution may be compromised in up to 3.5% of cases. all taxa which apparently share barcodes are discussed in detail. one case of modest divergence (1.4%) revealed a species overlooked by the current taxonomic system: eupithecia goossensiata mabille, 1869 stat.n. is raised from synonymy with eupithecia absinthiata (clerck, 1759) to species rank. deep intraspecific sequence divergences (>2%) were detected in 20 traditionally recognized species. conclusions/significance the study emphasizes the effectiveness of dna barcoding as a tool for monitoring biodiversity. open access is provided to a data set that includes records for 1,395 geometrid specimens (331 species) from bavaria, with 69 additional species from neighbouring regions. taxa with deep intraspecific sequence divergences are undergoing more detailed analysis to ascertain if they represent cases of cryptic diversity."
                },
                {
                    "id": "R140197",
                    "label": "DNA barcodes distinguish species of tropical Lepidoptera",
                    "doi": "10.1073/pnas.0510466103",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "although central to much biological research, the identification of species is often difficult. the use of dna barcodes, short dna sequences from a standardized region of the genome, has recently been proposed as a tool to facilitate species identification and discovery. however, the effectiveness of dna barcoding for identifying specimens in species-rich tropical biotas is unknown. here we show that cytochrome c oxidase i dna barcodes effectively discriminate among species in three lepidoptera families from area de conservaci\u00f3n guanacaste in northwestern costa rica. we found that 97.9% of the 521 species recognized by prior taxonomic work possess distinctive cytochrome c oxidase i barcodes and that the few instances of interspecific sequence overlap involve very similar species. we also found two or more barcode clusters within each of 13 supposedly single species. covariation between these clusters and morphological and/or ecological traits indicates overlooked species complexes. if these results are general, dna barcoding will significantly aid species identification and discovery in tropical settings."
                },
                {
                    "id": "R140252",
                    "label": "Species-Level Para- and Polyphyly in DNA Barcode Gene Trees: Strong Operational Bias in European Lepidoptera",
                    "doi": "10.1093/sysbio/syw044",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "the proliferation of dna data is revolutionizing all fields of systematic research. dna barcode sequences, now available for millions of specimens and several hundred thousand species, are increasingly used in algorithmic species delimitations. this is complicated by occasional incongruences between species and gene genealogies, as indicated by situations where conspecific individuals do not form a monophyletic cluster in a gene tree. in two previous reviews, non-monophyly has been reported as being common in mitochondrial dna gene trees. we developed a novel web service \u201cmonophylizer\u201d to detect non-monophyly in phylogenetic trees and used it to ascertain the incidence of species non-monophyly in coi (a.k.a. cox1) barcode sequence data from 4977 species and 41,583 specimens of european lepidoptera, the largest data set of dna barcodes analyzed from this regard. particular attention was paid to accurate species identification to ensure data integrity. we investigated the effects of tree-building method, sampling effort, and other methodological issues, all of which can influence estimates of non-monophyly. we found a 12% incidence of non-monophyly, a value significantly lower than that observed in previous studies. neighbor joining (nj) and maximum likelihood (ml) methods yielded almost equal numbers of non-monophyletic species, but 24.1% of these cases of non-monophyly were only found by one of these methods. non-monophyletic species tend to show either low genetic distances to their nearest neighbors or exceptionally high levels of intraspecific variability. cases of polyphyly in coi trees arising as a result of deep intraspecific divergence are negligible, as the detected cases reflected misidentifications or methodological errors. taking into consideration variation in sampling effort, we estimate that the true incidence of non-monophyly is \u223c23%, but with operational factors still being included. within the operational factors, we separately assessed the frequency of taxonomic limitations (presence of overlooked cryptic and oversplit species) and identification uncertainties. we observed that operational factors are potentially present in more than half (58.6%) of the detected cases of non-monophyly. furthermore, we observed that in about 20% of non-monophyletic species and entangled species, the lineages involved are either allopatric or parapatric\u2014conditions where species delimitation is inherently subjective and particularly dependent on the species concept that has been adopted. these observations suggest that species-level non-monophyly in coi gene trees is less common than previously supposed, with many cases reflecting misidentifications, the subjectivity of species delimitation or other operational factors."
                },
                {
                    "id": "R142471",
                    "label": "DNA barcoding of Northern Nearctic Muscidae (Diptera) reveals high correspondence between morphological and molecular species limits",
                    "doi": "10.1186/1472-6785-12-24",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "abstract \\n \\n background \\n various methods have been proposed to assign unknown specimens to known species using their dna barcodes, while others have focused on using genetic divergence thresholds to estimate \u201cspecies\u201d diversity for a taxon, without a well-developed taxonomy and/or an extensive reference library of dna barcodes. the major goals of the present work were to: a) conduct the largest species-level barcoding study of the muscidae to date and characterize the range of genetic divergence values in the northern nearctic fauna; b) evaluate the correspondence between morphospecies and barcode groupings defined using both clustering-based and threshold-based approaches; and c) use the reference library produced to address taxonomic issues. \\n \\n \\n results \\n our data set included 1114 individuals and their coi sequences (951 from churchill, manitoba), representing 160 morphologically-determined species from 25 genera, covering 89% of the known fauna of churchill and 23% of the nearctic fauna. following an iterative process through which all specimens belonging to taxa with anomalous divergence values and/or monophyly issues were re-examined, identity was modified for 9 taxa, including the reinstatement of phaonia luteva (walker) stat. nov. as a species distinct from phaonia errans (meigen). in the post-reassessment data set, no distinct gap was found between maximum pairwise intraspecific distances (range 0.00-3.01%) and minimum interspecific distances (range: 0.77-11.33%). nevertheless, using a clustering-based approach, all individuals within 98% of species grouped with their conspecifics with high (&gt;95%) bootstrap support; in contrast, a maximum species discrimination rate of 90% was obtained at the optimal threshold of 1.2%. dna barcoding enabled the determination of females from 5 ambiguous species pairs and confirmed that 16 morphospecies were genetically distinct from named taxa. there were morphological differences among all distinct genetic clusters; thus, no cases of cryptic species were detected. \\n \\n \\n conclusions \\n our findings reveal the great utility of building a well-populated, species-level reference barcode database against which to compare unknowns. when such a library is unavailable, it is still possible to obtain a fairly accurate (within ~10%) rapid assessment of species richness based upon a barcode divergence threshold alone, but this approach is most accurate when the threshold is tuned to a particular taxon. \\n"
                },
                {
                    "id": "R142517",
                    "label": "A DNA barcode library for 5,200 German flies and midges (Insecta: Diptera) and its implications for metabarcoding\u2010based biomonitoring",
                    "doi": "10.1111/1755-0998.13022",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "this study summarizes results of a dna barcoding campaign on german diptera, involving analysis of 45,040 specimens. the resultant dna barcode library includes records for 2,453 named species comprising a total of 5,200 barcode index numbers (bins), including 2,700 coi haplotype clusters without species\u2010level assignment, so called \u201cdark taxa.\u201d overall, 88 out of 117 families (75%) recorded from germany were covered, representing more than 50% of the 9,544 known species of german diptera. until now, most of these families, especially the most diverse, have been taxonomically inaccessible. by contrast, within a few years this study provided an intermediate taxonomic system for half of the german dipteran fauna, which will provide a useful foundation for subsequent detailed, integrative taxonomic studies. using dna extracts derived from bulk collections made by malaise traps, we further demonstrate that species delineation using bins and operational taxonomic units (otus) constitutes an effective method for biodiversity studies using dna metabarcoding. as the reference libraries continue to grow, and gaps in the species catalogue are filled, bin lists assembled by metabarcoding will provide greater taxonomic resolution. the present study has three main goals: (a) to provide a dna barcode library for 5,200 bins of diptera; (b) to demonstrate, based on the example of bulk extractions from a malaise trap experiment, that dna barcode clusters, labelled with globally unique identifiers (such as otus and/or bins), provide a pragmatic, accurate solution to the \u201ctaxonomic impediment\u201d; and (c) to demonstrate that interim names based on bins and otus obtained through metabarcoding provide an effective method for studies on species\u2010rich groups that are usually neglected in biodiversity research projects because of their unresolved taxonomy."
                },
                {
                    "id": "R142535",
                    "label": "DNA Barcodes for the Northern European Tachinid Flies (Diptera: Tachinidae)",
                    "doi": "10.1371/journal.pone.0164933",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "this data release provides coi barcodes for 366 species of parasitic flies (diptera: tachinidae), enabling the dna based identification of the majority of northern european species and a large proportion of palearctic genera, regardless of the developmental stage. the data will provide a tool for taxonomists and ecologists studying this ecologically important but challenging parasitoid family. a comparison of minimum distances between the nearest neighbors revealed the mean divergence of 5.52% that is approximately the same as observed earlier with comparable sampling in lepidoptera, but clearly less than in coleoptera. full barcode-sharing was observed between 13 species pairs or triplets, equaling to 7.36% of all species. delimitation based on barcode index number (bin) system was compared with traditional classification of species and interesting cases of possible species oversplits and cryptic diversity are discussed. overall, dna barcodes are effective in separating tachinid species and provide novel insight into the taxonomy of several genera."
                },
                {
                    "id": "R145296",
                    "label": "Molecular identification of mosquitoes (Diptera: Culicidae) in southeastern Australia",
                    "doi": "10.1002/ece3.2095",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "abstract dna barcoding is a modern species identification technique that can be used to distinguish morphologically similar species, and is particularly useful when using small amounts of starting material from partial specimens or from immature stages. in order to use dna barcoding in a surveillance program, a database containing mosquito barcode sequences is required. this study obtained cytochrome oxidase i (coi) sequences for 113 morphologically identified specimens, representing 29 species, six tribes and 12 genera; 17 of these species have not been previously barcoded. three of the 29 species \u2500 culex palpalis, macleaya macmillani, and an unknown species originally identified as tripteroides atripes \u2500 were initially misidentified as they are difficult to separate morphologically, highlighting the utility of dna barcoding. while most species grouped separately (reciprocally monophyletic), the cx. pipiens subgroup could not be genetically separated using coi. the average conspecific and congeneric p\u2010distance was 0.8% and 7.6%, respectively. in our study, we also demonstrate the utility of dna barcoding in distinguishing exotics from endemic mosquitoes by identifying a single intercepted stegomyia aegypti egg at an international airport. the use of dna barcoding dramatically reduced the identification time required compared with rearing specimens through to adults, thereby demonstrating the value of this technique in biosecurity surveillance. the dna barcodes produced by this study have been uploaded to the \u2018mosquitoes of australia\u2013victoria\u2019 project on the barcode of life database (bold), which will serve as a resource for the victorian arbovirus disease control program and other national and international mosquito surveillance programs."
                },
                {
                    "id": "R145304",
                    "label": "Analyzing Mosquito (Diptera: Culicidae) Diversity in Pakistan by DNA Barcoding",
                    "doi": "10.1371/journal.pone.0097268",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "background although they are important disease vectors mosquito biodiversity in pakistan is poorly known. recent epidemics of dengue fever have revealed the need for more detailed understanding of the diversity and distributions of mosquito species in this region. dna barcoding improves the accuracy of mosquito inventories because morphological differences between many species are subtle, leading to misidentifications. methodology/principal findings sequence variation in the barcode region of the mitochondrial coi gene was used to identify mosquito species, reveal genetic diversity, and map the distribution of the dengue-vector species in pakistan. analysis of 1684 mosquitoes from 491 sites in punjab and khyber pakhtunkhwa during 2010\u20132013 revealed 32 species with the assemblage dominated by culex quinquefasciatus (61% of the collection). the genus aedes (stegomyia) comprised 15% of the specimens, and was represented by six taxa with the two dengue vector species, ae. albopictus and ae. aegypti, dominant and broadly distributed. anopheles made up another 6% of the catch with an. subpictus dominating. barcode sequence divergence in conspecific specimens ranged from 0\u20132.4%, while congeneric species showed from 2.3\u201317.8% divergence. a global haplotype analysis of disease-vectors showed the presence of multiple haplotypes, although a single haplotype of each dengue-vector species was dominant in most countries. geographic distribution of ae. aegypti and ae. albopictus showed the later species was dominant and found in both rural and urban environments. conclusions as the first dna-based analysis of mosquitoes in pakistan, this study has begun the construction of a barcode reference library for the mosquitoes of this region. levels of genetic diversity varied among species. because of its capacity to differentiate species, even those with subtle morphological differences, dna barcoding aids accurate tracking of vector populations."
                },
                {
                    "id": "R145434",
                    "label": "DNA Barcoding of Neotropical Sand Flies (Diptera, Psychodidae, Phlebotominae): Species Identification and Discovery within Brazil",
                    "doi": "10.1371/journal.pone.0140636",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "dna barcoding has been an effective tool for species identification in several animal groups. here, we used dna barcoding to discriminate between 47 morphologically distinct species of brazilian sand flies. dna barcodes correctly identified approximately 90% of the sampled taxa (42 morphologically distinct species) using clustering based on neighbor-joining distance, of which four species showed comparatively higher maximum values of divergence (range 4.23\u201319.04%), indicating cryptic diversity. the dna barcodes also corroborated the resurrection of two species within the shannoni complex and provided an efficient tool to differentiate between morphologically indistinguishable females of closely related species. taken together, our results validate the effectiveness of dna barcoding for species identification and the discovery of cryptic diversity in sand flies from brazil."
                },
                {
                    "id": "R145437",
                    "label": "DNA Barcoding to Improve the Taxonomy of the Afrotropical Hoverflies (Insecta: Diptera: Syrphidae)",
                    "doi": "10.1371/journal.pone.0140264",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "the identification of afrotropical hoverflies is very difficult because of limited recent taxonomic revisions and the lack of comprehensive identification keys. in order to assist in their identification, and to improve the taxonomy of this group, we constructed a reference dataset of 513 coi barcodes of 90 of the more common nominal species from ghana, togo, benin and nigeria (w africa) and added ten publically available coi barcodes from nine nominal afrotropical species to this (total: 523 coi barcodes; 98 nominal species; 26 genera). the identification accuracy of this dataset was evaluated with three methods (k2p distance-based, neighbor-joining (nj) / maximum likelihood (ml) analysis, and using speciesidentifier). results of the three methods were highly congruent and showed a high identification success. nine species pairs showed a low ( 0.03) maximum intraspecific k2p distance was observed in eight species and barcodes of these species not always formed single clusters in the nj / ml analayses which may indicate the occurrence of cryptic species. optimal k2p thresholds to differentiate intra- from interspecific k2p divergence were highly different among the three subfamilies (eristalinae: 0.037, syrphinae: 0.06, microdontinae: 0.007\u20130.02), and among the different general suggesting that optimal thresholds are better defined at the genus level. in addition to providing an alternative identification tool, our study indicates that dna barcoding improves the taxonomy of afrotropical hoverflies by selecting (groups of) taxa that deserve further taxonomic study, and by attributing the unknown sex to species for which only one of the sexes is known."
                },
                {
                    "id": "R145468",
                    "label": "DNA barcoding of Neotropical black flies (Diptera: Simuliidae): Species identification and discovery of cryptic diversity in Mesoamerica",
                    "doi": "10.11646/zootaxa.3936.1.5",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "although correct taxonomy is paramount for disease control programs and epidemiological studies, morphology-based taxonomy of black flies is extremely difficult. in the present study, the utility of a partial sequence of the coi gene, the dna barcoding region, for the identification of species of black flies from mesoamerica was assessed. a total of 32 morphospecies were analyzed, one belonging to the genus gigantodax and 31 species to the genus simulium and six of its subgenera (aspathia, eusimulium, notolepria, psaroniocompsa, psilopelmia, trichodagmia). the neighbour joining tree (nj) derived from the dna barcodes grouped most specimens according to species or species groups recognized by morphotaxonomic studies. intraspecific sequence divergences within morphologically distinct species ranged from 0.07% to 1.65%, while higher divergences (2.05%-6.13%) in species complexes suggested the presence of cryptic diversity. the existence of well-defined groups within s. callidum (dyar & shannon), s. quadrivittatum loew, and s. samboni jennings revealed the likely inclusion of cryptic species within these taxa. in addition, the suspected presence of sibling species within s. paynei vargas and s. tarsatum macquart was supported. dna barcodes also showed that specimens of species that are difficult to delimit morphologically such as s. callidum, s. pseudocallidum d\u00edaz n\u00e1jera, s. travisi vargas, vargas & ram\u00edrez-p\u00e9rez, relatives of the species complexes such as s. metallicum bellardi s.l. (e.g., s. horacioi okazawa & onishi, s. jobbinsi vargas, mart\u00ednez palacios, d\u00edaz n\u00e1jera, and s. puigi vargas, mart\u00ednez palacios & d\u00edaz n\u00e1jera), and s. virgatum coquillett complex (e.g., s. paynei and s. tarsatum) grouped together in the nj analysis, suggesting they represent valid species. dna barcoding combined with a sound morphotaxonomic framework provided an effective approach for the identification of medically important black flies species in mesoamerica and for the discovery of hidden diversity within this group."
                },
                {
                    "id": "R145491",
                    "label": "DNA barcoding of tropical black flies (Diptera: Simuliidae) of Thailand",
                    "doi": "10.1111/1755-0998.12174",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "the ecological and medical importance of black flies drives the need for rapid and reliable identification of these minute, structurally uniform insects. we assessed the efficiency of dna barcoding for species identification of tropical black flies. a total of 351 cytochrome c oxidase subunit 1 sequences were obtained from 41 species in six subgenera of the genus simulium in thailand. despite high intraspecific genetic divergence (mean = 2.00%, maximum = 9.27%), dna barcodes provided 96% correct identification. barcodes also differentiated cytoforms of selected species complexes, albeit with varying levels of success. perfect differentiation was achieved for two cytoforms of simulium feuerborni, and 91% correct identification was obtained for the simulium angulistylum complex. low success (33%), however, was obtained for the simulium siamense complex. the differential efficiency of dna barcodes to discriminate cytoforms was attributed to different levels of genetic structure and demographic histories of the taxa. dna barcode trees were largely congruent with phylogenies based on previous molecular, chromosomal and morphological analyses, but revealed inconsistencies that will require further evaluation."
                },
                {
                    "id": "R145495",
                    "label": "DNA Barcoding for the Identification of Sand Fly Species (Diptera, Psychodidae, Phlebotominae) in Colombia",
                    "doi": "10.1371/journal.pone.0085496",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "sand flies include a group of insects that are of medical importance and that vary in geographic distribution, ecology, and pathogen transmission. approximately 163 species of sand flies have been reported in colombia. surveillance of the presence of sand fly species and the actualization of species distribution are important for predicting risks for and monitoring the expansion of diseases which sand flies can transmit. currently, the identification of phlebotomine sand flies is based on morphological characters. however, morphological identification requires considerable skills and taxonomic expertise. in addition, significant morphological similarity between some species, especially among females, may cause difficulties during the identification process. dna-based approaches have become increasingly useful and promising tools for estimating sand fly diversity and for ensuring the rapid and accurate identification of species. a partial sequence of the mitochondrial cytochrome oxidase gene subunit i (coi) is currently being used to differentiate species in different animal taxa, including insects, and it is referred as a barcoding sequence. the present study explored the utility of the dna barcode approach for the identification of phlebotomine sand flies in colombia. we sequenced 700 bp of the coi gene from 36 species collected from different geographic localities. the coi barcode sequence divergence within a single species was <2% in most cases, whereas this divergence ranged from 9% to 26.6% among different species. these results indicated that the barcoding gene correctly discriminated among the previously morphologically identified species with an efficacy of nearly 100%. analyses of the generated sequences indicated that the observed species groupings were consistent with the morphological identifications. in conclusion, the barcoding gene was useful for species discrimination in sand flies from colombia."
                },
                {
                    "id": "R145497",
                    "label": "Half of the European fruit fly species barcoded (Diptera, Tephritidae); a feasibility test for molecular identification",
                    "doi": "10.3897/zookeys.365.5819",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "abstract a feasibility test of molecular identification of european fruit flies (diptera: tephritidae) based on coi barcode sequences has been executed. a dataset containing 555 sequences of 135 ingroup species from three subfamilies and 42 genera and one single outgroup species has been analysed. 73.3% of all included species could be identified based on their coi barcode gene, based on similarity and distances. the low success rate is caused by singletons as well as some problematic groups: several species groups within the genus terellia and especially the genus urophora. with slightly more than 100 sequences \u2013 almost 20% of the total \u2013 this genus alone constitutes the larger part of the failure for molecular identification for this dataset. deleting the singletons and urophora results in a success-rate of 87.1% of all queries and 93.23% of the not discarded queries as correctly identified. urophora is of special interest due to its economic importance as beneficial species for weed control, therefore it is desirable to have alternative markers for molecular identification. we demonstrate that the success of dna barcoding for identification purposes strongly depends on the contents of the database used to blast against. especially the necessity of including multiple specimens per species of geographically distinct populations and different ecologies for the understanding of the intra- versus interspecific variation is demonstrated. furthermore thresholds and the distinction between true and false positives and negatives should not only be used to increase the reliability of the success of molecular identification but also to point out problematic groups, which should then be flagged in the reference database suggesting alternative methods for identification."
                },
                {
                    "id": "R145502",
                    "label": "Barcoding of biting midges in the genus Culicoides: a tool for species determination",
                    "doi": "10.1111/j.1365-2915.2012.01050.x",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "biting midges of the genus culicoides (diptera: ceratopogonidae) are insect vectors of economically important veterinary diseases such as african horse sickness virus and bluetongue virus. however, the identification of culicoides based on morphological features is difficult. the sequencing of mitochondrial cytochrome oxidase subunit i (coi), referred to as dna barcoding, has been proposed as a tool for rapid identification to species. hence, a study was undertaken to establish dna barcodes for all morphologically determined culicoides species in swedish collections. in total, 237 specimens of culicoides representing 37 morphologically distinct species were used. the barcoding generated 37 supported clusters, 31 of which were in agreement with the morphological determination. however, two pairs of closely related species could not be separated using the dna barcode approach. moreover, culicoides obsoletus meigen and culicoides newsteadi austen showed relatively deep intraspecific divergence (more than 10 times the average), which led to the creation of two cryptic species within each of c. obsoletus and c. newsteadi. the use of coi barcodes as a tool for the species identification of biting midges can differentiate 95% of species studied. identification of some closely related species should employ a less conserved region, such as a ribosomal internal transcribed spacer."
                },
                {
                    "id": "R145506",
                    "label": "Identification of Nearctic black flies using DNA barcodes (Diptera: Simuliidae)",
                    "doi": "10.1111/j.1755-0998.2009.02648.x",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "dna barcoding has gained increased recognition as a molecular tool for species identification in various groups of organisms. in this preliminary study, we tested the efficacy of a 615\u2010bp fragment of the cytochrome c oxidase i (coi) as a dna barcode in the medically important family simuliidae, or black flies. a total of 65 (25%) morphologically distinct species and sibling species in species complexes of the 255 recognized nearctic black fly species were used to create a preliminary barcode profile for the family. genetic divergence among congeners averaged 14.93% (range 2.83\u201315.33%), whereas intraspecific genetic divergence between morphologically distinct species averaged 0.72% (range 0\u20133.84%). dna barcodes correctly identified nearly 100% of the morphologically distinct species (87% of the total sampled taxa), whereas in species complexes (13% of the sampled taxa) maximum values of divergence were comparatively higher (max. 4.58\u20136.5%), indicating cryptic diversity. the existence of sibling species in prosimulium travisi and p. neomacropyga was also demonstrated, thus confirming previous cytological evidence about the existence of such cryptic diversity in these two taxa. we conclude that dna barcoding is an effective method for species identification and discovery of cryptic diversity in black flies."
                },
                {
                    "id": "R145509",
                    "label": "Identifying Canadian mosquito species through DNA barcodes",
                    "doi": "10.1111/j.1365-2915.2006.00653.x",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "abstract a short fragment of mt dna from the cytochrome c oxidase 1 (co1) region was used to provide the first co1 barcodes for 37 species of canadian mosquitoes (diptera: culicidae) from the provinces ontario and new brunswick. sequence variation was analysed in a 617\u2010bp fragment from the 5\u2032 end of the co1 region. sequences of each mosquito species formed barcode clusters with tight cohesion that were usually clearly distinct from those of allied species. co1 sequence divergences were, on average, nearly 20 times higher for congeneric species than for members of a species; divergences between congeneric species averaged 10.4% (range 0.2\u201317.2%), whereas those for conspecific individuals averaged 0.5% (range 0.0\u20133.9%)."
                },
                {
                    "id": "R145554",
                    "label": "Identifying the Main Mosquito Species in China Based on DNA Barcoding",
                    "doi": "10.1371/journal.pone.0047051",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "mosquitoes are insects of the diptera, nematocera, and culicidae families, some species of which are important disease vectors. identifying mosquito species based on morphological characteristics is difficult, particularly the identification of specimens collected in the field as part of disease surveillance programs. because of this difficulty, we constructed dna barcodes of the cytochrome c oxidase subunit 1, the coi gene, for the more common mosquito species in china, including the major disease vectors. a total of 404 mosquito specimens were collected and assigned to 15 genera and 122 species and subspecies on the basis of morphological characteristics. individuals of the same species grouped closely together in a neighborhood-joining tree based on coi sequence similarity, regardless of collection site. coi gene sequence divergence was approximately 30 times higher for species in the same genus than for members of the same species. divergence in over 98% of congeneric species ranged from 2.3% to 21.8%, whereas divergence in conspecific individuals ranged from 0% to 1.67%. cryptic species may be common and a few pseudogenes were detected."
                },
                {
                    "id": "R146639",
                    "label": "DNA barcodes for species delimitation in Chironomidae (Diptera): a case study on the genus Labrundinia",
                    "doi": "10.4039/tce.2013.44",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "abstract in this study, we analysed the applicability of dna barcodes for delimitation of 79 specimens of 13 species of nonbiting midges in the subfamily tanypodinae (diptera: chironomidae) from s\u00e3o paulo state, brazil. our results support dna barcoding as an excellent tool for species identification and for solving taxonomic conflicts in genus labrundinia. molecular analysis of cytochrome c oxidase subunit i (coi) gene sequences yielded taxon identification trees, supporting 13 cohesive species clusters, of which three similar groups were subsequently linked to morphological variation at the larval and pupal stage. additionally, another cluster previously described by means of morphology was linked to molecular markers. we found a distinct barcode gap, and in some species substantial interspecific pairwise divergences (up to 19.3%) were observed, which permitted identification of all analysed species. the results also indicated that barcodes can be used to associate life stages of chironomids since coi was easily amplified and sequenced from different life stages with universal barcode primers."
                },
                {
                    "id": "R146643",
                    "label": "Revision of Nearctic Dasysyrphus Enderlein (Diptera: Syrphidae)",
                    "doi": "10.11646/zootaxa.3660.1.1",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "dasysyrphus enderlein (diptera: syrphidae) has posed taxonomic challenges to researchers in the past, primarily due to their lack of interspecific diagnostic characters. in the present study, dna data (mitochondrial cytochrome c oxidase sub-unit i\u2014coi) were combined with morphology to help delimit species. this led to two species being resurrected from synonymy (d. laticaudus and d. pacificus) and the discovery of one new species (d. occidualis sp. nov.). an additional new species was described based on morphology alone (d. richardi sp. nov.), as the specimens were too old to obtain coi. part of the taxonomic challenge presented by this group arises from missing type specimens. neotypes are designated here for d. pauxillus and d. pinastri to bring stability to these names. an illustrated key to 13 nearctic species is presented, along with descriptions, maps and supplementary data. a phylogeny based on coi is also presented and discussed."
                },
                {
                    "id": "R146646",
                    "label": "Comprehensive evaluation of DNA barcoding for the molecular species identification of forensically important Australian Sarcophagidae (Diptera)",
                    "doi": "10.1071/is12008",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "carrion-breeding sarcophagidae (diptera) can be used to estimate the post-mortem interval in forensic cases. difficulties with accurate morphological identifications at any life stage and a lack of documented thermobiological profiles have limited their current usefulness. the molecular-based approach of dna barcoding, which utilises a 648-bp fragment of the mitochondrial cytochrome oxidase subuniti gene, was evaluated in a pilot study for discrimination between 16 australian sarcophagids. the current study comprehensively evaluated barcoding for a larger taxon set of 588 australian sarcophagids. in total, 39 of the 84 known australian species were represented by 580 specimens, which includes 92% of potentially forensically important species. a further eight specimens could not be identified, but were included nonetheless as six unidentifiable taxa. a neighbour-joining tree was generated and nucleotide sequence divergences were calculated. all species except sarcophaga (fergusonimyia) bancroftorum, known for high morphological variability, were resolved as monophyletic (99.2% of cases), with bootstrap support of 100. excluding s. bancroftorum, the mean intraspecific and interspecific variation ranged from 1.12% and 2.81\u201311.23%, respectively, allowing for species discrimination. dna barcoding was therefore validated as a suitable method for molecular identification of australian sarcophagidae, which will aid in the implementation of this fauna in forensic entomology."
                },
                {
                    "id": "R146932",
                    "label": "DNA barcodes reveal cryptic genetic diversity within the blackfly subgenus Trichodagmia Enderlein (Diptera: Simuliidae: Simulium) and related taxa in the New World",
                    "doi": "10.11646/zootaxa.3514.1.3",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "in this paper we investigate the utility of the coi dna barcoding region for species identification and for revealing hidden diversity within the subgenus trichodagmia and related taxa in the new world. in total, 24 morphospecies within the current expanded taxonomic concept of trichodagmia were analyzed. three species in the subgenus aspathia and 10 species in the subgenus simulium s.str. were also included in the analysis because of their putative phylogenetic relationship with trichodagmia. in the neighbour joining analysis tree (nj) derived from the dna barcodes most of the specimens grouped together according to species or species groups as recognized by other morphotaxonomic studies. the interspecific genetic divergence averaged 11.2% (range 2.8\u201319.5%), whereas intraspecific genetic divergence within morphologically distinct species averaged 0.5% (range 0\u20131.2%). higher values of genetic divergence (3.2\u20133.7%) in species complexes suggest the presence of cryptic diversity. the existence of well defined groups within s. piperi, s. duodenicornium, s. canadense and s. rostratum indicate the possible presence of cryptic species within these taxa. also, the suspected presence of a sibling species in s. tarsatum and s. paynei is supported. dna barcodes also showed that specimens from species that were taxonomically difficult to delimit such as s. hippovorum, s. rubrithorax, s. paynei, and other related taxa (s. solarii), grouped together in the nj analysis, confirming the validity of their species status. the recovery of partial barcodes from specimens in collections was time consuming and pcr success was low from specimens more than 10 years old. however, when a sequence was obtained, it provided good resolution for species identification. larvae preserved in \u2018weak\u2019 carnoy\u2019s solution (9:1 ethanol:acetic acid) provided full dna barcodes. adding legs directly to the pcr mix from recently collected and preserved adults was an inexpensive, fast methodology to obtain full barcodes. in summary, dna barcoding combined with a sound morphotaxonomic framework provides an effective approach for the delineation of species and for the discovery of hidden diversity in the subgenus trichodagmia."
                },
                {
                    "id": "R146938",
                    "label": "Evaluation of DNA barcoding and identification of new haplomorphs in Canadian deerflies and horseflies",
                    "doi": "10.1111/j.1365-2915.2010.00896.x",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "this paper reports the first tests of the suitability of the standardized mitochondrial cytochrome c oxidase subunit i (coi) barcoding system for the identification of canadian deerflies and horseflies. two additional mitochondrial molecular markers were used to determine whether unambiguous species recognition in tabanids can be achieved. our 332 canadian tabanid samples yielded 650 sequences from five genera and 42 species. standard coi barcodes demonstrated a strong a + t bias (mean 68.1%), especially at third codon positions (mean 93.0%). our preliminary test of this system showed that the standard coi barcode worked well for canadian tabanidae: the target dna can be easily recovered from small amounts of insect tissue and aligned for all tabanid taxa. each tabanid species possessed distinctive sets of coi haplotypes which discriminated well among species. average conspecific kimura two\u2010parameter (k2p) divergence (0.49%) was 12 times lower than the average divergence within species. both the neighbour\u2010joining and the bayesian methods produced trees with identical monophyletic species groups. two species, chrysops dawsoni philip and chrysops montanus osten sacken (diptera: tabanidae), showed relatively deep intraspecific sequence divergences (\u223c10 times the average) for all three mitochondrial gene regions analysed. we suggest provisional differentiation of ch. montanus into two haplotypes, namely, ch. montanus haplomorph 1 and ch. montanus haplomorph 2, both defined by their molecular sequences and by newly discovered differences in structural features near their ocelli."
                },
                {
                    "id": "R157039",
                    "label": "DNA barcode library for European Gelechiidae (Lepidoptera) suggests greatly underestimated species diversity",
                    "doi": "10.3897/zookeys.921.49199",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "for the first time, a nearly complete barcode library for european gelechiidae is provided. dna barcode sequences (coi gene \u2013 cytochrome c oxidase 1) from 751 out of 865 nominal species, belonging to 105 genera, were successfully recovered. a total of 741 species represented by specimens with sequences \u2265 500bp and an additional ten species represented by specimens with shorter sequences were used to produce 53 nj trees. intraspecific barcode divergence averaged only 0.54% whereas distance to the nearest-neighbour species averaged 5.58%. of these, 710 species possessed unique dna barcodes, but 31 species could not be reliably discriminated because of barcode sharing or partial barcode overlap. species discrimination based on the barcode index system (bin) was successful for 668 out of 723 species which clustered from minimum one to maximum 22 unique bins. fifty-five species shared a bin with up to four species and identification from dna barcode data is uncertain. finally, 65 clusters with a unique bin remained unidentified to species level. these putative taxa, as well as 114 nominal species with more than one bin, suggest the presence of considerable cryptic diversity, cases which should be examined in future revisionary studies."
                },
                {
                    "id": "R157051",
                    "label": "A Transcontinental Challenge \u2014 A Test of DNA Barcode Performance for 1,541 Species of Canadian Noctuoidea (Lepidoptera)",
                    "doi": "10.1371/journal.pone.0092797",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "this study provides a first, comprehensive, diagnostic use of dna barcodes for the canadian fauna of noctuoids or \u201cowlet\u201d moths (lepidoptera: noctuoidea) based on vouchered records for 1,541 species (99.1% species coverage), and more than 30,000 sequences. when viewed from a canada-wide perspective, dna barcodes unambiguously discriminate 90% of the noctuoid species recognized through prior taxonomic study, and resolution reaches 95.6% when considered at a provincial scale. barcode sharing is concentrated in certain lineages with 54% of the cases involving 1.8% of the genera. deep intraspecific divergence exists in 7.7% of the species, but further studies are required to clarify whether these cases reflect an overlooked species complex or phylogeographic variation in a single species. non-native species possess higher nearest-neighbour (nn) distances than native taxa, whereas generalist feeders have lower nn distances than those with more specialized feeding habits. we found high concordance between taxonomic names and sequence clusters delineated by the barcode index number (bin) system with 1,082 species (70%) assigned to a unique bin. the cases of discordance involve both bin mergers and bin splits with 38 species falling into both categories, most likely reflecting bidirectional introgression. one fifth of the species are involved in a bin merger reflecting the presence of 158 species sharing their barcode sequence with at least one other taxon, and 189 species with low, but diagnostic coi divergence. a very few cases (13) involved species whose members fell into both categories. most of the remaining 140 species show a split into two or three bins per species, while virbia ferruginosa was divided into 16. the overall results confirm that dna barcodes are effective for the identification of canadian noctuoids. this study also affirms that bins are a strong proxy for species, providing a pathway for a rapid, accurate estimation of animal diversity."
                },
                {
                    "id": "R157056",
                    "label": "A DNA Barcode Library for North American Pyraustinae (Lepidoptera: Pyraloidea: Crambidae)",
                    "doi": "10.1371/journal.pone.0161449",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "although members of the crambid subfamily pyraustinae are frequently important crop pests, their identification is often difficult because many species lack conspicuous diagnostic morphological characters. dna barcoding employs sequence diversity in a short standardized gene region to facilitate specimen identifications and species discovery. this study provides a dna barcode reference library for north american pyraustines based upon the analysis of 1589 sequences recovered from 137 nominal species, 87% of the fauna. data from 125 species were barcode compliant (>500bp, <1% n), and 99 of these taxa formed a distinct cluster that was assigned to a single bin. the other 26 species were assigned to 56 bins, reflecting frequent cases of deep intraspecific sequence divergence and a few instances of barcode sharing, creating a total of 155 bins. two systems for otu designation, abgd and bin, were examined to check the correspondence between current taxonomy and sequence clusters. the bin system performed better than abgd in delimiting closely related species, while otu counts with abgd were influenced by the value employed for relative gap width. different species with low or no interspecific divergence may represent cases of unrecognized synonymy, whereas those with high intraspecific divergence require further taxonomic scrutiny as they may involve cryptic diversity. the barcode library developed in this study will also help to advance understanding of relationships among species of pyraustinae."
                },
                {
                    "id": "R157062",
                    "label": "A Comprehensive DNA Barcode Library for the Looper Moths (Lepidoptera: Geometridae) of British Columbia, Canada",
                    "doi": "10.1371/journal.pone.0018290",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "background the construction of comprehensive reference libraries is essential to foster the development of dna barcoding as a tool for monitoring biodiversity and detecting invasive species. the looper moths of british columbia (bc), canada present a challenging case for species discrimination via dna barcoding due to their considerable diversity and limited taxonomic maturity. methodology/principal findings by analyzing specimens held in national and regional natural history collections, we assemble barcode records from representatives of 400 species from bc and surrounding provinces, territories and states. sequence variation in the barcode region unambiguously discriminates over 93% of these 400 geometrid species. however, a final estimate of resolution success awaits detailed taxonomic analysis of 48 species where patterns of barcode variation suggest cases of cryptic species, unrecognized synonymy as well as young species. conclusions/significance a catalog of these taxa meriting further taxonomic investigation is presented as well as the supplemental information needed to facilitate these investigations."
                },
                {
                    "id": "R136201",
                    "label": "DNA barcode analysis of butterfly species from Pakistan points towards regional endemism",
                    "doi": "10.1111/1755-0998.12131",
                    "research_field": {
                        "id": "R136127",
                        "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
                    },
                    "abstract": "dna barcodes were obtained for 81 butterfly species belonging to 52 genera from sites in north\u2010central pakistan to test the utility of barcoding for their identification and to gain a better understanding of regional barcode variation. these species represent 25% of the butterfly fauna of pakistan and belong to five families, although the nymphalidae were dominant, comprising 38% of the total specimens. barcode analysis showed that maximum conspecific divergence was 1.6%, while there was 1.7\u201314.3% divergence from the nearest neighbour species. barcode records for 55 species showed <2% sequence divergence to records in the barcode of life data systems (bold), but only 26 of these cases involved specimens from neighbouring india and central asia. analysis revealed that most species showed little incremental sequence variation when specimens from other regions were considered, but a threefold increase was noted in a few cases. there was a clear gap between maximum intraspecific and minimum nearest neighbour distance for all 81 species. neighbour\u2010joining cluster analysis showed that members of each species formed a monophyletic cluster with strong bootstrap support. the barcode results revealed two provisional species that could not be clearly linked to known taxa, while 24 other species gained their first coverage. future work should extend the barcode reference library to include all butterfly species from pakistan as well as neighbouring countries to gain a better understanding of regional variation in barcode sequences in this topographically and climatically complex region."
                }
            ]
        },
        {
            "id": "R150089",
            "label": "Epidemiological surveillance systems design and implementation",
            "research_fields": [
                {
                    "id": "R278",
                    "label": "Information Science"
                }
            ],
            "properties": [
                "Software Used",
                "Software development approach",
                "Related work",
                "Epidemiological surveillance system purpose",
                "Epidemiological surveillance process",
                "Epidemiological surveillance users",
                "Statistical analysis techniques",
                "Epidemiological surveillance architecture",
                "Epidemiological software development approach",
                "Advantage provided by the system",
                "Limit of the system"
            ],
            "papers": [
                {
                    "id": "R145039",
                    "label": "Statewide System of Electronic Notifiable Disease Reporting From Clinical Laboratories: Comparing Automated Reporting With Conventional Methods",
                    "doi": "10.1001/jama.282.19.1845",
                    "research_field": {
                        "id": "R278",
                        "label": "Information Science"
                    },
                    "abstract": "context\\nnotifiable disease surveillance is essential to rapidly identify and respond to outbreaks so that further illness can be prevented. automating reports from clinical laboratories has been proposed to reduce underreporting and delays.\\n\\n\\nobjective\\nto compare the timeliness and completeness of a prototypal electronic reporting system with that of conventional laboratory reporting.\\n\\n\\ndesign\\nlaboratory-based reports for 5 conditions received at a state health department between july 1 and december 31, 1998, were reviewed. completeness of coverage for each reporting system was estimated using capture-recapture methods.\\n\\n\\nsetting\\nthree statewide private clinical laboratories in hawaii.\\n\\n\\nmain outcome measures\\nthe number and date of reports received, by reporting system, laboratory, and pathogen; completeness of data fields.\\n\\n\\nresults\\na total of 357 unique reports of illness were identified; 201 (56%) were received solely through the automated electronic system, 32 (9%) through the conventional system only, and 124 (35%) through both. thus, electronic reporting resulted in a 2.3-fold (95% confidence interval [ci], 2.0-2.6) increase in reports. electronic reports arrived an average of 3.8 (95% ci, 2.6-5.0) days earlier than conventional reports. of 21 data fields common to paper and electronic formats, electronic reports were significantly more likely to be complete for 12 and for 1 field with the conventional system. the estimated completeness of coverage for electronic reporting was 80% (95% ci, 75%-85%) [corrected] compared with 38% (95% ci, 36%-41%) [corrected] for the conventional system.\\n\\n\\nconclusions\\nin this evaluation, electronic reporting more than doubled the total number of laboratory-based reports received. on average, the electronic reports were more timely and more complete, suggesting that electronic reporting may ultimately facilitate more rapid and comprehensive institution of disease control measures."
                },
                {
                    "id": "R145065",
                    "label": "Description and validation of a new automated surveillance system for Clostridium difficile in Denmark",
                    "doi": "10.1017/s0950268817001315",
                    "research_field": {
                        "id": "R278",
                        "label": "Information Science"
                    },
                    "abstract": "summary the surveillance of clostridium difficile (cd) in denmark consists of laboratory based data from departments of clinical microbiology (dcms) sent to the national registry of enteric pathogens (nrep). we validated a new surveillance system for cd based on the danish microbiology database (miba). miba automatically collects microbiological test results from all danish dcms. we built an algorithm to identify positive test results for cd recorded in miba. a cd case was defined as a person with a positive culture for cd or pcr detection of toxin a and/or b and/or binary toxin. we compared cd cases identified through the miba-based surveillance with those reported to nrep and locally in five dcms representing different danish regions. during 2010\u20132014, nrep reported 13 896 cd cases, and the miba-based surveillance 21 252 cd cases. there was a 99\u00b79% concordance between the local datasets and the miba-based surveillance. surveillance based on miba was superior to the current surveillance system, and the findings show that the number of cd cases in denmark hitherto has been under-reported. there were only minor differences between local data and the miba-based surveillance, showing the completeness and validity of cd data in miba. this nationwide electronic system can greatly strengthen surveillance and research in various applications."
                },
                {
                    "id": "R145085",
                    "label": "Developing open source, self-contained disease surveillance software applications for use in resource-limited settings",
                    "doi": "10.1186/1472-6947-12-99",
                    "research_field": {
                        "id": "R278",
                        "label": "Information Science"
                    },
                    "abstract": "abstract background emerging public health threats often originate in resource-limited countries. in recognition of this fact, the world health organization issued revised international health regulations in 2005, which call for significantly increased reporting and response capabilities for all signatory nations. electronic biosurveillance systems can improve the timeliness of public health data collection, aid in the early detection of and response to disease outbreaks, and enhance situational awareness. methods as components of its suite for automated global biosurveillance (sages) program, the johns hopkins university applied physics laboratory developed two open-source, electronic biosurveillance systems for use in resource-limited settings. openessence provides web-based data entry, analysis, and reporting. essence desktop edition provides similar capabilities for settings without internet access. both systems may be configured to collect data using locally available cell phone technologies. results essence desktop edition has been deployed for two years in the republic of the philippines. local health clinics have rapidly adopted the new technology to provide daily reporting, thus eliminating the two-to-three week data lag of the previous paper-based system. conclusions openessence and essence desktop edition are two open-source software products with the capability of significantly improving disease surveillance in a wide range of resource-limited settings. these products, and other emerging surveillance technologies, can assist resource-limited countries compliance with the revised international health regulations."
                },
                {
                    "id": "R145318",
                    "label": "Electronic Surveillance System for the Early Notification of Community-Based Epidemics (ESSENCE): Overview, Components, and Public Health Applications",
                    "doi": "10.2196/26303",
                    "research_field": {
                        "id": "R278",
                        "label": "Information Science"
                    },
                    "abstract": "\\n background \\n the electronic surveillance system for the early notification of community-based epidemics (essence) is a secure web-based tool that enables health care practitioners to monitor health indicators of public health importance for the detection and tracking of disease outbreaks, consequences of severe weather, and other events of concern. the essence concept began in an internally funded project at the johns hopkins university applied physics laboratory, advanced with funding from the state of maryland, and broadened in 1999 as a collaboration with the walter reed army institute for research. versions of the system have been further developed by johns hopkins university applied physics laboratory in multiple military and civilian programs for the timely detection and tracking of health threats. \\n \\n \\n objective \\n this study aims to describe the components and development of a biosurveillance system increasingly coordinating all-hazards health surveillance and infectious disease monitoring among large and small health departments, to list the key features and lessons learned in the growth of this system, and to describe the range of initiatives and accomplishments of local epidemiologists using it. \\n \\n \\n methods \\n the features of essence include spatial and temporal statistical alerting, custom querying, user-defined alert notifications, geographical mapping, remote data capture, and event communications. to expedite visualization, configurable and interactive modes of data stratification and filtering, graphical and tabular customization, user preference management, and sharing features allow users to query data and view geographic representations, time series and data details pages, and reports. these features allow essence users to gather and organize the resulting wealth of information into a coherent view of population health status and communicate findings among users. \\n \\n \\n results \\n the resulting broad utility, applicability, and adaptability of this system led to the adoption of essence by the centers for disease control and prevention, numerous state and local health departments, and the department of defense, both nationally and globally. the open-source version of suite for automated global electronic biosurveillance is available for global, resource-limited settings. resourceful users of the us national syndromic surveillance program essence have applied it to the surveillance of infectious diseases, severe weather and natural disaster events, mass gatherings, chronic diseases and mental health, and injury and substance abuse. \\n \\n \\n conclusions \\n with emerging high-consequence communicable diseases and other health conditions, the continued user requirement\u2013driven enhancements of essence demonstrate an adaptable disease surveillance capability focused on the everyday needs of public health. the challenge of a live system for widely distributed users with multiple different data sources and high throughput requirements has driven a novel, evolving architecture design. \\n"
                },
                {
                    "id": "R145901",
                    "label": "Evaluating the electronic tuberculosis register surveillance system in Eden District, Western Cape, South Africa, 2015",
                    "doi": "10.1080/16549716.2017.1360560",
                    "research_field": {
                        "id": "R278",
                        "label": "Information Science"
                    },
                    "abstract": "abstract background: tuberculosis (tb) surveillance data are crucial to the effectiveness of national tb control programs. in south africa, few surveillance system evaluations have been undertaken to provide a rigorous assessment of the platform from which the national and district health systems draws data to inform programs and policies. objective: evaluate the attributes of eden district\u2019s tb surveillance system, western cape province, south africa. methods: data quality, sensitivity and positive predictive value were assessed using secondary data from 40,033 tb cases entered in eden district\u2019s etr.net from 2007 to 2013, and 79 purposively selected tb blue cards (tbcs), a medical patient file and source document for data entered into etr.net. simplicity, flexibility, acceptability, stability and usefulness of the etr.net were assessed qualitatively through interviews with tb nurses, information health officers, sub-district and district coordinators involved in the tb surveillance. results: tb surveillance system stakeholders report that eden district\u2019s etr.net system was simple, acceptable, flexible and stable, and achieves its objective of informing tb control program, policies and activities. data were less complete in the etr.net (66\u2013100%) than in the tbcs (76\u2013100%), and concordant for most variables except pre-treatment smear results, antiretroviral therapy (art) and treatment outcome. the sensitivity of recorded variables in etr.net was 98% for gender, 97% for patient category, 93% for art, 92% for treatment outcome and 90% for pre-treatment smear grading. conclusions: our results reveal that the system provides useful information to guide tb control program activities in eden district. however, urgent attention is needed to address gaps in clinical recording on the tbc and data capturing into the etr.net system. we recommend continuous training and support of tb personnel involved with tb care, management and surveillance on tb data recording into the tbcs and etr.net as well as the implementation of a well-structured quality control and assurance system."
                },
                {
                    "id": "R146244",
                    "label": "Improvements in Timeliness Resulting from Implementation of Electronic Laboratory Reporting and an Electronic Disease Surveillance System",
                    "doi": "10.1177/003335491312800510",
                    "research_field": {
                        "id": "R278",
                        "label": "Information Science"
                    },
                    "abstract": "objectives. electronic laboratory reporting (elr) reduces the time between communicable disease diagnosis and case reporting to local health departments (lhds). however, it also imposes burdens on public health agencies, such as increases in the number of unique and duplicate case reports. we assessed how elr affects the timeliness and accuracy of case report processing within public health agencies. methods. using data from may\u2013august 2010 and january\u2013march 2012, we assessed timeliness by calculating the time between receiving a case at the lhd and reporting the case to the state (first stage of reporting) and between submitting the report to the state and submitting it to the centers for disease control and prevention (second stage of reporting). we assessed accuracy by calculating the proportion of cases returned to the lhd for changes or additional information. we compared timeliness and accuracy for elr and non-elr cases. results. elr was associated with decreases in case processing time (median = 40 days for elr cases vs. 52 days for non-elr cases in 2010; median = 20 days for elr cases vs. 25 days for non-elr cases in 2012; both p<0.001). elr also allowed time to reduce the backlog of unreported cases. finally, elr was associated with higher case reporting accuracy (in 2010, 2% of elr case reports vs. 8% of non-elr case reports were returned; in 2012, 2% of elr case reports vs. 6% of non-elr case reports were returned; both p<0.001). conclusion. the overall impact of increased elr is more efficient case processing at both local and state levels."
                },
                {
                    "id": "R146256",
                    "label": "Improving national surveillance of Lyme neuroborreliosis in Denmark through electronic reporting of specific antibody index testing from 2010 to 2012",
                    "doi": "10.2807/1560-7917.es2015.20.28.21184",
                    "research_field": {
                        "id": "R278",
                        "label": "Information Science"
                    },
                    "abstract": "our aim was to evaluate the results of automated surveillance of lyme neuroborreliosis (lnb) in denmark using the national microbiology database (miba), and to describe the epidemiology of laboratory-confirmed lnb at a national level. miba-based surveillance includes electronic transfer of laboratory results, in contrast to the statutory surveillance based on manually processed notifications. antibody index (ai) testing is the recommend laboratory test to support the diagnosis of lnb in denmark. in the period from 2010 to 2012, 217 clinical cases of lnb were notified to the statutory surveillance system, while 533 cases were reported ai positive by the miba system. thirty-five unconfirmed cases (29 ai-negative and 6 not tested) were notified, but not captured by miba. using miba, the number of reported cases was increased almost 2.5 times. furthermore, the reporting was timelier (median lag time: 6 vs 58 days). average annual incidence of ai-confirmed lnb in denmark was 3.2/100,000 population and incidences stratified by municipality ranged from none to above 10/100,000. this is the first study reporting nationwide incidence of lnb using objective laboratory criteria. laboratory-based surveillance with electronic data-transfer was more accurate, complete and timely compared to the surveillance based on manually processed notifications. we propose using ai test results for lnb surveillance instead of clinical reporting.\\n"
                },
                {
                    "id": "R146321",
                    "label": "Introduction of software tools for epidemiological surveillance in infection control in Colombia",
                    "doi": "10.25100/cm.v46i2.1548",
                    "research_field": {
                        "id": "R278",
                        "label": "Information Science"
                    },
                    "abstract": "introduction:\\n\\nhealthcare-associated infections (hai) are a challenge for patient safety in the hospitals. infection control committees (icc) should follow cdc definitions when monitoring hai. the handmade method of epidemiological surveillance (es) may affect the sensitivity and specificity of the monitoring system, while electronic surveillance can improve the performance, quality and traceability of recorded information.\\nobjective:\\n\\nto assess the implementation of a strategy for electronic surveillance of hai, bacterial resistance and antimicrobial consumption by the icc of 23 high-complexity clinics and hospitals in colombia, during the period 2012-2013.\\nmethods:\\n\\nan observational study evaluating the introduction of electronic tools in the icc was performed; we evaluated the structure and operation of the icc, the degree of incorporation of the software hai solutions and the adherence to record the required information.\\nresults:\\n\\nthirty-eight percent of hospitals (8/23) had active surveillance strategies with standard criteria of the cdc, and 87% of institutions adhered to the module of identification of cases using the hai solutions software. in contrast, compliance with the diligence of the risk factors for device-associated hais was 33%.\\nconclusions:\\n\\nthe introduction of es could achieve greater adherence to a model of active surveillance, standardized and prospective, helping to improve the validity and quality of the recorded information."
                },
                {
                    "id": "R146490",
                    "label": "Rapid implementation of mobile technology for real-time epidemiology of COVID-19",
                    "doi": "10.1126/science.abc0473",
                    "research_field": {
                        "id": "R278",
                        "label": "Information Science"
                    },
                    "abstract": "mobile symptom tracking \\n \\n the rapidity with which severe acute respiratory syndrome coronavirus 2 (sars-cov-2) spreads through a population is defying attempts at tracking it, and quantitative polymerase chain reaction testing so far has been too slow for real-time epidemiology. taking advantage of existing longitudinal health care and research patient cohorts, drew\\n et al. \\n pushed software updates to participants to encourage reporting of potential coronavirus disease 2019 (covid-19) symptoms. the authors recruited about 2 million users (including health care workers) to the covid symptom study (previously known as the covid symptom tracker) from across the united kingdom and the united states. the prevalence of combinations of symptoms (three or more), including fatigue and cough, followed by diarrhea, fever, and/or anosmia, was predictive of a positive test verification for sars-cov-2. as exemplified by data from wales, united kingdom, mathematical modeling predicted geographical hotspots of incidence 5 to 7 days in advance of official public health reports.\\n \\n \\n science \\n , this issue p.\\n 1362 \\n"
                },
                {
                    "id": "R146576",
                    "label": "Comparative evaluation of three surveillance systems for infectious equine diseases in France and implications for future synergies",
                    "doi": "10.1017/s0950268815000217",
                    "research_field": {
                        "id": "R278",
                        "label": "Information Science"
                    },
                    "abstract": "summary it is necessary to assess surveillance systems for infectious animal diseases to ensure they meet their objectives and provide high-quality health information. each system is generally dedicated to one disease and often comprises various components. in many animal industries, several surveillance systems are implemented separately even if they are based on similar components. this lack of synergy may prevent optimal surveillance. the purpose of this study was to assess several surveillance systems within the same industry using the semi-quantitative oasis method and to compare the results of the assessments in order to propose improvements, including future synergies. we have focused on the surveillance of three major equine diseases in france. we have identified the mutual and specific strengths and weaknesses of each surveillance system. furthermore, the comparative assessment has highlighted many possible synergies that could improve the effectiveness and efficiency of surveillance as a whole, including the implementation of new joint tools or the pooling of existing teams, tools or skills. our approach is an original application of the oasis method, which requires minimal financial resources and is not very time-consuming. such a comparative evaluation could conceivably be applied to other surveillance systems, other industries and other countries. this approach would be especially relevant to enhance the efficiency of surveillance activities when resources are limited."
                },
                {
                    "id": "R146600",
                    "label": "Coronavirus disease 2019 (COVID-19) surveillance system: Development of COVID-19 minimum data set and interoperable reporting framework",
                    "doi": "10.4103/jehp.jehp_456_20",
                    "research_field": {
                        "id": "R278",
                        "label": "Information Science"
                    },
                    "abstract": "introduction: the 2019 coronavirus disease (covid-19) is a major global health concern. joint efforts for effective surveillance of covid-19 require immediate transmission of reliable data. in this regard, a standardized and interoperable reporting framework is essential in a consistent and timely manner. thus, this research aimed at to determine data requirements towards interoperability. materials and methods: in this cross-sectional and descriptive study, a combination of literature study and expert consensus approach was used to design covid-19 minimum data set (mds). a mds checklist was extracted and validated. the definitive data elements of the mds were determined by applying the delphi technique. then, the existing messaging and data standard templates (health level seven-clinical document architecture [hl7-cda] and snomed-ct) were used to design the surveillance interoperable framework. results: the proposed mds was divided into administrative and clinical sections with three and eight data classes and 29 and 40 data fields, respectively. then, for each data field, structured data values along with snomed-ct codes were defined and structured according hl7-cda standard. discussion and conclusion: the absence of effective and integrated system for covid-19 surveillance can delay critical public health measures, leading to increased disease prevalence and mortality. the heterogeneity of reporting templates and lack of uniform data sets hamper the optimal information exchange among multiple systems. thus, developing a unified and interoperable reporting framework is more effective to prompt reaction to the covid-19 outbreak."
                },
                {
                    "id": "R150170",
                    "label": "Epidemiologic Surveillance in Developing Countries",
                    "doi": "10.1146/annurev.pu.12.050191.001353",
                    "research_field": {
                        "id": "R278",
                        "label": "Information Science"
                    },
                    "abstract": "developed countries in many ways. most people are poorer, less educated, more likely to die at a young age, and less knowledgeable about factors that cause, prevent, or cure disease. biological and physical hazards are more common, which results in greater incidence, disability, and death. although disease is common, both the people and government have much fewer resources for prevention or medical care. many efficacious drugs arc too expensive and not readily available for those in greatest need. salaries are so low that government physicians or nurses must work after-hours in private clinics to feed, clothe, and educate their families. the establishment and maintenance of an epidemiological surveillance system in such an environment requires a differ\\xad ent orientation from that found in wealthier nations. the scarcity of resources is a dominant concern. salaried time spent gathering data is lost to service activities, such as treating gastrointestinal problems or preventing childhood diseases. as a result, components in a surveillance system must be justified, as are purchases of examination tables or radiographic equipment. a costly, extensive surveillance system may cause more harm than good. in this article 1 will define epidemiologic surveillance. 1 also will describe the various components of a surveillance program, show how microcomputers and existing software can be used to increase effectiveness, and illustrate how"
                }
            ]
        },
        {
            "id": "R150595",
            "label": "Tailored Forming Contribution",
            "research_fields": [
                {
                    "id": "R137654",
                    "label": "Mechanical Process Engineering"
                }
            ],
            "properties": [
                "Has result",
                "research problem",
                "has material",
                "realizes"
            ],
            "papers": [
                {
                    "id": "R171846",
                    "label": "Investigation of the material combination 20MnCr5 and X45CrSi9-3 in the Tailored Forming of shafts with bearing seats",
                    "doi": "10.1007/s11740-022-01119-w",
                    "research_field": {
                        "id": "R137654",
                        "label": "Mechanical Process Engineering"
                    },
                    "abstract": "abstract the tailored forming process chain is used to manufacture hybrid components and consists of a joining process or additive manufacturing for various materials (e.g. deposition welding), subsequent hot forming, machining and heat treatment. in this way, components can be produced with materials adapted to the load case. for this paper, hybrid shafts are produced by deposition welding of a cladding made of x45crsi9-3 onto a workpiece made from 20mncr5. the hybrid shafts are then formed by means of cross-wedge rolling. it is investigated, how the thickness of the cladding and the type of cooling after hot forming (in air or in water) affect the properties of the cladding. the hybrid shafts are formed without layer separation. however, slight core loosening occurres in the area of the bearing seat due to the mannesmann effect. the microhardness of the cladding is only slightly effected by the cooling strategy, while the microhardness of the base material is significantly higher in water cooled shafts. the microstructure of the cladding after both cooling strategies consists mainly of martensite. in the base material, air cooling results in a mainly ferritic microstructure with grains of ferrite-pearlite. quenching in water results in a microstructure containing mainly martensite."
                },
                {
                    "id": "R145720",
                    "label": "Investigations on Tailored Forming of AISI 52100 as Rolling Bearing Raceway",
                    "doi": "10.3390/met10101363",
                    "research_field": {
                        "id": "R137654",
                        "label": "Mechanical Process Engineering"
                    },
                    "abstract": "hybrid cylindrical roller thrust bearing washers of type 81212 were manufactured by tailored forming. an aisi 1022m base material, featuring a sufficient strength for structural loads, was cladded with the bearing steel aisi 52100 by plasma transferred arc welding (pta). though aisi 52100 is generally regarded as non-weldable, it could be applied as a cladding material by adjusting pta parameters. the cladded parts were investigated after each individual process step and subsequently tested under rolling contact load. welding defects that could not be completely eliminated by the subsequent hot forming were characterized by means of scanning acoustic microscopy and micrographs. below the surface, pores with a typical size of ten \u00b5m were found to a depth of about 0.45 mm. in the material transition zone and between individual weld seams, larger voids were observed. grinding of the surface after heat treatment caused compressive residual stresses near the surface with a relatively small depth. fatigue tests were carried out on an fe8 test rig. eighty-two percent of the calculated rating life for conventional bearings was achieved. a high failure slope of the weibull regression was determined. a relationship between the weld defects and the fatigue behavior is likely."
                },
                {
                    "id": "R145729",
                    "label": "Manufacturing and Evaluation of Multi-Material Axial-Bearing Washers by Tailored Forming",
                    "doi": "10.3390/met9020232",
                    "research_field": {
                        "id": "R137654",
                        "label": "Mechanical Process Engineering"
                    },
                    "abstract": "components subject to rolling contact fatigue, such as gears and rolling bearings, are among the fundamental machine elements in mechanical and vehicle engineering. rolling bearings are generally not designed to be fatigue-resistant, as the necessary oversizing is not technically and economically marketable. in order to improve the load-bearing capacity, resource efficiency and application possibilities of rolling bearings and other possible multi-material solid components, a new process chain was developed at leibniz university hannover as a part of the collaborative research centre 1153 \u201ctailored forming\u201d. semi-finished products, already joined before the forming process, are used here to allow a further optimisation of joint quality by forming and finishing. in this paper, a plasma-powder-deposition welding process is presented, which enables precise material deposition and control of the welding depth. for this study, bearing washers (serving as rolling bearing raceways) of a cylindrical roller thrust bearing, similar to type 81212 with a multi-layer structure, were manufactured. a previously non-weldable high-performance material, steel aisi 5140, was used as the cladding layer. depending on the degree of forming, grain-refinement within the welded material was achieved by thermo-mechanical treatment of the joining zone during the forming process. this grain-refinements lead to an improvement of the mechanical properties and thus, to a higher lifetime for washers of an axial cylindrical roller bearing, which were examined as an exemplary component on a fatigue test bench. to evaluate the bearing washers, the results of the bearing tests were compared with industrial bearings and deposition welded axial-bearing washers without subsequent forming. in addition, the bearing washers were analysed micro-tribologically and by scanning acoustic microscopy both after welding and after the forming process. nano-scratch tests were carried out on the bearing washers to analyse the layer properties. together with the results of additional microscopic images of the surface and cross-sections, the causes of failure due to fatigue and wear were identified."
                },
                {
                    "id": "R145732",
                    "label": "Tribological Study on Tailored-Formed Axial Bearing Washers",
                    "doi": "10.2474/trol.13.320",
                    "research_field": {
                        "id": "R137654",
                        "label": "Mechanical Process Engineering"
                    },
                    "abstract": "to enhance tribological contacts under cyclic load, high performance materials are required. utilizing the same high-strength material for the whole machine element is not resource-efficient. in order to manufacture machine elements with extended functionality and specific properties, a combination of different materials can be used in a single component for a more efficient material utilization. by combining different joining techniques with subsequent forming, multi-material or tailored components can be manufactured. to reduce material costs and energy consumption during the component service life, a less expensive lightweight material should be used for regions remote from the highly stressed zones. the scope is not only to obtain the desired shape and dimensions for the finishing process, but also to improve properties like the bond strength between different materials and the microscopic structure of the material. the multi-material approach can be applied to all components requiring different properties in separate component regions such as shafts, bearings or bushes. the current study exemplarily presents the process route for the production of an axial bearing washer by means of tailored forming technology. the bearing washers were chosen to fit axial roller bearings (type 81212). the manufacturing process starts with the laser wire cladding of a hard facing made of martensitic chromium silicon steel (1.4718) on a base substrate of s235 (1.0038) steel. subsequently, the bearing washers are forged. after finishing, the surfaces of the bearing washers were tested in thrust bearings on an fe-8 test rig. the operational test of the bearings consists in a run-in phase at 250 rpm. a bearing failure is determined by a condition monitoring system. before and after this, the bearings were inspected by optical and ultrasonic microscopy in order to examine whether the bond of the coat is resistant against rolling contact fatigue. the feasibility of the approach could be proven by endurance test. the joining zone was able to withstand the rolling contact stresses and the bearing failed due to material-induced fatigue with high cycle stability."
                },
                {
                    "id": "R162731",
                    "label": "Cross-wedge rolling of PTA-welded hybrid steel billets with rolling bearing steel and hard material coatings",
                    "doi": "10.1063/1.5112553",
                    "research_field": {
                        "id": "R137654",
                        "label": "Mechanical Process Engineering"
                    },
                    "abstract": "within the collaborative research centre 1153 \u201ctailored forming\u201c a process chain for the manufacturing of hybrid high performance components is developed. exemplary process steps consist of deposit welding of high performance steel on low-cost steel, pre-shaping by cross-wedge rolling and finishing by milling.hard material coatings such as stellite 6 or delcrome 253 are used as wear or corrosion protection coatings in industrial applications. scientists of the institute of material science welded these hard material alloys onto a base material, in this case c22.8, to create a hybrid workpiece. scientists of the institut fur integrierte produktion hannover have shown that these hybrid workpieces can be formed without defects (e.g. detachment of the coating) by cross-wedge rolling. after forming, the properties of the coatings are retained or in some cases even improved (e.g. the transition zone between base material and coating). by adjustments in the welding process, it was possible to apply the 100cr6 rolling bearing steel, as of now declared as non-weldable, on the low-cost steel c22.8. 100cr6 was formed afterwards in its hybrid bonding state with c22.8 by cross-wedge rolling, thus a component-integrated bearing seat was produced. even after welding and forming, the rolling bearing steel coating could still be quench-hardened to a hardness of over 60 hrc. this paper shows the potential of forming hybrid billets to tailored parts. since industrially available standard materials can be used for hard material coatings by this approach, even though they are not weldable by conventional methods, it is not necessary to use expensive, for welding designed materials to implement a hybrid component concept.within the collaborative research centre 1153 \u201ctailored forming\u201c a process chain for the manufacturing of hybrid high performance components is developed. exemplary process steps consist of deposit welding of high performance steel on low-cost steel, pre-shaping by cross-wedge rolling and finishing by milling.hard material coatings such as stellite 6 or delcrome 253 are used as wear or corrosion protection coatings in industrial applications. scientists of the institute of material science welded these hard material alloys onto a base material, in this case c22.8, to create a hybrid workpiece. scientists of the institut fur integrierte produktion hannover have shown that these hybrid workpieces can be formed without defects (e.g. detachment of the coating) by cross-wedge rolling. after forming, the properties of the coatings are retained or in some cases even improved (e.g. the transition zone between base material and coating). by adjustments in the welding process, it was possible to apply the 100cr6 ro..."
                }
            ]
        },
        {
            "id": "R152828",
            "label": "Xray laser pumped",
            "research_fields": [
                {
                    "id": "R185",
                    "label": "Plasma and Beam Physics"
                }
            ],
            "properties": [
                "has system qualities"
            ],
            "papers": [
                {
                    "id": "R156333",
                    "label": "Demonstration of a Soft X-Ray Amplifier",
                    "doi": "10.1103/physrevlett.54.110",
                    "research_field": {
                        "id": "R185",
                        "label": "Plasma and Beam Physics"
                    },
                    "abstract": "we report observations of amplified spontaneous emission at soft x-ray wavelengths. an optical laser ionized thin foils of selenium to produce a population inversion of the $2{p}^{5}3p$ and $2{p}^{5}3s$ levels of the neonlike ion. using three time-resolved, spectroscopic measurements we demonstrated gain-length products up to 6.5 and gain coefficients of 5.5\\\\ifmmode\\\\pm\\\\else\\\\textpm\\\\fi{}1.0 ${\\\\mathrm{cm}}^{\\\\ensuremath{-}1}$ for the $j=2 \\\\mathrm{to} 1$ lines at 206.3 and 209.6 \\\\aa{}. we also observed considerable amplification for the same transitions in yttrium at 155.0 and 157.1 \\\\aa{}."
                },
                {
                    "id": "R156404",
                    "label": "Amplification of stimulated soft x-ray emission in a confined plasma column",
                    "doi": "10.1103/physrevlett.55.1753",
                    "research_field": {
                        "id": "R185",
                        "label": "Plasma and Beam Physics"
                    },
                    "abstract": "\"une amplification atteignant 100 de l'emission stimulee sur l'emission spontanee de la raie cvi 182 a a ete mesuree dans une colonne de plasma magnetiquement confinee, par deux methodes independantes utilisant des monochromateurs uv extreme etalonnes en intensite. une confirmation supplementaire que l'amplification est due a l'emission stimulee a ete obtenue avec un miroir rx mou: avec 12% de reflectivite du miroir effective mesuree, une augmentation de 120% de l'intensite de la raie cvi 182 a dans la direction axiale a ete observee\""
                },
                {
                    "id": "R156448",
                    "label": "Soft x-ray lasing in neonlike germanium and copper plasmas",
                    "doi": "10.1103/physrevlett.59.1185",
                    "research_field": {
                        "id": "R185",
                        "label": "Plasma and Beam Physics"
                    },
                    "abstract": "soft x-ray 3p\\\\ensuremath{\\\\rightarrow}3s lasing in neonlike germanium (${\\\\mathrm{ge}}^{22+}$) and copper (${\\\\mathrm{cu}}^{19+}$) in the wavelength interval of 195 to 285 a\\\\r{} is observed for the first time, with gain coefficients ranging from 1.7 to 4.1 ${\\\\mathrm{cm}}^{\\\\mathrm{\\\\ensuremath{-}}1}$, the higher gain with germanium. the lasing plasmas are produced by focusing a driving laser beam (\\\\ensuremath{\\\\lambda}=1.05 \\\\ensuremath{\\\\mu}m, 2-ns fwhm) into an 18-mm-long line onto thin films and slab targets. the measured j=0 to 1 gain coefficients are comparable to those of the j=2 to 1 transitions. the measured wavelengths of the six lasing lines compared favorably with recent calculations."
                },
                {
                    "id": "R156490",
                    "label": "Soft-x-ray amplification by lithiumlike ions in recombining hot plasmas",
                    "doi": "10.1364/josab.4.000563",
                    "research_field": {
                        "id": "R185",
                        "label": "Plasma and Beam Physics"
                    },
                    "abstract": "this paper describes calculations and experiments about soft-x-ray amplification by lithiumlike ions in recombining laser-produced plasmas. time- and space-dependent population inversion densities calculated with a collisional-radiative model used as the postprocessor of a hydrodynamic code are reported. amplification diagnostic accuracy in plasma experiments is discussed. time-integrated and time-resolved measurements of gain are presented, especially at 105.7 a in lithiumlike aluminum. it is shown that, in a plasma produced by a 3-nsec laser pulse, the peak of amplified radiation occurs about 7 nsec after the top of the pulse. the maximum gain\u2013length product measured previously was 2\u20132.5. a short description of a future experiment designed for producing a much larger gain is presented."
                },
                {
                    "id": "R156576",
                    "label": "Demonstration of x-ray amplifiers near the carbonKedge",
                    "doi": "10.1103/physrevlett.65.420",
                    "research_field": {
                        "id": "R185",
                        "label": "Plasma and Beam Physics"
                    },
                    "abstract": "\"the ni-like 4d-4p laser scheme has been extended to wavelengths near the k absorption edge of carbon. a gain of 2.3 ${\\\\mathrm{cm}}^{\\\\mathrm{\\\\ensuremath{-}}1}$ with a duration of 250 psec was observed in ni-like ta at 44.83 \\\\aa{} (a wavelength close to optimal for holographic imaging of live cells). ni-like w produced a gain of 2.6 ${\\\\mathrm{cm}}^{\\\\mathrm{\\\\ensuremath{-}}1}$ with a total of seven gainlengths of amplification at 43.18 \\\\aa{}. this is the first demonstration of an x-ray amplifier on the short-wavelength side of the carbon k edge, within the ``water window.'' both lasers should be scalable to coherent power sufficient for holographic imaging and other applications.\""
                },
                {
                    "id": "R156620",
                    "label": "Demonstration of population inversion by resonant photopumping in a neon gas cell irradiated by a sodiumZpinch",
                    "doi": "10.1103/physrevlett.68.796",
                    "research_field": {
                        "id": "R185",
                        "label": "Plasma and Beam Physics"
                    },
                    "abstract": "the broadband radiation emitted from a na z pinch is used to photoionize ne to the he-like ground state and radiation from the na 1s 2 -1s2p 1 p 1 transition is used to resonantly photoexcite the ne 1s 2 -1s4p 1 p 1 transition. time-resolved and time-integrated spectral measurements of the ne k-shell emission demonstrate the first population inversion driven by a z pinch"
                },
                {
                    "id": "R156663",
                    "label": "Short wavelength x\u2010ray laser research at the Lawrence Livermore National Laboratory",
                    "doi": "10.1063/1.860203",
                    "research_field": {
                        "id": "R185",
                        "label": "Plasma and Beam Physics"
                    },
                    "abstract": "laboratory x\u2010ray lasers are currently being studied by researchers worldwide. this paper reviews some of the recent work carried out at lawrence livermore national laboratory. laser action has been demonstrated at wavelengths as short as 35.6 a while saturation of the small signal gain has been observed with longer wavelength schemes. some of the most successful schemes to date have been collisionally pumped x\u2010ray lasers that use the thermal electron distribution within a laser\u2010produced plasma to excite electrons from closed shells in neon\u2010 and nickel\u2010like ions to metastable levels in the next shell. attempts to quantify and improve the longitudinal and transverse coherence of collisionally pumped x\u2010ray lasers are motivated by the desire to produce sources for specific applications. toward this goal there is a large effort underway to enhance the power output of the ni\u2010like ta x\u2010ray laser at 44.83 a as a source for x\u2010ray imaging of live cells. improving the efficiency of x\u2010ray lasers in order to produce s..."
                },
                {
                    "id": "R156770",
                    "label": "Femtosecond-pulse-driven 10-Hz 418-nm laser in Xe ix",
                    "doi": "10.1364/josab.13.000180",
                    "research_field": {
                        "id": "R185",
                        "label": "Plasma and Beam Physics"
                    },
                    "abstract": "we report the observation of extreme uv lasing at 41.81 nm on the 4d95d1s0\u22124d95p1p1 transition in xe ix, as proposed by lemoff [ opt. lett.19, 569 ( 1994)]. a 10-hz circularly polarized 800-nm laser pulse with an energy of \u223c70 mj and a duration of \u223c40 fs is longitudinally focused to a peak intensity of >3 \u00d7 1016 w/cm2 over a length of 8.4 nm in a differentially pumped cell containing 12 torr of xe gas. laser amplification was observed with an estimated gain coefficient of 13 cm\u22121 and a total gain of exp(11)."
                },
                {
                    "id": "R156819",
                    "label": "A Saturated X-ray Laser Beam at 7 Nanometers",
                    "doi": "10.1126/science.276.5315.1097",
                    "research_field": {
                        "id": "R185",
                        "label": "Plasma and Beam Physics"
                    },
                    "abstract": "a saturated nickel-like samarium x-ray laser beam at 7 nanometers has been demonstrated with an output energy of 0.3 millijoule in 50-picosecond pulses, demonstrating that saturated operation of a laser at wavelengths shorter than 10 nanometers can be achieved. the narrow divergence, short wavelength, short pulse duration, high efficiency, and high brightness of this samarium laser make it an ideal candidate for many x-ray laser applications."
                },
                {
                    "id": "R156908",
                    "label": "Saturated and Short Pulse Duration X-Ray Lasers",
                    "doi": "10.1063/1.1521033",
                    "research_field": {
                        "id": "R185",
                        "label": "Plasma and Beam Physics"
                    },
                    "abstract": "the basis of a model of the relationship between gain and output laser intensity is reviewed and the measurement of the duration of x\u2010ray lasing with a streak camera with 700 fs temporal resolution is described. combined with a temporal smearing due to the spectrometer employed, we have measured x\u2010ray laser pulse durations for ni\u2010like silver at 13.9 nm and ne\u2010like nickel at 23.1 nm with a total time resolution of 1.1 ps. an extension of the model is shown to consistently relate the measured x\u2010ray laser pulse duration to estimates of the gain duration obtained by temporally resolving resonance line emission from states near in energy to the upper lasing level."
                }
            ]
        },
        {
            "id": "R154390",
            "label": "Lignin decomposition",
            "research_fields": [
                {
                    "id": "R129",
                    "label": "Organic Chemistry"
                }
            ],
            "properties": [
                "substrate",
                "catalyst",
                "has temperature value",
                "Solvent",
                "Pressure",
                "Reactor",
                "Conversion",
                "Product"
            ],
            "papers": [
                {
                    "id": "R154399",
                    "label": "Selective catalytic conversion of guaiacol to phenols over a molybdenum carbide catalyst",
                    "doi": "10.1039/c5cc01900a",
                    "research_field": {
                        "id": "R129",
                        "label": "Organic Chemistry"
                    },
                    "abstract": "an activated carbon supported \u03b1-molybdenum carbide catalyst (\u03b1-moc 1\u2212x /ac) showed remarkable activity in the selective deoxygenation of guaiacol to substituted mono-phenols in low carbon number alcohol solvents."
                },
                {
                    "id": "R154426",
                    "label": "Catalysis Meets Nonthermal Separation for the Production of (Alkyl)phenols and Hydrocarbons from Pyrolysis Oil",
                    "doi": "10.1002/anie.201610405",
                    "research_field": {
                        "id": "R129",
                        "label": "Organic Chemistry"
                    },
                    "abstract": "a simple and efficient hydrodeoxygenation strategy is described to selectively generate and separate high-value alkylphenols from pyrolysis bio-oil, produced directly from lignocellulosic biomass. the overall process is efficient and only requires low pressures of hydrogen gas (5\\u2005bar). initially, an investigation using model compounds indicates that mocx /c is a promising catalyst for targeted hydrodeoxygenation, enabling selective retention of the desired ar-oh substituents. by applying this procedure to pyrolysis bio-oil, the primary products (phenol/4-alkylphenols and hydrocarbons) are easily separable from each other by short-path column chromatography, serving as potential valuable feedstocks for industry. the strategy requires no prior fractionation of the lignocellulosic biomass, no further synthetic steps, and no input of additional (e.g., petrochemical) platform molecules."
                },
                {
                    "id": "R154440",
                    "label": "Anatase TiO2 Activated by Gold Nanoparticles for Selective Hydrodeoxygenation of Guaiacol to Phenolics",
                    "doi": "10.1021/acscatal.6b02368",
                    "research_field": {
                        "id": "R129",
                        "label": "Organic Chemistry"
                    },
                    "abstract": "gold nanoparticles on a number of supporting materials, including anatase tio2 (tio2-a, in 40 nm and 45 \u03bcm), rutile tio2 (tio2-r), zro2, al2o3, sio2 , and activated carbon, were evaluated for hydrodeoxygenation of guaiacol in 6.5 mpa initial h2 pressure at 300 \u00b0c. the presence of gold nanoparticles on the supports did not show distinguishable performance compared to that of the supports alone in the conversion level and in the product distribution, except for that on a tio2-a-40 nm. the lack of marked catalytic activity on supports other than tio2-a-40 nm suggests that au nanoparticles are not catalytically active on these supports. most strikingly, the gold nanoparticles on the least-active tio2-a-40 nm support stood out as the best catalyst exhibiting high activity with excellent stability and remarkable selectivity to phenolics from guaiacol hydrodeoxygenation. the conversion of guaiacol (\u223c43.1%) over gold on the tio2-a-40 nm was about 33 times that (1.3%) over the tio2-a-40 nm alone. the selectivity o..."
                },
                {
                    "id": "R154455",
                    "label": "Hydrodeoxygenation of Guaiacol over Carbon-Supported Metal Catalysts",
                    "doi": "10.1002/cctc.201300096",
                    "research_field": {
                        "id": "R129",
                        "label": "Organic Chemistry"
                    },
                    "abstract": "catalytic bio\u2010oil upgrading to produce renewable fuels has attracted increasing attention in response to the decreasing oil reserves and the increased fuel demand worldwide. herein, the catalytic hydrodeoxygenation (hdo) of guaiacol with carbon\u2010supported non\u2010sulfided metal catalysts was investigated. catalytic tests were performed at 4.0\\u2005mpa and temperatures ranging from 623 to 673\\u2005k. both ru/c and mo/c catalysts showed promising catalytic performance in hdo. the selectivity to benzene was 69.5 and 83.5\\u2009% at 653\\u2005k over ru/c and 10mo/c catalysts, respectively. phenol, with a selectivity as high as 76.5\\u2009%, was observed mainly on 1mo/c. however, the reaction pathway over both catalysts is different. over the ru/c catalyst, the o\\uf8ffch3 bond was cleaved to form the primary intermediate catechol, whereas only traces of catechol were detected over mo/c catalysts. in addition, two types of active sites were detected over mo samples after reduction in h2 at 973\\u2005k. catalytic studies showed that the demethoxylation of guaiacol is performed over residual moox sites with high selectivity to phenol whereas the consecutive hdo of phenol is performed over molybdenum carbide species, which is widely available only on the 10mo/c sample. different deactivation patterns were also observed over ru/c and mo/c catalysts."
                },
                {
                    "id": "R154460",
                    "label": "A highly stable Ru/LaCO3OH catalyst consisting of support-coated Ru nanoparticles in aqueous-phase hydrogenolysis reactions",
                    "doi": "10.1039/c7gc02414b",
                    "research_field": {
                        "id": "R129",
                        "label": "Organic Chemistry"
                    },
                    "abstract": "a hydrothermally stable ru/laco 3 oh catalyst consisting of ru nanoparticles partially encapsulated by the support with a strong metal\u2013support interaction is developed."
                },
                {
                    "id": "R154468",
                    "label": "Atmospheric Hydrodeoxygenation of Guaiacol over Alumina-, Zirconia-, and Silica-Supported Nickel Phosphide Catalysts",
                    "doi": "10.1021/sc300157d",
                    "research_field": {
                        "id": "R129",
                        "label": "Organic Chemistry"
                    },
                    "abstract": "this study investigated atmospheric hydrodeoxygenation (hdo) of guaiacol over ni2p-supported catalysts. alumina, zirconia, and silica served as the supports of ni2p catalysts. the physicochemical properties of these catalysts were surveyed by n2 physisorption, x-ray diffraction (xrd), co chemisorption, h2 temperature-programmed reduction (h2-tpr), h2 temperature-programmed desorption (h2-tpd), and nh3 temperature-programmed desorption (nh3-tpd). the catalytic performance of these catalysts was tested in a continuous fixed-bed system. this paper proposes a plausible network of atmospheric guaiacol hdo, containing demethoxylation (dmo), demethylation (dme), direct deoxygenation (ddo), hydrogenation (hyd), transalkylation, and methylation. pseudo-first-order kinetics analysis shows that the intrinsic activity declined in the following order: ni2p/zro2 > ni2p/al2o3 > ni2p/sio2. product selectivity at zero guaiacol conversion indicates that ni2p/sio2 promotes dmo and ddo routes, whereas ni2p/zro2 and ni2p/al2o..."
                }
            ]
        },
        {
            "id": "R155844",
            "label": "Photocatalysts",
            "research_fields": [
                {
                    "id": "R130",
                    "label": "Physical Chemistry"
                }
            ],
            "properties": [
                "Wavelength of maximum absorption",
                "Energy band gap",
                "Photocatalyst",
                "Wavelength of maximum emission",
                "Emission lifetime",
                "Ground-state oxidation potential",
                "Ground-state reduction potential",
                "Excited-state oxidation potential",
                "Excited-state reduction potential"
            ],
            "papers": [
                {
                    "id": "R155854",
                    "label": "Visible Light Photoredox Catalysis with Transition Metal Complexes: Applications in Organic Synthesis",
                    "doi": "10.1021/cr300503r",
                    "research_field": {
                        "id": "R130",
                        "label": "Physical Chemistry"
                    },
                    "abstract": "a fundamental aim in the field of catalysis is the development of new modes of small molecule activation. one approach toward the catalytic activation of organic molecules that has received much attention recently is visible light photoredox catalysis. in a general sense, this approach relies on the ability of metal complexes and organic dyes to engage in single-electron-transfer (set) processes with organic substrates upon photoexcitation with visible light. \\n \\nmany of the most commonly employed visible light photocatalysts are polypyridyl complexes of ruthenium and iridium, and are typified by the complex tris(2,2\u2032-bipyridine) ruthenium(ii), or ru(bpy)32+ (figure 1). these complexes absorb light in the visible region of the electromagnetic spectrum to give stable, long-lived photoexcited states.1,2 the lifetime of the excited species is sufficiently long (1100 ns for ru(bpy)32+) that it may engage in bimolecular electron-transfer reactions in competition with deactivation pathways.3 although these species are poor single-electron oxidants and reductants in the ground state, excitation of an electron affords excited states that are very potent single-electron-transfer reagents. importantly, the conversion of these bench stable, benign catalysts to redox-active species upon irradiation with simple household lightbulbs represents a remarkably chemoselective trigger to induce unique and valuable catalytic processes. \\n \\n \\n \\nfigure 1 \\n \\nruthenium polypyridyl complexes: versatile visible light photocatalysts. \\n \\n \\n \\nthe ability of ru(bpy)32+ and related complexes to function as visible light photocatalysts has been recognized and extensively investigated for applications in inorganic and materials chemistry. in particular, photoredox catalysts have been utilized to accomplish the splitting of water into hydrogen and oxygen4 and the reduction of carbon dioxide to methane.5 ru(bpy)32+ and its analogues have been used (i) as components of dye-sensitized solar cells6 and organic light-emitting diodes,7 (ii) to initiate polymerization reactions,8 and (iii) in photo-dynamic therapy.9 \\n \\nuntil recently, however, these complexes had been only sporadically employed as photocatalysts in the area of organic synthesis. the limited exploration of this area is perhaps surprising, as single-electron, radical processes have long been employed in c\u2013c bond construction and often provide access to reactivity that is complementary to that of closed-shell, two-electron pathways.10 in 2008, concurrent reports from the yoon group and our own lab detailed the use of ru(bpy)32+ as a visible light photoredox catalyst to perform a [2 + 2] cycloaddition11 and an \u03b1-alkylation of aldehydes,12 respectively. shortly thereafter, stephenson and co-workers disclosed a photoredox reductive dehalogenation of activated alkyl halides mediated by the same catalyst.13 the combined efforts of these three research groups have helped to initiate a renewed interest in this field, prompting a diversity of studies into the utility of photoredox catalysis as a conceptually novel approach to synthetic organic reaction development. \\n \\nmuch of the promise of visible light photoredox catalysis hinges on its ability to achieve unique, if not exotic bond constructions that are not possible using established protocols. for instance, photoredox catalysis may be employed to perform overall redox neutral reactions. as both oxidants and reductants may be transiently generated in the same reaction vessel, photoredox approaches may be used to develop reactions requiring both the donation and the reception of electrons at disparate points in the reaction mechanism. this approach stands in contrast to methods requiring stoichiometric chemical oxidants and reductants, which are often incompatible with each other, as well as to electrochemical approaches, which are not amenable to redox neutral transformations. furthermore, single-electron-transfer events often provide access to radical ion intermediates having reactivity patterns fundamentally different from those of their ground electronic or excited states.14 access to these intermediates using other means of activation is often challenging or requires conditions under which their unique reactivity cannot be productively harnessed. \\n \\nat the same time, photoredox catalysts such as ru(bpy)32+ may also be employed to generate radicals for use in a diverse range of established radical chemistries. photoredox reactions occur under extremely mild conditions, with most reactions proceeding at room temperature without the need for highly reactive radical initiators. the irradiation source is typically a commercial household light bulb, a significant advantage over the specialized equipment required for processes employing high-energy ultraviolet (uv) light. additionally, because organic molecules generally do not absorb visible light, there is little potential for deleterious side reactions that might arise from photoexcitation of the substrate itself. finally, photoredox catalysts may be employed at very low loadings, with 1 mole % or less being typical. \\n \\nthis review will highlight the early work on the use of transition metal complexes as photoredox catalysts to promote reactions of organic compounds (prior to 2008), as well as cover the surge of work that has appeared since 2008. we have for the most part grouped reactions according to whether the organic substrate undergoes reduction, oxidation, or a redox neutral reaction and throughout have sought to highlight the variety of reactive intermediates that may be accessed via this general reaction manifold.15 \\n \\nstudies on the use of transition metal complexes as visible light photocatalysts for organic synthesis have benefited tremendously from advances in the related fields of organic and semiconductor photocatalysis. many organic molecules may function as visible light photocatalysts; analogous to metal complexes such as ru(bpy)32+, organic dyes such as eosin y, 9,10-dicyanoanthracene, and triphenylpyrylium salts absorb light in the visible region to give excited states capable of single-electron transfer. these catalysts have been employed to achieve a vast range of bond-forming reactions of broad utility in organic synthesis.16 visible light photocatalysis has also been carried out with heterogeneous semiconductors such as mesoporous carbon nitride17 and various metal oxides and sulfides.18 these approaches are often complementary to photoredox catalysis with transition metal-polypyridyl complexes, and we have referred to work in these areas when it is similar to the chemistry under discussion. however, an in-depth discussion of the extensive literature in these fields is outside the scope of this review, and readers are directed to existing reviews on these topics.16\u201318"
                },
                {
                    "id": "R156187",
                    "label": "Enhanced Luminescent Iridium(III) Complexes Bearing Aryltriazole Cyclometallated Ligands",
                    "doi": "10.1021/ic2014013",
                    "research_field": {
                        "id": "R130",
                        "label": "Physical Chemistry"
                    },
                    "abstract": "herein we report the synthesis of 4-aryl-1-benzyl-1h-1,2,3-triazoles (atl), made via \"click chemistry\" and their incorporation as cyclometallating ligands into new heteroleptic iridium(iii) complexes containing diimine (n(^)n) ancillary ligands 2,2\\'-bipyridine (bpy) and 4,4\\'-di-tert-butyl-2,2\\'-bipyridine (dtbubpy). depending on decoration, these complexes emit from the yellow to sky blue in acetonitrile (acn) solution at room temperature (rt). their emission energies are slightly blue-shifted and their photoluminescent quantum efficiencies are markedly higher (between 25 and 80%) than analogous (c(^)n)(2)ir(n(^)n)(+) type complexes, where c(^)n is a decorated 2-phenylpyridinato ligand. this increased brilliance is in part due to the presence of the benzyl groups, which act to sterically shield the iridium metal center. x-ray crystallographic analyses of two of the atl complexes corroborate this assertion. their electrochemistry is reversible, thus making these complexes amenable for inclusion in light-emitting electrochemical cells (leecs). a parallel computational investigation supports the experimental findings and demonstrates that for all complexes included in this study, the highest occupied molecular orbital (homo) is located on both the aryl fragment of the atl ligands and the iridium metal while the lowest unoccupied molecular orbital (lumo) is located essentially exclusively on the ancillary ligand."
                },
                {
                    "id": "R156216",
                    "label": "Electrophosphorescent homo- and heteroleptic copper(i) complexes prepared from various bis-phosphine ligands",
                    "doi": "10.1039/b707398d",
                    "research_field": {
                        "id": "R130",
                        "label": "Physical Chemistry"
                    },
                    "abstract": "homo- and heteroleptic copper(i) complexes obtained from various chelating bis-phosphine ligands and cu(ch3cn)4bf4 have been used for the preparation of light emitting devices."
                },
                {
                    "id": "R156228",
                    "label": "Cationic and Thiol\u2013Ene Photopolymerization upon Red Lights Using Anthraquinone Derivatives as Photoinitiators",
                    "doi": "10.1021/ma401513b",
                    "research_field": {
                        "id": "R130",
                        "label": "Physical Chemistry"
                    },
                    "abstract": "anthraquinone derivatives in combination with an iodonium salt (and optionally n-vinylcarbazole) have been used as photoinitiating systems. one of them (oil blue n) that is particularly efficient for cationic, ipn, and thiol\u2013ene polymerization upon red lights (laser diode at 635 nm or household red led bulb at 630 nm) belongs to the very few systems available at this long wavelength in such experimental conditions (low light intensity in the 10\u2013100 mw/cm2 range). their abilities to initiate the cationic photopolymerization of epoxides or vinyl ethers under very soft halogen lamp irradiation have been also investigated. the photochemical mechanisms are studied by steady state photolysis, fluorescence, cyclic voltammetry, and electron spin resonance spin trapping techniques."
                },
                {
                    "id": "R156272",
                    "label": "A Noble-Metal-Free System for Photocatalytic Hydrogen Production from Water",
                    "doi": "10.1002/chem.201302091",
                    "research_field": {
                        "id": "R130",
                        "label": "Physical Chemistry"
                    },
                    "abstract": "a series of heteroleptic copper(i) complexes with bidentate pp and nn chelate ligands was prepared and successfully applied as photosensitizers in the light-driven production of hydrogen, by using [fe3(co)12] as a water-reduction catalyst (wrc). these systems efficiently reduces protons from water/thf/triethylamine mixtures, in which the amine serves as a sacrificial electron donor (sr). turnover numbers (for h) up to 1330 were obtained with these fully noble-metal-free systems. the new complexes were electrochemically and photophysically characterized. they exhibited a correlation between the lifetimes of the mlct excited state and their efficiency as photosensitizers in proton-reduction systems. within these experiments, considerably long excited-state lifetimes of up to 54 \u03bcs were observed. quenching studies with the sr, in the presence and absence of the wrc, showed that intramolecular deactivation was more efficient in the former case, thus suggesting the predominance of an oxidative quenching pathway."
                },
                {
                    "id": "R156292",
                    "label": "Simple Cu(I) Complexes with Unprecedented Excited-State Lifetimes",
                    "doi": "10.1021/ja012247h",
                    "research_field": {
                        "id": "R130",
                        "label": "Physical Chemistry"
                    },
                    "abstract": "this report describes new, readily accessible copper(i) complexes that can exhibit unusually long-lived, high quantum yield emissions in fluid solution. the complexes are of the form [cu(nn)(pop)]+ where nn denotes 1,10-phenanthroline (phen), 2,9-dimethyl-1,10-phenanthroline (dmp) or 2,9-di-n-butyl-1,10-phenanthroline (dbp) and pop denotes bis[2-(diphenylphosphino)phenyl] ether. modes of characterization include x-ray crystallography and cyclic voltammetry. the complexes each have a pseudotetrahedral coordination geometry and a cu(ii)/cu(i) potential upward of +1.2 v vs ag/agcl. in room-temperature dichloromethane solution, charge-transfer excited states of the dmp and dbp derivatives exhibit respective emission quantum yields of 0.15 and 0.16 and corresponding excited-state lifetimes of 14.3 and 16.1 mus, respectively. despite the fact that coordinating solvents usually quench charge-transfer emission from copper systems, the photoexcited dmp (dbp) complex retains a lifetime of 2.4 mus (5.4 mus) in methanol."
                }
            ]
        },
        {
            "id": "R156306",
            "label": "Research objective",
            "research_fields": [
                {
                    "id": "R185",
                    "label": "Plasma and Beam Physics"
                }
            ],
            "properties": [
                "Research objective"
            ],
            "papers": [
                {
                    "id": "R156333",
                    "label": "Demonstration of a Soft X-Ray Amplifier",
                    "doi": "10.1103/physrevlett.54.110",
                    "research_field": {
                        "id": "R185",
                        "label": "Plasma and Beam Physics"
                    },
                    "abstract": "we report observations of amplified spontaneous emission at soft x-ray wavelengths. an optical laser ionized thin foils of selenium to produce a population inversion of the $2{p}^{5}3p$ and $2{p}^{5}3s$ levels of the neonlike ion. using three time-resolved, spectroscopic measurements we demonstrated gain-length products up to 6.5 and gain coefficients of 5.5\\\\ifmmode\\\\pm\\\\else\\\\textpm\\\\fi{}1.0 ${\\\\mathrm{cm}}^{\\\\ensuremath{-}1}$ for the $j=2 \\\\mathrm{to} 1$ lines at 206.3 and 209.6 \\\\aa{}. we also observed considerable amplification for the same transitions in yttrium at 155.0 and 157.1 \\\\aa{}."
                },
                {
                    "id": "R156404",
                    "label": "Amplification of stimulated soft x-ray emission in a confined plasma column",
                    "doi": "10.1103/physrevlett.55.1753",
                    "research_field": {
                        "id": "R185",
                        "label": "Plasma and Beam Physics"
                    },
                    "abstract": "\"une amplification atteignant 100 de l'emission stimulee sur l'emission spontanee de la raie cvi 182 a a ete mesuree dans une colonne de plasma magnetiquement confinee, par deux methodes independantes utilisant des monochromateurs uv extreme etalonnes en intensite. une confirmation supplementaire que l'amplification est due a l'emission stimulee a ete obtenue avec un miroir rx mou: avec 12% de reflectivite du miroir effective mesuree, une augmentation de 120% de l'intensite de la raie cvi 182 a dans la direction axiale a ete observee\""
                },
                {
                    "id": "R156448",
                    "label": "Soft x-ray lasing in neonlike germanium and copper plasmas",
                    "doi": "10.1103/physrevlett.59.1185",
                    "research_field": {
                        "id": "R185",
                        "label": "Plasma and Beam Physics"
                    },
                    "abstract": "soft x-ray 3p\\\\ensuremath{\\\\rightarrow}3s lasing in neonlike germanium (${\\\\mathrm{ge}}^{22+}$) and copper (${\\\\mathrm{cu}}^{19+}$) in the wavelength interval of 195 to 285 a\\\\r{} is observed for the first time, with gain coefficients ranging from 1.7 to 4.1 ${\\\\mathrm{cm}}^{\\\\mathrm{\\\\ensuremath{-}}1}$, the higher gain with germanium. the lasing plasmas are produced by focusing a driving laser beam (\\\\ensuremath{\\\\lambda}=1.05 \\\\ensuremath{\\\\mu}m, 2-ns fwhm) into an 18-mm-long line onto thin films and slab targets. the measured j=0 to 1 gain coefficients are comparable to those of the j=2 to 1 transitions. the measured wavelengths of the six lasing lines compared favorably with recent calculations."
                },
                {
                    "id": "R156490",
                    "label": "Soft-x-ray amplification by lithiumlike ions in recombining hot plasmas",
                    "doi": "10.1364/josab.4.000563",
                    "research_field": {
                        "id": "R185",
                        "label": "Plasma and Beam Physics"
                    },
                    "abstract": "this paper describes calculations and experiments about soft-x-ray amplification by lithiumlike ions in recombining laser-produced plasmas. time- and space-dependent population inversion densities calculated with a collisional-radiative model used as the postprocessor of a hydrodynamic code are reported. amplification diagnostic accuracy in plasma experiments is discussed. time-integrated and time-resolved measurements of gain are presented, especially at 105.7 a in lithiumlike aluminum. it is shown that, in a plasma produced by a 3-nsec laser pulse, the peak of amplified radiation occurs about 7 nsec after the top of the pulse. the maximum gain\u2013length product measured previously was 2\u20132.5. a short description of a future experiment designed for producing a much larger gain is presented."
                },
                {
                    "id": "R156576",
                    "label": "Demonstration of x-ray amplifiers near the carbonKedge",
                    "doi": "10.1103/physrevlett.65.420",
                    "research_field": {
                        "id": "R185",
                        "label": "Plasma and Beam Physics"
                    },
                    "abstract": "\"the ni-like 4d-4p laser scheme has been extended to wavelengths near the k absorption edge of carbon. a gain of 2.3 ${\\\\mathrm{cm}}^{\\\\mathrm{\\\\ensuremath{-}}1}$ with a duration of 250 psec was observed in ni-like ta at 44.83 \\\\aa{} (a wavelength close to optimal for holographic imaging of live cells). ni-like w produced a gain of 2.6 ${\\\\mathrm{cm}}^{\\\\mathrm{\\\\ensuremath{-}}1}$ with a total of seven gainlengths of amplification at 43.18 \\\\aa{}. this is the first demonstration of an x-ray amplifier on the short-wavelength side of the carbon k edge, within the ``water window.'' both lasers should be scalable to coherent power sufficient for holographic imaging and other applications.\""
                },
                {
                    "id": "R156620",
                    "label": "Demonstration of population inversion by resonant photopumping in a neon gas cell irradiated by a sodiumZpinch",
                    "doi": "10.1103/physrevlett.68.796",
                    "research_field": {
                        "id": "R185",
                        "label": "Plasma and Beam Physics"
                    },
                    "abstract": "the broadband radiation emitted from a na z pinch is used to photoionize ne to the he-like ground state and radiation from the na 1s 2 -1s2p 1 p 1 transition is used to resonantly photoexcite the ne 1s 2 -1s4p 1 p 1 transition. time-resolved and time-integrated spectral measurements of the ne k-shell emission demonstrate the first population inversion driven by a z pinch"
                },
                {
                    "id": "R156663",
                    "label": "Short wavelength x\u2010ray laser research at the Lawrence Livermore National Laboratory",
                    "doi": "10.1063/1.860203",
                    "research_field": {
                        "id": "R185",
                        "label": "Plasma and Beam Physics"
                    },
                    "abstract": "laboratory x\u2010ray lasers are currently being studied by researchers worldwide. this paper reviews some of the recent work carried out at lawrence livermore national laboratory. laser action has been demonstrated at wavelengths as short as 35.6 a while saturation of the small signal gain has been observed with longer wavelength schemes. some of the most successful schemes to date have been collisionally pumped x\u2010ray lasers that use the thermal electron distribution within a laser\u2010produced plasma to excite electrons from closed shells in neon\u2010 and nickel\u2010like ions to metastable levels in the next shell. attempts to quantify and improve the longitudinal and transverse coherence of collisionally pumped x\u2010ray lasers are motivated by the desire to produce sources for specific applications. toward this goal there is a large effort underway to enhance the power output of the ni\u2010like ta x\u2010ray laser at 44.83 a as a source for x\u2010ray imaging of live cells. improving the efficiency of x\u2010ray lasers in order to produce s..."
                },
                {
                    "id": "R156770",
                    "label": "Femtosecond-pulse-driven 10-Hz 418-nm laser in Xe ix",
                    "doi": "10.1364/josab.13.000180",
                    "research_field": {
                        "id": "R185",
                        "label": "Plasma and Beam Physics"
                    },
                    "abstract": "we report the observation of extreme uv lasing at 41.81 nm on the 4d95d1s0\u22124d95p1p1 transition in xe ix, as proposed by lemoff [ opt. lett.19, 569 ( 1994)]. a 10-hz circularly polarized 800-nm laser pulse with an energy of \u223c70 mj and a duration of \u223c40 fs is longitudinally focused to a peak intensity of >3 \u00d7 1016 w/cm2 over a length of 8.4 nm in a differentially pumped cell containing 12 torr of xe gas. laser amplification was observed with an estimated gain coefficient of 13 cm\u22121 and a total gain of exp(11)."
                },
                {
                    "id": "R156819",
                    "label": "A Saturated X-ray Laser Beam at 7 Nanometers",
                    "doi": "10.1126/science.276.5315.1097",
                    "research_field": {
                        "id": "R185",
                        "label": "Plasma and Beam Physics"
                    },
                    "abstract": "a saturated nickel-like samarium x-ray laser beam at 7 nanometers has been demonstrated with an output energy of 0.3 millijoule in 50-picosecond pulses, demonstrating that saturated operation of a laser at wavelengths shorter than 10 nanometers can be achieved. the narrow divergence, short wavelength, short pulse duration, high efficiency, and high brightness of this samarium laser make it an ideal candidate for many x-ray laser applications."
                },
                {
                    "id": "R156908",
                    "label": "Saturated and Short Pulse Duration X-Ray Lasers",
                    "doi": "10.1063/1.1521033",
                    "research_field": {
                        "id": "R185",
                        "label": "Plasma and Beam Physics"
                    },
                    "abstract": "the basis of a model of the relationship between gain and output laser intensity is reviewed and the measurement of the duration of x\u2010ray lasing with a streak camera with 700 fs temporal resolution is described. combined with a temporal smearing due to the spectrometer employed, we have measured x\u2010ray laser pulse durations for ni\u2010like silver at 13.9 nm and ne\u2010like nickel at 23.1 nm with a total time resolution of 1.1 ps. an extension of the model is shown to consistently relate the measured x\u2010ray laser pulse duration to estimates of the gain duration obtained by temporally resolving resonance line emission from states near in energy to the upper lasing level."
                }
            ]
        },
        {
            "id": "R159441",
            "label": "City Digital Twin Potentials",
            "research_fields": [
                {
                    "id": "R137681",
                    "label": "Information Systems, Process and Knowledge Management"
                }
            ],
            "properties": [
                "Visualization",
                "Situational Awareness",
                "Planning and Prediction",
                "Integration and Collaboration",
                "Data management"
            ],
            "papers": [
                {
                    "id": "R160244",
                    "label": "Hybrid Automaton Implementation For Intelligent Agents\u2019 Behavior Modelling",
                    "doi": "10.1109/icisct47635.2019.9011955",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "it is predicted that the future of our world might be considered as an urban future, and this future should be happy, or, at least, livable. to describe ways and conditions for creating this future, the \u201csmart city\u201d concept has been worked out. \u201csmart city\u201d in this concept is under consideration as a cyber-physical system. digital twins have become the main elements of these systems, while computer simulation is to be the main technology for these systems investigation. in this paper, the process of a hybrid (continuous-discrete) automaton implementation for the aim of intelligent agents\u2019 behavior modelling is under discussion."
                },
                {
                    "id": "R159487",
                    "label": "Urban Intelligence: a Modular, Fully Integrated, and Evolving Model for Cities Digital Twinning",
                    "doi": "10.1109/honet.2019.8907962",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "the urban intelligence (ui) paradigm proposes an ecosystem of technologies to improve urban environment, wellbeing, quality of life and smart city systems. it fosters the definition of a digital twin of the city, namely a cyber-physical counterpart of all the city systems and sub-systems. here we propose a novel approach to ui that extends available frameworks combining advanced multidisciplinary modelling of the city, simulation and learning tools with numerical optimization techniques, each of them specialized for the digital representation of city systems and subsystems, including not only city infrastructures, but also city users and their interactions. ui provides sets of candidate policies in complex scenarios and supports policy makers and stakeholders in designing sustainable and personalized solutions. the main characteristics of the proposed ui architecture are (a) fully multidisciplinary integration of city layers, (b) connection and evolution with the city, (c) integration of participative strategies to include \u201chuman-oriented\u201d information, and (d) modularity of application."
                },
                {
                    "id": "R159450",
                    "label": "Urban Digital Twins for Smart Cities and Citizens: The Case Study of Herrenberg, Germany",
                    "doi": "10.3390/su12062307",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "cities are complex systems connected to economic, ecological, and demographic conditions and change. they are also characterized by diverging perceptions and interests of citizens and stakeholders. thus, in the arena of urban planning, we are in need of approaches that are able to cope not only with urban complexity but also allow for participatory and collaborative processes to empower citizens. this to create democratic cities. connected to the field of smart cities and citizens, we present in this paper, the prototype of an urban digital twin for the 30,000-people town of herrenberg in germany. urban digital twins are sophisticated data models allowing for collaborative processes. the herein presented prototype comprises (1) a 3d model of the built environment, (2) a street network model using the theory and method of space syntax, (3) an urban mobility simulation, (4) a wind flow simulation, and (5) a number of empirical quantitative and qualitative data using volunteered geographic information (vgi). in addition, the urban digital twin was implemented in a visualization platform for virtual reality and was presented to the general public during diverse public participatory processes, as well as in the framework of the \u201cmorgenstadt werkstatt\u201d (tomorrow\u2019s cities workshop). the results of a survey indicated that this method and technology could significantly aid in participatory and collaborative processes. further understanding of how urban digital twins support urban planners, urban designers, and the general public as a collaboration and communication tool and for decision support allows us to be more intentional when creating smart cities and sustainable cities with the help of digital twins. we conclude the paper with a discussion of the presented results and further research directions."
                },
                {
                    "id": "R159456",
                    "label": "Geospatial Artificial Intelligence: Potentials of Machine Learning for 3D Point Clouds and Geospatial Digital Twins",
                    "doi": "10.1007/s41064-020-00102-3",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "abstract artificial intelligence (ai) is changing fundamentally the way how it solutions are implemented and operated across all application domains, including the geospatial domain. this contribution outlines ai-based techniques for 3d point clouds and geospatial digital twins as generic components of geospatial ai. first, we briefly reflect on the term \u201cai\u201d and outline technology developments needed to apply ai to it solutions, seen from a software engineering perspective. next, we characterize 3d point clouds as key category of geodata and their role for creating the basis for geospatial digital twins; we explain the feasibility of machine learning (ml) and deep learning (dl) approaches for 3d point clouds. in particular, we argue that 3d point clouds can be seen as a corpus with similar properties as natural language corpora and formulate a \u201cnaturalness hypothesis\u201d for 3d point clouds. in the main part, we introduce a workflow for interpreting 3d point clouds based on ml/dl approaches that derive domain-specific and application-specific semantics for 3d point clouds without having to create explicit spatial 3d models or explicit rule sets. finally, examples are shown how ml/dl enables us to efficiently build and maintain base data for geospatial digital twins such as virtual 3d city models, indoor models, or building information models."
                },
                {
                    "id": "R159459",
                    "label": "RESEARCH ON CONSTRUCTION OF SPATIO-TEMPORAL DATA VISUALIZATION PLATFORM FOR GIS AND BIM FUSION",
                    "doi": "10.5194/isprs-archives-xlii-3-w10-555-2020",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "abstract. the visualization model of gis and bim fusion can provide data bearing platform and main technical support for future urban operation centers, digital twin cities, and smart cities. based on the analysis of the features and advantages of gis and bim fusion, this paper proposes a construction method of the spatio-temporal data visualization platform for gis and bim fusion. it expounds and analyzes the overall architecture design of platform, multi-dimensional and multi-spatial scales visualization, space analysis for gis and bim fusion, and platform applications and so on. the urban virtual simulation spatio-temporal data platform project of teda new district in tianjin has verified and demonstrated that the effect of application is good. this provides a feasible solution for the construction of spatio-temporal data visualization platform.\\n"
                },
                {
                    "id": "R159465",
                    "label": "A Socio-Technical Perspective on Urban Analytics: The Case of City-Scale Digital Twins",
                    "doi": "10.1080/10630732.2020.1798177",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "abstract this paper demonstrates that a shift from a purely technical to a more socio-technical perspective has significant implications for the conceptualization, design, and implementation of smart city technologies. such implications are discussed and illustrated through the case of an emerging urban analytics tool, the city-scale digital twin. based on interdisciplinary insights and a participatory knowledge co-production and tool co-development process, including both researchers and prospective users, we conclude that in order to move beyond a mere \u201chype technology,\u201d city-scale digital twins must reflect the specifics of the urban and socio-political context."
                },
                {
                    "id": "R159473",
                    "label": "Participatory Sensing and Digital Twin City: Updating Virtual City Models for Enhanced Risk-Informed Decision-Making",
                    "doi": "10.1061/(asce)me.1943-5479.0000748",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "abstractthe benefits of a digital twin city have been assessed based on real-time data collected from preinstalled internet of things (iot) sensors (e.g.,\\xa0traffic, energy use, air pollution, water ..."
                },
                {
                    "id": "R159481",
                    "label": "The Digital Twin of the City of Zurich for Urban Planning",
                    "doi": "10.1007/s41064-020-00092-2",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "abstract population growth will confront the city of zurich with a variety of challenges in the coming years, as the increase in the number of inhabitants and jobs will lead to densification and competing land uses. the tasks for the city administration have become more complex, whereas tools and methods are often based on traditional, static approaches while involving a limited number of citizens and stakeholders in relevant decisions. the digital transformation of more and more\\xa0pieces of the planning and decision-making process will make both increasingly more illustrative, easier to understand and more comprehensible. an important data basis for these processes is the digital twin of the city of zurich. 3d spatial data and their models transform themes of the city, such as buildings, bridges, vegetation, etc., to the digital world, are being updated when required, and create advantages in digital space. these benefits need to be highlighted and published. an important step in public awareness is the release of 3d spatial data under open government data. this allows the development of applications, the promotion of understanding, and the simplification of the creation of different collaborative platforms. by\\xa0visualization and analysis of digital prototypes and the demonstration of interactions with the built environment, scenarios can be digitally developed and discussed in decision-making bodies. questions about the urban climate can be simulated with the help of the digital twin and results can be linked to the existing 3d spatial data. thus, the 3d spatial data set, the models and their descriptions through\\xa0metadata become the reference and must be updated according to the requirements. depending on requirements and questions, further 3d spatial data must be added. the description of the 3d spatial data and their models or the lifecycle management of the digital twin must be carried out with great care. only in this way, decision processes can be supported in a comprehensible way."
                },
                {
                    "id": "R159484",
                    "label": "Smart city dvelopment with digital twin technology",
                    "doi": "10.18690/978-961-286-362-3.20",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "growing urban areas are major consumers of natural resources, energy and raw materials. understanding cities\u00b4 urban metabolism is salient when developing sustainable and resilient cities. this paper addresses concepts of smart city and digital twin technology as means to foster more sustainable urban development. smart city has globally been well adopted concept in urban development. with smart city development cities aim to optimize overall performance of the city, its infrastructures, processes and services, but also to improve socio-economic wellbeing. dynamic digital twins are constituted to form real-time connectivity between virtual and physical objects. digital twin combines virtual objects to its physical counterparts. this conceptual paper provides additionally examples from dynamic digital twin platforms and digital twin of helsinki, finland."
                },
                {
                    "id": "R160211",
                    "label": "Architecting Smart City Digital Twins: Combined Semantic Model and Machine Learning Approach",
                    "doi": "10.1061/(asce)me.1943-5479.0000774",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "abstractthis work was motivated by the premise that next-generation smart city systems will be enabled by widespread adoption of sensing and communication technologies deeply embedded within the ph..."
                },
                {
                    "id": "R160217",
                    "label": "Methodological Framework for Digital Transition and Performance Assessment of Smart Cities",
                    "doi": "10.23919/splitech.2019.8783170",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "the ultimate goal of smart cities is to improve citizens\u2019 quality of life in a scenario where technological solutions challenge urban governance. however, the knowledge and framework for data use for smart cities remain relatively unknown. the actual translation of city problems into diverse actions requires specific methodologies to guide digital transitions of cities and to assess to what extent the smart cities\u2019 initiatives pursue sustainable development goals. this paper proposes a methodological framework for digital modelling of cities allowing assessment of their performance and supporting decision making. the city model adopts the concept of digital twin as a powerful tool for discussion between stakeholders, as well as citizens to find the smartest solutions and get valuable insight after their deployment. the methodological framework is presented as a set of digital twin concept, stages of digital twinning and implementation strategy. furthermore, the most common city information models, suitable for implementation of digital twins are summarized."
                },
                {
                    "id": "R160231",
                    "label": "Using big data analytics and IoT principles to keep an eye on underground infrastructure",
                    "doi": "10.1109/bigdata.2017.8258503",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "a concept development study by the open geospatial consortium (ogc) has highlighted the importance of high-quality feature data for underground urban infrastructure (ugi). analysis of large survey datasets, including both visual and non-visual methods, is essential for creating and maintaining ugi geodata. connecting hidden features with diverse, high-velocity sensing streams and realistic predictive models that effectively characterize them is key to lower construction costs, efficient infrastructure operation, sound disaster preparedness, and new smart city services. iot principles that combine ogc geodata and sensor web observation standards may offer the best chance for working towards functional \u201cdigital twins\u201d of such hidden infrastructure that are both cost effective and scalable with the increasing complexity and instrumentation of the underground built environment. technical and policy challenges remain, however, before this can be achieved."
                },
                {
                    "id": "R160241",
                    "label": "AUTOMATIC 3D BUILDINGS COMPACT RECONSTRUCTION FROM LIDAR POINT CLOUDS",
                    "doi": "10.5194/isprs-archives-xliii-b2-2020-473-2020",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "abstract. point clouds generated from aerial lidar and photogrammetric techniques are great ways to obtain valuable spatial insights over large scale. however, their nature hinders the direct extraction and sharing of underlying information. the generation of consistent large-scale 3d city models from this real-world data is a major challenge. specifically, the integration in workflows usable by decision-making scenarios demands that the data is structured, rich and exchangeable. citygml permits new advances in terms of interoperable endeavour to use city models in a collaborative way. efforts have led to render good-looking digital twins of cities but few of them take into account their potential use in finite elements simulations (wind, floods, heat radiation model, etc.). in this paper, we target the automatic reconstruction of consistent 3d city buildings highlighting closed solids, coherent surface junctions, perfect snapping of vertices, etc. it specifically investigates the topological and geometrical consistency of generated models from aerial lidar point cloud, formatted following the cityjson specifications. these models are then usable to store relevant information and provides geometries usable within complex computations such as computational fluid dynamics, free of local inconsistencies (e.g. holes and unclosed solids).\\n"
                },
                {
                    "id": "R160247",
                    "label": "Smart city digital twins",
                    "doi": "10.1109/ssci.2017.8285439",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "\"driven by the challenges of rapid urbanization, cities are determined to implement advanced socio-technological changes and transform into smarter cities. the success of such transformation, however, greatly relies on a thorough understanding of the city's states of spatiotemporal flux. the ability to understand such fluctuations in context and in terms of interdependencies that exist among various entities across time and space is crucial, if cities are to maintain their smart growth. here, we introduce a smart city digital twin paradigm that can enable increased visibility into cities' human-infrastructure-technology interactions, in which spatiotemporal fluctuations of the city are integrated into an analytics platform at the real-time intersection of reality-virtuality. through learning and exchange of spatiotemporal information with the city, enabled through virtualization and the connectivity offered by internet of things (iot), this digital twin of the city becomes smarter over time, able to provide predictive insights into the city's smarter performance and growth.\""
                },
                {
                    "id": "R160253",
                    "label": "The Potential of Digital Twin Model Integrated With Artificial Intelligence Systems",
                    "doi": "10.1109/eeeic/icpseurope49358.2020.9160810",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "the paper explores the use of a \u201cdigital twin model\u201d applied to the case study of a residential district, and organized as a three-dimensional data system able to participate to the intelligent optimization and automation of the energy management and efficiency of the building system. the case study focuses on the area called rinascimento iii in rome, consisting of 16 eight-floor building hosting 216 apartment units with an overall percentage of self-renewable energy produced by the building complex equal to 70%. this already quite high percentage means that the building complex can be defined as a near zero energy building (nzeb), i.e. a building that has a very high energy performance, and the nearly-zero or very low amount of energy required should be covered to a very significant extent by energy from renewable sources, including energy from renewable source produced on-site or nearby."
                },
                {
                    "id": "R160256",
                    "label": "Devising a Game Theoretic Approach to Enable Smart City Digital Twin Analytics",
                    "doi": "10.24251/hicss.2019.241",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "despite investments in advancing information and communications technology (ict)-integrated infrastructure systems toward becoming smarter cities, cities often face a large gap between smart sustainable supply and demand. here, we review the core concepts of ict-integrated infrastructure systems as they pertain to developing smart and sustainable cities, and describe how a game theoretic-based digital twin of a city can enable more visibility and insight into the successful implementation of such systems. this study is a foundational step toward enabling participation of all city stakeholders (i.e., government, industry, and citizens) in the decision making process and the creation of smart sustainable cities. engaging city stakeholders in such a manner allows for collective participation in changes, which can enable continuous adaptation toward more sustaining growth and prosperity. 1. smart sustainable cities 1.1. urbanization, growth of supply and demand, and urge for efficiency between 1950 and 2018, the world\u2019s urban population have grown from 751 to more than 4.2 billion. projections anticipate that, by 2050, they will constitute nearly 70% of the world population [1]. in the united states, currently the most urbanized region in the world, this percentage is expected to increase to"
                },
                {
                    "id": "R175410",
                    "label": "The FAIR Data Maturity Model: An Approach to Harmonise FAIR Assessments",
                    "doi": "10.5334/dsj-2020-041",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "in the past years, many methodologies and tools have been developed to assess the fairness of research data. these different methodologies and tools have been based on various interpretations of the fair principles, which makes comparison of the results of the assessments difficult. the work in the rda fair data maturity model working group reported here has delivered a set of indicators with priorities and guidelines that provide a \u2018lingua franca\u2019 that can be used to make the results of the assessment using those methodologies and tools comparable. the model can act as a tool that can be used by various stakeholders, including researchers, data stewards, policy makers and funding agencies, to gain insight into the current fairness of data as well as into the aspects that can be improved to increase the potential for reuse of research data. through increased efficiency and effectiveness, it helps research activities to solve societal challenges and to support evidence-based decisions. the maturity model is publicly available and the working group is encouraging application of the model in practice. experience with the model will be taken into account in the further development of the model."
                }
            ]
        },
        {
            "id": "R160259",
            "label": "Digital city twin review",
            "research_fields": [
                {
                    "id": "R137681",
                    "label": "Information Systems, Process and Knowledge Management"
                }
            ],
            "properties": [
                "Application area"
            ],
            "papers": [
                {
                    "id": "R159450",
                    "label": "Urban Digital Twins for Smart Cities and Citizens: The Case Study of Herrenberg, Germany",
                    "doi": "10.3390/su12062307",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "cities are complex systems connected to economic, ecological, and demographic conditions and change. they are also characterized by diverging perceptions and interests of citizens and stakeholders. thus, in the arena of urban planning, we are in need of approaches that are able to cope not only with urban complexity but also allow for participatory and collaborative processes to empower citizens. this to create democratic cities. connected to the field of smart cities and citizens, we present in this paper, the prototype of an urban digital twin for the 30,000-people town of herrenberg in germany. urban digital twins are sophisticated data models allowing for collaborative processes. the herein presented prototype comprises (1) a 3d model of the built environment, (2) a street network model using the theory and method of space syntax, (3) an urban mobility simulation, (4) a wind flow simulation, and (5) a number of empirical quantitative and qualitative data using volunteered geographic information (vgi). in addition, the urban digital twin was implemented in a visualization platform for virtual reality and was presented to the general public during diverse public participatory processes, as well as in the framework of the \u201cmorgenstadt werkstatt\u201d (tomorrow\u2019s cities workshop). the results of a survey indicated that this method and technology could significantly aid in participatory and collaborative processes. further understanding of how urban digital twins support urban planners, urban designers, and the general public as a collaboration and communication tool and for decision support allows us to be more intentional when creating smart cities and sustainable cities with the help of digital twins. we conclude the paper with a discussion of the presented results and further research directions."
                },
                {
                    "id": "R159465",
                    "label": "A Socio-Technical Perspective on Urban Analytics: The Case of City-Scale Digital Twins",
                    "doi": "10.1080/10630732.2020.1798177",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "abstract this paper demonstrates that a shift from a purely technical to a more socio-technical perspective has significant implications for the conceptualization, design, and implementation of smart city technologies. such implications are discussed and illustrated through the case of an emerging urban analytics tool, the city-scale digital twin. based on interdisciplinary insights and a participatory knowledge co-production and tool co-development process, including both researchers and prospective users, we conclude that in order to move beyond a mere \u201chype technology,\u201d city-scale digital twins must reflect the specifics of the urban and socio-political context."
                },
                {
                    "id": "R159473",
                    "label": "Participatory Sensing and Digital Twin City: Updating Virtual City Models for Enhanced Risk-Informed Decision-Making",
                    "doi": "10.1061/(asce)me.1943-5479.0000748",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "abstractthe benefits of a digital twin city have been assessed based on real-time data collected from preinstalled internet of things (iot) sensors (e.g.,\\xa0traffic, energy use, air pollution, water ..."
                },
                {
                    "id": "R159481",
                    "label": "The Digital Twin of the City of Zurich for Urban Planning",
                    "doi": "10.1007/s41064-020-00092-2",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "abstract population growth will confront the city of zurich with a variety of challenges in the coming years, as the increase in the number of inhabitants and jobs will lead to densification and competing land uses. the tasks for the city administration have become more complex, whereas tools and methods are often based on traditional, static approaches while involving a limited number of citizens and stakeholders in relevant decisions. the digital transformation of more and more\\xa0pieces of the planning and decision-making process will make both increasingly more illustrative, easier to understand and more comprehensible. an important data basis for these processes is the digital twin of the city of zurich. 3d spatial data and their models transform themes of the city, such as buildings, bridges, vegetation, etc., to the digital world, are being updated when required, and create advantages in digital space. these benefits need to be highlighted and published. an important step in public awareness is the release of 3d spatial data under open government data. this allows the development of applications, the promotion of understanding, and the simplification of the creation of different collaborative platforms. by\\xa0visualization and analysis of digital prototypes and the demonstration of interactions with the built environment, scenarios can be digitally developed and discussed in decision-making bodies. questions about the urban climate can be simulated with the help of the digital twin and results can be linked to the existing 3d spatial data. thus, the 3d spatial data set, the models and their descriptions through\\xa0metadata become the reference and must be updated according to the requirements. depending on requirements and questions, further 3d spatial data must be added. the description of the 3d spatial data and their models or the lifecycle management of the digital twin must be carried out with great care. only in this way, decision processes can be supported in a comprehensible way."
                },
                {
                    "id": "R160211",
                    "label": "Architecting Smart City Digital Twins: Combined Semantic Model and Machine Learning Approach",
                    "doi": "10.1061/(asce)me.1943-5479.0000774",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "abstractthis work was motivated by the premise that next-generation smart city systems will be enabled by widespread adoption of sensing and communication technologies deeply embedded within the ph..."
                },
                {
                    "id": "R160263",
                    "label": "Evaluation of Urban-Scale Building Energy-Use Models and Tools\u2014Application for the City of Fribourg, Switzerland",
                    "doi": "10.3390/su13041595",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "building energy-use models and tools can simulate and represent the distribution of energy consumption of buildings located in an urban area. the aim of these models is to simulate the energy performance of buildings at multiple temporal and spatial scales, taking into account both the building shape and the surrounding urban context. this paper investigates existing models by simulating the hourly space heating consumption of residential buildings in an urban environment. existing bottom-up urban-energy models were applied to the city of fribourg in order to evaluate the accuracy and flexibility of energy simulations. two common energy-use models\u2014a machine learning model and a gis-based engineering model\u2014were compared and evaluated against anonymized monitoring data. the study shows that the simulations were quite precise with an annual mean absolute percentage error of 12.8 and 19.3% for the machine learning and the gis-based engineering model, respectively, on residential buildings built in different periods of construction. moreover, a sensitivity analysis using the morris method was carried out on the gis-based engineering model in order to assess the impact of input variables on space heating consumption and to identify possible optimization opportunities of the existing model."
                },
                {
                    "id": "R160274",
                    "label": "Smart City Digital Twin\u2013Enabled Energy Management: Toward Real-Time Urban Building Energy Benchmarking",
                    "doi": "10.1061/(asce)me.1943-5479.0000741",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "abstractto meet energy-reduction goals, cities are challenged with assessing building energy performance and prioritizing efficiency upgrades across existing buildings. although current top-down bu..."
                },
                {
                    "id": "R160291",
                    "label": "A Smart Campus\u2019 Digital Twin for Sustainable Comfort Monitoring",
                    "doi": "10.3390/su12219196",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "interdisciplinary cross-cultural and cross-organizational research offers great opportunities for innovative breakthroughs in the field of smart cities, yet it also presents organizational and knowledge development hurdles. smart cities must be large towns able to sustain the needs of their citizens while promoting environmental sustainability. smart cities foment the widespread use of novel information and communication technologies (icts); however, experimenting with these technologies in such a large geographical area is unfeasible. consequently, smart campuses (scs), which are universities where technological devices and applications create new experiences or services and facilitate operational efficiency, allow experimentation on a smaller scale, the concept of scs as a testbed for a smart city is gaining momentum in the research community. nevertheless, while universities acknowledge the academic role of a smart and sustainable approach to higher education, campus life and other student activities remain a mystery, which have never been universally solved. this paper proposes a sc concept to investigate the integration of building information modeling tools with internet of things- (iot)-based wireless sensor networks in the fields of environmental monitoring and emotion detection to provide insights into the level of comfort. additionally, it explores the ability of universities to contribute to local sustainability projects by sharing knowledge and experience across a multi-disciplinary team. preliminary results highlight the significance of monitoring workspaces because productivity has been proven to be directly influenced by environment parameters. the comfort-monitoring infrastructure could also be reused to monitor physical parameters from educational premises to increase energy efficiency."
                },
                {
                    "id": "R160298",
                    "label": "A Digital Twin of Bridges for Structural Health Monitoring",
                    "doi": "10.12783/shm2019/32287",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "\"\u00a9 international workshop on structural health monitoring. all rights reserved. bridges are critical infrastructure systems connecting different regions and providing widespread social and economic benefits. it is therefore essential that they are designed, constructed and maintained properly to adapt to changing conditions of use and climate-driven events. with the rapid development in capability of collecting bridge monitoring data, a data challenge emerges due to insufficient capability in managing, processing and interpreting large monitoring datasets to extract useful information which is of practical value to the industry. one emerging area of research which focuses on addressing this challenge is the creation of 'digital twins' for bridges. a digital twin serves as a virtual representation of the physical infrastructure (i.e. the physical twin), which can be updated in near real time as new data is collected, provide feedback into the physical twin and perform 'what-if scenarios for assessing asset risks and predicting asset performance. this paper presents and broadly discusses two years of exploratory study towards creating a digital twin of bridges for structural health monitoring purposes. in particular, it has involved an interdisciplinary collaboration between civil engineers at the cambridge centre for smart infrastructure and construction (csic) and statisticians at the alan turing institute (ati), using two monitored railway bridges in staffordshire, uk as a case study. four areas of research were investigated: (i) real-time data management using bim, (ii) physics-based approaches, (iii) data-driven approaches, and (iv) data-centric engineering approaches (i.e. synthesis of physics-based and data-driven approaches). a framework for creating a digital twin of bridges, particularly for structural health monitoring purposes, is proposed and briefly discussed.\""
                },
                {
                    "id": "R160301",
                    "label": "Digital Twin Aided Vulnerability Assessment and Risk-Based Maintenance Planning of Bridge Infrastructures Exposed to Extreme Conditions",
                    "doi": "10.3390/su13042051",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "over the past centuries, millions of bridge infrastructures have been constructed globally. many of those bridges are ageing and exhibit significant potential risks. frequent risk-based inspection and maintenance management of highway bridges is particularly essential for public safety. at present, most bridges rely on manual inspection methods for management. the efficiency is extremely low, causing the risk of bridge deterioration and defects to increase day by day, reducing the load-bearing capacity of bridges, and restricting the normal and safe use of them. at present, the applications of digital twins in the construction industry have gained significant momentum and the industry has gradually entered the information age. in order to obtain and share relevant information, engineers and decision makers have adopted digital twins over the entire life cycle of a project, but their applications are still limited to data sharing and visualization. this study has further demonstrated the unprecedented applications of digital twins to sustainability and vulnerability assessments, which can enable the next generation risk-based inspection and maintenance framework. this study adopts the data obtained from a constructor of zhongcheng village bridge in zhejiang province, china as a case study. the applications of digital twins to bridge model establishment, information collection and sharing, data processing, inspection and maintenance planning have been highlighted. then, the integration of \u201cdigital twins (or building information modelling, bim) + bridge risk inspection model\u201d has been established, which will become a more effective information platform for all stakeholders to mitigate risks and uncertainties of exposure to extreme weather conditions over the entire life cycle."
                },
                {
                    "id": "R160306",
                    "label": "Mobile Mapping, Machine Learning and Digital Twin for Road Infrastructure Monitoring and Maintenance: Case Study of Mohammed VI Bridge in Morocco",
                    "doi": "10.1109/morgeo49228.2020.9121882",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "the concepts of digital twin has been recently introduced, it refers to functional connections between a complex physical system and its high-fidelity digital replica. digital twin process workflow is proposed in case of mohammed vi bridge modeling in morocco. the current maintenance of a road infrastructure is based on a manual inspection and a system based on traditional tools. aging infrastructures require a new approach to maintenance in terms of inspection, bridge maintenance system, simulation and systematic evaluation. this system now exists and is called the digital twin. digital twin can be thought of as a virtual prototype in service that changes dynamically in near real time as its physical twin changes. an urban infrastructure digital twin is a virtual instance of his physical twin that is continuously updated with multisource, multisensor and multitemporal data that can be used for monitoring, simulating and forecasting any potential problem that may appear in the structure and proposing planning for repair and maintenance of health status throughout the life cycle of this infrastructure. this work presents a general vision and a justification for integrating dt technology with geospatial data. the paper examines the benefits of integrating 3d gis data acquired by automated mobile mapping (mms) workflows for modeling the reality of a major bridge infrastructure in morocco. this allowed to study the future performance of this bridge structure on virtual twin structures under different environmental conditions. cloud point data are acquired by a mobile mapping system on mohammed vi bridge and converted in bim model by a scan to bim process and is integrated in a gis and bim virtual environment and shows the efficiency of volumetric auscultation in terms of surface flatness and distortion inspection. this project provides a new bridge maintenance system using the concept of a digital twin. this digital model is a platform that allows to collect, organize and share the maintenance history of this important road infrastructure in morocco."
                },
                {
                    "id": "R160311",
                    "label": "Digital Twin and CyberGIS for Improving Connectivity and Measuring the Impact of Infrastructure Construction Planning in Smart Cities",
                    "doi": "10.3390/ijgi9040240",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "smart technologies are advancing, and smart cities can be made smarter by increasing the connectivity and interactions of humans, the environment, and smart devices. this paper discusses selective technologies that can potentially contribute to developing an intelligent environment and smarter cities. while the connectivity and efficiency of smart cities is important, the analysis of the impact of construction development and large projects in the city is crucial to decision and policy makers, before the project is approved. this raises the question of assessing the impact of a new infrastructure project on the community prior to its commencement\u2014what type of technologies can potentially be used for creating a virtual representation of the city? how can a smart city be improved by utilizing these technologies? there are a wide range of technologies and applications available but understanding their function, interoperability, and compatibility with the community requires more discussion around system designs and architecture. these questions can be the basis of developing an agenda for further investigations. in particular, the need for advanced tools such as mobile scanners, geospatial artificial intelligence, unmanned aerial vehicles, geospatial augmented reality apps, light detection, and ranging in smart cities is discussed. in line with smart city technology development, this special issue includes eight accepted articles covering trending topics, which are briefly reviewed."
                },
                {
                    "id": "R160319",
                    "label": "The Circular Economy: A New Development Strategy in China",
                    "doi": "10.1162/108819806775545321",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "activities over the past several years, however, clearly show that ce is emerging as an economic strategy rather than a purely environmental strategy. the major objective of the government is to promote the sustainable development of economy and society, while it also helps to achieve sustainable environmental protection. powers, increasing the wealth of the population and providing employment and business opportunities. the rapid economic growth, however, has engendered serious natural resource depletion and environmental pollution, and the continuing increase of population has exacerbated this situation greatly. recent research has pointed out that growth of the gross domestic product (gdp) in china has significantly reduced the opportunities of future generations to enjoy natural and environmental resources.1 the central government promised in 2002 to build a prosperous society in a comprehensive way by 2020. by then, gdp per capita is anticipated to reach u.s. $3,000 and the total gdp to quadruple. obviously, it is unrealistic for china to expect to realize this ambitious objective in terms of natural resource use if it continues its current development pathway, with population increasing to 1.45 billion in 2020 (qu 2004), low productivity, and the absence of eco-efficiency."
                },
                {
                    "id": "R160323",
                    "label": "Circular cities",
                    "doi": "10.1177/0042098018806133",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "a circular approach to the way in which we manage the resources consumed and produced in cities \u2013 materials, energy, water and land \u2013 will significantly reduce the consumption of finite resources globally. it will also help to address urban problems including resource security, waste disposal, greenhouse gas emissions, pollution, heating, drought and flooding. taking a circular approach can also tackle many other socio-economic problems afflicting cities, for example, providing access to affordable accommodation, expanding and diversifying the economic base, building more engaged and collaborative communities in cities. thus it has great potential to improve our urban living environments. to date, the industrial ecologists and economists have tended to dominate the circularity debate, focusing on closed-loop industrial systems and circular economy (circular businesses and systems of provision). in this paper i investigate why the current state-of-the-art conceptualisation for circular economy (resolve) is inadequate when applied to a city. through this critique and a broader review of the literature i identify the principles and components which are lacking from the circular economy (ce) conceptualisation when applied to a city. i then use this to develop my own definition and conceptualisation of a circular approach to urban resource management."
                },
                {
                    "id": "R160331",
                    "label": "An Architecture for Blockchain over Edge-enabled IoT for Smart Circular Cities",
                    "doi": "10.1109/dcoss.2019.00092",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "\"circular economy is a novel economic model, where every 'asset' is not wasted but reused and upscaled. the internet of things-iot paradigm can underpin the transition to a circular economy by enabling fine-grained and continuous asset tracking. however, there are issues related to security and privacy of iot devices that generate and handle sensitive and personal data. the use of blockchain technology provides an answer to this issue, however, its application raises issues related to the highly-constrained nature of these networks. in this paper, edge computing is presented as a solution to this issue, providing a way in which blockchain and edge computing can be used together to address the constrained nature of iot. furthermore, we present the challenges that this combination poses and the opportunities that it brings. we propose an architecture that decreases the iot devices requirements for memory capacity and increases the overall performance. we also discuss the architecture design and the challenges that it has, comparing it to the traditional blockchain architecture as well as an edge computing architecture for mobile blockchain. the paper closes with a discussion and future extensions of our work are presented, as well.\""
                },
                {
                    "id": "R160337",
                    "label": "Digital Twin in Circular Economy: Remanufacturing in Construction",
                    "doi": "10.1088/1755-1315/588/3/032014",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "abstract \\n global warming attracts increasing public attention. however, in the past few decades, the contribution of construction to greenhouse gas emissions is around 40% of total emissions. the promotion of construction waste remanufacturing faces challenges. the application of digital twins in the remanufacturing of construction waste contributes to the tracking, recycling and management of construction waste. this article reviews the current research on construction waste remanufacturing and the application of digital twin in construction and remanufacturing, aiming at finding the current challenge of construction waste remanufacturing and the opportunity of digital twin to solve it. then, the digital twin platform concept for construction waste remanufacturing is provided as a solution for the current challenges. theoretically, this paper points out the shortcomings of the current research in construction waste remanufacturing based on literature review. meanwhile, this article proposes the application of digital twin in construction waste remanufacturing, which expands the research scope of circular economy in construction. in fact, this research has driven the digital twin application in more industries. besides, this research proposes a concept of potential solutions for the current challenges of construction waste in circular economy."
                },
                {
                    "id": "R160340",
                    "label": "Integrating Virtual Reality and Digital Twin in Circular Economy Practices: A Laboratory Application Case",
                    "doi": "10.3390/su12062286",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "the increasing awareness of customers toward climate change effects, the high demand instability affecting several industrial sectors, and the fast automation and digitalization of production systems are forcing companies to re-think their business strategies and models in view of both the circular economy (ce) and industry 4.0 (i4.0) paradigms. some studies have already assessed the relations between ce and i4.0, their benefits, and barriers. however, a practical demonstration of their potential impact in real contexts is still lacking. the aim of this paper is to present a laboratory application case showing how i4.0-based technologies can support ce practices by virtually testing a waste from electrical and electronic equipment (weee) disassembly plant configuration through a set of dedicated simulation tools. our results highlight that service-oriented, event-driven processing and information models can support the integration of smart and digital solutions in current ce practices at the factory level."
                },
                {
                    "id": "R160349",
                    "label": "Leveraging Digital Twin for Sustainability Assessment of an Educational Building",
                    "doi": "10.3390/su13020480",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "the eu green deal, beginning in 2019, promoted a roadmap for operating the transition to a sustainable eu economy by turning climate issues and environmental challenges into opportunities in all policy areas and making the transition fair and inclusive for all. focusing on the built environment, the voluntary adoption of rating systems for sustainability assessment is growing, with an increasing market value, and is perceived as a social responsibility both by public administration and by private companies. this paper proposes a framework for shifting from a static sustainability assessment to a digital twin (dt)-based and internet of things (iot)-enabled dynamic approach. this new approach allows for a real-time evaluation and control of a wide range of sustainability criteria with a user-centered point of view. a pilot building, namely, the elux lab cognitive building in the university of brescia, was used to test the framework with some sample applications. the educational building accommodates the daily activities of the engineering students by constantly interacting with the sensorized asset monitoring indoor comfort and air quality conditions as well as the energy behavior of the building in order to optimize the trade-off with renewable energy production. the framework is the cornerstone of a methodology exploiting the digital twin approach to support the decision processes related to sustainability through the whole building\u2019s life cycle."
                },
                {
                    "id": "R160363",
                    "label": "Beyond the State of the Art of Electric Vehicles: A Fact-Based Paper of the Current and Prospective Electric Vehicle Technologies",
                    "doi": "10.3390/wevj12010020",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "today, there are many recent developments that focus on improving the electric vehicles and their components, particularly regarding advances in batteries, energy management systems, autonomous features and charging infrastructure. this plays an important role in developing next electric vehicle generations, and encourages more efficient and sustainable eco-system. this paper not only provides insights in the latest knowledge and developments of electric vehicles (evs), but also the new promising and novel ev technologies based on scientific facts and figures\u2014which could be from a technological point of view feasible by 2030. in this paper, potential design and modelling tools, such as digital twin with connected internet-of-things (iot), are addressed. furthermore, the potential technological challenges and research gaps in all ev aspects from hard-core battery material sciences, power electronics and powertrain engineering up to environmental assessments and market considerations are addressed. the paper is based on the knowledge of the 140+ fte counting multidisciplinary research centre mobi-vub, that has a 40-year track record in the field of electric vehicles and e-mobility."
                },
                {
                    "id": "R160374",
                    "label": "A Digital Twin Paradigm: Vehicle-to-Cloud Based Advanced Driver Assistance Systems",
                    "doi": "10.1109/vtc2020-spring48590.2020.9128938",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "digital twin, an emerging representation of cyberphysical systems, has attracted increasing attentions very recently. it opens the way to real-time monitoring and synchronization of real-world activities with the virtual counterparts. in this study, we develop a digital twin paradigm using an advanced driver assistance system (adas) for connected vehicles. by leveraging vehicle-to-cloud (v2c) communication, on-board devices can upload the data to the server through cellular network. the server creates a virtual world based on the received data, processes them with the proposed models, and sends them back to the connected vehicles. drivers can benefit from this v2c based adas, even if all computations are conducted on the cloud. the cooperative ramp merging case study is conducted, and the field implementation results show the proposed digital twin framework can benefit the transportation systems regarding mobility and environmental sustainability with acceptable communication delays and packet losses."
                },
                {
                    "id": "R160377",
                    "label": "Time series behavior modeling with digital twin for Internet of Vehicles",
                    "doi": "10.1186/s13638-019-1589-8",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "abstract electric vehicle (ev) is considered eco-friendly with low carbon emission and maintenance costs. given the current battery and charging technology, driving experience of evs relies heavily on the availability and reachability of ev charging infrastructure. as the number of charging piles increases, carefully designed arrangement of resources and efficient utilization of the infrastructure is essential to the future development of ev industry. the mobility and distribution of evs determine the charging demand and the load of power distribution grid. then, dynamic traffic pattern of numerous interconnected evs poses great impact on charging plans and charging infrastructure. in this paper, we introduce the digital twin of a real-world ev by modeling the mobility based on a time series behaviors of evs to evaluate the charging algorithm and pile arrangement policy. the introduced digital twin ev is a virtually simulated equivalence with same traffic behaviors and charging activities as the ev in real world. the behavior and route choice of evs is dynamically simulated base on the time-varying driving operations, travel intent, and charging plan in a simulated large-scale charging scenario composed of concurrently moving evs and correspondingly equipped charging piles. different ev navigation algorithms and charging algorithms of internet of vehicle can be exactly evaluated in the dynamic simulation of the digital twins of the moving evs and charging infrastructure. then we analyze the collected data such as energy consumption, charging capacity, charging frequency, and waiting time in queue on both the ev side and the charging pile side to evaluate the charging efficiency. the simulation is used to study the relations between the scheduled charging operation of evs and the deployment of piles. the proposed model helps evaluate and validate the design of the charging recommendation and the deployment plan regarding to the arrangement and distribution of charging piles."
                },
                {
                    "id": "R160381",
                    "label": "Roads Infrastructure Digital Twin: A Step Toward Smarter Cities Realization",
                    "doi": "10.1109/mnet.011.2000398",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "digital twin is a new concept that consists of creating an up-to-date virtual asset in cyberspace which mimics the original physical asset in most of its aspects, ultimately to monitor, analyze, test, and optimize the physical asset. in this article, we investigate and discuss the use of the digital twin concept of the roads as a step toward realizing the dream of smart cities. to this end, we propose the deployment of a digital twin box to the roads that is composed of a 360\u00b0 camera and a set of iot devices connected to a single onboard computer. the digital twin box creates a digital twin of the physical road asset by constantly sending real-time data to the edge/cloud, including the 360\u00b0 live stream, gps location, and measurements of the temperature and humidity. this data will be used for realtime monitoring and other purposes by displaying the live stream via head-mounted devices or using a 360\u00b0 web-based player. additionally, we perform an object detection process to extract all possible objects from the captured stream. for some specific objects (person and vehicle), an identification module and a tracking module are employed to identify the corresponding objects and keep track of all video frames where these objects appeared. the outcome of the latter step would be of utmost importance to many other services and domains such as national security. to show the viability of the proposed solution, we have implemented and conducted real-world experiments where we focus more on the detection and recognition processes. the achieved results show the effectiveness of the proposed solution in creating a digital twin of the roads, a step forward to enable self-driving vehicles as a crucial component of smart mobility, using the digital twin box."
                },
                {
                    "id": "R160384",
                    "label": "A Digital Twin-based Privacy Enhancement Mechanism for the Automotive Industry",
                    "doi": "10.1109/is.2018.8710526",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "this paper discusses a digital twin demonstrator for privacy enhancement in the automotive industry. here, the digital twin demonstrator is presented as a method for the design and implementation of privacy enhancement mechanisms, and is used to detect privacy concerns and minimize breaches and associated risks to which smart car drivers can be exposed through connected infotainment applications and services. the digital twin-based privacy enhancement demonstrator is designed to simulate variety of conditions that can occur in the smart car ecosystem. we firstly identify the core stakeholders (actors) in the smart car ecosystem, their roles and exposure to privacy vulnerabilities and associated risks. secondly, we identify assets that consume and generate sensitive privacy data in smart cars, their functionalities, and relevant privacy concerns and risks. thirdly, we design an infrastructure for collecting (i) real-time sensor data from smart cars and their assets, and (ii) environmental data, road and traffic data, generated through operational driving lifecycle. in order to ensure compliance of the collected data with privacy policies and regulations, e.g. with gdpr requirements for enforcement of the data subject\u2019s rights, we design methods for the digital twin-based privacy enhancement demonstrator that are based on behavioural analytics informed by gdpr. we also perform data anonymization to minimize privacy risks and enable actions such as sending an automatic informed consent to the stakeholders."
                },
                {
                    "id": "R160390",
                    "label": "Collaborative city digital twin for the COVID-19 pandemic: A federated learning solution",
                    "doi": "10.26599/tst.2021.9010026",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "\"in this work, we propose a collaborative city digital twin based on fl, a novel paradigm that allowing multiple city dt to share the local strategy and status in a timely manner. in particular, an fl central server manages the local updates of multiple collaborators (city dt), provides a global model which is trained in multiple iterations at different city dt systems, until the model gains the correlations between various response plan and infection trend. that means, a collaborative city dt paradigm based on fl techniques can obtain knowledge and patterns from multiple dts, and eventually establish a `global view' for city crisis management. meanwhile, it also helps to improve each city digital twin selves by consolidating other dt's respective data without violating privacy rules. to validate the proposed solution, we take covid-19 pandemic as a case study. the experimental results on the real dataset with various response plan validate our proposed solution and demonstrate the superior performance.\""
                },
                {
                    "id": "R160395",
                    "label": "Building and exploiting a Digital Twin for the management of drinking water distribution networks",
                    "doi": "10.1080/1573062x.2020.1771382",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "abstract digital twins (dts) are starting to be exploited to improve the management of water distribution systems (wdss) and, in the future, they will be crucial for decision making. in this paper, the authors propose several requirements that a dt of a water distribution system should accomplish. developing a dt is a challenge, and a continuous process of adjustments and learning is required. due to the advantages of having a dt of the wds always available, during the last years a strategy to build and maintain a dt of the water distribution network of valencia (spain) and its metropolitan area (1.6 million inhabitants) was developed. this is one of the first dts built of a water utility, being currently in operation. the great benefits of their use in the daily operation of the system ensure that they will begin to be usual in the most advanced smart cities."
                },
                {
                    "id": "R160402",
                    "label": "BIM and IoT: A Synopsis from GIS Perspective",
                    "doi": "10.5194/isprsarchives-xl-2-w4-33-2015",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "abstract. internet-of-things (iot) focuses on enabling communication between all devices, things that are existent in real life or that are virtual. building information models (bims) and building information modelling is a hype that has been the buzzword of the construction industry for last 15 years. bims emerged as a result of a push by the software companies, to tackle the problems of inefficient information exchange between different software and to enable true interoperability. in bim approach most up-to-date an accurate models of a building are stored in shared central databases during the design and the construction of a project and at post-construction stages. gis based city monitoring / city management applications require the fusion of information acquired from multiple resources, bims, city models and sensors. this paper focuses on providing a method for facilitating the gis based fusion of information residing in digital building \u201cmodels\u201d and information acquired from the city objects i.e. \u201cthings\u201d. once this information fusion is accomplished, many fields ranging from emergency response, urban surveillance, urban monitoring to smart buildings will have potential benefits.\\n"
                },
                {
                    "id": "R160408",
                    "label": "Using Smart City Technology to Make Healthcare Smarter",
                    "doi": "10.1109/jproc.2017.2787688",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "smart cities use information and communication technologies (icts) to scale services include utilities and transportation to a growing population. in this paper, we discuss how smart city icts can also improve healthcare effectiveness and lower healthcare cost for smart city residents. we survey current literature and introduce original research to offer an overview of how smart city infrastructure supports strategic healthcare using both mobile and ambient sensors combined with machine learning. finally, we consider challenges that will be faced as healthcare providers make use of these opportunities."
                },
                {
                    "id": "R160415",
                    "label": "Digital Twins: From Personalised Medicine to Precision Public Health",
                    "doi": "10.3390/jpm11080745",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "a digital twin is a virtual model of a physical entity, with dynamic, bi-directional links between the physical entity and its corresponding twin in the digital domain. digital twins are increasingly used today in different industry sectors. applied to medicine and public health, digital twin technology can drive a much-needed radical transformation of traditional electronic health/medical records (focusing on individuals) and their aggregates (covering populations) to make them ready for a new era of precision (and accuracy) medicine and public health. digital twins enable learning and discovering new knowledge, new hypothesis generation and testing, and in silico experiments and comparisons. they are poised to play a key role in formulating highly personalised treatments and interventions in the future. this paper provides an overview of the technology\u2019s history and main concepts. a number of application examples of digital twins for personalised medicine, public health, and smart healthy cities are presented, followed by a brief discussion of the key technical and other challenges involved in such applications, including ethical issues that arise when digital twins are applied to model humans."
                },
                {
                    "id": "R160422",
                    "label": "A Novel Cloud-Based Framework for the Elderly Healthcare Services Using Digital Twin",
                    "doi": "10.1109/access.2019.2909828",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "with the development of technologies, such as big data, cloud computing, and the internet of things (iot), digital twin is being applied in industry as a precision simulation technology from concept to practice. further, simulation plays a very important role in the healthcare field, especially in research on medical pathway planning, medical resource allocation, medical activity prediction, etc. by combining digital twin and healthcare, there will be a new and efficient way to provide more accurate and fast services for elderly healthcare. however, how to achieve personal health management throughout the entire lifecycle of elderly patients, and how to converge the medical physical world and the virtual world to realize real smart healthcare, are still two key challenges in the era of precision medicine. in this paper, a framework of the cloud healthcare system is proposed based on digital twin healthcare (clouddth). this is a novel, generalized, and extensible framework in the cloud environment for monitoring, diagnosing and predicting aspects of the health of individuals using, for example, wearable medical devices, toward the goal of personal health management, especially for the elderly. clouddth aims to achieve interaction and convergence between medical physical and virtual spaces. accordingly, a novel concept of digital twin healthcare (dth) is proposed and discussed, and a dth model is implemented. next, a reference framework of clouddth based on dth is constructed, and its key enabling technologies are explored. finally, the feasibility of some application scenarios and a case study for real-time supervision are demonstrated."
                },
                {
                    "id": "R160428",
                    "label": "An ISO/IEEE 11073 Standardized Digital Twin Framework for Health and Well-Being in Smart Cities",
                    "doi": "10.1109/access.2020.2999871",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "the use of the digital twin has been quickly adopted in industry in recent years and continues to gain momentum. the recent redefinition of the digital twin from the digital replica of a physical asset to the replica of a living or nonliving entity has increased its potential. the digital twin not only disrupts industrial processes, but also expands the domain of health and well-being towards fostering smart healthcare services in smart cities. in this paper, we propose an iso/ieee 11073 standardized digital twin framework architecture for health and well-being. this framework encompasses the process of data collection from personal health devices, the analysis of this data, and conveying the feedback to the user in a loop cycle. the framework proposes a solution to include not only x73 compliant devices, but also noncompliant health devices, by interfacing them with an x73 wrapper module as we explain in this paper. besides, we propose a configurable x73 mobile application, designed to work with any x73 compliant device. we designed and implemented the proposed framework, and the x73 mobile app, and conducted an experiment as a proof of concept of the digital twin in the domain of health and well-being in smart cities. the experiment shows promising results and the potential of benefiting from the proposed framework, by gaining insights on the health and well-being of individuals, and providing feedback to the individual and caregiver."
                }
            ]
        },
        {
            "id": "R161545",
            "label": "Recycling methods",
            "research_fields": [
                {
                    "id": "R131",
                    "label": "Polymer Chemistry"
                }
            ],
            "properties": [
                "method",
                "Recycling type",
                "has property"
            ],
            "papers": [
                {
                    "id": "R161549",
                    "label": "Mechanical Recycling of Packaging Plastics: A Review",
                    "doi": "10.1002/marc.202000415",
                    "research_field": {
                        "id": "R131",
                        "label": "Polymer Chemistry"
                    },
                    "abstract": "the current global plastics economy is highly linear, with the exceptional performance and low carbon footprint of polymeric materials at odds with dramatic increases in plastic waste. transitioning to a circular economy that retains plastic in its highest value condition is essential to reduce environmental impacts, promoting reduction, reuse, and recycling. mechanical recycling is an essential tool in an environmentally and economically sustainable economy of plastics, but current mechanical recycling processes are limited by cost, degradation of mechanical properties, and inconsistent quality products. this review covers the current methods and challenges for the mechanical recycling of the five main packaging plastics: poly(ethylene terephthalate), polyethylene, polypropylene, polystyrene, and poly(vinyl chloride) through the lens of a circular economy. their reprocessing induced degradation mechanisms are introduced and strategies to improve their recycling are discussed. additionally, this review briefly examines approaches to improve polymer blending in mixed plastic waste streams and applications of lower quality recyclate."
                },
                {
                    "id": "R161568",
                    "label": "Current Technologies in Depolymerization Process and the Road Ahead",
                    "doi": "10.3390/polym13030449",
                    "research_field": {
                        "id": "R131",
                        "label": "Polymer Chemistry"
                    },
                    "abstract": "although plastic is considered an indispensable commodity, plastic pollution is a major concern around the world due to its rapid accumulation rate, complexity, and lack of management. some political policies, such as the chinese import ban on plastic waste, force us to think about a long-term solution to eliminate plastic wastes. converting waste plastics into liquid and gaseous fuels is considered a promising technique to eliminate the harm to the environment and decrease the dependence on fossil fuels, and recycling waste plastic by converting it into monomers is another effective solution to the plastic pollution problem. this paper presents the critical situation of plastic pollution, various methods of plastic depolymerization based on different kinds of polymers defined in the society of the plastics industry (spi) resin identification coding system, and the opportunities and challenges in the future."
                },
                {
                    "id": "R161598",
                    "label": "Hydrolysis and Solvolysis as Benign Routes for the End-of-Life Management of Thermoset Polymer Waste",
                    "doi": "10.1146/annurev-chembioeng-120919-012253",
                    "research_field": {
                        "id": "R131",
                        "label": "Polymer Chemistry"
                    },
                    "abstract": "the production of thermoset polymers is increasing globally owing to their advantageous properties, particularly when applied as composite materials. though these materials are traditionally used in more durable, longer-lasting applications, ultimately, they become waste at the end of their usable lifetimes. current recycling practices are not applicable to traditional thermoset waste, owing to their network structures and lack of processability. recently, researchers have been developing thermoset polymers with the right functionalities to be chemically degraded under relatively benign conditions postuse, providing a route to future management of thermoset waste. this review presents thermosets containing hydrolytically or solvolytically cleavable bonds, such as esters and acetals. hydrolysis and solvolysis mechanisms are discussed, and various factors that influence the degradation rates are examined. degradable thermosets with impressive mechanical, thermal, and adhesion behavior are discussed, illustrating that the design of material end-of-life need not limit material performance."
                }
            ]
        },
        {
            "id": "R161736",
            "label": "Xray laser applications",
            "research_fields": [
                {
                    "id": "R114008",
                    "label": "Applied Physics"
                }
            ],
            "properties": [
                "Paper type",
                "Research objective",
                "has system qualities"
            ],
            "papers": [
                {
                    "id": "R161811",
                    "label": "Beam optics of exploding foil plasma x\u2010ray lasers",
                    "doi": "10.1063/1.866565",
                    "research_field": {
                        "id": "R114008",
                        "label": "Applied Physics"
                    },
                    "abstract": "in soft x\u2010ray lasers, amplification is achieved as the x rays propagate down a long narrow plasma column. refraction, due to electron density gradients, tends to direct the x\u2010rays out of high density regions. this can have the undesirable effect of shortening the distance that the x ray stay within the plasma, thereby limiting the amount of amplification. the exploding foil design lessens refraction, but does not eliminate it. in this paper, a quantitative analysis of propagation and amplification in an exploding foil x\u2010ray laser is presented. the density and gain profiles within the plasma are modeled in an approximate manner, which enables considerable analytic progress. it is found that refraction introduces a loss term to the laser amplification. the beam pattern from a parabolic gain profile laser has a dominant peak on the x\u2010ray laser axis. the pattern from a quartic gain profile having a dip on\u2010axis can produce a profile with off\u2010axis peaks, in better agreement with recent experimental data."
                },
                {
                    "id": "R161856",
                    "label": "Science with Soft X Rays",
                    "doi": "10.1063/1.1349609",
                    "research_field": {
                        "id": "R114008",
                        "label": "Applied Physics"
                    },
                    "abstract": "synchrotron radiation with photon energies at or below 1 kev is giving new insights into such areas as wet cell biology, condensed matter physics and extreme ultraviolet optics technology."
                },
                {
                    "id": "R161886",
                    "label": "Picosecond X-Ray Laser Interferometry of Dense Plasmas",
                    "doi": "10.1103/physrevlett.89.065004",
                    "research_field": {
                        "id": "R114008",
                        "label": "Applied Physics"
                    },
                    "abstract": "we present the first results from picosecond interferometry of dense laser-produced plasmas using a soft x-ray laser. the picosecond duration and short wavelength of the 14.7 nm ni-like pd laser mitigates effects associated with motion blurring and refraction through millimeter-scale plasmas. this enables direct measurement of the electron-density profile to within 10 microm of the target surface. a series of high-quality two-dimensional (2d) density measurements provide unambiguous characterization of the time evolution in a fast-evolving plasma suitable for validation of existing 1d and 2d hydrodynamic codes."
                },
                {
                    "id": "R161940",
                    "label": "Picosecond Snapshot of the Speckles from FerroelectricBaTiO3by Means of X-Ray Lasers",
                    "doi": "10.1103/physrevlett.89.257602",
                    "research_field": {
                        "id": "R114008",
                        "label": "Applied Physics"
                    },
                    "abstract": "a picosecond x-ray laser speckle has been conducted to study the dynamics of a disordered surface domain structure (batio3 with 90 degrees c/a domains) as a function of temperature for the first time. the transient surface structures induced by ferroelectric domains decrease as temperature increases towards the curie temperature t(c) and completely disappear above t(c). the dramatic change of the spatial configuration of the c/a domains was observed to occur from a temperature 2 degrees c below t(c), near which the average correlated domain size at equilibrium decreases as (t(c)-t)(0.37+/-0.02)."
                },
                {
                    "id": "R162021",
                    "label": "Sub-38 nm resolution tabletop microscopy with 13 nm wavelength laser light",
                    "doi": "10.1364/ol.31.001214",
                    "research_field": {
                        "id": "R114008",
                        "label": "Applied Physics"
                    },
                    "abstract": "we have acquired images with a spatial resolution better than 38 nm by using a tabletop microscope that combines 13 nm wavelength light from a high-brightness tabletop laser and fresnel zone plate optics. these results open a gateway to the development of compact and widely available extreme-ultraviolet imaging tools capable of inspecting samples in a variety of environments with a 15-20 nm spatial resolution and a picosecond time resolution."
                },
                {
                    "id": "R162049",
                    "label": "High-Brightness Injection-Seeded Soft-X-Ray-Laser Amplifier Using a Solid Target",
                    "doi": "10.1103/physrevlett.97.123901",
                    "research_field": {
                        "id": "R114008",
                        "label": "Applied Physics"
                    },
                    "abstract": "we demonstrate the generation of an intense soft-x-ray-laser beam by saturated amplification of high harmonic seed pulses in a dense transient collisional soft-x-ray-laser plasma amplifier created by heating a titanium target. amplification in the 32.6 nm line of ne-like ti generates laser pulses of subpicosecond duration that are measured to approach full spatial coherence. the peak spectral brightness is estimated to be approximately 2 x 10(26) photons/(s mm(2) mrad(2) 0.01% bandwidth). the scheme is scalable to produce extremely bright lasers at very short wavelengths with full temporal and spatial coherence."
                },
                {
                    "id": "R162104",
                    "label": "Single-shot soft-x-ray digital holographic microscopy with an adjustable field of view and magnification",
                    "doi": "10.1364/ol.34.000623",
                    "research_field": {
                        "id": "R114008",
                        "label": "Applied Physics"
                    },
                    "abstract": "single-shot digital holographic microscopy with an adjustable field of view and magnification was demonstrated by using a tabletop 32.8 nm soft-x-ray laser. the holographic images were reconstructed with a two-dimensional fast-fourier-transform algorithm, and a new configuration of imaging was developed to overcome the pixel-size limit of the recording device without reducing the effective na. the image of an atomic-force-microscope cantilever was reconstructed with a lateral resolution of 480 nm, and the phase contrast image of a 20 nm carbon mesh foil demonstrated that profiles of sample thickness can be reconstructed with few-nanometers uncertainty. the ultrashort x-ray pulse duration combined with single-shot capability offers great advantage for flash imaging of delicate samples."
                },
                {
                    "id": "R162132",
                    "label": "Coherent imaging of biological samples with femtosecond pulses at the free-electron laser FLASH",
                    "doi": "10.1088/1367-2630/12/3/035003",
                    "research_field": {
                        "id": "R114008",
                        "label": "Applied Physics"
                    },
                    "abstract": "coherent x-ray imaging represents a new window to imaging non-crystalline, biological specimens at unprecedented resolutions. the advent of free-electron lasers (fel) allows extremely high flux densities to be delivered to a specimen resulting in stronger scattered signal from these samples to be measured. in the best case scenario, the diffraction pattern is measured before the sample is destroyed by these intense pulses, as the processes involved in radiation damage may be substantially slower than the pulse duration. in this case, the scattered signal can be interpreted and reconstructed to yield a faithful image of the sample at a resolution beyond the conventional radiation damage limit. we employ coherent x-ray diffraction imaging (cxdi) using the free-electron laser in hamburg (flash) in a non-destructive regime to compare images of a biological sample reconstructed using different, single, femtosecond pulses of fel radiation. furthermore, for the first time, we demonstrate cxdi, in-line holography and fourier transform holography (fth) of the same unicellular marine organism using an fel and present diffraction data collected using the third harmonic of flash, reaching into the water window. we provide quantitative results for the resolution of the cxdi images as a function of pulse intensity, and compare this with the resolutions achieved with in-line holography and fth."
                },
                {
                    "id": "R162160",
                    "label": "Temporal coherence and spectral linewidth of an injection-seeded transient collisional soft x-ray laser",
                    "doi": "10.1364/oe.19.012087",
                    "research_field": {
                        "id": "R114008",
                        "label": "Applied Physics"
                    },
                    "abstract": "the temporal coherence of an injection-seeded transient 18.9 nm molybdenum soft x-ray laser was measured using a wavefront division interferometer and compared to model simulations. the seeded laser is found to have a coherence time similar to that of the unseeded amplifier, ~1 ps, but a significantly larger degree of temporal coherence. the measured coherence time for the unseeded amplifier is only a small fraction of the pulsewidth, while in the case of the seeded laser it approaches full temporal coherence. the measurements confirm that the bandwidth of the solid target amplifiers is significantly wider than that of soft x-ray lasers that use gaseous targets, an advantage for the development of sub-picosecond soft x-ray lasers."
                },
                {
                    "id": "R162188",
                    "label": "Sequential single-shot imaging of nanoscale dynamic interactions with a table-top soft x-ray laser",
                    "doi": "10.1364/ol.37.002994",
                    "research_field": {
                        "id": "R114008",
                        "label": "Applied Physics"
                    },
                    "abstract": "we demonstrate the first real-space recording of nanoscale dynamic interactions using single-shot soft x-ray (sxr) full-field laser microscopy. a sequence of real-space flash images acquired with a table-top sxr laser was used to capture the motion of a rapidly oscillating magnetic nanoprobe. changes of 30 nm in the oscillation amplitude were detected when the nanoprobe was made to interact with stray fields from a magnetic sample. the table-top visualization of nanoscale dynamics in real space can significantly contribute to the understanding of nanoscale processes and can accelerate the development of new nanodevices."
                },
                {
                    "id": "R162216",
                    "label": "Defect-tolerant extreme ultraviolet nanoscale printing",
                    "doi": "10.1364/ol.37.003633",
                    "research_field": {
                        "id": "R114008",
                        "label": "Applied Physics"
                    },
                    "abstract": "we present a defect-free lithography method for printing periodic features with nanoscale resolution using coherent extreme ultraviolet light. this technique is based on the self-imaging effect known as the talbot effect, which is produced when coherent light is diffracted by a periodic mask. we present a numerical simulation and an experimental verification of the method with a compact extreme ultraviolet laser. furthermore, we explore the extent of defect tolerance by testing masks with different defect layouts. the experimental results are in good agreement with theoretical calculations."
                },
                {
                    "id": "R162244",
                    "label": "Single-shot soft x-ray laser linewidth measurement using a grating interferometer",
                    "doi": "10.1364/ol.38.005004",
                    "research_field": {
                        "id": "R114008",
                        "label": "Applied Physics"
                    },
                    "abstract": "the linewidth of a 14.7 nm wavelength ni-like pd soft x-ray laser was measured in a single shot using a soft x-ray diffraction grating interferometer. the instrument uses the time delay introduced by the gratings across the beam to measure the temporal coherence. the spectral linewidth of the 4d1s0-4p1p1 ni-like pd lasing line was measured to be \u03b4\u03bb/\u03bb=3\u00d710(-5) from the fourier transform of the fringe visibility. this single shot linewidth measurement technique provides a rapid and accurate way to determine the temporal coherence of soft x-ray lasers that can contribute to the development of femtosecond plasma-based soft x-ray lasers."
                },
                {
                    "id": "R162271",
                    "label": "Tabletop single-shot extreme ultraviolet Fourier transform holography of an extended object",
                    "doi": "10.1364/oe.21.009959",
                    "research_field": {
                        "id": "R114008",
                        "label": "Applied Physics"
                    },
                    "abstract": "we demonstrate single and multi-shot fourier transform holography with the use of a tabletop extreme ultraviolet laser. the reference wave was produced by a fresnel zone plate with a central opening that allowed the incident beam to illuminate the sample directly. the high reference wave intensity allows for larger objects to be imaged compared to mask-based lensless fourier transform holography techniques. we obtain a spatial resolution of 169 nm from a single laser pulse and a resolution of 128 nm from an accumulation of 20 laser pulses for an object ~11x11\u03bcm(2) in size. this experiment utilized a tabletop extreme ultraviolet laser that produces a highly coherent ~1.2 ns laser pulse at 46.9 nm wavelength."
                }
            ]
        },
        {
            "id": "R172526",
            "label": "Video process",
            "research_fields": [
                {
                    "id": "",
                    "label": ""
                }
            ],
            "properties": [
                "has study",
                "research problem",
                "Application ",
                "production"
            ],
            "papers": [
                {
                    "id": "R158044",
                    "label": "Software Cinema-Video-based Requirements Engineering",
                    "doi": "10.1109/re.2006.59",
                    "research_field": {
                        "id": "R11",
                        "label": "Science"
                    },
                    "abstract": "the dialogue between end-user and developer presents several challenges in requirements development. one issue is the gap between the conceptual models of end-users and formal specification/analysis models of developers. this paper presents a novel technique for the video analysis of scenarios, relating the use of video-based requirements to process models of software development. it uses a knowledge model-an rdf graph-based on a semiotic interpretation of film language, which allows mapping conceptual into formal models. it can be queried with rdql, a query language for rdf. the technique has been implemented with a tool which lets the analyst annotate objects as well as spatial or temporal relationships in the video, to represent the conceptual model. the video can be arranged in a scenario graph effectively representing a multi-path video. it can be viewed in linear time order to facilitate the review of individual scenarios by end-users. each multi-path scene from the conceptual model is mapped to a uml use case in the formal model. a uml sequence diagram can also be generated from the annotations, which shows the direct mapping of film language to uml. this sequence diagram can be edited by the analyst, refining the conceptual model to reflect deeper understanding of the application domain. the use of the software cinema technique is demonstrated with several prototypical applications. one example is a loan application scenario for a financial services consulting firm which acted as an end-user"
                },
                {
                    "id": "R159714",
                    "label": "Towards a Framework for Real Time Requirements Elicitation",
                    "doi": "10.1109/mere.2006.6",
                    "research_field": {
                        "id": "R11",
                        "label": "Science"
                    },
                    "abstract": "eliciting complete and correct requirements is a major challenge in software engineering and incorrect requirements are a constant source of defects. it often happens that requirements are either recorded only partially or not at all. also, commonly, the rationale behind the requirements is not recorded or may be recorded, but is not accessible for the developers who need this information to support the decision making process when requirements change or need clarification. our proposed framework is designed to solve those problems by using video to record the requirements elicitation meetings and automatically extract important stakeholder statements. those statements are made available to the project members as video clips by using an re database to access the statements and/or by the integration with the sysiphus system."
                },
                {
                    "id": "R159585",
                    "label": "On the effect of visual refinement upon user feedback in the context of video prototyping",
                    "doi": "10.1145/2038476.2038497",
                    "research_field": {
                        "id": "R11",
                        "label": "Science"
                    },
                    "abstract": "there has been extensive discussion and research surrounding fidelity or refinement of prototypes in paper and software form, especially focusing on how the nature of prototypes influences the feedback that this prototype can help elicit during user testing. we extend this debate to the domain of video prototypes, where use scenarios are acted out on video. this study examines how the visual refinement (a.k.a. visual fidelity) of design representations presented in such videos impacts user feedback. an experiment was performed where two video prototypes were compared, one where the product is portrayed with high visual refinement and the other looking rough and sketchy. our results could not identify any significant effects upon the number or type of comments returned by users. this finding contrasts widely held contentions relating to fidelity of software and paper prototypes, though it agrees with similar experiments done with non video prototypes. in practice our results support the validity of testing with low fidelity videos and suggest that the choice of visual fidelity in video prototypes should be based on pragmatic project concerns, e.g., whether the video should be used also for communication and the resources that are available for prototyping."
                },
                {
                    "id": "R159590",
                    "label": "Interactive Multimedia Storyboard for Facilitating Stakeholder Interaction: Supporting Continuous Improvement in IT-ecosystems",
                    "doi": "10.1109/quatic.2012.35",
                    "research_field": {
                        "id": "R11",
                        "label": "Science"
                    },
                    "abstract": "in order to stay competitive, elicitation and validation of user requirements are crucial tasks of a software system provider. however, reaching common ground among stakeholders and engineers is still difficult. in the special context of it-ecosystems, continuous improvement efforts call for non-standard requirements engineering methods. we propose a new kind of interaction between engineers and stakeholders that is based on multimedia technologies. our interactive storyboard aims to facilitate stakeholder interaction and enable requirements engineering optimized for fast-paced situations with severe time limitations. requirements and visions of new systems are documented in a multimedia representation. this representation is easy to create and requires no formal preparation for stakeholders to understand. thereby, we aim to improve comprehension of requirements in stakeholder meetings and strengthen stakeholder involvement. the use of video, photo and audio acts as a catalyst for fast-paced stakeholder interaction."
                },
                {
                    "id": "R159596",
                    "label": "At home with agents: exploring attitudes towards future smart energy infrastructures",
                    "doi": "10.1145/2470654.2466152",
                    "research_field": {
                        "id": "R11",
                        "label": "Science"
                    },
                    "abstract": "energy systems researchers are proposing a broad range of future \"smart\" energy infrastructures to promote more efficient management of energy resources. this paper considers how consumers might relate to these future smart grids within the uk. to address this challenge we exploited a combination of demonstration and animated sketches to convey the nature of a future smart energy infrastructure based on software agents. users\\' reactions suggested that although they felt an obligation to engage with energy issues, they were principally disinterested. users showed a considerable lack of trust in energy companies raising a dilemma of design. while users might welcome agents to help in engaging with complex energy infrastructures, they had little faith in those that might provide them. this suggests the need to consider how to design software agents to enhance trust in these socio-economic settings."
                },
                {
                    "id": "R159604",
                    "label": "Speculative Requirements: Design Fiction and RE",
                    "doi": "10.1109/re.2018.00-20",
                    "research_field": {
                        "id": "R11",
                        "label": "Science"
                    },
                    "abstract": "\"many innovative software products are conceived, developed and deployed without any conventional attempt to elicit stakeholder requirements. rather, they are the result of the vision and intuition of a small number of creative individuals, facilitated by the emergence of a new technology. in this paper we consider how the conditions that enable new products' emergence might be better anticipated, making innovations a little less reliant on individual vision and a little more informed by stakeholder need. this is particularly important where a new technology would have the potential for social impact, good or bad. speculative design seeks to explore this landscape. we describe a case study using a variant called design fiction to explore how plausible new technologies might impact on dementia care.\""
                },
                {
                    "id": "R159618",
                    "label": "From pixels to bytes: evolutionary scenario based design with video",
                    "doi": "10.1145/2393596.2393631",
                    "research_field": {
                        "id": "R11",
                        "label": "Science"
                    },
                    "abstract": "change and user involvement are two major challenges in agile software projects. as change and user involvement usually arise spontaneously, reaction to change, validation and communication are thereby expected to happen in a continuous way in the project lifecycle. we propose evolutionary scenario based design, which employs video in fulfilling this goal, and present a new idea that supports video production using secondlife-like virtual world technology."
                },
                {
                    "id": "R159623",
                    "label": "User model and system model: the yin and yang in user-centered software development",
                    "doi": "10.1145/2509578.2514737",
                    "research_field": {
                        "id": "R11",
                        "label": "Science"
                    },
                    "abstract": "software systems can be viewed from both external and internal perspectives. they are called user model and system model respectively in the human-computer interaction community. in this paper, we employ the yin-yang principle as an analytical tool for reviewing the relationship between the user model and the system model. in the traditional system-centered approach, the engineer is more concerned with the system model and does not pay much attention to the user model. however, as the user-centered approach has gained increasing acceptance in a number of projects, we claim that the user model and system model are the yin and yang in user-centered software development and, following the yin-yang principle, call for equal emphasis on both models. particularly, we propose using video-based scenarios as the representation of user models and reveal the benefits of the use of video in software development. as a case study, we describe how we have employed scenario videos in a project course and share best practices that we have identified for the creation of demo scenario videos."
                },
                {
                    "id": "R159637",
                    "label": "Using video to re-present the user",
                    "doi": "10.1145/203356.203368",
                    "research_field": {
                        "id": "R11",
                        "label": "Science"
                    },
                    "abstract": "advocates of user-centered design and participatory design, also referred to as \u201cwork practice practitioners\u201d include computer scientists, systems designers, software engineers, social scientists, industrial and graphic designers, marketing, sales, and service personnel. working singly or in teams, we have been identifying and combining effective techniques and methods of: gathering data, interacting with user participants, representing activities and observations, and integrating findings with the design and construction of new technologies."
                },
                {
                    "id": "R159642",
                    "label": "Requirements elicitation and validation with real world scenes",
                    "doi": "10.1109/32.738338",
                    "research_field": {
                        "id": "R11",
                        "label": "Science"
                    },
                    "abstract": "a requirements specification defines the requirements for the future system at a conceptual level (i.e., class or type level). in contrast, a scenario represents a concrete example of current or future system usage. in early re phases, scenarios are used to support the definition of high level requirements (goals) to be achieved by the new system. in many cases, those goals can to a large degree be elicited by observing, documenting and analyzing scenarios about current system usage. to support the elicitation and validation of the goals achieved by the existing system and to illustrate problems of the old system, we propose to capture current system usage using rich media (e.g., video, speech, pictures, etc.) and to interrelate those observations with the goal definitions. thus, we aim at making the abstraction process which leads to the definition of the conceptual models more transparent and traceable. we relate the parts of the observations which have caused the definition of a goal or against which a goal was validated with the corresponding goal. these interrelations provide the basis for: 1) explaining and illustrating a goal model to, e.g., untrained stakeholders and/or new team members; 2) detecting, analyzing, and resolving a different interpretation of the observations; 3) comparing different observations using computed goal annotations; and 4) refining or detailing a goal model during later process phases. using the prime implementation framework, we have implemented the prime-crews environment, which supports the interrelation of conceptual models and captured system usage observations. we report on our experiences with prime-crews gained in an experimental case study."
                },
                {
                    "id": "R159647",
                    "label": "Video brainstorming and prototyping: techniques for participatory design",
                    "doi": "10.1145/632716.632790",
                    "research_field": {
                        "id": "R11",
                        "label": "Science"
                    },
                    "abstract": "this tutorial is designed for hci designers and researchers interested in learning specific techniques for using video to support a range of participatory design activities. based on a combination of lectures, video demonstrations and hands-on exercises, the tutorial will give participants practical experience using video to observe users in laboratory and field settings, to analyze multimedia data, to explore and capture design ideas (video brainstorming), to simulate interaction techniques with users (wizard-of-oz and video prototyping) and to present video-based design ideas to users and managers. participants will gain experience shooting video and will address practical issues such as maintaining video archives and ethical issues such as obtaining informed consent. although these video techniques are applicable in a variety of design settings, the emphasis here is on participatory design, using video as a tool to help users, researchers and designers gather and communicate design ideas."
                },
                {
                    "id": "R159657",
                    "label": "Supporting requirements with video-based analysis",
                    "doi": "10.1109/ms.2006.84",
                    "research_field": {
                        "id": "R11",
                        "label": "Science"
                    },
                    "abstract": "the dealing-room study is one of many studies that have used video to support requirements elicitation and the general design process. a growing body of experience with video-based ethnographies supports technology development in various domains, including air traffic and other control rooms, healthcare, public settings such as museums, and more experimental technologies, including media spaces and ubiquitous computing"
                },
                {
                    "id": "R159662",
                    "label": "Capturing Multimedia Requirements Descriptions with Mobile RE Tools",
                    "doi": "10.1109/mere.2006.1",
                    "research_field": {
                        "id": "R11",
                        "label": "Science"
                    },
                    "abstract": "as tools for requirements engineering (re) become available on mobile devices using their multimedia capabilities to capture requirements descriptions is an obvious opportunity. this paper reports on two different approaches enabling mobile analysts and endusers to add multimedia descriptions to requirements. based on our mobile tool for scenario-based re we compare a solution based on the cots package microsoft pocket word with a novel plug-in solution providing more flexibility for tool users."
                },
                {
                    "id": "R159671",
                    "label": "MEGORE: Multimedia Enhanced Goal-Oriented Requirement Elicitation Experience in China",
                    "doi": "10.1109/mere.2008.4",
                    "research_field": {
                        "id": "R11",
                        "label": "Science"
                    },
                    "abstract": "based on the survey results of chinese requirements engineering practices, this paper reports a few general patterns of chinese-culture-related ways of thinking and their influence to requirements elicitation activities. a multimedia enhanced goal oriented requirement elicitation method is proposed, in which media measures are used to help capture user requirements and preferences more easily. the method is applied in the design of an electronic marine chart navigation system. some lessons are summarized. we argue that a media-enhanced approach can help improve the efficiency of requirement elicitation process."
                },
                {
                    "id": "R159675",
                    "label": "Applying a Video-based Requirements Engineering Technique to an Airport Scenario",
                    "doi": "10.1109/mere.2008.2",
                    "research_field": {
                        "id": "R11",
                        "label": "Science"
                    },
                    "abstract": "in the development of software-intensive systems, the interaction between customer and supplier is usually text-based. we argue that with agile project management gaining momentum, the inclusion of end-user feedback and a better mutual understanding between customer and supplier on hardware and software design goals becomes increasingly important. we propose the use of video techniques, video-based requirements engineering (vbre), to support the communication between all stakeholders. the key ingredients of vbre are user-centric videos and an exploratory environment for creating multi-path scenarios. in this workshop session, the participants will get hands-on experience with vbre techniques and tools while working on a fictitious airport baggage handling system."
                },
                {
                    "id": "R159681",
                    "label": "An experiment in teaching innovation in software engineering: video presentation",
                    "doi": "10.1145/1449814.1449868",
                    "research_field": {
                        "id": "R11",
                        "label": "Science"
                    },
                    "abstract": "the dolli project was a large-scale educational student project course with a real customer, offered to students in their second year. in the time frame of a single semester a functional system was developed and delivered to the customer. we experimented with a shift from a traditional life-cycle to an agile process during the project, and used video techniques for defining requirements and meeting capture."
                },
                {
                    "id": "R159686",
                    "label": "Contravision: exploring users' reactions to futuristic technology",
                    "doi": "10.1145/1753326.1753350",
                    "research_field": {
                        "id": "R11",
                        "label": "Science"
                    },
                    "abstract": "\"how can we best explore the range of users' reactions when developing future technologies that may be controversial, such as personal healthcare systems? our approach -- contravision -- uses futuristic videos, or other narrative forms, that convey either negative or positive aspects of the proposed technology for the same scenarios. we conducted a user study to investigate what range of responses the different versions elicited. our findings show that the use of two systematically comparable representations of the same technology can elicit a wider spectrum of reactions than a single representation can. we discuss why this is so and the value of obtaining breadth in user feedback for potentially controversial technologies.\""
                },
                {
                    "id": "R159700",
                    "label": "Feed me, feed me: an exemplar for engineering adaptive software",
                    "doi": "10.1145/2897053.2897071",
                    "research_field": {
                        "id": "R11",
                        "label": "Science"
                    },
                    "abstract": "the internet of things (iot) promises to deliver improved quality of life for citizens, through pervasive connectivity and quantified monitoring of devices, people, and their environment. as such, the iot presents a major new opportunity for research in adaptive software engineering. however, there are currently no shared exemplars that can support software engineering researchers to explore and potentially address the challenges of engineering adaptive software for the iot, and to comparatively evaluate proposed solutions. in this paper, we present feed me, feed me, an exemplar that represents an iot-based ecosystem to support food security at different levels of granularity: individuals, families, cities, and nations. we describe this exemplar using animated videos which highlight the requirements that have been informally observed to play a critical role in the success or failure of iot-based software systems. these requirements are: security and privacy, interoperability, adaptation, and personalisation. to elicit a wide spectrum of user reactions, we created these animated videos based on the contravision empirical methodology, which specifically supports the elicitation of end-user requirements for controversial or futuristic technologies. our deployment of contravision presented our pilot study subjects with an equal number of utopian and dystopian scenarios, derived from the food security domain, and described them at the different level of granularity. our synthesis of the preliminary empirical findings suggests a number of key requirements and software engineering research challenges in this area. we offer these to the research community, together with a rich exemplar and associated scenarios available in both their textual form in the paper, and as a series of animated videos (http://sead1.open.ac.uk/fmfm/)"
                },
                {
                    "id": "R159710",
                    "label": "Video Variants for CrowdRE: How to Create Linear Videos, Vision Videos, and Interactive Videos",
                    "doi": "10.1109/rew.2019.00039",
                    "research_field": {
                        "id": "R11",
                        "label": "Science"
                    },
                    "abstract": "in crowdre, heterogenous crowds of stakeholders are involved in requirements elicitation. one major challenge is to inform several people about a complex and sophisticated piece of software so that they can effectively contextualize and contribute their opinions and insights. overly technical or boring textual representations might lead to misunderstandings or even repel some people. videos may be better suited for this purpose. there are several variants of video available: linear videos have been used for tutorials on youtube and similar platforms. interactive media have been proposed for activating commitment and valuable feedback. vision videos were explicitly introduced to solicit feedback about product visions and software requirements. in this paper, we describe essential steps of creating a useful video, making it interactive, and presenting it to stakeholders. we consider four potentially useful types of videos for crowdre and how to produce them. to evaluate feasibility of this approach for creating video variants, all presented steps were performed in a case study."
                },
                {
                    "id": "R159667",
                    "label": "Using Video Clips to Support Requirements Elicitation in Focus Groups - An Experience Report.",
                    "doi": "",
                    "research_field": {
                        "id": "R11",
                        "label": "Science"
                    },
                    "abstract": "abstract this paper reports on a methodological experiment, which was carried out in two large collaborative research projects targeted at innovative products. video material was produced in order to visualize the project vision and solution ideas, and this video material was used in focus group discussions. the paper describes the process, the experiences gained and gives a number of hints which may be helpful for projects planning to use a similar approach."
                },
                {
                    "id": "R159719",
                    "label": "Design of a hyper media tool to support requirements elicitation meetings",
                    "doi": "10.1109/case.1995.465308",
                    "research_field": {
                        "id": "R11",
                        "label": "Science"
                    },
                    "abstract": "\"introduces a hypermedia tool for requirements elicitation meetings. we consider such a meeting to be a consensus-making process among the participants, who have their own roles. participants in the meeting usually repeat the following activities to reach the final specification: (a) preparing the agenda and/or final specification for the next meeting while referring both to their own memory and the secretary's minutes, if accessible; and (b) pursuing the arguments in a meeting while referring either to the agenda for the meeting or to the minutes and their own memory about previous meetings. from observations of several real meetings, most of the final specifications were inconsistent, and unnecessary and redundant communication had occurred in the meetings because a large amount of verbal data in the meetings made each participant's memory and meeting minutes incomplete and ambiguous. from this point of view, our tool gives the participants the following facilities: (1) a plain record of the meetings; (2) a repository for the minutes and agenda extracted from the record; and (3) multi-modal and graphical user interfaces for referring to the repository. such facilities can be available for the participants to develop both suitable minutes and agendas. our tool improves the efficiency of a consensus-making process by suppressing unnecessary and redundant communication. >\""
                },
                {
                    "id": "R159762",
                    "label": "Video artifacts for design: bridging the Gap between abstraction and detail",
                    "doi": "10.1145/347642.347666",
                    "research_field": {
                        "id": "R11",
                        "label": "Science"
                    },
                    "abstract": "video artifacts help bridge the gap between abstraction and detail in the design process. this paper describes how our use and re-use of video artifacts affected the re-design of a graphical editor for building, simulating, and analyzing coloured petri nets. the two primary goals of the project were to create design abstractions that integrate recent advances in graphical interaction techniques and to explicitly support specific patterns of use of petri nets in real-world settings.\\nusing a participatory design process, we organized a series of video-based design activities that helped us manage the tension between finding useful design abstractions and specifying the details of the user interface. video artifacts resulting from one activity became the basis for the next, facilitating communication among members of the multi-disciplinary design team. the video artifacts provided an efficient way of capturing and incorporating subtle aspects of petri nets in use into our design and ensured that the implementation of our design principles was grounded in real-world work practices."
                },
                {
                    "id": "R159798",
                    "label": "Keep Your Stakeholders Engaged: Interactive Vision Videos in Requirements Engineering",
                    "doi": "10.1109/rew53955.2021.00014",
                    "research_field": {
                        "id": "R11",
                        "label": "Science"
                    },
                    "abstract": "one of the most important issues in requirements engineering (re) is the alignment of stakeholders\u2019 mental models. making sure that all stakeholders share the same vision of a changing system is crucial to the success of any project. misaligned mental models of stakeholders can lead to conflicting requirements. a promising approach to this problem is the use of video showing a system vision, so-called vision videos, which help stakeholders to disclose, discuss, and align their mental models of the future system. however, videos have the drawback of allowing viewers to adopt a passive role, as has been shown in research on e-learning. in this role, viewers tend to be inactive, unfocused and bored while watching a video. in this paper, we learn and adopt findings from scientific literature in the field of e-learning on how to mitigate this passive role while watching vision videos in requirements engineering. in this way, we developed concepts that incorporate interactive elements into vision videos to help viewers stay focused. these elements include questions that are asked during the video and ways for viewers to decide what happens next in the video. in a preliminary evaluation with twelve participants, we found statistically significant differences when comparing the interactive vision videos with their traditional form. using an interactive vision videos, viewers are noticeably more engaged and gather more information on the shown system."
                },
                {
                    "id": "R159803",
                    "label": "Using Vision Videos in a Virtual Focus Group: Experiences and Recommendations",
                    "doi": "",
                    "research_field": {
                        "id": "R11",
                        "label": "Science"
                    },
                    "abstract": "facilitated meetings are an established practice for the requirements engineering activities elicitation and validation. focus groups are one well-known technique to implement this practice. several researchers already reported the successful use of vision videos to stimulate active discussions among the participants of on-site focus groups, e.g., for validating scenarios and eliciting feedback. these vision videos show scenarios of a system vision. in this way, the videos serve all parties involved as a visual reference point to actively disclose, discuss, and align their mental models of the future system to achieve shared understanding. in the joint project trusd, we had planned to conduct such an on-site focus group using a vision video to validate a scenario of a future software tool, the so-called privacy dashboard. however, the covid-19 pandemic and its associated measures led to an increase in home and remote working, which also affected us. therefore, we had to replan and conduct the focus group virtually. in this paper, we report about our experiences and recommendations for the use of vision videos in virtual focus groups."
                },
                {
                    "id": "R159783",
                    "label": "Anforderungen kl\u00e4ren mit Videoclips",
                    "doi": "",
                    "research_field": {
                        "id": "R11",
                        "label": "Science"
                    },
                    "abstract": "viele gro\u00dfe softwaresysteme sind heute vernetzt, in ger\u00e4te eingebettet und mit anzeigesystemen verbunden. sie erscheinen den nutzern als komplizierte, computergest\u00fctzte umwelt, deren bestandteile kaum zu unterscheiden sind. die ger\u00e4te und ihre umgebung, die software und die bed\u00fcrfnisse der benutzer entwickeln sich w\u00e4hrend des betriebs st\u00e4ndig weiter. damit ein systemteil n\u00fctzlich und wettbewerbsf\u00e4hig bleibt, braucht man fortw\u00e4hrend r\u00fcckmeldungen und bewertungen. die techniken des klassischen requirements engineering reichen dazu aber nicht aus. in diesem beitrag stellen wir einen ansatz vor, um mit kurzen und einfachen videoclips in dieser situation an feedback heranzukommen. bei deren auswertung werden aktuelle anforderungen identifiziert und gekl\u00e4rt."
                },
                {
                    "id": "R164396",
                    "label": "Engaging older people using participatory design",
                    "doi": "10.1145/2207676.2208570",
                    "research_field": {
                        "id": "R11",
                        "label": "Science"
                    },
                    "abstract": "the use of digital technologies is increasingly proposed in health and social care to address the aging population phenomenon but, in practice, the designers of these technologies are ill equipped to design for older people. we suggest participatory design as an approach to improving the quality of design for older people but, based on previous work and our own experiences, identify four central issues that participatory design approaches need to address. we describe an approach to early engagement in design with older people that address each of these issues and some of our experiences applying the approach in a variety of different design projects. we conclude by discussing some of the issues that have been highlighted when attempting apply this approach in different design contexts and the issues that have been raised when working with partners who are less committed to the idea of engaging with older adults in participatory design."
                },
                {
                    "id": "R164400",
                    "label": "Invisible design: exploring insights and ideas through ambiguous film scenarios",
                    "doi": "10.1145/2317956.2318036",
                    "research_field": {
                        "id": "R11",
                        "label": "Science"
                    },
                    "abstract": "invisible design is a technique for generating insights and ideas with workshop participants in the early stages of concept development. it involves the creation of ambiguous films in which characters discuss a technology that is not directly shown. the technique builds on previous work in hci on scenarios, persona, theatre, film and ambiguity. the invisible design approach is illustrated with three examples from unrelated projects; biometric daemon, panini and smart money. the paper presents a qualitative analysis of data from a series of workshops where these invisible designs were discussed. the analysis outlines responses to the films in terms of; existing problems, concerns with imagined technologies and design speculation. it is argued that invisible design can help to create a space for critical and creative dialogue during participatory concept development."
                },
                {
                    "id": "R159793",
                    "label": "Viewing Vision Videos Online: Opportunities for Distributed Stakeholders",
                    "doi": "10.1109/rew53955.2021.00054",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "creating shared understanding between stakeholders is essential for the success of software projects. conflicting requirements originating from misaligned mental models can hinder the development process. the use of videos to present abstract system visions is one approach to counteract this problem. these videos are usually shown in in-person meetings. however, face-to-face meetings are not suited to every situation and every stakeholder, for example due to scheduling constraints. methods for the use of vision videos in online settings are necessary. furthermore, methods enabling an asynchronous use of vision videos are needed for cases when conjoined meetings are impossible even in an online setting.in this paper, we compare synchronous and asynchronous viewings of vision videos in online settings. the two methods are piloted in a preliminary experiment. the results show a difference in the amount of arguments regarding the presented visions. on average, participants who took part in asynchronous meetings stated more arguments. our results point to multiple advantages and disadvantages as well as use cases for each type. for example, a synchronous meeting could be chosen when all involved stakeholders can attend the appointment to discuss the vision and to quickly resolve ambiguities. an asynchronous meeting could be held if a joint meeting is not feasible due to time constraints. we also discuss how our findings can be applied to the elicitation of requirements from a crowd of stakeholders."
                },
                {
                    "id": "R74547",
                    "label": "Supporting Requirements Elicitation by Tool-Supported Video Analysis",
                    "doi": "10.1109/re.2016.10",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"workshops are an established technique for requirements elicitation. a lot of information is revealed during a workshop, which is generally captured via textual minutes. the scribe suffers from a cognitive overload due to the difficulty of gathering all information, listening and writing at the same time. video recording is used as additional option to capture more information, including non-verbal gestures. since a workshop can take several hours, the recorded video will be long and may be disconnected from the scribe's notes. therefore, the weak and unclear structure of the video complicates the access to the recorded information, for example in subsequent requirements engineering activities. we propose the combination of textual minutes and video with a software tool. our objective is connecting textual notes with the corresponding part of the video. by highlighting relevant sections of a video and attaching notes that summarize those sections, a more useful structure can be achieved. this structure allows an easy and fast access to the relevant information and their corresponding video context. thus, a scribe's overload can be mitigated and further use of a video can be simplified. tool-supported analysis of such an enriched video can facilitate the access to all communicated information of a workshop. this allows an easier elicitation of high-quality requirements. we performed a preliminary evaluation of our approach in an experimental set-up with 12 participants. they were able to elicit higher-quality requirements with our software tool.\""
                },
                {
                    "id": "R74688",
                    "label": "Video as a By-Product of Digital Prototyping: Capturing the Dynamic Aspect of Interaction",
                    "doi": "10.1109/rew.2017.16",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "requirements engineering provides several practices to analyze how a user wants to interact with a future software. mockups, prototypes, and scenarios are suitable to understand usability issues and user requirements early. nevertheless, users are often dissatisfied with the usability of a resulting software. apparently, previously explored information was lost or no longer accessible during the development phase.scenarios are one effective practice to describe behavior. however, they are commonly notated in natural language which is often improper to capture and communicate interaction knowledge comprehensible to developers and users. the dynamic aspect of interaction is lost if only static descriptions are used. digital prototyping enables the creation of interactive prototypes by adding responsive controls to hand-or digitally drawn mockups. we propose to capture the events of these controls to obtain a representation of the interaction. from this data, we generate videos, which demonstrate interaction sequences, as additional support for textual scenarios.variants of scenarios can be created by modifying the captured event sequences and mockups. any change is unproblematic since videos only need to be regenerated. thus, we achieve video as a by-product of digital prototyping. this reduces the effort compared to video recording such as screencasts. a first evaluation showed that such a generated video supports a faster understanding of a textual scenario compared to static mockups."
                },
                {
                    "id": "R159770",
                    "label": "Pimp my Spec: Tuning von Spezifikationen durch Videos",
                    "doi": "",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "motivation die dokumentation geh\u00f6rt zu den wichtigsten aufgaben in projekten \u2013 erfreut sich jedoch leider selten gro\u00dfer beliebtheit. daher haben wir uns von sophist die frage gestellt, welche formen der dokumentation einfache und effektive alternativen zu klassischen vorgehensweisen bieten. das medium video scheint daf\u00fcr ideal, denn die dokumentation kann gleichzeitig zu erund vermittlungszwecken von anforderungen genutzt werden und erspart somit kostbare zeit. zur wirkungsvollen umsetzung gilt es aber einiges zu beachten."
                },
                {
                    "id": "R159779",
                    "label": "Return of the Vision Video: Can Corporate Vision Videos Serve as Setting for Participation?",
                    "doi": "",
                    "research_field": {
                        "id": "R265",
                        "label": "Computer-Aided Engineering and Design"
                    },
                    "abstract": "this paper examines the role of corporate vision videos as a possible setting for participation when exploring the future potentials (and pitfalls) of new technological concepts. we propose that through the recent decade\u2019s rise web 2.0 platforms, and the viral effects of user sharing, the corporate vision video of today might take on a significantly different role than before, and act as a participatory design approach. this address the changing landscaping for participatory and user-involved design processes, in the wake of new digital forms of participation, communication and collaboration, which have radically changed the possible power dynamics of the production life cycle of new product developments. through a case study, we pose the question of whether the online engagements around corporate vision videos can be viewed as a form of participation in a design process, and thus revitalize the relevance of vision videos as a design resource?"
                }
            ]
        },
        {
            "id": "R178304",
            "label": "Dataset",
            "research_fields": [
                {
                    "id": "",
                    "label": ""
                }
            ],
            "properties": [
                "inter-annotator agreement",
                "alternateName",
                "assesses",
                "creator",
                "description",
                "disambiguatingDescription",
                "encoding",
                "exampleOfWork",
                "genre",
                "inLanguage",
                "isBasedOn",
                "license",
                "name",
                "sameAs",
                "size",
                "sourceOrganization",
                "url",
                "version"
            ],
            "papers": [
                {
                    "id": "R182258",
                    "label": "A food image recognition system with Multiple Kernel Learning",
                    "doi": "10.1109/icip.2009.5413400",
                    "research_field": {
                        "id": "R133",
                        "label": "Artificial Intelligence"
                    },
                    "abstract": "\"since health care on foods is drawing people's attention recently, a system that can record everyday meals easily is being awaited. in this paper, we propose an automatic food image recognition system for recording people's eating habits. in the proposed system, we use the multiple kernel learning (mkl) method to integrate several kinds of image features such as color, texture and sift adaptively. mkl enables to estimate optimal weights to combine image features for each category. in addition, we implemented a prototype system to recognize food images taken by cellular-phone cameras. in the experiment, we have achieved the 61.34% classification rate for 50 kinds of foods. to the best of our knowledge, this is the first report of a food image classification system which can be applied for practical use.\""
                },
                {
                    "id": "R182290",
                    "label": "PFID: Pittsburgh fast-food image dataset",
                    "doi": "10.1109/icip.2009.5413511",
                    "research_field": {
                        "id": "R133",
                        "label": "Artificial Intelligence"
                    },
                    "abstract": "we introduce the first visual dataset of fast foods with a total of 4,545 still images, 606 stereo pairs, 303 360\u00b0 videos for structure from motion, and 27 privacy-preserving videos of eating events of volunteers. this work was motivated by research on fast food recognition for dietary assessment. the data was collected by obtaining three instances of 101 foods from 11 popular fast food chains, and capturing images and videos in both restaurant conditions and a controlled lab setting. we benchmark the dataset using two standard approaches, color histogram and bag of sift features in conjunction with a discriminative classifier. our dataset and the benchmarks are designed to stimulate research in this area and will be released freely to the research community."
                },
                {
                    "id": "R182302",
                    "label": "Personal dietary assessment using mobile devices",
                    "doi": "10.1117/12.813556",
                    "research_field": {
                        "id": "R133",
                        "label": "Artificial Intelligence"
                    },
                    "abstract": "dietary intake provides valuable insights for mounting intervention programs for prevention of disease. with growing concern for adolescent obesity, the need to accurately measure diet becomes imperative. assessment among adolescents is problematic as this group has irregular eating patterns and have less enthusiasm for recording food intake. preliminary studies among adolescents suggest that innovative use of technology may improve the accuracy of diet information from young people. in this paper we describe further development of a novel dietary assessment system using mobile devices. this system will generate an accurate account of daily food and nutrient intake among adolescents. the mobile computing device provides a unique vehicle for collecting dietary information that reduces burden on records that are obtained using more classical approaches. images before and after foods are eaten can be used to estimate the amount of food consumed."
                },
                {
                    "id": "R182311",
                    "label": "Image Recognition of 85 Food Categories by Feature Fusion",
                    "doi": "10.1109/ism.2010.51",
                    "research_field": {
                        "id": "R133",
                        "label": "Artificial Intelligence"
                    },
                    "abstract": "recognition of food images is challenging due to their diversity and practical for health care on foods for people. in this paper, we propose an automatic food image recognition system for 85 food categories by fusing various kinds of image features including bag-of-features~(bof), color histogram, gabor features and gradient histogram with multiple kernel learning~(mkl). in addition, we implemented a prototype system to recognize food images taken by cellular-phone cameras. in the experiment, we have achieved the 62.52% classification rate for 85 food categories."
                },
                {
                    "id": "R182316",
                    "label": "Automatic Chinese food identification and quantity estimation",
                    "doi": "10.1145/2407746.2407775",
                    "research_field": {
                        "id": "R133",
                        "label": "Artificial Intelligence"
                    },
                    "abstract": "computer-aided food identification and quantity estimation have caught more attention in recent years because of the growing concern of our health. the identification problem is usually defined as an image categorization or classification problem and several researches have been proposed. in this paper, we address the issues of feature descriptors in the food identification problem and introduce a preliminary approach for the quantity estimation using depth information. sparse coding is utilized in the sift and local binary pattern feature descriptors, and these features combined with gabor and color features are used to represent food items. a multi-label svm classifier is trained for each feature, and these classifiers are combined with multi-class adaboost algorithm. for evaluation, 50 categories of worldwide food are used, and each category contains 100 photographs from different sources, such as manually taken or from internet web albums. an overall accuracy of 68.3% is achieved, and success at top-n candidates achieved 80.6%, 84.8%, and 90.9% accuracy accordingly when n equals 2, 3, and 5, thus making mobile application practical. the experimental results show that the proposed methods greatly improve the performance of original sift and lbp feature descriptors. on the other hand, for quantity estimation using depth information, a straight forward method is proposed for certain food, while transparent food ingredients such as pure water and cooked rice are temporarily excluded."
                },
                {
                    "id": "R182336",
                    "label": "A Food Recognition System for Diabetic Patients Based on an Optimized Bag-of-Features Model",
                    "doi": "10.1109/JBHI.2014.2308928",
                    "research_field": {
                        "id": "R133",
                        "label": "Artificial Intelligence"
                    },
                    "abstract": "\"computer vision-based food recognition could be used to estimate a meal's carbohydrate content for diabetic patients. this study proposes a methodology for automatic food recognition, based on the bag-of-features (bof) model. an extensive technical investigation was conducted for the identification and optimization of the best performing components involved in the bof architecture, as well as the estimation of the corresponding parameters. for the design and evaluation of the prototype system, a visual dataset with nearly 5000 food images was created and organized into 11 classes. the optimized system computes dense local features, using the scale-invariant feature transform on the hsv color space, builds a visual dictionary of 10000 visual words by using the hierarchical k-means clustering and finally classifies the food images with a linear support vector machine classifier. the system achieved classification accuracy of the order of 78%, thus proving the feasibility of the proposed approach in a very challenging image dataset.\""
                },
                {
                    "id": "R182352",
                    "label": "Real-Time Mobile Food Recognition System",
                    "doi": "10.1109/cvprw.2013.5",
                    "research_field": {
                        "id": "R133",
                        "label": "Artificial Intelligence"
                    },
                    "abstract": "\"we propose a mobile food recognition system the poses of which are estimating calorie and nutritious of foods and recording a user's eating habits. since all the processes on image recognition performed on a smart-phone, the system does not need to send images to a server and runs on an ordinary smartphone in a real-time way. to recognize food items, a user draws bounding boxes by touching the screen first, and then the system starts food item recognition within the indicated bounding boxes. to recognize them more accurately, we segment each food item region by grubcut, extract a color histogram and surf-based bag-of-features, and finally classify it into one of the fifty food categories with linear svm and fast 2 kernel. in addition, the system estimates the direction of food regions where the higher svm output score is expected to be obtained, show it as an arrow on the screen in order to ask a user to move a smartphone camera. this recognition process is performed repeatedly about once a second. we implemented this system as an android smartphone application so as to use multiple cpu cores effectively for real-time recognition. in the experiments, we have achieved the 81.55% classification rate for the top 5 category candidates when the ground-truth bounding boxes are given. in addition, we obtained positive evaluation by user study compared to the food recording system without object recognition.\""
                },
                {
                    "id": "R164455",
                    "label": "BioNLP Shared Task 2011 - Bacteria Biotope",
                    "doi": "",
                    "research_field": {
                        "id": "R322",
                        "label": "Computational Linguistics"
                    },
                    "abstract": "this paper presents the bacteria biotope task as part of the bionlp shared tasks 2011. the bacteria biotope task aims at extracting the location of bacteria from scientific web pages. bacteria location is a crucial knowledge in biology for phenotype studies. the paper details the corpus specification, the evaluation metrics, summarizes and discusses the participant results."
                },
                {
                    "id": "R162526",
                    "label": "Overview of the BioCreative VI text-mining services for Kinome Curation Track",
                    "doi": "10.1093/database/bay104",
                    "research_field": {
                        "id": "R145261",
                        "label": "Natural Language Processing"
                    },
                    "abstract": "abstract the text-mining services for kinome curation track, part of biocreative vi, proposed a competition to assess the effectiveness of text mining to perform literature triage. the track has exploited an unpublished curated data set from the nextprot database. this data set contained comprehensive annotations for 300 human protein kinases. for a given protein and a given curation axis [diseases or gene ontology (go) biological processes], participants\u2019 systems had to identify and rank relevant articles in a collection of 5.2 m medline citations (task 1) or 530 000 full-text articles (task 2). explored strategies comprised named-entity recognition and machine-learning frameworks. for that latter approach, participants developed methods to derive a set of negative instances, as the databases typically do not store articles that were judged as irrelevant by curators. the supervised approaches proposed by the participating groups achieved significant improvements compared to the baseline established in a previous study and compared to a basic pubmed search."
                },
                {
                    "id": "R162546",
                    "label": "Overview of the BioCreative VI Precision Medicine Track: mining protein interactions and mutations for precision medicine",
                    "doi": "10.1093/database/bay147",
                    "research_field": {
                        "id": "R145261",
                        "label": "Natural Language Processing"
                    },
                    "abstract": "abstract the precision medicine initiative is a multicenter effort aiming at formulating personalized treatments leveraging on individual patient data (clinical, genome sequence and functional genomic data) together with the information in large knowledge bases (kbs) that integrate genome annotation, disease association studies, electronic health records and other data types. the biomedical literature provides a rich foundation for populating these kbs, reporting genetic and molecular interactions that provide the scaffold for the cellular regulatory systems and detailing the influence of genetic variants in these interactions. the goal of biocreative vi precision medicine track was to extract this particular type of information and was organized in two tasks: (i) document triage task, focused on identifying scientific literature containing experimentally verified protein\u2013protein interactions (ppis) affected by genetic mutations and (ii) relation extraction task, focused on extracting the affected interactions (protein pairs). to assist system developers and task participants, a large-scale corpus of pubmed documents was manually annotated for this task. ten teams worldwide contributed 22 distinct text-mining models for the document triage task, and six teams worldwide contributed 14 different text-mining systems for the relation extraction task. when comparing the text-mining system predictions with human annotations, for the triage task, the best f-score was 69.06%, the best precision was 62.89%, the best recall was 98.0% and the best average precision was 72.5%. for the relation extraction task, when taking homologous genes into account, the best f-score was 37.73%, the best precision was 46.5% and the best recall was 54.1%. submitted systems explored a wide range of methods, from traditional rule-based, statistical and machine learning systems to state-of-the-art deep learning methods. given the level of participation and the individual team results we find the precision medicine track to be successful in engaging the text-mining research community. in the meantime, the track produced a manually annotated corpus of 5509 pubmed documents developed by biogrid curators and relevant for precision medicine. the data set is freely available to the community, and the specific interactions have been integrated into the biogrid data set. in addition, this challenge provided the first results of automatically identifying pubmed articles that describe ppi affected by mutations, as well as extracting the affected relations from those articles. still, much progress is needed for computer-assisted precision medicine text mining to become mainstream. future work should focus on addressing the remaining technical challenges and incorporating the practical benefits of text-mining tools into real-world precision medicine information-related curation."
                },
                {
                    "id": "R163190",
                    "label": "Cross-lingual Name Tagging and Linking for 282 Languages",
                    "doi": "10.18653/v1/p17-1178",
                    "research_field": {
                        "id": "R145261",
                        "label": "Natural Language Processing"
                    },
                    "abstract": "the ambitious goal of this work is to develop a cross-lingual name tagging and linking framework for 282 languages that exist in wikipedia. given a document in any of these languages, our framework is able to identify name mentions, assign a coarse-grained or fine-grained type to each mention, and link it to an english knowledge base (kb) if it is linkable. we achieve this goal by performing a series of new kb mining methods: generating \u201csilver-standard\u201d annotations by transferring annotations from english to other languages through cross-lingual links and kb properties, refining annotations through self-training and topic selection, deriving language-specific morphology features from anchor links, and mining word translation pairs from cross-lingual links. both name tagging and linking results for 282 languages are promising on wikipedia data and on-wikipedia data."
                },
                {
                    "id": "R162457",
                    "label": "Overview of the CHEMDNER patents task",
                    "doi": "",
                    "research_field": {
                        "id": "R145261",
                        "label": "Natural Language Processing"
                    },
                    "abstract": "a considerable effort has been made to extract biological and chemical entities, as well as their relationships, from the scientific literature, either manually through traditional literature curation or by using information extraction and text mining technologies. medicinal chemistry patents contain a wealth of information, for instance to uncover potential biomarkers that might play a role in cancer treatment and prognosis. however, current biomedical annotation databases do not cover such information, partly due to limitations of publicly available biomedical patent mining software. as part of the biocreative v chemdner patents track, we present the results of the first named entity recognition (ner) assignment carried out to detect mentions of chemical compounds and genes/proteins in running patent text. more specifically, this task aimed to evaluate the performance of automatic name recognition strategies capable of isolating chemical names and gene and gene product mentions from surrounding text within patent titles and abstracts. a total of 22 unique teams submitted results for at least one of the three chemdner subtasks. the first subtask, called the cemp (chemical entity mention in patents) task, focused on the detection of chemical named entity mentions in patents, requesting teams to return the start and end indices corresponding to all the chemical entities found in a given record. a total of 21 teams submitted 93 runs, for this subtask. the top performing team reached an f-measure of 0.89 with a precision of 0.87 and a recall of 0.91. the cpd (chemical passage detection) task required the classification of patent titles and abstracts whether they do or do not contain chemical compound mentions. nine teams returned predictions for this task (40 runs). the top run in terms of matthew\u2019s correlation coefficient (mcc) had a score of 0.88, the highest sensitivity ? corresponding author"
                },
                {
                    "id": "R162349",
                    "label": "BioCreAtIvE Task 1A: gene mention finding evaluation",
                    "doi": "10.1186/1471-2105-6-s1-s2",
                    "research_field": {
                        "id": "R145261",
                        "label": "Natural Language Processing"
                    },
                    "abstract": "abstract \\n \\n background \\n the biological research literature is a major repository of knowledge. as the amount of literature increases, it will get harder to find the information of interest on a particular topic. there has been an increasing amount of work on text mining this literature, but comparing this work is hard because of a lack of standards for making comparisons. to address this, we worked with colleagues at the protein design group, cnb-csic, madrid to develop biocreative (critical assessment for information extraction in biology), an open common evaluation of systems on a number of biological text mining tasks. we report here on task 1a, which deals with finding mentions of genes and related entities in text. \"finding mentions\" is a basic task, which can be used as a building block for other text mining tasks. the task makes use of data and evaluation software provided by the (us) national center for biotechnology information (ncbi). \\n \\n \\n results \\n 15 teams took part in task 1a. a number of teams achieved scores over 80% f-measure (balanced precision and recall). the teams that tried to use their task 1a systems to help on other biocreative tasks reported mixed results. \\n \\n \\n conclusion \\n the 80% plus f-measure results are good, but still somewhat lag the best scores achieved in some other domains such as newswire, due in part to the complexity and length of gene names, compared to person or organization names in newswire. \\n"
                },
                {
                    "id": "R162352",
                    "label": "Evaluation of BioCreAtIvE assessment of task 2",
                    "doi": "10.1186/1471-2105-6-s1-s16",
                    "research_field": {
                        "id": "R145261",
                        "label": "Natural Language Processing"
                    },
                    "abstract": "abstract \\n \\n background \\n molecular biology accumulated substantial amounts of data concerning functions of genes and proteins. information relating to functional descriptions is generally extracted manually from textual data and stored in biological databases to build up annotations for large collections of gene products. those annotation databases are crucial for the interpretation of large scale analysis approaches using bioinformatics or experimental techniques. due to the growing accumulation of functional descriptions in biomedical literature the need for text mining tools to facilitate the extraction of such annotations is urgent. in order to make text mining tools useable in real world scenarios, for instance to assist database curators during annotation of protein function, comparisons and evaluations of different approaches on full text articles are needed. \\n \\n \\n results \\n the critical assessment for information extraction in biology (biocreative) contest consists of a community wide competition aiming to evaluate different strategies for text mining tools, as applied to biomedical literature. we report on task two which addressed the automatic extraction and assignment of gene ontology (go) annotations of human proteins, using full text articles. the predictions of task 2 are based on triplets of protein \u2013 go term \u2013 article passage . the annotation-relevant text passages were returned by the participants and evaluated by expert curators of the go annotation (goa) team at the european institute of bioinformatics (ebi). each participant could submit up to three results for each sub-task comprising task 2. in total more than 15,000 individual results were provided by the participants. the curators evaluated in addition to the annotation itself, whether the protein and the go term were correctly predicted and traceable through the submitted text fragment. \\n \\n \\n conclusion \\n concepts provided by go are currently the most extended set of terms used for annotating gene products, thus they were explored to assess how effectively text mining tools are able to extract those annotations automatically. although the obtained results are promising, they are still far from reaching the required performance demanded by real world applications. among the principal difficulties encountered to address the proposed task, were the complex nature of the go terms and protein names (the large range of variants which are used to express proteins and especially go terms in free text), and the lack of a standard training set. a range of very different strategies were used to tackle this task. the dataset generated in line with the biocreative challenge is publicly available and will allow new possibilities for training information extraction methods in the domain of molecular biology. \\n"
                },
                {
                    "id": "R163186",
                    "label": "WikiNEuRal: Combined Neural and Knowledge-based Silver Data Creation for Multilingual NER",
                    "doi": "10.18653/v1/2021.findings-emnlp.215",
                    "research_field": {
                        "id": "R145261",
                        "label": "Natural Language Processing"
                    },
                    "abstract": "multilingual named entity recognition (ner) is a key intermediate task which is needed in many areas of nlp. in this paper, we address the well-known issue of data scarcity in ner, especially relevant when moving to a multilingual scenario, and go beyond current approaches to the creation of multilingual silver data for the task. we exploit the texts of wikipedia and introduce a new methodology based on the effective combination of knowledge-based approaches and neural models, together with a novel domain adaptation technique, to produce high-quality training corpora for ner. we evaluate our datasets extensively on standard benchmarks for ner, yielding substantial improvements of up to 6 span-based f1-score points over previous state-of-the-art systems for data creation."
                },
                {
                    "id": "R163542",
                    "label": "Overview of the Regulatory Network of Plant Seed Development\n            (SeeDev) Task at the BioNLP Shared Task 2016.",
                    "doi": "10.18653/v1/w16-3001",
                    "research_field": {
                        "id": "R145261",
                        "label": "Natural Language Processing"
                    },
                    "abstract": "this paper presents the seedev task of the bionlp shared task 2016. the purpose of the seedev task is the extraction from scientific articles of the descriptions of genetic and molecular mechanisms involved in seed development of the model plant, arabidopsis thaliana. the seedev task consists in the extraction of many different event types that involve a wide range of entity types so that they accurately reflect the complexity of the biological mechanisms. the corpus is composed of paragraphs selected from the full-texts of relevant scientific articles. in this paper, we describe the organization of the seedev task, the corpus characteristics, and the metrics used for the evaluation of participant systems. we analyze and discuss the final results of the seven participant systems to the test. the best f-score is 0.432, which is similar to the scores achieved in similar tasks on molecular biology."
                },
                {
                    "id": "R163595",
                    "label": "Overview of the Bacteria Biotope Task at BioNLP Shared Task 2016",
                    "doi": "10.18653/v1/w16-3002",
                    "research_field": {
                        "id": "R145261",
                        "label": "Natural Language Processing"
                    },
                    "abstract": "this paper presents the bacteria biotope task of the bionlp shared task 2016, which follows the previous 2013 and 2011 editions. the task focuses on the extraction of the locations (biotopes and geographical places) of bacteria from pubme abstracts and the characterization of bacteria and their associated habitats with\\nrespect to reference knowledge sources (ncbi taxonomy, ontobiotope ontology). the task is motivated by the importance of the knowledge on bacteria habitats for fundamental research and applications in microbiology. the paper describes the different proposed subtasks, the corpus characteristics, the challenge organization, and the evaluation metrics. we also provide an analysis of the results obtained by participants."
                },
                {
                    "id": "R163616",
                    "label": "CRAFT Shared Tasks 2019 Overview \u2013- Integrated Structure, Semantics, and Coreference",
                    "doi": "10.18653/v1/d19-5725",
                    "research_field": {
                        "id": "R145261",
                        "label": "Natural Language Processing"
                    },
                    "abstract": "as part of the bionlp open shared tasks 2019, the craft shared tasks 2019 provides a platform to gauge the state of the art for three fundamental language processing tasks \u2014 dependency parse construction, coreference resolution, and ontology concept identification \u2014 over full-text biomedical articles. the structural annotation task requires the automatic generation of dependency parses for each sentence of an article given only the article text. the coreference resolution task focuses on linking coreferring base noun phrase mentions into chains using the symmetrical and transitive identity relation. the ontology concept annotation task involves the identification of concept mentions within text using the classes of ten distinct ontologies in the biomedical domain, both unmodified and augmented with extension classes. this paper provides an overview of each task, including descriptions of the data provided to participants and the evaluation metrics used, and discusses participant results relative to baseline performances for each of the three tasks."
                },
                {
                    "id": "R163050",
                    "label": "Named Entity Recognition in Wikipedia",
                    "doi": "",
                    "research_field": {
                        "id": "R145261",
                        "label": "Natural Language Processing"
                    },
                    "abstract": "\"named entity recognition (ner) is used in many domains beyond the newswire text that comprises current gold-standard corpora. recent work has used wikipedia's link structure to automatically generate near gold-standard annotations. until now, these resources have only been evaluated on newswire corpora or themselves. \\n \\nwe present the first ner evaluation on a wikipedia gold standard (wg) corpus. our analysis of cross-corpus performance on wg shows that wikipedia text may be a harder ner domain than newswire. we find that an automatic annotation of wikipedia has high agreement with wg and, when used as training data, outperforms newswire models by up to 7.7%.\""
                },
                {
                    "id": "R163109",
                    "label": "WiNER: A Wikipedia Annotated Corpus for Named Entity Recognition",
                    "doi": "",
                    "research_field": {
                        "id": "R145261",
                        "label": "Natural Language Processing"
                    },
                    "abstract": "we revisit the idea of mining wikipedia in order to generate named-entity annotations. we propose a new methodology that we applied to english wikipedia to build winer, a large, high quality, annotated corpus. we evaluate its usefulness on 6 ner tasks, comparing 4 popular state-of-the art approaches. we show that lstm-crf is the approach that benefits the most from our corpus. we report impressive gains with this model when using a small portion of winer on top of the conll training material. last, we propose a simple but efficient method for exploiting the full range of winer, leading to further improvements."
                },
                {
                    "id": "R163499",
                    "label": "Overview of the Pathway Curation (PC) task of BioNLP Shared Task 2013",
                    "doi": "",
                    "research_field": {
                        "id": "R145261",
                        "label": "Natural Language Processing"
                    },
                    "abstract": "we present the pathway curation (pc) task, a main event extraction task of the bionlp shared task (st) 2013. the pc task concerns the automatic extraction of biomolecular reactions from text. the task setting, representation and semantics are defined with respect to pathway model standards and ontologies (sbml, biopax, sbo) and documents selected by relevance to specific model reactions. two bionlp st 2013 participants successfully completed the pc task. the highest achieved fscore, 52.8%, indicates that event extraction is a promising approach to supporting pathway curation efforts. the pc task continues as an open challenge with data, resources and tools available from http://2013.bionlp-st.org/"
                },
                {
                    "id": "R162482",
                    "label": "BioCreative V track 4: a shared task for the extraction of causal network information using the Biological Expression Language",
                    "doi": "10.1093/database/baw067",
                    "research_field": {
                        "id": "R145261",
                        "label": "Natural Language Processing"
                    },
                    "abstract": "automatic extraction of biological network information is one of the most desired and most complex tasks in biological and medical text mining. track 4 at biocreative v attempts to approach this complexity using fragments of large-scale manually curated biological networks, represented in biological expression language (bel), as training and test data. bel is an advanced knowledge representation format which has been designed to be both human readable and machine processable. the specific goal of track 4 was to evaluate text mining systems capable of automatically constructing bel statements from given evidence text, and of retrieving evidence text for given bel statements. given the complexity of the task, we designed an evaluation methodology which gives credit to partially correct statements. we identified various levels of information expressed by bel statements, such as entities, functions, relations, and introduced an evaluation framework which rewards systems capable of delivering useful bel fragments at each of these levels. the aim of this evaluation method is to help identify the characteristics of the systems which, if combined, would be most useful for achieving the overall goal of automatically constructing causal biological networks from text."
                },
                {
                    "id": "R163656",
                    "label": "PharmaCoNER: Pharmacological Substances, Compounds and proteins Named Entity Recognition track",
                    "doi": "10.18653/v1/d19-5701",
                    "research_field": {
                        "id": "R145261",
                        "label": "Natural Language Processing"
                    },
                    "abstract": "one of the biomedical entity types of relevance for medicine or biosciences are chemical compounds and drugs. the correct detection these entities is critical for other text mining applications building on them, such as adverse drug-reaction detection, medication-related fake news or drug-target extraction. although a significant effort was made to detect mentions of drugs/chemicals in english texts, so far only very limited attempts were made to recognize them in medical documents in other languages. taking into account the growing amount of medical publications and clinical records written in spanish, we have organized the first shared task on detecting drug and chemical entities in spanish medical documents. additionally, we included a clinical concept-indexing sub-track asking teams to return snomed-ct identifiers related to drugs/chemicals for a collection of documents. for this task, named pharmaconer, we generated annotation guidelines together with a corpus of 1,000 manually annotated clinical case studies. a total of 22 teams participated in the sub-track 1, (77 system runs), and 7 teams in the sub-track 2 (19 system runs). top scoring teams used sophisticated deep learning approaches yielding very competitive results with f-measures above 0.91. these results indicate that there is a real interest in promoting biomedical text mining efforts beyond english. we foresee that the pharmaconer annotation guidelines, corpus and participant systems will foster the development of new resources for clinical and biomedical text mining systems of spanish medical data."
                },
                {
                    "id": "R163666",
                    "label": "An Overview of the Active Gene Annotation Corpus and the BioNLP OST 2019 AGAC Track Tasks",
                    "doi": "10.18653/v1/d19-5710",
                    "research_field": {
                        "id": "R145261",
                        "label": "Natural Language Processing"
                    },
                    "abstract": "the active gene annotation corpus (agac) was developed to support knowledge discovery for drug repurposing. based on the corpus, the agac track of the bionlp open shared tasks 2019 was organized, to facilitate cross-disciplinary collaboration across bionlp and pharmacoinformatics communities, for drug repurposing. the agac track consists of three subtasks: 1) named entity recognition, 2) thematic relation extraction, and 3) loss of function (lof) / gain of function (gof) topic classification. the agac track was participated by five teams, of which the performance are compared and analyzed. the the results revealed a substantial room for improvement in the design of the task, which we analyzed in terms of \u201cimbalanced data\u201d, \u201cselective annotation\u201d and \u201clatent topic annotation\u201d."
                },
                {
                    "id": "R163702",
                    "label": "Bacteria Biotope at BioNLP Open Shared Tasks 2019",
                    "doi": "10.18653/v1/d19-5719",
                    "research_field": {
                        "id": "R145261",
                        "label": "Natural Language Processing"
                    },
                    "abstract": "this paper presents the fourth edition of the bacteria biotope task at bionlp open shared tasks 2019. the task focuses on the extraction of the locations and phenotypes of microorganisms from pubmed abstracts and full-text excerpts, and the characterization of these entities with respect to reference knowledge sources (ncbi taxonomy, ontobiotope ontology). the task is motivated by the importance of the knowledge on biodiversity for fundamental research and applications in microbiology. the paper describes the different proposed subtasks, the corpus characteristics, and the challenge organization. we also provide an analysis of the results obtained by participants, and inspect the evolution of the results since the last edition in 2016."
                },
                {
                    "id": "R163875",
                    "label": "The role of software in science: a knowledge graph-based analysis of software mentions in PubMed Central",
                    "doi": "10.7717/peerj-cs.835",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "science across all disciplines has become increasingly data-driven, leading to additional needs with respect to software for collecting, processing and analysing data. thus, transparency about software used as part of the scientific process is crucial to understand provenance of individual research data and insights, is a prerequisite for reproducibility and can enable macro-analysis of the evolution of scientific methods over time. however, missing rigor in software citation practices renders the automated detection and disambiguation of software mentions a challenging problem. in this work, we provide a large-scale analysis of software usage and citation practices facilitated through an unprecedented knowledge graph of software mentions and affiliated metadata generated through supervised information extraction models trained on a unique gold standard corpus and applied to more than 3 million scientific articles. our information extraction approach distinguishes different types of software and mentions, disambiguates mentions and outperforms the state-of-the-art significantly, leading to the most comprehensive corpus of 11.8 m software mentions that are described through a knowledge graph consisting of more than 300 m triples. our analysis provides insights into the evolution of software usage and citation patterns across various fields, ranks of journals, and impact of publications. whereas, to the best of our knowledge, this is the most comprehensive analysis of software use and citation at the time, all data and models are shared publicly to facilitate further research into scientific use and citation of software."
                },
                {
                    "id": "R164003",
                    "label": "SoMeSci- A 5 Star Open Data Gold Standard Knowledge Graph of Software Mentions in Scientific Articles",
                    "doi": "10.1145/3459637.3482017",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "knowledge about software used in scientific investigations is important for several reasons, for instance, to enable an understanding of provenance and methods involved in data handling. however, software is usually not formally cited, but rather mentioned informally within the scholarly description of the investigation, raising the need for automatic information extraction and disambiguation. given the lack of reliable ground truth data, we present somesci-software mentions in science-a gold standard knowledge graph of software mentions in scientific articles. it contains high quality annotations (irr: k=.82) of 3756 software mentions in 1367 pubmed central articles. besides the plain mention of the software, we also provide relation labels for additional information, such as the version, the developer, a url or citations. moreover, we distinguish between different types, such as application, plugin or programming environment, as well as different types of mentions, such as usage or creation. to the best of our knowledge, somesci is the most comprehensive corpus about software mentions in scientific articles, providing training samples for named entity recognition, relation extraction, entity disambiguation, and entity linking. finally, we sketch potential use cases and provide baseline results."
                },
                {
                    "id": "R166497",
                    "label": "Softcite dataset: A dataset of software mentions in biomedical and economic research publications",
                    "doi": "10.1002/asi.24454",
                    "research_field": {
                        "id": "R137681",
                        "label": "Information Systems, Process and Knowledge Management"
                    },
                    "abstract": "software contributions to academic research are relatively invisible, especially to the formalized scholarly reputation system based on bibliometrics. in this article, we introduce a gold\u2010standard dataset of software mentions from the manual annotation of 4,971 academic pdfs in biomedicine and economics. the dataset is intended to be used for automatic extraction of software mentions from pdf format research publications by supervised learning at scale. we provide a description of the dataset and an extended discussion of its creation process, including improved text conversion of academic pdfs. finally, we reflect on our challenges and lessons learned during the dataset creation, in hope of encouraging more discussion about creating datasets for machine learning use."
                },
                {
                    "id": "R165975",
                    "label": "Named Entity Recognition for Astronomy Literature",
                    "doi": "",
                    "research_field": {
                        "id": "R145261",
                        "label": "Natural Language Processing"
                    },
                    "abstract": "we present a system for named entity recognition (ner) in astronomy journal articles. we have developed this system on a ne corpus comprising approximately 200,000 words of text from astronomy articles. these have been manually annotated with \u223c40 entity types of interest to astronomers. we report on the challenges involved in extracting the corpus, defining entity classes and annotating scientific text. we investigate which features of an existing state-of-the-art maximum entropy approach perform well on astronomy text. our system achieves an f-score of 87.8%."
                },
                {
                    "id": "R166174",
                    "label": "POLYGLOT-NER: Massive Multilingual Named Entity Recognition",
                    "doi": "10.1137/1.9781611974010.66",
                    "research_field": {
                        "id": "R145261",
                        "label": "Natural Language Processing"
                    },
                    "abstract": "the increasing diversity of languages used on the web introduces a new level of complexity to information retrieval (ir) systems. we can no longer assume that textual content is written in one language or even the same language family. in this paper, we demonstrate how to build massive multilingual annotators with minimal human expertise and intervention. we describe a system that builds named entity recognition (ner) annotators for 40 major languages using wikipedia and freebase. our approach does not require ner human annotated datasets or language specific resources like treebanks, parallel corpora, and orthographic rules. the novelty of approach lies therein - using only language agnostic techniques, while achieving competitive performance. \\nour method learns distributed word representations (word embeddings) which encode semantic and syntactic features of words in each language. then, we automatically generate datasets from wikipedia link structure and freebase attributes. finally, we apply two preprocessing stages (oversampling and exact surface form matching) which do not require any linguistic expertise. \\nour evaluation is two fold: first, we demonstrate the system performance on human annotated datasets. second, for languages where no gold-standard benchmarks are available, we propose a new method, distant evaluation, based on statistical machine translation."
                },
                {
                    "id": "R166335",
                    "label": "Overview of BioCreAtIvE task 1B: normalized gene lists",
                    "doi": "10.1186/1471-2105-6-s1-s11",
                    "research_field": {
                        "id": "R145261",
                        "label": "Natural Language Processing"
                    },
                    "abstract": "abstract \\n \\n background \\n our goal in biocreative has been to assess the state of the art in text mining, with emphasis on applications that reflect real biological applications, e.g., the curation process for model organism databases. this paper summarizes the biocreative task 1b, the \"normalized gene list\" task, which was inspired by the gene list supplied for each curated paper in a model organism database. the task was to produce the correct list of unique gene identifiers for the genes and gene products mentioned in sets of abstracts from three model organisms (yeast, fly, and mouse). \\n \\n \\n results \\n eight groups fielded systems for three data sets (yeast, fly, and mouse). for yeast, the top scoring system (out of 15) achieved 0.92 f-measure (harmonic mean of precision and recall); for mouse and fly, the task was more difficult, due to larger numbers of genes, more ambiguity in the gene naming conventions (particularly for fly), and complex gene names (for mouse). for fly, the top f-measure was 0.82 out of 11 systems and for mouse, it was 0.79 out of 16 systems. \\n \\n \\n conclusion \\n this assessment demonstrates that multiple groups were able to perform a real biological task across a range of organisms. the performance was dependent on the organism, and specifically on the naming conventions associated with each organism. these results hold out promise that the technology can provide partial automation of the curation process in the near future. \\n"
                },
                {
                    "id": "R171842",
                    "label": "BC4GO: a full-text corpus for the BioCreative IV GO task",
                    "doi": "10.1093/database/bau074",
                    "research_field": {
                        "id": "R145261",
                        "label": "Natural Language Processing"
                    },
                    "abstract": "gene function curation via gene ontology (go) annotation is a common task among model organism database groups. owing to its manual nature, this task is considered one of the bottlenecks in literature curation. there have been many previous attempts at automatic identification of go terms and supporting information from full text. however, few systems have delivered an accuracy that is comparable with humans. one recognized challenge in developing such systems is the lack of marked sentence-level evidence text that provides the basis for making go annotations. we aim to create a corpus that includes the go evidence text along with the three core elements of go annotations: (i) a gene or gene product, (ii) a go term and (iii) a go evidence code. to ensure our results are consistent with real-life go data, we recruited eight professional go curators and asked them to follow their routine go annotation protocols. our annotators marked up more than 5000 text passages in 200 articles for 1356 distinct go terms. for evidence sentence selection, the inter-annotator agreement (iaa) results are 9.3% (strict) and 42.7% (relaxed) in f1-measures. for go term selection, the iaas are 47% (strict) and 62.9% (hierarchical). our corpus analysis further shows that abstracts contain \u223c10% of relevant evidence sentences and 30% distinct go terms, while the results/experiment section has nearly 60% relevant sentences and >70% go terms. further, of those evidence sentences found in abstracts, less than one-third contain enough experimental detail to fulfill the three core criteria of a go annotation. this result demonstrates the need of using full-text articles for text mining go annotations. through its use at the biocreative iv go (bc4go) task, we expect our corpus to become a valuable resource for the bionlp research community. database url: http://www.biocreative.org/resources/corpora/bc-iv-go-task-corpus/."
                },
                {
                    "id": "R166184",
                    "label": "Mining Wiki Resources for Multilingual Named Entity Recognition",
                    "doi": "",
                    "research_field": {
                        "id": "R145261",
                        "label": "Natural Language Processing"
                    },
                    "abstract": "\"in this paper, we describe a system by which the multilingual characteristics of wikipedia can be utilized to annotate a large corpus of text with named entity recognition (ner) tags requiring minimal human intervention and no linguistic expertise. this process, though of value in languages for which resources exist, is particularly useful for less commonly taught languages. we show how the wikipedia format can be used to identify possible named entities and discuss in detail the process by which we use the category structure inherent to wikipedia to determine the named entity type of a proposed entity. we further describe the methods by which english language data can be used to bootstrap the ner process in other languages. we demonstrate the system by using the generated corpus as training sets for a variant of bbn's identifinder in french, ukrainian, spanish, polish, russian, and portuguese, achieving overall f-scores as high as 84.7% on independent, human-annotated corpora, comparable to a system trained on up to 40,000 words of human-annotated newswire.\""
                },
                {
                    "id": "R166178",
                    "label": "Exploiting Wikipedia as external knowledge for named entity recognition",
                    "doi": "",
                    "research_field": {
                        "id": "R145261",
                        "label": "Natural Language Processing"
                    },
                    "abstract": "we explore the use of wikipedia as external knowledge to improve named entity recognition (ner). our method retrieves the corresponding wikipedia entry for each candidate word sequence and extracts a category label from the first sentence of the entry, which can be thought of as a definition part. these category labels are used as features in a crf-based ne tagger. we demonstrate using the conll 2003 dataset that the wikipedia category labels extracted by such a simple method actually improve the accuracy of ner."
                },
                {
                    "id": "R164531",
                    "label": "BioNLP Shared Task 2013 \u2013 An overview of the Genic Regulation Network Task",
                    "doi": "",
                    "research_field": {
                        "id": "R145261",
                        "label": "Natural Language Processing"
                    },
                    "abstract": "the goal of the genic regulation network task (grn) is to extract a regulation network that links and integrates a v a r i e y o f m o l e a interactions between genes and proteins of the well-studied model bacterium bacillus subtilis. it is an extension of the bi task of bionlp-st\u201911. the corpus is composed of sentences selected from publicly available pubmed scientific"
                },
                {
                    "id": "R166235",
                    "label": "WEXEA: Wikipedia EXhaustive Entity Annotation",
                    "doi": "",
                    "research_field": {
                        "id": "R145261",
                        "label": "Natural Language Processing"
                    },
                    "abstract": "building predictive models for information extraction from text, such as named entity recognition or the extraction of semantic relationships between named entities in text, requires a large corpus of annotated text. wikipedia is often used as a corpus for these tasks where the annotation is a named entity linked by a hyperlink to its article. however, editors on wikipedia are only expected to link these mentions in order to help the reader to understand the content, but are discouraged from adding links that do not add any benefit for understanding an article. therefore, many mentions of popular entities (such as countries or popular events in history), or previously linked articles, as well as the article\u2019s entity itself, are not linked. in this paper, we discuss wexea, a wikipedia exhaustive entity annotation system, to create a text corpus based on wikipedia with exhaustive annotations of entity mentions, i.e. linking all mentions of entities to their corresponding articles. this results in a huge potential for additional annotations that can be used for downstream nlp tasks, such as relation extraction. we show that our annotations are useful for creating distantly supervised datasets for this task. furthermore, we publish all code necessary to derive a corpus from a raw wikipedia dump, so that it can be reproduced by everyone."
                },
                {
                    "id": "R164551",
                    "label": "BioNLP shared Task 2013 \u2013 An Overview of the Bacteria Biotope Task",
                    "doi": "",
                    "research_field": {
                        "id": "R145261",
                        "label": "Natural Language Processing"
                    },
                    "abstract": "this paper presents the bacteria biotope task of the bionlp shared task 2013, which follows bionlp-st-11. the bacteria biotope task aims to extract the location of bacteria from scientific web pages and to characterize these locations with respect to the ontobiotope ontology. bacteria locations are crucil knowledge in biology for phenotype studies. the paper details the corpus specifications, the evaluation metrics, and it summarizes and discusses the participant results."
                },
                {
                    "id": "R165926",
                    "label": "Transforming Wikipedia into Named Entity Training Data",
                    "doi": "",
                    "research_field": {
                        "id": "R145261",
                        "label": "Natural Language Processing"
                    },
                    "abstract": "statistical named entity recognisers require costly hand-labelled training data and, as a result, most existing corpora are small. we exploit wikipedia to create a massive corpus of named entity annotated text. we transform wikipedia\u2019s links into named entity annotations by classifying the target articles into common entity types (e.g. person, organisation and location). comparing to muc, conll and bbn corpora, wikipedia generally performs better than other cross-corpus train/test pairs."
                },
                {
                    "id": "R166181",
                    "label": "Multilingual named entity recognition using parallel data and metadata from wikipedia",
                    "doi": "",
                    "research_field": {
                        "id": "R145261",
                        "label": "Natural Language Processing"
                    },
                    "abstract": "in this paper we propose a method to automatically label multi-lingual data with named entity tags. we build on prior work utilizing wikipedia metadata and show how to effectively combine the weak annotations stemming from wikipedia metadata with information obtained through english-foreign language parallel wikipedia sentences. the combination is achieved using a novel semi-crf model for foreign sentence tagging in the context of a parallel english sentence. the model outperforms both standard annotation projection methods and methods based solely on wikipedia metadata."
                }
            ]
        },
        {
            "id": "R182248",
            "label": "Food photo dataset",
            "research_fields": [
                {
                    "id": "R133",
                    "label": "Artificial Intelligence"
                }
            ],
            "properties": [
                "Year",
                "Name",
                "Annotation",
                "Number of images",
                "Task",
                "Number of classes",
                "Acquisition",
                "type"
            ],
            "papers": [
                {
                    "id": "R182290",
                    "label": "PFID: Pittsburgh fast-food image dataset",
                    "doi": "10.1109/icip.2009.5413511",
                    "research_field": {
                        "id": "R133",
                        "label": "Artificial Intelligence"
                    },
                    "abstract": "we introduce the first visual dataset of fast foods with a total of 4,545 still images, 606 stereo pairs, 303 360\u00b0 videos for structure from motion, and 27 privacy-preserving videos of eating events of volunteers. this work was motivated by research on fast food recognition for dietary assessment. the data was collected by obtaining three instances of 101 foods from 11 popular fast food chains, and capturing images and videos in both restaurant conditions and a controlled lab setting. we benchmark the dataset using two standard approaches, color histogram and bag of sift features in conjunction with a discriminative classifier. our dataset and the benchmarks are designed to stimulate research in this area and will be released freely to the research community."
                },
                {
                    "id": "R182302",
                    "label": "Personal dietary assessment using mobile devices",
                    "doi": "10.1117/12.813556",
                    "research_field": {
                        "id": "R133",
                        "label": "Artificial Intelligence"
                    },
                    "abstract": "dietary intake provides valuable insights for mounting intervention programs for prevention of disease. with growing concern for adolescent obesity, the need to accurately measure diet becomes imperative. assessment among adolescents is problematic as this group has irregular eating patterns and have less enthusiasm for recording food intake. preliminary studies among adolescents suggest that innovative use of technology may improve the accuracy of diet information from young people. in this paper we describe further development of a novel dietary assessment system using mobile devices. this system will generate an accurate account of daily food and nutrient intake among adolescents. the mobile computing device provides a unique vehicle for collecting dietary information that reduces burden on records that are obtained using more classical approaches. images before and after foods are eaten can be used to estimate the amount of food consumed."
                },
                {
                    "id": "R182311",
                    "label": "Image Recognition of 85 Food Categories by Feature Fusion",
                    "doi": "10.1109/ism.2010.51",
                    "research_field": {
                        "id": "R133",
                        "label": "Artificial Intelligence"
                    },
                    "abstract": "recognition of food images is challenging due to their diversity and practical for health care on foods for people. in this paper, we propose an automatic food image recognition system for 85 food categories by fusing various kinds of image features including bag-of-features~(bof), color histogram, gabor features and gradient histogram with multiple kernel learning~(mkl). in addition, we implemented a prototype system to recognize food images taken by cellular-phone cameras. in the experiment, we have achieved the 62.52% classification rate for 85 food categories."
                },
                {
                    "id": "R182316",
                    "label": "Automatic Chinese food identification and quantity estimation",
                    "doi": "10.1145/2407746.2407775",
                    "research_field": {
                        "id": "R133",
                        "label": "Artificial Intelligence"
                    },
                    "abstract": "computer-aided food identification and quantity estimation have caught more attention in recent years because of the growing concern of our health. the identification problem is usually defined as an image categorization or classification problem and several researches have been proposed. in this paper, we address the issues of feature descriptors in the food identification problem and introduce a preliminary approach for the quantity estimation using depth information. sparse coding is utilized in the sift and local binary pattern feature descriptors, and these features combined with gabor and color features are used to represent food items. a multi-label svm classifier is trained for each feature, and these classifiers are combined with multi-class adaboost algorithm. for evaluation, 50 categories of worldwide food are used, and each category contains 100 photographs from different sources, such as manually taken or from internet web albums. an overall accuracy of 68.3% is achieved, and success at top-n candidates achieved 80.6%, 84.8%, and 90.9% accuracy accordingly when n equals 2, 3, and 5, thus making mobile application practical. the experimental results show that the proposed methods greatly improve the performance of original sift and lbp feature descriptors. on the other hand, for quantity estimation using depth information, a straight forward method is proposed for certain food, while transparent food ingredients such as pure water and cooked rice are temporarily excluded."
                },
                {
                    "id": "R182336",
                    "label": "A Food Recognition System for Diabetic Patients Based on an Optimized Bag-of-Features Model",
                    "doi": "10.1109/JBHI.2014.2308928",
                    "research_field": {
                        "id": "R133",
                        "label": "Artificial Intelligence"
                    },
                    "abstract": "\"computer vision-based food recognition could be used to estimate a meal's carbohydrate content for diabetic patients. this study proposes a methodology for automatic food recognition, based on the bag-of-features (bof) model. an extensive technical investigation was conducted for the identification and optimization of the best performing components involved in the bof architecture, as well as the estimation of the corresponding parameters. for the design and evaluation of the prototype system, a visual dataset with nearly 5000 food images was created and organized into 11 classes. the optimized system computes dense local features, using the scale-invariant feature transform on the hsv color space, builds a visual dictionary of 10000 visual words by using the hierarchical k-means clustering and finally classifies the food images with a linear support vector machine classifier. the system achieved classification accuracy of the order of 78%, thus proving the feasibility of the proposed approach in a very challenging image dataset.\""
                },
                {
                    "id": "R182352",
                    "label": "Real-Time Mobile Food Recognition System",
                    "doi": "10.1109/cvprw.2013.5",
                    "research_field": {
                        "id": "R133",
                        "label": "Artificial Intelligence"
                    },
                    "abstract": "\"we propose a mobile food recognition system the poses of which are estimating calorie and nutritious of foods and recording a user's eating habits. since all the processes on image recognition performed on a smart-phone, the system does not need to send images to a server and runs on an ordinary smartphone in a real-time way. to recognize food items, a user draws bounding boxes by touching the screen first, and then the system starts food item recognition within the indicated bounding boxes. to recognize them more accurately, we segment each food item region by grubcut, extract a color histogram and surf-based bag-of-features, and finally classify it into one of the fifty food categories with linear svm and fast 2 kernel. in addition, the system estimates the direction of food regions where the higher svm output score is expected to be obtained, show it as an arrow on the screen in order to ask a user to move a smartphone camera. this recognition process is performed repeatedly about once a second. we implemented this system as an android smartphone application so as to use multiple cpu cores effectively for real-time recognition. in the experiments, we have achieved the 81.55% classification rate for the top 5 category candidates when the ground-truth bounding boxes are given. in addition, we obtained positive evaluation by user study compared to the food recording system without object recognition.\""
                }
            ]
        },
        {
            "id": "R184022",
            "label": "Xray spectroscopy",
            "research_fields": [
                {
                    "id": "R175",
                    "label": "Atomic, Molecular and Optical Physics"
                }
            ],
            "properties": [
                "Paper type",
                "Research objective",
                "has system qualities"
            ],
            "papers": [
                {
                    "id": "R184047",
                    "label": "Theoretical energies for the n\u2002=\u20021 and 2 states of the helium isoelectronic sequence up to Z\u2002=\u2002100",
                    "doi": "10.1139/p88-100",
                    "research_field": {
                        "id": "R175",
                        "label": "Atomic, Molecular and Optical Physics"
                    },
                    "abstract": "the unified method described previously for combining high-precision nonrelativistic variational calculations with relativistic and quantum electrodynamic corrections is applied to the 1s 2 \\u2002 1 s 0 , 1s2s\\u2002 1 s 0 , 1s2s\\u2002 3 s 1 , 1s2p\\u2002 1 p 1 , and 1s2p\\u2002 3 p 0,1,2 staters of helium-like ions. detailed tabulations are presented for all ions in the range 2\\u2002\u2264\\u2002z\\u2002\u2264\\u2002100 and are compared with a wide range of experimental data up to 34 kr + . the results for 90 u + significantly alter the recent lamb shift measurement of munger and gould from 70.4\\u2002\u00b1\\u20028.3 to 71.0\\u2002\u00b1\\u20028.3\\u2002ev, in comparison with a revised theoretical value of 74.3\\u2002\u00b1\\u20020.4\\u2002ev. the improved agreement is due to the inclusion of higher order two-electron corrections in the present work."
                },
                {
                    "id": "R184054",
                    "label": "Theoretical energies for the <i>n</i>\u2002=\u20021 and 2 states of the helium isoelectronic sequence up to <i>Z</i>\u2002=\u2002100",
                    "doi": "10.1139/p88-100",
                    "research_field": {
                        "id": "R175",
                        "label": "Atomic, Molecular and Optical Physics"
                    },
                    "abstract": "the unified method described previously for combining high-precision nonrelativistic variational calculations with relativistic and quantum electrodynamic corrections is applied to the 1s 2 \\u2002 1 s 0 , 1s2s\\u2002 1 s 0 , 1s2s\\u2002 3 s 1 , 1s2p\\u2002 1 p 1 , and 1s2p\\u2002 3 p 0,1,2 staters of helium-like ions. detailed tabulations are presented for all ions in the range 2\\u2002\u2264\\u2002z\\u2002\u2264\\u2002100 and are compared with a wide range of experimental data up to 34 kr + . the results for 90 u + significantly alter the recent lamb shift measurement of munger and gould from 70.4\\u2002\u00b1\\u20028.3 to 71.0\\u2002\u00b1\\u20028.3\\u2002ev, in comparison with a revised theoretical value of 74.3\\u2002\u00b1\\u20020.4\\u2002ev. the improved agreement is due to the inclusion of higher order two-electron corrections in the present work."
                },
                {
                    "id": "R184163",
                    "label": "Absolute measurement of the resonance lines in heliumlike vanadium on an electron-beam ion trap",
                    "doi": "10.1103/physreva.62.042501",
                    "research_field": {
                        "id": "R175",
                        "label": "Atomic, Molecular and Optical Physics"
                    },
                    "abstract": "trap are reported. the 1 s2p 1 p1!1s 2 ,1 s2p 3 p2!1s 2 ~1s2p 3 p1!1s 2 and 1s2p 3 p0!1s 2 ! blend and 1s2s 3 s1!1s 2 transitions are 5205.10 6 0.14 ev, 5189.12 6 0.21 ev, 5180.22 6 0.17 ev, and 5153.82 6 0.14 ev, respectively. this agrees with recent theoretical calculations and the experimental precision lies at the same level as the current uncertainty in theory ~0.1 ev!. these measurements represent a 5.7\u20108% determination of the qed contribution to the transition energies and are the most precise measurements of heliumlike resonance lines in the z519\u2010 31 range. the measurement of the 1s2s 3 s1!1s 2 transition is also sensitive to the 1 s2s 3 s1 qed contribution at the 40% level. the intensity of the strong 1 s2p 3 p1!1s 2 component of the blend compared to the total blend intensity is 94%612%. this is in accord with current theoretical predictions but disagrees with an earlier reported ratio."
                },
                {
                    "id": "R184195",
                    "label": "Absolute measurement of the resonance lines in heliumlike vanadium on an electron-beam ion trap",
                    "doi": "10.1103/physreva.62.042501",
                    "research_field": {
                        "id": "R175",
                        "label": "Atomic, Molecular and Optical Physics"
                    },
                    "abstract": "trap are reported. the 1 s2p 1 p1!1s 2 ,1 s2p 3 p2!1s 2 ~1s2p 3 p1!1s 2 and 1s2p 3 p0!1s 2 ! blend and 1s2s 3 s1!1s 2 transitions are 5205.10 6 0.14 ev, 5189.12 6 0.21 ev, 5180.22 6 0.17 ev, and 5153.82 6 0.14 ev, respectively. this agrees with recent theoretical calculations and the experimental precision lies at the same level as the current uncertainty in theory ~0.1 ev!. these measurements represent a 5.7\u20108% determination of the qed contribution to the transition energies and are the most precise measurements of heliumlike resonance lines in the z519\u2010 31 range. the measurement of the 1s2s 3 s1!1s 2 transition is also sensitive to the 1 s2s 3 s1 qed contribution at the 40% level. the intensity of the strong 1 s2p 3 p1!1s 2 component of the blend compared to the total blend intensity is 94%612%. this is in accord with current theoretical predictions but disagrees with an earlier reported ratio."
                },
                {
                    "id": "R184318",
                    "label": "Absolute measurement of the resonance lines in heliumlike vanadium on an electron-beam ion trap",
                    "doi": "10.1103/physreva.62.042501",
                    "research_field": {
                        "id": "R175",
                        "label": "Atomic, Molecular and Optical Physics"
                    },
                    "abstract": "trap are reported. the 1 s2p 1 p1!1s 2 ,1 s2p 3 p2!1s 2 ~1s2p 3 p1!1s 2 and 1s2p 3 p0!1s 2 ! blend and 1s2s 3 s1!1s 2 transitions are 5205.10 6 0.14 ev, 5189.12 6 0.21 ev, 5180.22 6 0.17 ev, and 5153.82 6 0.14 ev, respectively. this agrees with recent theoretical calculations and the experimental precision lies at the same level as the current uncertainty in theory ~0.1 ev!. these measurements represent a 5.7\u20108% determination of the qed contribution to the transition energies and are the most precise measurements of heliumlike resonance lines in the z519\u2010 31 range. the measurement of the 1s2s 3 s1!1s 2 transition is also sensitive to the 1 s2s 3 s1 qed contribution at the 40% level. the intensity of the strong 1 s2p 3 p1!1s 2 component of the blend compared to the total blend intensity is 94%612%. this is in accord with current theoretical predictions but disagrees with an earlier reported ratio."
                },
                {
                    "id": "R184351",
                    "label": "Absolute measurement of the resonance lines in heliumlike vanadium on an electron-beam ion trap",
                    "doi": "10.1103/physreva.62.042501",
                    "research_field": {
                        "id": "R175",
                        "label": "Atomic, Molecular and Optical Physics"
                    },
                    "abstract": "trap are reported. the 1 s2p 1 p1!1s 2 ,1 s2p 3 p2!1s 2 ~1s2p 3 p1!1s 2 and 1s2p 3 p0!1s 2 ! blend and 1s2s 3 s1!1s 2 transitions are 5205.10 6 0.14 ev, 5189.12 6 0.21 ev, 5180.22 6 0.17 ev, and 5153.82 6 0.14 ev, respectively. this agrees with recent theoretical calculations and the experimental precision lies at the same level as the current uncertainty in theory ~0.1 ev!. these measurements represent a 5.7\u20108% determination of the qed contribution to the transition energies and are the most precise measurements of heliumlike resonance lines in the z519\u2010 31 range. the measurement of the 1s2s 3 s1!1s 2 transition is also sensitive to the 1 s2s 3 s1 qed contribution at the 40% level. the intensity of the strong 1 s2p 3 p1!1s 2 component of the blend compared to the total blend intensity is 94%612%. this is in accord with current theoretical predictions but disagrees with an earlier reported ratio."
                },
                {
                    "id": "R184383",
                    "label": "Absolute measurement of the resonance lines in heliumlike vanadium on an electron-beam ion trap",
                    "doi": "10.1103/physreva.62.042501",
                    "research_field": {
                        "id": "R175",
                        "label": "Atomic, Molecular and Optical Physics"
                    },
                    "abstract": "trap are reported. the 1 s2p 1 p1!1s 2 ,1 s2p 3 p2!1s 2 ~1s2p 3 p1!1s 2 and 1s2p 3 p0!1s 2 ! blend and 1s2s 3 s1!1s 2 transitions are 5205.10 6 0.14 ev, 5189.12 6 0.21 ev, 5180.22 6 0.17 ev, and 5153.82 6 0.14 ev, respectively. this agrees with recent theoretical calculations and the experimental precision lies at the same level as the current uncertainty in theory ~0.1 ev!. these measurements represent a 5.7\u20108% determination of the qed contribution to the transition energies and are the most precise measurements of heliumlike resonance lines in the z519\u2010 31 range. the measurement of the 1s2s 3 s1!1s 2 transition is also sensitive to the 1 s2s 3 s1 qed contribution at the 40% level. the intensity of the strong 1 s2p 3 p1!1s 2 component of the blend compared to the total blend intensity is 94%612%. this is in accord with current theoretical predictions but disagrees with an earlier reported ratio."
                },
                {
                    "id": "R185033",
                    "label": "Absolute measurement of the resonance lines in heliumlike vanadium on an electron-beam ion trap",
                    "doi": "10.1103/physreva.62.042501",
                    "research_field": {
                        "id": "R175",
                        "label": "Atomic, Molecular and Optical Physics"
                    },
                    "abstract": "trap are reported. the 1 s2p 1 p1!1s 2 ,1 s2p 3 p2!1s 2 ~1s2p 3 p1!1s 2 and 1s2p 3 p0!1s 2 ! blend and 1s2s 3 s1!1s 2 transitions are 5205.10 6 0.14 ev, 5189.12 6 0.21 ev, 5180.22 6 0.17 ev, and 5153.82 6 0.14 ev, respectively. this agrees with recent theoretical calculations and the experimental precision lies at the same level as the current uncertainty in theory ~0.1 ev!. these measurements represent a 5.7\u20108% determination of the qed contribution to the transition energies and are the most precise measurements of heliumlike resonance lines in the z519\u2010 31 range. the measurement of the 1s2s 3 s1!1s 2 transition is also sensitive to the 1 s2s 3 s1 qed contribution at the 40% level. the intensity of the strong 1 s2p 3 p1!1s 2 component of the blend compared to the total blend intensity is 94%612%. this is in accord with current theoretical predictions but disagrees with an earlier reported ratio."
                },
                {
                    "id": "R185065",
                    "label": "Fe XXV Drake 1/2",
                    "doi": "10.1139/p88-100",
                    "research_field": {
                        "id": "R175",
                        "label": "Atomic, Molecular and Optical Physics"
                    },
                    "abstract": "the unified method described previously for combining high-precision nonrelativistic variational calculations with relativistic and quantum electrodynamic corrections is applied to the 1s 2 \\u2002 1 s 0 , 1s2s\\u2002 1 s 0 , 1s2s\\u2002 3 s 1 , 1s2p\\u2002 1 p 1 , and 1s2p\\u2002 3 p 0,1,2 staters of helium-like ions. detailed tabulations are presented for all ions in the range 2\\u2002\u2264\\u2002z\\u2002\u2264\\u2002100 and are compared with a wide range of experimental data up to 34 kr + . the results for 90 u + significantly alter the recent lamb shift measurement of munger and gould from 70.4\\u2002\u00b1\\u20028.3 to 71.0\\u2002\u00b1\\u20028.3\\u2002ev, in comparison with a revised theoretical value of 74.3\\u2002\u00b1\\u20020.4\\u2002ev. the improved agreement is due to the inclusion of higher order two-electron corrections in the present work."
                },
                {
                    "id": "R185098",
                    "label": "Theoretical energies for the <i>n</i>\u2002=\u20021 and 2 states of the helium isoelectronic sequence up to <i>Z</i>\u2002=\u2002100",
                    "doi": "10.1139/p88-100",
                    "research_field": {
                        "id": "R175",
                        "label": "Atomic, Molecular and Optical Physics"
                    },
                    "abstract": "the unified method described previously for combining high-precision nonrelativistic variational calculations with relativistic and quantum electrodynamic corrections is applied to the 1s 2 \\u2002 1 s 0 , 1s2s\\u2002 1 s 0 , 1s2s\\u2002 3 s 1 , 1s2p\\u2002 1 p 1 , and 1s2p\\u2002 3 p 0,1,2 staters of helium-like ions. detailed tabulations are presented for all ions in the range 2\\u2002\u2264\\u2002z\\u2002\u2264\\u2002100 and are compared with a wide range of experimental data up to 34 kr + . the results for 90 u + significantly alter the recent lamb shift measurement of munger and gould from 70.4\\u2002\u00b1\\u20028.3 to 71.0\\u2002\u00b1\\u20028.3\\u2002ev, in comparison with a revised theoretical value of 74.3\\u2002\u00b1\\u20020.4\\u2002ev. the improved agreement is due to the inclusion of higher order two-electron corrections in the present work."
                },
                {
                    "id": "R185136",
                    "label": "Absolute measurement of the resonance lines in heliumlike vanadium on an electron-beam ion trap",
                    "doi": "10.1103/physreva.62.042501",
                    "research_field": {
                        "id": "R175",
                        "label": "Atomic, Molecular and Optical Physics"
                    },
                    "abstract": "trap are reported. the 1 s2p 1 p1!1s 2 ,1 s2p 3 p2!1s 2 ~1s2p 3 p1!1s 2 and 1s2p 3 p0!1s 2 ! blend and 1s2s 3 s1!1s 2 transitions are 5205.10 6 0.14 ev, 5189.12 6 0.21 ev, 5180.22 6 0.17 ev, and 5153.82 6 0.14 ev, respectively. this agrees with recent theoretical calculations and the experimental precision lies at the same level as the current uncertainty in theory ~0.1 ev!. these measurements represent a 5.7\u20108% determination of the qed contribution to the transition energies and are the most precise measurements of heliumlike resonance lines in the z519\u2010 31 range. the measurement of the 1s2s 3 s1!1s 2 transition is also sensitive to the 1 s2s 3 s1 qed contribution at the 40% level. the intensity of the strong 1 s2p 3 p1!1s 2 component of the blend compared to the total blend intensity is 94%612%. this is in accord with current theoretical predictions but disagrees with an earlier reported ratio."
                },
                {
                    "id": "R185172",
                    "label": "Theoretical energies for the <i>n</i>\u2002=\u20021 and 2 states of the helium isoelectronic sequence up to <i>Z</i>\u2002=\u2002100",
                    "doi": "10.1139/p88-100",
                    "research_field": {
                        "id": "R175",
                        "label": "Atomic, Molecular and Optical Physics"
                    },
                    "abstract": "the unified method described previously for combining high-precision nonrelativistic variational calculations with relativistic and quantum electrodynamic corrections is applied to the 1s 2 \\u2002 1 s 0 , 1s2s\\u2002 1 s 0 , 1s2s\\u2002 3 s 1 , 1s2p\\u2002 1 p 1 , and 1s2p\\u2002 3 p 0,1,2 staters of helium-like ions. detailed tabulations are presented for all ions in the range 2\\u2002\u2264\\u2002z\\u2002\u2264\\u2002100 and are compared with a wide range of experimental data up to 34 kr + . the results for 90 u + significantly alter the recent lamb shift measurement of munger and gould from 70.4\\u2002\u00b1\\u20028.3 to 71.0\\u2002\u00b1\\u20028.3\\u2002ev, in comparison with a revised theoretical value of 74.3\\u2002\u00b1\\u20020.4\\u2002ev. the improved agreement is due to the inclusion of higher order two-electron corrections in the present work."
                },
                {
                    "id": "R185211",
                    "label": "Theoretical energies for the <i>n</i>\u2002=\u20021 and 2 states of the helium isoelectronic sequence up to <i>Z</i>\u2002=\u2002100",
                    "doi": "10.1139/p88-100",
                    "research_field": {
                        "id": "R175",
                        "label": "Atomic, Molecular and Optical Physics"
                    },
                    "abstract": "the unified method described previously for combining high-precision nonrelativistic variational calculations with relativistic and quantum electrodynamic corrections is applied to the 1s 2 \\u2002 1 s 0 , 1s2s\\u2002 1 s 0 , 1s2s\\u2002 3 s 1 , 1s2p\\u2002 1 p 1 , and 1s2p\\u2002 3 p 0,1,2 staters of helium-like ions. detailed tabulations are presented for all ions in the range 2\\u2002\u2264\\u2002z\\u2002\u2264\\u2002100 and are compared with a wide range of experimental data up to 34 kr + . the results for 90 u + significantly alter the recent lamb shift measurement of munger and gould from 70.4\\u2002\u00b1\\u20028.3 to 71.0\\u2002\u00b1\\u20028.3\\u2002ev, in comparison with a revised theoretical value of 74.3\\u2002\u00b1\\u20020.4\\u2002ev. the improved agreement is due to the inclusion of higher order two-electron corrections in the present work."
                },
                {
                    "id": "R185000",
                    "label": "Absolute measurement of the resonance lines in heliumlike vanadium on an electron-beam ion trap",
                    "doi": "10.1103/physreva.62.042501",
                    "research_field": {
                        "id": "R175",
                        "label": "Atomic, Molecular and Optical Physics"
                    },
                    "abstract": "trap are reported. the 1 s2p 1 p1!1s 2 ,1 s2p 3 p2!1s 2 ~1s2p 3 p1!1s 2 and 1s2p 3 p0!1s 2 ! blend and 1s2s 3 s1!1s 2 transitions are 5205.10 6 0.14 ev, 5189.12 6 0.21 ev, 5180.22 6 0.17 ev, and 5153.82 6 0.14 ev, respectively. this agrees with recent theoretical calculations and the experimental precision lies at the same level as the current uncertainty in theory ~0.1 ev!. these measurements represent a 5.7\u20108% determination of the qed contribution to the transition energies and are the most precise measurements of heliumlike resonance lines in the z519\u2010 31 range. the measurement of the 1s2s 3 s1!1s 2 transition is also sensitive to the 1 s2s 3 s1 qed contribution at the 40% level. the intensity of the strong 1 s2p 3 p1!1s 2 component of the blend compared to the total blend intensity is 94%612%. this is in accord with current theoretical predictions but disagrees with an earlier reported ratio."
                },
                {
                    "id": "R189876",
                    "label": "Precision Wavelength Determination of 2^1P_1 - 1^1S_0 and 2^3P_1 - 1^1S_0 Transitions in Helium-Like Sulfur Ions",
                    "doi": "10.1088/0031-8949/25/6b/004",
                    "research_field": {
                        "id": "R114010",
                        "label": "Atomic Physics"
                    },
                    "abstract": "transitions from the 21p1 - and 23p1 -state to the ground state 11s0 in helium-like sulphur ions have been measured with an accuracy of 4 \u00d7 10-5. energy calibration is described in detail and two reference wavelengths have been reevaluated. substantial line-blending was observed, due to long-lived spectator electrons. the two transition energies were corrected for doppler shift and compared with most refined theoretical calculations, including terms of order \u03b14z6 in the breit operator and terms of order \u03b15z6 in the quantum-electrodynamical corrections. the experimental contributions to the ground-state qed shifts agree within its error (\u223c 15%) with the theoretical values."
                },
                {
                    "id": "R189910",
                    "label": "Spectroscopy of hydrogenlike and heliumlike argon",
                    "doi": "10.1103/physreva.28.1413",
                    "research_field": {
                        "id": "R114010",
                        "label": "Atomic Physics"
                    },
                    "abstract": "the x-ray transitions ($n=2\\\\ensuremath{\\\\rightarrow}n=1$) emitted by fast hydrogenlike and heliumlike argon ions have been studied. the absolute energy of the lyman $\\\\ensuremath{\\\\alpha}$ lines of hydrogenlike ions has been measured and the value of the ($1s$) lamb shift of argon evaluated for the first time. the $^{3}p_{1,2}$ and $^{1}p_{1}$ transitions of heliumlike argon have also been studied at very high precision and their energies compared to multiconfiguration dirac-fock calculations. the energies of lyman $\\\\ensuremath{\\\\alpha}$ lines are of 3323.2\\\\ifmmode\\\\pm\\\\else\\\\textpm\\\\fi{}0.5 ev ($\\\\mathrm{ly}{\\\\ensuremath{\\\\alpha}}_{1}$) and 3318.1\\\\ifmmode\\\\pm\\\\else\\\\textpm\\\\fi{}0.5 ev ($\\\\mathrm{ly}{\\\\ensuremath{\\\\alpha}}_{2}$), and those of $n=2\\\\ensuremath{\\\\rightarrow}n=1$ transitions for heliumlike argon are of 3123.6\\\\ifmmode\\\\pm\\\\else\\\\textpm\\\\fi{}0.25 ev ($^{3}p_{1}$), 3126.4\\\\ifmmode\\\\pm\\\\else\\\\textpm\\\\fi{}0.4 ev ($^{3}p_{2}$), and 3139.6\\\\ifmmode\\\\pm\\\\else\\\\textpm\\\\fi{}0.25 ev ($^{1}p_{1}$)."
                },
                {
                    "id": "R190068",
                    "label": "Precision X-ray wavelength measurements in helium-like argon recoil ions",
                    "doi": "10.1088/0022-3700/17/21/001",
                    "research_field": {
                        "id": "R114010",
                        "label": "Atomic Physics"
                    },
                    "abstract": "\"the authors report precise wavelength measurements of the 1s2 1s0-1s2p3p1,2,1p1 transitions in ar16+ produced by collisions of 5.9 mev amu-1 u66+ ions with an argon gas target. by use of this 'recoil source', the precision is not limited by doppler shifts while the influence of spectator electrons is minimised by observation of their relative importance as a function of gas pressure. the accuracy obtained is at the 12 p.p.m. level dominated by the x-ray calibration standard. the measurement is thus sensitive to quantum-electrodynamic (qed) and electron correlation effects.\""
                },
                {
                    "id": "R190165",
                    "label": "Precision measurement of the K_\u03b1 transitions in heliumlike Ge^30+",
                    "doi": "10.1103/physreva.45.329",
                    "research_field": {
                        "id": "R114010",
                        "label": "Atomic Physics"
                    },
                    "abstract": "a measurement of the 1{ital s}2{ital p} {sup 1}{ital p}{sub 1}{r arrow}1{ital s}{sup 2} {ital s}{sub 0} resonance transition in heliumlike germanium (ge{sup 30+}) has been made on the lawrence livermore national laboratory electron-beam ion trap to a precision of 21 ppm. the result is compared with theoretical values and confirms a trend previously seen in the differences between experiment and theory for this transition as a function of {ital z}. results for the 1{ital s}2{ital p} {sup 3}{ital p}{sub 1}{r arrow}1{ital s}{sup 2} {ital s}{sub 0} and 1{ital s}2{ital p} {sup 3}{ital p}{sub 2}{r arrow}1{ital s}{sup 2} {ital s}{sub 0} intercombination lines and for the 1{ital s}2{ital s} {sup 3}{ital s}{sub 1}{r arrow}1{ital s}{sup 2} {sup 1}{ital s}{sub 0} forbidden line are also presented and show similar differences with theoretical predictions."
                },
                {
                    "id": "R190520",
                    "label": "High Precision Spectroscopic Studies of Few Electron Ions",
                    "doi": "10.1063/1.2948737",
                    "research_field": {
                        "id": "R114010",
                        "label": "Atomic Physics"
                    },
                    "abstract": "in this report some experiments are described in which the lyman ..cap alpha.. lines of hydrogen-like and helium-like argon and iron ions have been measured. the principle of all these experiments was to study with a crystal spectrometer the x rays emitted in flight by the ions, at 90/sup 0/ with respect to the direction of the beam. 5 references, 14 figures, 9 tables."
                },
                {
                    "id": "R190553",
                    "label": "High-precision spectroscopic study of heliumlike iron",
                    "doi": "10.1103/physreva.29.3143",
                    "research_field": {
                        "id": "R114010",
                        "label": "Atomic Physics"
                    },
                    "abstract": "the x-ray spectrum emitted by high-velocity heliumlike iron ions has been studied with a crystal spectrometer. the absolute energies of the n = 2..-->..n = 1 lines have been measured with a precision of 40 ppm. a very good agreement has been found between our experimental values and a very accurate multiconfiguration dirac-fock calculation. the precision of the measurement and the calculation of the energies are such that for the first time the magnetic correlation energy (spin-spin) as well as the screening of quantum-electrodynamic effects can now be appreciated. the contamination of the considered lines by the so-called dielectronic recombination satellites has been studied in great detail by varying the nature and the thickness of the targets."
                }
            ]
        },
        {
            "id": "R186491",
            "label": "Research Practices in RE Contribution",
            "research_fields": [
                {
                    "id": "",
                    "label": ""
                }
            ],
            "properties": [
                "Data analysis",
                "research  question ",
                "Threat to validity",
                "data collection method",
                "research paradigm",
                "research question answer"
            ],
            "papers": [
                {
                    "id": "R108199",
                    "label": "A Little Bird Told Me: Mining Tweets for Requirements and Software Evolution",
                    "doi": "10.1109/re.2017.88",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"twitter is one of the most popular social networks. previous research found that users employ twitter to communicate about software applications via short messages, commonly referred to as tweets, and that these tweets can be useful for requirements engineering and software evolution. however, due to their large number---in the range of thousands per day for popular applications---a manual analysis is unfeasible.in this work we present alertme, an approach to automatically classify, group and rank tweets about software applications. we apply machine learning techniques for automatically classifying tweets requesting improvements, topic modeling for grouping semantically related tweets and a weighted function for ranking tweets according to specific attributes, such as content category, sentiment and number of retweets. we ran our approach on 68,108 collected tweets from three software applications and compared its results against software practitioners' judgement. our results show that alertme is an effective approach for filtering, summarizing and ranking tweets about software applications. alertme enables the exploitation of twitter as a feedback channel for information relevant to software evolution, including end-user requirements.\""
                },
                {
                    "id": "R111988",
                    "label": "A Needle in a Haystack: What Do Twitter Users Say about Software?",
                    "doi": "10.1109/re.2016.67",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "users of the twitter microblogging platform share a vast amount of information about various topics through short messages on a daily basis. some of these so called tweets include information that is relevant for software companies and could, for example, help requirements engineers to identify user needs. therefore, tweets have the potential to aid in the continuous evolution of software applications. despite the existence of such relevant tweets, little is known about their number and content. in this paper we report on the results of an exploratory study in which we analyzed the usage characteristics, content and automatic classification potential of tweets about software applications by using descriptive statistics, content analysis and machine learning techniques. although the manual search of relevant information within the vast stream of tweets can be compared to looking for a needle in a haystack, our analysis shows that tweets provide a valuable input for software companies. furthermore, our results demonstrate that machine learning techniques have the capacity to identify and harvest relevant information automatically."
                },
                {
                    "id": "R112434",
                    "label": "Users \u2014 The Hidden Software Product Quality Experts?: A Study on How App Users Report Quality Aspects in Online Reviews",
                    "doi": "10.1109/re.2017.73",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "[context and motivation] research on eliciting requirements from a large number of online reviews using automated means has focused on functional aspects. assuring the quality of an app is vital for its success. this is why user feedback concerning quality issues should be considered as well [question/problem] but to what extent do online reviews of apps address quality characteristics? and how much potential is there to extract such knowledge through automation? [principal ideas/results] by tagging online reviews, we found that users mainly write about \"usability\" and \"reliability\", but the majority of statements are on a subcharacteristic level, most notably regarding \"operability\", \"adaptability\", \"fault tolerance\", and \"interoperability\". a set of 16 language patterns regarding \"usability\" correctly identified 1,528 statements from a large dataset far more efficiently than our manual analysis of a small subset. [contribution] we found that statements can especially be derived from online reviews about qualities by which users are directly affected, although with some ambiguity. language patterns can identify statements about qualities with high precision, though the recall is modest at this time. nevertheless, our results have shown that online reviews are an unused big data source for quality requirements."
                },
                {
                    "id": "R113008",
                    "label": "Canary: Extracting Requirements-Related Information from Online Discussions",
                    "doi": "10.1109/re.2017.83",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "online discussions about software applications generate a large amount of requirements-related information. this information can potentially be usefully applied in requirements engineering; however currently, there are few systematic approaches for extracting such information. to address this gap, we propose canary, an approach for extracting and querying requirements-related information in online discussions. the highlight of our approach is a high-level query language that combines aspects of both requirements and discussion in online forums. we give the semantics of the query language in terms of relational databases and sql. we demonstrate the usefulness of the language using examples on real data extracted from online discussions. our approach relies on human annotations of online discussions. we highlight the subtleties involved in interpreting the content in online discussions and the assumptions and choices we made to effectively address them. we demonstrate the feasibility of generating high-quality annotations by obtaining them from lay amazon mechanical turk users."
                },
                {
                    "id": "R113067",
                    "label": "Mining User Rationale from Software Reviews",
                    "doi": "10.1109/re.2017.86",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "rationale refers to the reasoning and justification behind human decisions, opinions, and beliefs. in software engineering, rationale management focuses on capturing design and requirements decisions and on organizing and reusing project knowledge. this paper takes a different view on rationale written by users in online reviews. we studied 32,414 reviews for 52 software applications in the amazon store. through a grounded theory approach and peer content analysis, we investigated how users argue and justify their decisions, e.g. about upgrading, installing, or switching software applications. we also studied the occurrence frequency of rationale concepts such as issues encountered or alternatives considered in the reviews and found that assessment criteria like performance, compatibility, and usability represent the most pervasive concept. we then used the truth set of manually labeled review sentences to explore how accurately we can mine rationale concepts from the reviews. support vector classifier, naive bayes, and logistic regression, trained on the review metadata, syntax tree of the review text, and influential terms, achieved a precision around 80% for predicting sentences with alternatives and decisions, with top recall values of 98%. on the review level, precision was up to 13% higher with recall values reaching 99%. we discuss the findings and the rationale importance for supporting deliberation in user communities and synthesizing the reviews for developers."
                },
                {
                    "id": "R113160",
                    "label": "Customer Rating Reactions Can Be Predicted Purely using App Features",
                    "doi": "10.1109/re.2018.00018",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "in this paper we provide empirical evidence that the rating that an app attracts can be accurately predicted from the features it offers. our results, based on an analysis of 11,537 apps from the samsung android and blackberry world app stores, indicate that the rating of 89% of these apps can be predicted with 100% accuracy. our prediction model is built by using feature and rating information from the existing apps offered in the app store and it yields highly accurate rating predictions, using only a few (11-12) existing apps for case-based prediction. these findings may have important implications for requirements engineering in app stores: they indicate that app developers may be able to obtain (very accurate) assessments of the customer reaction to their proposed feature sets (requirements), thereby providing new opportunities to support the requirements elicitation process for app developers."
                },
                {
                    "id": "R113204",
                    "label": "Mining Android App Descriptions for Permission Requirements Recommendation",
                    "doi": "10.1109/re.2018.00024",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"during the development or maintenance of an android app, the app developer needs to determine the app's security and privacy requirements such as permission requirements. permission requirements include two folds. first, what permissions (i.e., access to sensitive resources, e.g., location or contact list) the app needs to request. second, how to explain the reason of permission usages to users. in this paper, we focus on the multiple challenges that developers face when creating permission-usage explanations. we propose a novel framework, clap, that mines potential explanations from the descriptions of similar apps. clap leverages information retrieval and text summarization techniques to find frequent permission usages. we evaluate clap on a large dataset containing 1.4 million android apps. the evaluation results outperform existing state-of-the-art approaches, showing great promise of clap as a tool for assisting developers and permission requirements discovery.\""
                },
                {
                    "id": "R192345",
                    "label": "On the impact of using different templates on creating and understanding user stories",
                    "doi": "10.1109/re51729.2021.00026",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "context: user stories are often used for elicitation and prioritisation of requirements. however, the lack of a widely adopted user story template, covering benefit and the usage (or not) of a persona, can affect user stories\u2019 quality, leading to ambiguity, lack of completeness, or accidental complexity. objectives: our goal was to analyse the differences between 4 alternative user story templates when creating and understanding user stories. methods: we conducted a quasi-experiment. we asked 41 participants to perform creation and understanding tasks with the user story templates. we measured their accuracy, using metrics of task success; their speed, with task duration; visual effort, collected with an eye-tracker; and participants\u2019 perceived effort, evaluated with nasa-tlx. results: regarding the impact of the different templates in creating user stories, we observed statistically significant differences in some of the metrics for accuracy, speed and visual effort. for understanding user stories, we observed small differences in terms of visual effort. conclusions: although some templates outperformed others in a few metrics, no template obtained the best overall result. as such, we found no compelling evidence that one template is \"better\" than the others."
                },
                {
                    "id": "R192386",
                    "label": "A Survey of Instructional Approaches in the Requirements Engineering Education Literature",
                    "doi": "10.1109/re51729.2021.00030",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "requirements engineering (re) has established itself as a core software engineering discipline. it is well acknowledged that good re leads to higher quality software and considerably reduces the risk of failure or exceeding budgets of software development projects. therefore, it is of vital importance to train future software engineers in re and educate future requirements engineers to adequately manage requirements in various projects. however, to date there exists no central concept of what the most useful educational approaches are in re education in order to best interweave theory with practice. to lay the foundation for this important mission, we conducted a systematic literature review. in this paper, we report on the results and provide a synthesis of instructional approaches in re education. findings show that experiential learning through projects, collaboration, and realistic stakeholder involvement are among the most promising trends to teach both re theory and develop student soft skills."
                },
                {
                    "id": "R192398",
                    "label": "Agile Teams\u2019 Perception in Privacy Requirements Elicitation: LGPD\u2019s compliance in Brazil",
                    "doi": "10.1109/re51729.2021.00013",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "context: the implementation of the brazilian general data protection law (lgpd) may impact activities carried out by the software development teams. it is necessary for developers to know the existing techniques and tools to carry out privacy requirements elicitation. objectives: in this research, we investigated the perception of agile software development team members from different organizations, regarding the impact that lgpd will have on the activities of the software development process. methods: we conducted an online survey and a systematic literature review to identify the techniques, methodologies and tools used in the literature to perform privacy requirements elicitation in the context of agile software development (asd). in addition, we also investigated the perception of an agile team from a federal public administration organization regarding the impacts of the obligation to develop software in accordance with the lgpd. results: our findings reveal that agile teams know the concepts related to data privacy legislation, but they do not use the techniques proposed in the literature to perform privacy requirements elicitation. in addition, agile teams face problems with outdated software requirements specifications and stakeholders\u2019 lack of knowledge regarding data privacy. conclusions: agile teams need to improve their knowledge on privacy requirements."
                },
                {
                    "id": "R192428",
                    "label": "Ambiguity and Generality in Natural Language Privacy Policies",
                    "doi": "10.1109/re51729.2021.00014",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "privacy policies are legal documents containing application data practices. these documents are well-established sources of requirements in software engineering. however, privacy policies are written in natural language, thus subject to ambiguity and abstraction. eliciting requirements from privacy policies is a challenging task as these ambiguities can result in more than one interpretation of a given information type (e.g., ambiguous information type \"device information\" in the statement \"we collect your device information\"). to address this challenge, we propose an automated approach to infer semantic relations among information types and construct an ontology to guide requirements authors in the selection of the most appropriate information type terms. our solution utilizes word embeddings and convolutional neural networks (cnn) to classify information type pairs as either hypernymy, synonymy, or unknown. we evaluate our model on a manually-built ontology, yielding predictions that identify hypernymy relations in information type pairs with 0.904 f-1 score, suggesting a large reduction in effort required for ontology construction."
                },
                {
                    "id": "R192445",
                    "label": "Automated Traceability for Domain Modelling Decisions Empowered by Artificial Intelligence",
                    "doi": "10.1109/re51729.2021.00023",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "domain modelling abstracts real-world entities and their relationships in the form of class diagrams for a given domain problem space. modellers often perform domain modelling to reduce the gap between understanding the problem description which expresses requirements in natural language and the concise interpretation of these requirements. however, the manual practice of domain modelling is both time-consuming and error-prone. these issues are further aggravated when problem descriptions are long, which makes it hard to trace modelling decisions from domain models to problem descriptions or vice-versa leading to completeness and conciseness issues. automated support for tracing domain modelling decisions in both directions is thus advantageous. in this paper, we propose an automated approach that uses artificial intelligence techniques to extract domain models along with their trace links. we present a traceability information model to enable traceability of modelling decisions in both directions and provide its proof-of-concept in the form of a tool. the evaluation on a set of unseen problem descriptions shows that our approach is promising with an overall median f2 score of 82.04%. we conduct an exploratory user study to assess the benefits and limitations of our approach and present the lessons learned from this study."
                },
                {
                    "id": "R193022",
                    "label": "Design Decisions in the Construction of Traceability Information Models for Safe Automotive Systems",
                    "doi": "",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "traceability management relies on a supporting model, the traceability information model (tim), that defines which types of relationships exist between which artifacts and contains additional constraints such as multiplicities. constructing a tim that is fit for purpose is crucial to ensure that a traceability strategy yields the desired benefits. however, which design decisions are critical in the construction of tims and which impact they have on the usefulness and applicability of traceability is still an open question. in this paper, we use two cases of tims constructed for safety-critical, automotive systems with industrial safety experts, to identify key design decisions. we also propose a comparison scheme for tims based on a systematic literature review and evaluate the two cases as well as tims from the literature according to the scheme. based on our analyses, we thus derive key insights into tim construction and the design decisions that ensure that a tim is fit for purpose."
                },
                {
                    "id": "R193035",
                    "label": "Exploring explainability: a definition, a model, and a knowledge catalogue",
                    "doi": "",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "the growing complexity of software systems and the influence of software-supported decisions in our society awoke the need for software that is transparent, accountable, and trust-worthy. explainability has been identified as a means to achieve these qualities. it is recognized as an emerging non-functional requirement (nfr) that has a significant impact on system quality. however, in order to incorporate this nfr into systems, we need to understand what explainability means from a software engineering perspective and how it impacts other quality aspects in a system. this allows for an early analysis of the benefits and possible design issues that arise from interrelationships between different quality aspects. nevertheless, explainability is currently under-researched in the domain of requirements engineering and there is a lack of conceptual models and knowledge catalogues that support the requirements engineering process and system design. in this work, we bridge this gap by proposing a definition, a model, and a catalogue for explainability. they illustrate how explainability interacts with other quality aspects and how it may impact various quality dimensions of a system. to this end, we conducted an interdisciplinary systematic literature review and validated our findings with experts in workshops."
                },
                {
                    "id": "R193044",
                    "label": "From Ideas to Expressed Needs: an Empirical Study on the Evolution of Requirements during Elicitation",
                    "doi": "",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "requirements are elicited from the customer and other stakeholders through an iterative process of interviews, prototyping, and other interactive sessions. many communication phenomena may emerge in these early iterations, that lead initial ideas to be transformed, renegotiated, or reframed. understanding how this process takes place can help in solving possible communication issues as well as their consequences. in this work, we perform an exploratory study of descriptive nature to understand in which way requirements get transformed from initial ideas into documented needs. to this end, we select 30 subjects that act as requirements analysts, and we perform a set of elicitation sessions with a fictional customer. the customer is required to study a sample requirements document for a system beforehand and to answer the questions of the analysts about the system. after the elicitation sessions, the analysts produce user stories for the system. these are compared with the original ones by two researchers to assess to which extent and in which way the initial requirements evolved throughout the interactive sessions. our results show that between 30% and 38% of the produced user stories include content that can be fully traced to the initial ones, while the rest of the content is dedicated to new requirements. we also show what types of requirements are introduced through the elicitation process, and how they vary depending on the analyst. our work contributes to theory in requirements engineering, with empirically grounded, quantitative data, concerning the impact of elicitation activities with respect to initial ideas."
                },
                {
                    "id": "R193070",
                    "label": "Non-functional requirements for machine learning: understanding current use and challenges in industry",
                    "doi": "",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "machine learning (ml) is an application of artificial intelligence (ai) that uses big data to produce complex predictions and decision-making systems, which would be challenging to obtain otherwise. to ensure the success of ml-enabled systems, it is essential to be aware of certain qualities of ml solutions (performance, transparency, fairness), known from a requirement engineering (re) perspective as non-functional requirements (nfrs). however, when systems involve ml, nfrs for traditional software may not apply in the same ways; some nfrs may become more prominent or less important; nfrs may be defined over the ml model, data, or the entire system; and nfrs for ml may be measured differently. in this work, we aim to understand the state-of-the-art and challenges of dealing with nfrs for ml in industry. we interviewed ten engineering practitioners working with nfrs and ml. we find examples of (1) the identification and measurement of nfrs for ml, (2) identification of more and less important nfrs for ml, and (3) the challenges associated with nfrs and ml in the industry. this knowledge paints a picture of how ml-related nfrs are treated in practice and helps to guide future re for ml efforts."
                },
                {
                    "id": "R193089",
                    "label": "On the Role of User Feedback in Software Evolution: a Practitioners\u2019 Perspective",
                    "doi": "",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "user feedback is indispensable in software evolution. previous work has proposed ways for automatically extracting requirements, bug reports and other valuable information from feedback. however, little is actually known about how user feedback\u2014 especially the one available through newer channels, such as social media\u2014is incorporated in development processes. to date, only a few case studies discuss the matter and the results are not always consistent. we carried out a mixed methods study to understand the current state of practice of harnessing user feedback in software development. qualitatively, we performed interviews with 18 software practitioners to get a deeper understanding of the role of user feedback in software evolution. quantitatively, we surveyed 101 software practitioners to cross-validate the interview findings and improve the generalizability of the results. we found that feedback is captured to (1) identify bugs, features and usability issues, (2) get a better understanding of the user, and (3) prioritize requirements. our results indicate that analyzing feedback is time-consuming and has a number of challenges. among them, feedback is typically analyzed manually and is spread over a wide range of channels and company departments. our findings stress the current importance for cross-department cooperation and call for the exploration of tools that can centralize user feedback."
                },
                {
                    "id": "R193108",
                    "label": "Perspectives on Regulatory Compliance in Software Engineering",
                    "doi": "",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"compliance reviews within a software organization are internal attempts to verify regulatory and security requirements during product development before its release. however, these reviews are not enough to adequately assess and address regulatory and security requirements throughout a software's development lifecycle. we believe requirements engineers can benefit from an improved understanding of how software practitioners treat and perceive compliance requirements. this paper describes an interview study seeking to understand how regulatory and security standard requirements are addressed, how burdensome they may be for businesses, and how our participants perceived them in the software development lifecycle. we interviewed 15 software practitioners from 13 organizations with different roles in the software development process and working in various industry domains, including big tech, healthcare, data analysis, finance, and small businesses. our findings suggest that, for our participants, the software release process is the ultimate focus for regulatory and security compliance reviews. also, most participants suggested that having a defined process for addressing compliance requirements was freeing rather than burdensome. finally, participants generally saw compliance requirements as an investment for both employees and customers. these findings may be unintuitive, and we discuss seven lessons this work may hold for requirements engineering.\""
                },
                {
                    "id": "R193295",
                    "label": "The practical role of context modeling in the elicitation of context-aware functionalities: a survey",
                    "doi": "",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "context-aware functionalities are functionalities that consider the context to produce a certain system behavior, typically an adaptation or recommendation. as contextual elements such as time, location, weather, user activity, device characteristics, network status, and countless others are becoming increasingly more accessible, the potential for adding context awareness to applications is enormous. identifying novel, unexpected, and even delightful context-aware functionalities in practice can be challenging, though: what context information is relevant for a given user task? how can contextual elements be combined? what if there is a large number of contextual elements? context modeling has been described in the literature as an essential aspect in the elicitation of context-aware functionalities; however, reports on the state of the practice are rare. in this study, we conducted a survey with industrial practitioners, mostly experienced professionals from large enterprises, to investigate how context models and context-modeling activities have been used to support the elicitation of context-aware functionalities. the results indicate a gap between research and industry: context models are rarely used in practice, and context-modeling activities such as analysis of relevance and especially analysis of combinations of contextual elements have been overlooked due to their high complexity, despite practitioners recognizing their importance."
                },
                {
                    "id": "R193308",
                    "label": "The Role of Linguistic Relativity on the Identification of Sustainability Requirements: An Empirical Study",
                    "doi": "",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "linguistic-relativity-theory states that language and its structure influence people\u2019s world view and cognition. we investigate how this theory impacts the identification of requirements in practice. to this end, we conducted two controlled experiments with 101 participants. we randomly showed participants a set of requirements dimensions (i.e. a language structure) either with a focus on software quality or on sustainability and asked them to identify the requirements for a grocery shopping app according to these dimensions. participants of the control group were not given any dimensions. the results show that the use of requirements dimensions significantly increases the number of identified requirements in comparison to the control group. furthermore, participants who were given the sustainability dimensions identified more sustainability requirements. in follow up interviews with 16 practitioners, the interviewees reported benefits of the dimensions such as a holistic guidance but were also concerned about the customers acceptance. furthermore, they stated challenges of implementing sustainability dimensions in the daily business but also suggested solutions like establishing sustainability as a common standard. our study indicates that carefully structuring requirements engineering along sustainability dimensions can guide development teams towards considering and ensuring software sustainability."
                },
                {
                    "id": "R193330",
                    "label": "Towards Achieving Trust Through Transparency and Ethics",
                    "doi": "",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "the ubiquitous presence of software in the products we use, together with artificial intelligence in these products, has led to an increasing need for consumer trust. consumers often lose faith in products, and the lack of trust propagates to the companies behind them. this is even more so in mission-critical systems such as autonomous vehicles and clinical support systems. this paper follows grounded theory principles to elicit knowledge related to trust, ethics, and transparency. we approach these qualities as non-functional requirements (nfrs), aiming to build catalogs to subsidize the construction of socially responsible software. the corpus we have used was built on a selected collection of literature on corporate social responsibility, with an emphasis on business ethics. our challenge is how to encode the social perspective knowledge, mainly through the view of corporate social responsibility, on how organizations or institutions achieve trustworthiness. since our ground perspective is that of nfrs, results are presented by a catalogue of trust as a non-functional requirement, represented as a softgoal interdependency graph (sig). the sig language helps software engineers in understanding alternatives they have to improve trust in software products."
                },
                {
                    "id": "R193336",
                    "label": "Unsupervised Topic Discovery in User Comments",
                    "doi": "",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "on social media platforms like twitter, users regularly share their opinions and comments with software vendors and service providers. popular software products might get thousands of user comments per day. research has shown that such comments contain valuable information for stakeholders, such as feature ideas, problem reports, or support inquiries. however, it is hard to manually manage and grasp a large amount of user comments, which can be redundant and of a different quality. consequently, researchers suggested automated approaches to extract valuable comments, e.g., through problem report classifiers. however, these approaches do not aggregate semantically similar comments into specific aspects to provide insights like how often users reported a certain problem.we introduce an approach for automatically discovering topics composed of semantically similar user comments based on deep bidirectional natural language processing algorithms. stakeholders can use our approach without the need to configure critical parameters like the number of clusters. we present our approach and report on a rigorous multiple-step empirical evaluation to assess how cohesive and meaningful the resulting clusters are. each evaluation step was peer-coded and resulted in inter-coder agreements of up to 98%, giving us high confidence in the approach. we also report a thematic analysis on the topics discovered from tweets in the telecommunication domain."
                },
                {
                    "id": "R193341",
                    "label": "What can Open Domain Model Tell Us about the Missing Software Requirements: A Preliminary Study",
                    "doi": "",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "completeness is one of the most important attributes of software requirement specification. unfortunately, incompleteness is one of the most difficult violations to detect. some approaches have been proposed to detect missing requirements based on the requirement-oriented domain model. however, these kinds of models are actually lack for lots of domains. fortunately, the domain models constructed for different purposes can usually be found online. this raises a question: whether or not these domain models are useful for finding the missing functional information in requirement specification? to explore this question, we design and conduct a preliminary study by computing the overlapping rate between the entities in domain models and the concepts of natural language software requirements, and then digging into four regularities of the occurrence of these entities(concepts) based on two example domains. the usefulness of these regularities, especially the one based our proposed metric ahme (with 54% and 70% of f2 on the two domains), has been initially evaluated with an additional experiment."
                },
                {
                    "id": "R193357",
                    "label": "What\u2019s up with Requirements Engineering for Artificial Intelligence Systems?",
                    "doi": "",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "in traditional approaches to building software systems (that do not include an artificial intelligent (ai) or machine learning (ml) component), requirements engineering (re) activities are well-established and researched. however, building software systems with one or more ai components may depend heavily on data with limited or no insight into the system\u2019s workings. therefore, engineering such systems poses significant new challenges to re. our search showed that literature has focused on using ai to manage re activities, with limited research on re for ai (re4ai). our study\u2019s main objective was to investigate current approaches in writing requirements for ai/ml systems, identify available tools and techniques used to model requirements, and find existing challenges and limitations. we performed a systematic literature review (slr) of current re4ai methods and identified 27 primary studies. using these studies, we analysed the key tools and techniques used to specify and model requirements and found several challenges and limitations of existing re4ai practices. we further provide recommendations for future research, based on our analysis of the primary studies and mapping to industry guidelines in google pair). the slr findings highlighted that present re applications were not adaptive to manage most ai/ml systems and emphasised the need to provide new techniques and tools to support re4ai."
                },
                {
                    "id": "R193371",
                    "label": "Automated recommendation of templates for legal requirements",
                    "doi": "10.1109/RE48521.2020.00027",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "[context] in legal requirements elicitation, requirements analysts need to extract obligations from legal texts. however, legal texts often express obligations only indirectly, for example, by attributing a right to the counterpart. this phenomenon has already been described in the requirements engineering (re) literature [1]. [objectives] we investigate the use of requirements templates for the systematic elicitation of legal requirements. our work is motivated by two observations: (1) the existing literature does not provide a harmonized view on the requirements templates that are useful for legal re; (2) despite the promising recent advancements in natural language processing (nlp), automated support for legal re through the suggestion of requirements templates has not been achieved yet. our objective is to take steps toward addressing these limitations. [methods] we review and reconcile the legal requirement templates proposed in re. subsequently, we conduct a qualitative study to define nlp rules for template recommendation. [results and conclusions] our contributions consist of (a) a harmonized list of requirements templates pertinent to legal re, and (b) rules for the automatic recommendation of such templates. we evaluate our rules through a case study on 400 statements from two legal domains. the results indicate a recall and precision of 82,3% and 79,8%, respectively. we show that introducing some limited interaction with the analyst considerably improves accuracy. specifically, our human-feedback strategy increases recall by 12% and precision by 10,8%, thus yielding an overall recall of 94,3% and overall precision of 90,6%."
                },
                {
                    "id": "R193385",
                    "label": "The way it makes you feel predicting users\u2019 engagement during interviews with biofeedback and supervised learning",
                    "doi": "",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "capturing users\u2019 engagement is crucial for gathering feedback about the features of a software product. in a market-driven context, current approaches to collect and analyze users\u2019 feedback are based on techniques leveraging information extracted from product reviews and social media. these approaches are hardly applicable in bespoke software development, or in contexts in which one needs to gather information from specific users. in such cases, companies need to resort to face-to-face interviews to get feedback on their products. in this paper, we propose to utilize biofeedback to complement interviews with information about the engagement of the user on the discussed features and topics. we evaluate our approach by interviewing users while gathering their biometric data using an empatica e4 wristband. our results show that we can predict users\u2019 engagement by training supervised machine learning algorithms on the biometric data. the results of our work can be used to facilitate the prioritization of product features and to guide the interview based on users\u2019 engagement."
                },
                {
                    "id": "R193406",
                    "label": "Theory as a source of software requirements",
                    "doi": "",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "today, when undertaking requirements elicitation, engineers attend to the needs and wants of the user groups considered relevant for the software system. however, answers to some relevant question (e.g., how to improve adoption of the intended system) cannot always be addressed through direct need and want elicitation. using an example of energy demand-response systems, this paper demonstrates that use of grounded theory analysis can help address such questions. the theory emerging from such analysis produces a set of additional requirements which cannot be directly elicited from individuals/groups, and would otherwise be missed out. thus, we demonstrate that the theory generated through grounded theory analysis can serve as an additional valuable source of software system requirements."
                },
                {
                    "id": "R193418",
                    "label": "Voice of the users: A demographic study of software feedback behaviour",
                    "doi": "",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "user feedback on mobile app stores, product forums, and on social media can contain product development insights. there has been a lot of recent research studying this feedback and developing methods to automatically extract requirement-related information. this feedback is generally considered to be the \u201cvoice of the users\u201d; however, only a subset of software users provide online feedback. if the demographics of the online feedback givers are not representative of the user base, this introduces the possibility of developing software that does not meet the needs of all users. it is, therefore, important to understand who provides online feedback to ensure the needs of underrepresented groups are not being missed.in this work, we directly survey 1040 software users about their feedback habits, software use, and demographic information. their responses indicate that there are statistically significant differences in who gives feedback on each online channel, with respect to traditional demographics (gender, age, etc). we also identify key differences in what motivates software users to engage with each of the three channels. our findings provide valuable context for requirements elicited from online feedback and show that considering information from all channels will provide a more comprehensive view of user needs."
                },
                {
                    "id": "R193467",
                    "label": "Cutting through the jungle: Disambiguating model-based traceability terminology",
                    "doi": "",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "traceability, a classic requirements engineering topic, is increasingly used in the context of model-based engineering. however, researchers and practitioners lack a concise terminology to discuss aspects of requirements traceability in situations in which engineers heavily rely on models and model-based engineering. while others have previously surveyed the domain, no one has so far provided a clear, unambiguous set of terms that can be used to discuss traceability in such a context. we therefore set out to cut a path through the jungle of terminology for model-based traceability, ground it in established terminology from requirements engineering, and derive an unambiguous set of relevant terms. we also map the terminology used in existing primary and secondary studies to our taxonomy to show differences and commonalities. the contribution of this paper is thus a terminology for model-based traceability that allows requirements engineers and engineers working with models to unambiguously discuss their joint traceability efforts."
                },
                {
                    "id": "R193473",
                    "label": "Continual human value analysis in software development: A goal model based approach",
                    "doi": "",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "software failures that demonstrate violations of human values can result in financial losses, reputation damages and social implications. therefore, integrating human values into software is vital to satisfy stakeholder needs. however, developing methodological approaches that allow systematic integration of human values throughout the software development life cycle is an open challenge. this paper proposes the continual value(s) assessment (cva) framework that uses extended goal and feature modeling techniques to support systematic integration, tracing and evaluation of human values in software systems. the cva framework prescribes (i) brainstorming of value implications of system features based on conventional system artefacts and (ii) the expansion of the existing set of system features to better serve stakeholder values expectations. in a pilot study, we use an emergency alarm system for the elderly to demonstrate the feasibility of the framework. we further discuss the challenges we faced while applying the framework and present the lessons learned from the pilot study."
                },
                {
                    "id": "R193481",
                    "label": "Extracting and classifying requirements from software engineering contracts",
                    "doi": "",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "in this paper, we present our work on extracting and classifying requirements from large software engineering contracts. typically, the process of requirements elicitation begins after a contractual agreement is signed by all participants. our interactions with the legal compliance team in a large vendor organization reveal that business contracts can help in the identification of high-level requirements relevant to the success of software engineering projects. we posit that requirements engineering as a discipline has an even wider scope than software engineering of which it is traditionally considered to be a sub-discipline. this is because software engineering-specific requirements are but a part of the success story of any large project. the requirements that emerge from contracts are obligatory in nature, whether or not they pertain to core software development. therefore, it is important that these are extracted and classified for the benefit of software engineers and other stakeholders responsible for a project. we discuss the results of an exploratory study and a range of experiments from the use of regular expressions to bidirectional encoder representations from transformers for automating the extraction and classification of requirements from software engineering contracts. with bidirectional encoder representations from transformers, we obtained a high f-score of greater than eighty four percent for classification of requirements."
                },
                {
                    "id": "R193490",
                    "label": "Which app features are being used? Learning app feature usages from interaction data",
                    "doi": "",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "in the dynamic and fast-growing app market, monitoring and understanding how past releases are actually being used is indispensable for successful app maintenance and evolution. current app usage analytics tools either log execution events, e.g., in stack traces, or general usage information such as the app activation time, location, and device. in this paper, we focus on analyzing the usages of the single app features as described in release notes and app pages. we suggest monitoring nine app-independent, privacy-friendly interaction events for training a machine learning model to learn app feature usages. we conducted a crowdsourcing study with 55 participants who labeled 5,815 feature usages of 170 unique apps for 18 days. our within-apps evaluation shows that we could achieve encouraging precision and recall values already with ten labeled feature usages. for certain popular features such as browse newsfeed or send an email, we achieved f1 values above 88%. betweenapps feature learning seems feasible with f1 values of up to 86%."
                },
                {
                    "id": "R193798",
                    "label": "Classifying user requirements from online feedback in small dataset environments using deep learning",
                    "doi": "",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "an overwhelming number of users access app repositories like app store/google play and social media platforms like twitter, where they provide feedback on digital experiences. this vast textual corpus comprising user feedback has the potential to unearth detailed insights regarding the users\u2019 opinions on products and services. various tools have been proposed that employ natural language processing (nlp) and traditional machine learning (ml) based models as an inexpensive mechanism to identify requirements in user feedback. however, they fall short on their classification accuracy over unseen data due to factors like the cost of generating voluminous de-biased labeled datasets and general inefficiency. recently, van vliet et al. [1] achieved state-of-the-art results extracting and classifying requirements from user reviews through traditional crowdsourcing. based on their reference classification tasks and outcomes, we successfully developed and validated a deep-learning-backed artificial intelligence pipeline to achieve a state-of-the-art averaged classification accuracy of \u223c87% on standard tasks for user feedback analysis. this approach, which comprises a bert-based sequence classifier, proved effective even in extremely low-volume dataset environments. additionally, our approach drastically reduces the time and costs of evaluation, and improves on the accuracy measures achieved using traditional ml-/nlp-based techniques."
                },
                {
                    "id": "R193828",
                    "label": "Mining reddit as a new source for software requirements",
                    "doi": "",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "mining app stores and social media has proven to be a good source for collecting user feedback to foster requirements engineering and software evolution. recent literature on mining software-related data from social platforms, such as twitter and facebook, shows that it complements app store mining. however, there are many other platforms where users discuss and provide feedback on software applications that are not thoroughly researched and analysed. one of such platforms is reddit. in this paper, we introduce reddit as a new potential data source and explore if and how requirements engineering and software evolution can benefit from obtaining user feedback from reddit. we also present an exploratory study in which we analysed the usage characteristics (i.e., frequency of posts, number of comments, and number of users for each subreddit) of reddit posts about software applications. furthermore, we examined the content of the posts and the results reveal that almost 54% of posts contain useful information. finally, we investigated the potential of automatic classification and applied machine learning algorithms to unstructured and noisy reddit data to perform automated classification into the categories of bug reports, feature related, and irrelevant. we found that the support vector machine algorithm with the f1-score of 84% can be effective in categorizing reddit posts. our results show that reddit posts provide useful feedback on software applications that can foster requirements engineering and software evolution."
                },
                {
                    "id": "R193849",
                    "label": "TEM: a transparency engineering methodology enabling users\u2019 trust judgement",
                    "doi": "",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "transparency is key to enhancing users\u2019 trust by enabling their judgment on the outcomes and consequences of a system\u2019s operations. this paper presents the transparency engineering methodology (tem) to generate transparency requirements that enable users\u2019 trust judgement. the idea is to identify where transparency is lacking and to address this through patterns augmenting the specification of data, use case, and process requirements. due to the complexity of software, it is impossible (and undesirable) to achieve full transparency throughout the system. however, transparency can be improved for selected system aspects. this is demonstrated using the results from an industrial case study with a medical technology company where, with the help of tem, existing functional requirements were refined, and transparency requirements generated systematically."
                },
                {
                    "id": "R193866",
                    "label": "The Rise and Fall of COVID-19 Contact-Tracing Apps: when NFRs Collide with Pandemic",
                    "doi": "10.1109/RE51729.2021.00017",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "to complement the manual contact-tracing methods, a flood of coronavirus-related apps was launched in the first half of 2020. despite the incredible promises made by the governments, contact-tracing apps did not live up to expectations. we provide a contextual perspective of the government commissioned contact-tracing apps from four countries to understand the non-functional requirements (nfrs) and socio-technical factors that hindered the success of these apps. we collected the user reviews from the app stores for ios and android versions and identified top news articles related to each app. our analysis revealed that the dominant factors behind the negligible success of these apps are complex and entangled with the cultural and political dimensions rather than being just technical. the multilayer diversity of the target users also impacted the design and development of contact-tracing apps in an extremely challenging situation. this perspective paper brings into light important elements, such as politics and socio-cultural aspects that should be studied in the design of contact-tracing apps, and public apps in general."
                },
                {
                    "id": "R193920",
                    "label": "Same same but different: Finding similar user feedback across multiple platforms and languages",
                    "doi": "10.1109/RE48521.2020.00017",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "users submit feedback about the software they use through application distributions platforms, i.e., app stores, and social media. previous research has found that this type of feedback contains valuable information for software evolution, such as bug reports, or feature requests. however, popular applications receive thousands of feedback entities per day, making their manual analysis unrealistic. in this work, we present an approach to automatically identify similar user feedback across different languages and platforms. at the core of the approach is a word aligner that aligns words based on their semantic similarity and the similarity of their local semantic contexts. additionally, we make use of machine translation, sentiment analysis, and text classification, to extract the sentiment polarity and content nature of user feedback written in different languages. we use the results of these components to compute a similarity score between user feedback pairs. we evaluated our approach on user feedback entities written in four different languages, and retrieved from five different mobile applications obtained from four different app stores and social networking sites. the obtained results are encouraging. compared to human assessment, the overall performance for monolingual user feedback pairs yielded a strong correlation of 0.79. for the crosslingual feedback pairs the correlation was also strong, with a value of 0.78."
                },
                {
                    "id": "R193929",
                    "label": "NoRBERT: Transfer learning for requirements classification",
                    "doi": "",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "classifying requirements is crucial for automatically handling natural language requirements. the performance of existing automatic classification approaches diminishes when applied to unseen projects because requirements usually vary in wording and style. the main problem is poor generalization. we propose norbert that fine-tunes bert, a language model that has proven useful for transfer learning. we apply our approach to different tasks in the domain of requirements classification. we achieve similar or better results f1-scores of up to 94%) on both seen and unseen projects for classifying functional and non-functional requirements on the promise nfr dataset. norbert outperforms recent approaches at classifying non-functional requirements subclasses. the most frequent classes are classified with an average f1-score of 87%. in an unseen project setup on a relabeled promise nfr dataset, our approach achieves an improvement of 15 percentage points in average f1 score compared to recent approaches. additionally, we propose to classify functional requirements according to the included concerns, i.e., function, data, and behavior. we labeled the functional requirements in the promise nfr dataset and applied our approach. norbert achieves an f1-score of up to 92%. overall, norbert improves requirements classification and can be applied to unseen projects with convincing results."
                },
                {
                    "id": "R74547",
                    "label": "Supporting Requirements Elicitation by Tool-Supported Video Analysis",
                    "doi": "10.1109/re.2016.10",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"workshops are an established technique for requirements elicitation. a lot of information is revealed during a workshop, which is generally captured via textual minutes. the scribe suffers from a cognitive overload due to the difficulty of gathering all information, listening and writing at the same time. video recording is used as additional option to capture more information, including non-verbal gestures. since a workshop can take several hours, the recorded video will be long and may be disconnected from the scribe's notes. therefore, the weak and unclear structure of the video complicates the access to the recorded information, for example in subsequent requirements engineering activities. we propose the combination of textual minutes and video with a software tool. our objective is connecting textual notes with the corresponding part of the video. by highlighting relevant sections of a video and attaching notes that summarize those sections, a more useful structure can be achieved. this structure allows an easy and fast access to the relevant information and their corresponding video context. thus, a scribe's overload can be mitigated and further use of a video can be simplified. tool-supported analysis of such an enriched video can facilitate the access to all communicated information of a workshop. this allows an easier elicitation of high-quality requirements. we performed a preliminary evaluation of our approach in an experimental set-up with 12 participants. they were able to elicit higher-quality requirements with our software tool.\""
                },
                {
                    "id": "R76792",
                    "label": "Mining Twitter Feeds for Software User Requirements",
                    "doi": "10.1109/re.2017.14",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"twitter enables large populations of end-users of software to publicly share their experiences and concerns about software systems in the form of micro-blogs. such data can be collected and classified to help software developers infer users' needs, detect bugs in their code, and plan for future releases of their systems. however, automatically capturing, classifying, and presenting useful tweets is not a trivial task. challenges stem from the scale of the data available, its unique format, diverse nature, and high percentage of irrelevant information and spam. motivated by these challenges, this paper reports on a three-fold study that is aimed at leveraging twitter as a main source of software user requirements. the main objective is to enable a responsive, interactive, and adaptive data-driven requirements engineering process. our analysis is conducted using 4,000 tweets collected from the twitter feeds of 10 software systems sampled from a broad range of application domains. the results reveal that around 50% of collected tweets contain useful technical information. the results also show that text classifiers such as support vector machines and naive bayes can be very effective in capturing and categorizing technically informative tweets. additionally, the paper describes and evaluates multiple summarization strategies for generating meaningful summaries of informative software-relevant tweets.\""
                },
                {
                    "id": "R76818",
                    "label": "App Review Analysis Via Active Learning: Reducing Supervision Effort without Compromising Classification Accuracy",
                    "doi": "10.1109/re.2018.00026",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "automated app review analysis is an important avenue for extracting a variety of requirements-related information. typically, a first step toward performing such analysis is preparing a training dataset, where developers (experts) identify a set of reviews and, manually, annotate them according to a given task. having sufficiently large training data is important for both achieving a high prediction accuracy and avoiding overfitting. given millions of reviews, preparing a training set is laborious. we propose to incorporate active learning, a machine learning paradigm, in order to reduce the human effort involved in app review analysis. our app review classification framework exploits three active learning strategies based on uncertainty sampling. we apply these strategies to an existing dataset of 4,400 app reviews for classifying app reviews as features, bugs, rating, and user experience. we find that active learning, compared to a training dataset chosen randomly, yields a significantly higher prediction accuracy under multiple scenarios."
                },
                {
                    "id": "R78392",
                    "label": "Bug report, feature request, or simply praise? On automatically classifying app reviews",
                    "doi": "10.1109/re.2015.7320414",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "app stores like google play and apple appstore have over 3 million apps covering nearly every kind of software and service. billions of users regularly download, use, and review these apps. recent studies have shown that reviews written by the users represent a rich source of information for the app vendors and the developers, as they include information about bugs, ideas for new features, or documentation of released features. this paper introduces several probabilistic techniques to classify app reviews into four types: bug reports, feature requests, user experiences, and ratings. for this we use review metadata such as the star rating and the tense, as well as, text classification, natural language processing, and sentiment analysis techniques. we conducted a series of experiments to compare the accuracy of the techniques and compared them with simple string matching. we found that metadata alone results in a poor classification accuracy. when combined with natural language processing, the classification precision got between 70-95% while the recall between 80-90%. multiple binary classifiers outperformed single multiclass classifiers. our results impact the design of review analytics tools which help app vendors, developers, and users to deal with the large amount of reviews, filter critical reviews, and assign them to the appropriate stakeholders."
                },
                {
                    "id": "R78432",
                    "label": "Software Feature Request Detection in Issue Tracking Systems",
                    "doi": "10.1109/re.2016.8",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "communication about requirements is often handled in issue tracking systems, especially in a distributed setting. as issue tracking systems also contain bug reports or programming tasks, the software feature requests of the users are often difficult to identify. this paper investigates natural language processing and machine learning features to detect software feature requests in natural language data of issue tracking systems. it compares traditional linguistic machine learning features, such as \"bag of words\", with more advanced features, such as subject-action-object, and evaluates combinations of machine learning features derived from the natural language and features taken from the issue tracking system meta-data. our investigation shows that some combinations of machine learning features derived from natural language and the issue tracking system meta-data outperform traditional approaches. we show that issues or data fields (e.g. descriptions or comments), which contain software feature requests, can be identified reasonably well, but hardly the exact sentence. finally, we show that the choice of machine learning algorithms should depend on the goal, e.g. maximization of the detection rate or balance between detection rate and precision. in addition, the paper contributes a double coded gold standard and an open-source implementation to further pursue this topic."
                },
                {
                    "id": "R193898",
                    "label": "A deep context-wise method for coreference detection in natural language requirements",
                    "doi": "",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "requirements are usually written by different stakeholders with diverse backgrounds and skills and evolve continuously. therefore inconsistency caused by specialized jargons and different domains, is inevitable. in particular, entity coreference in requirement engineering (re) is that different linguistic expressions refer to the same real-world entity. it leads to misconception about technical terminologies, and impacts the readability and understandability of requirements negatively. manual detection entity coreference is labor-intensive and time-consuming. in this paper, we propose a deep context-wise semantic method named deepcoref to entity coreference detection. it consists of one fine-tuning bert model for context representation and a word2vec-based network for entity representation. we use a multi-layer perception in the end to fuse and make a trade-off between two representations for obtaining a better representation of entities. the input of the network is requirement contextual text and related entities, and the output is the predictive label to infer whether two entities are coreferent. the evaluation on industry data shows that our approach significantly outperforms three baselines with average precision and recall of 96.10% and 96.06% respectively. we also compare deepcoref with three variants to demonstrate the performance enhancement from different components."
                },
                {
                    "id": "R193925",
                    "label": "Trace link recovery using semantic relation graphs and spreading activation",
                    "doi": "10.1109/RE48521.2020.00015",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "trace link recovery tries to identify and link related existing requirements with each other to support further engineering tasks. existing approaches are mainly based on algebraic information retrieval or machine-learning. machinelearning approaches usually demand reasonably large and labeled datasets to train. algebraic information retrieval approaches like distance between tf-idf scores also work on smaller datasets without training but are limited in providing explanations for trace links. in this work, we present a trace link recovery approach that is based on an explicit representation of the content of requirements as a semantic relation graph and uses spreading activation to answer trace queries over this graph. our approach is fully automated including an nlp pipeline to transform unrestricted natural language requirements into a graph. we evaluate our approach on five common datasets. depending on the selected configuration, the predictive power strongly varies. with the best tested configuration, the approach achieves a mean average precision of 40% and a lag of 50%. even though the predictive power of our approach does not outperform state-of-the-art approaches, we think that an explicit knowledge representation is an interesting artifact to explore in trace link recovery approaches to generate explanations and refine results."
                },
                {
                    "id": "R194054",
                    "label": "The Lack of Shared Understanding of Non-Functional Requirements in Continuous Software Engineering: Accidental or Essential?",
                    "doi": "10.1109/re48521.2020.00021",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "building shared understanding of requirements is key to ensuring downstream software activities are efficient and effective. however, in continuous software engineering (cse) some lack of shared understanding is an expected, and essential, part of a rapid feedback learning cycle. at the same time, there is a key trade-off with avoidable costs, such as rework, that come from accidental gaps in shared understanding. this tradeoff is even more challenging for non-functional requirements (nfrs), which have significant implications for product success. comprehending and managing nfrs is especially difficult in small, agile organizations. how such organizations manage shared understanding of nfrs in cse is understudied. we conducted a case study of three small organizations scaling up cse to further understand and identify factors that contribute to lack of shared understanding of nfrs, and its relationship to rework. our in-depth analysis identified 41 nfr-related software tasks as rework due to a lack of shared understanding of nfrs. of these 41 tasks 78% were due to avoidable (accidental) lack of shared understanding of nfrs. using a mixed-methods approach we identify factors that contribute to lack of shared understanding of nfrs, such as the lack of domain knowledge, rapid pace of change, and cross-organizational communication problems. we also identify recommended strategies to mitigate lack of shared understanding through more effective management of requirements knowledge in such organizations. we conclude by discussing the complex relationship between shared understanding of requirements, rework and, cse."
                },
                {
                    "id": "R194084",
                    "label": "Requirements Dependency Extraction by Integrating Active Learning with Ontology-Based Retrieval",
                    "doi": "10.1109/re48521.2020.00020",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "context: incomplete or incorrect detection of requirement dependencies has proven to result in reduced release quality and substantial rework. additionally, the extraction of dependencies is challenging since requirements are mostly documented in natural language, which makes it a cognitively difficult task. moreover, with ever-changing and new requirements, a manual analysis process must be repeated, which imposes extra hardship even for domain experts.objective: the three main objectives of this research are: 1) proposing a new dependency extraction method using a variant of active learning (al). 2) evaluating this al and ontology-based retrieval (obr) as baseline methods for dependency extraction on the two industrial data sets. 3) analyzing the value gained from integrating these diverse approaches to form two hybrid methods.method: building on the general al, ensemble and semi-supervised machine learning, a variant of al was developed, which was further integrated with obr to form two hybrid methods (hybrid1, hybrid2) for extracting three types of dependencies (requires, refines, other): hybrid1 used obr as a substitute for human expert; hybrid2 used dependencies extracted through the obr as an additional input for training set in al.results: for two industrial case studies, al extracted more dependencies than obr. hybrid1 showed improvement for both data sets. for one of them, f1 score increased to 82.6% compared to the al baseline score of 49.9%. hybrid2 increased the accuracy by 25% to the level of 75.8% compared to the al baseline accuracy. obr also complemented the al approach by reducing 50% of the human effort."
                },
                {
                    "id": "R194093",
                    "label": "OASIS: Weakening User Obligations for Security-critical Systems",
                    "doi": "10.1109/re48521.2020.00023",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "security-critical systems typically place some requirements on the behaviour of their users, obliging them to follow certain instructions when using those systems. security vulnerabilities can arise when users do not fully satisfy their obligations. in this paper, we propose an approach that improves system security by ensuring that attack scenarios are mitigated even when the users deviate from their expected behaviour. the approach uses structured transition systems to present and reason about user obligations. the aim is to identify potential vulnerabilities by weakening the assumptions on how the user will behave. we present an algorithm that combines iterative abstraction and controller synthesis to produce a new software specification that maintains the satisfaction of security requirements while weakening user obligations. we demonstrate the feasibility of our approach through two examples from the e-voting and e-commerce domains."
                },
                {
                    "id": "R194114",
                    "label": "How developers believe Invisibility impacts NFRs related to User Interaction",
                    "doi": "10.1109/re48521.2020.00022",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "the advance of ubiquitous computing (ubicomp) and internet of things (iot) brought a new set of non-functional requirements (nfrs), especially related to human-computer interaction (hci). invisibility is one of these nfrs, and it refers to either the merging of technology in the user environment or the decrease of the interaction workload. this new nfr may impact traditional nfrs (e.g., usability), revealing positive correlations, when one nfr helps another, and negative correlations, when a procedure favors an nfr but creates difficulty for another one. software engineers need to know about these correlations, so they can select appropriate strategies to satisfy invisibility and traditional nfrs. correlations between nfrs are usually stored in catalogs, which is a well-defined body of knowledge gathered from previous experience. although invisibility has been recently cataloged with development strategies, the literature still lacks catalogs with correlations for this nfr. therefore, this work aims at capturing and cataloging invisibility correlations for ubicomp and iot systems. to do that, we also propose to systematize the definition of correlations using the following well-defined research methods: interview, content analysis and questionnaire. as a result, we defined a catalog with 110 positive and negative correlations with 9 nfrs. this well-defined body of knowledge is useful for supporting software engineers to select strategies to satisfy invisibility and other nfrs related to user interaction."
                },
                {
                    "id": "R194125",
                    "label": "Data-driven Risk Management for Requirements Engineering: An Automated Approach based on Bayesian Networks",
                    "doi": "10.1109/re48521.2020.00024",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "requirements engineering (re) is a means to reduce the risk of delivering a product that does not fulfill the stakeholders\u2019 needs. therefore, a major challenge in re is to decide how much re is needed and what re methods to apply. the quality of such decisions is strongly based on the re expert\u2019s experience and expertise in carefully analyzing the context and current state of a project. recent work, however, shows that lack of experience and qualification are common causes for problems in re. we trained a series of bayesian networks on data from the napire survey to model relationships between re problems, their causes, and effects in projects with different contextual characteristics. these models were used to conduct (1) a post-mortem (diagnostic) analysis, deriving probable causes of suboptimal re performance, and (2) to conduct a preventive analysis, predicting probable issues a young project might encounter. the method was subject to a rigorous cross-validation procedure for both use cases before assessing its applicability to real-world scenarios with a case study."
                },
                {
                    "id": "R194139",
                    "label": "An AI-assisted Approach for Checking the Completeness of Privacy Policies Against GDPR",
                    "doi": "10.1109/re48521.2020.00025",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "privacy policies are critical for helping individuals make informed decisions about their personal data. in europe, privacy policies are subject to compliance with the general data protection regulation (gdpr). if done entirely manually, checking whether a given privacy policy complies with gdpr is both time-consuming and error-prone. automated support for this task is thus advantageous. at the moment, there is an evident lack of such support on the market. in this paper, we tackle an important dimension of gdpr compliance checking for privacy policies. specifically, we provide automated support for checking whether the content of a given privacy policy is complete according to the provisions stipulated by gdpr. to do so, we present: (1) a conceptual model to characterize the information content envisaged by gdpr for privacy policies, (2) an ai-assisted approach for classifying the information content in gdpr privacy policies and subsequently checking how well the classified content meets the completeness criteria of interest; and (3) an evaluation of our approach through a case study over 24 unseen privacy policies. for classification, we leverage a combination of natural language processing and supervised machine learning. our experimental material is comprised of 234 real privacy policies from the fund industry. our empirical results indicate that our approach detected 45 of the total of 47 incompleteness issues in the 24 privacy policies it was applied to. over these policies, the approach had eight false positives. the approach thus has a precision of 85% and recall of 96% over our case study."
                },
                {
                    "id": "R194237",
                    "label": "Do we Really Know What we are Building? Raising Awareness of Potential Sustainability Effects of Software Systems in Requirements Engineering",
                    "doi": "10.1109/re.2019.00013",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "integrating novel software systems in our society, economy, and environment can have far-reaching effects. as a result, software systems should be designed in such a way as to maintain or improve the sustainability of the socio-technical system of their destination. however, a paradigm shift is required to raise awareness of software professionals on the potential sustainability effects of software systems. while requirements engineering is considered the key to driving this change, requirements engineers lack the knowledge, experience and methodological support for doing so. this paper presents a question-based framework for raising awareness of the potential effects of software systems on sustainability, as the first step towards enabling the required paradigm shift. a feasibility study of the framework was carried out with two groups of computer science students. the results of the study indicate that the framework helps enable discussions about potential effects that software systems could have on sustainability."
                },
                {
                    "id": "R194253",
                    "label": "Can a Conversation Paint a Picture? Mining Requirements In Software Forums",
                    "doi": "10.1109/re.2019.00014",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "the modern software landscape is highly competitive. software companies need to quickly fix reported bugs and release requested new features, or they risk negative reviews and reduced market share. the amount of online user feedback prevents manual analysis. past research has investigated automated requirement mining techniques on online platforms like app stores and twitter, but online product forums have not been studied. in this paper, we show that online product forums are a rich source of user feedback that may be used to elicit product requirements. the information contained in forum questions is different from what has been described in the related work on app stores or twitter. users often provide detailed context to specific problems they encounter with a software product and other users respond with workarounds or to confirm the problem. through the analysis of two large forums, we identify 18 different types of information (classifications) contained in forums that can be relevant to maintenance and evolution tasks. we show that a state-of-the-art app store tool is unable to accurately classify forum data, which may be due to the differences in content. thus, specific techniques are likely needed to mine requirements from product forums. in an exploratory study, we developed classifiers with forum specific features. promising results are achieved for all classifiers with f-measure scores ranging from 70.3% to 89.8%."
                },
                {
                    "id": "R194281",
                    "label": "Learning Requirements Elicitation Interviews with Role-Playing, Self-Assessment and Peer-Review",
                    "doi": "10.1109/re.2019.00015",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "interviews are largely used in the practice of requirements elicitation. nevertheless, performing an effective interview often depends on soft-skills, and on knowledge acquired through experience. when it comes to requirements engineering education and training (reet), limited resources and few well-founded pedagogical approaches are available to allow students to acquire and improve their skills as interviewers. this paper presents a novel pedagogical approach that combines role-playing, peer-review and self-assessment to enable students to reflect on their mistakes, and improve their interview skills. we evaluate the approach through a controlled quasi-experiment. the study shows that the approach significantly reduces the amount of mistakes made by the students. feedback from the participants confirms the usefulness and easiness of the proposed training. this work contributes to the body of knowledge of reet with an empirically evaluated method for teaching inter-views. furthermore, we share the pedagogical material used, to enable other educators to apply and possibly tailor the approach."
                },
                {
                    "id": "R194317",
                    "label": "Optimizing for Recall in Automatic Requirements Classification: An Empirical Study",
                    "doi": "10.1109/re.2019.00016",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "using machine learning to solve requirements engineering problems can be a tricky task. even though certain algorithms have exceptional performance, their recall is usually below 100%. one key aspect in the implementation of machine learning tools is the balance between recall and precision. tools that do not find all correct answers may be considered useless. however, some tasks are very complicated and even requirements engineers struggle to solve them perfectly. if a tool achieves performance comparable to a trained engineer while reducing her workload considerably, it is considered to be useful. one such task is the classification of specification content elements into requirements and non-requirements. in this paper, we analyze this specific requirements classification problem and assess the importance of recall by performing an empirical study. we compared two groups of students who performed this task with and without tool support, respectively. we use the results to compute an estimate of f for the ff score, allowing us to choose the optimal balance between precision and recall. furthermore, we use the results to assess the practical time savings realized by the approach. by using the tool, users may not be able to find all defects in a document, however, they will be able to find close to all of them in a fraction of the time necessary. this demonstrates the practical usefulness of our approach and machine learning tools in general."
                },
                {
                    "id": "R194345",
                    "label": "A Machine Learning-Based Approach for Demarcating Requirements in Textual Specifications",
                    "doi": "10.1109/re.2019.00017",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "a simple but important task during the analysis of a textual requirements specification is to determine which statements in the specification represent requirements. in principle, by following suitable writing and markup conventions, one can provide an immediate and unequivocal demarcation of requirements at the time a specification is being developed. however, neither the presence nor a fully accurate enforcement of such conventions is guaranteed. the result is that, in many practical situations, analysts end up resorting to after-the-fact reviews for sifting requirements from other material in a requirements specification. this is both tedious and time-consuming. we propose an automated approach for demarcating requirements in free-form requirements specifications. the approach, which is based on machine learning, can be applied to a wide variety of specifications in different domains and with different writing styles. we train and evaluate our approach over an independently labeled dataset comprised of 30 industrial requirements specifications. over this dataset, our approach yields an average precision of 81.2% and an average recall of 95.7%. compared to simple baselines that demarcate requirements based on the presence of modal verbs and identifiers, our approach leads to an average gain of 16.4% in precision and 25.5% in recall."
                },
                {
                    "id": "R194360",
                    "label": "Analysis of Requirements-Related Arguments in User Forums",
                    "doi": "10.1109/re.2019.00018",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "in the past, users were asked to express their needs and intentions by writing a structured requirements document in natural language. due to the pervasive use of online forums and social media, user feedback is more accessible today. however, the information obtained is often fragmented, involving multipleperspectives from multiple parties on an on-going basis. in this paper, we propose a crowd-based requirements engineering approach by argumentation (crowdre-arg), which analyses the conversations from user forum, identifies the arguments in favor or opposing of a given requirements related discussion topic. by generating the argumentation model of the involved user statements, we are able to recover the conflicting viewpoints, to reason about the winning arguments for informed requirements decisions. the proposed approach is illustrated with a data set of sample conversations about the design of a new google-map feature from reddit. also, we apply natural language processing techniques and machine learning algorithms to support the automated execution of the crowdre-arg approach."
                },
                {
                    "id": "R194373",
                    "label": "The Role of Environment Assertions in Requirements-Based Testing",
                    "doi": "10.1109/re.2019.00019",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "software developers dedicate a major portion of their development effort towards testing and quality assurance (qa) activities, especially during and around the implementation phase. nevertheless, we continue to see an alarmingly increasing trend in the cost and consequences of software failure. in an attempt to mitigate such loss and address software issues at a much earlier stage, researchers have recently emphasized on the successful coordination of requirements engineering and testing. in addition, the notion of requirements-based testing (rbt) has also emerged with a focus on checking the correctness, completeness, unambiguity, and logical consistency of requirements. one seminal work points out that requirements reside in the environment which is comprised of certain problem domain phenomena. environmental assertions, which connect some of these phenomena in the indicative mood, play a key role in deciding whether a software solution is acceptable. despite that requirements are located in the environment, little is known about if and how the environment assertions would impact testing and qa activities. in order to address this gap, we present a detailed empirical study, with 114 developers, on the prominence of environment assertions in rbt. although the results suggest that paying attention to correct, complete, and useful environment assertions has a positive impact on rbt, developers often face difficulty in formulating good assertions from scratch. our work, to that end, illuminates the potential usefulness of automated support in generating environment assertions."
                },
                {
                    "id": "R194401",
                    "label": "An Approach for Reviewing Security-Related Aspects in Agile Requirements Specifications of Web Applications",
                    "doi": "10.1109/re.2019.00020",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "defects in requirements specifications can have severe consequences during the software development lifecycle. some of them result in overall project failure due to incorrect or missing quality characteristics such as security. there are several concerns that make security difficult to deal with; for instance, (1) when stakeholders discuss general requirements in meetings, they are often unaware that they should also discuss security-related topics, and (2) they typically do not have enough expertise in security. this often leads to unspecified or ill-defined security-related aspects. these concerns become even more challenging in agile contexts, where lightweight documentation is typically involved. the goal of this paper is to design and evaluate an approach for reviewing security-related aspects in agile requirements specifications of web applications. the approach considers user stories and security specifications as input and relates those user stories to security properties via natural language processing. based on the related security properties, our approach then identifies high-level security requirements from the open web application security project to be verified and generates a reading technique to support reviewers in detecting defects. we evaluate our approach via two controlled experiment trials. we compare the effectiveness and efficiency of novice inspectors verifying security aspects in agile requirements using our approach against using the complete list of high-level security requirements. the (statistically significant) results indicate that using our approach has a positive impact (with large effect size) on the performance of inspectors in terms of effectiveness and efficiency."
                },
                {
                    "id": "R194414",
                    "label": "Detecting Bad Smells in Use Case Descriptions",
                    "doi": "10.1109/re.2019.00021",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "use case modeling is very popular to represent the functionality of the system to be developed, and it consists of two parts: use case diagram and use case description. use case descriptions are written in structured natural language (nl), and the usage of nl can lead to poor descriptions such as ambiguous, inconsistent and/or incomplete descriptions, etc. poor descriptions lead to missing requirements and eliciting incorrect requirements as well as less comprehensiveness of produced use case models. this paper proposes a technique to automate detecting bad smells of use case descriptions, symptoms of poor descriptions. at first, to clarify bad smells, we analyzed existing use case models to discover poor use case descriptions concretely and developed the list of bad smells, i.e., a catalogue of bad smells. some of the bad smells can be refined into measures using the goal-question-metric paradigm to automate their detection. the main contribution of this paper is the automated detection of bad smells. we have implemented an automated smell detector for 22 bad smells at first and assessed its usefulness by an experiment. as a result, the first version of our tool got a precision ratio of 0.591 and recall ratio of 0.981."
                },
                {
                    "id": "R194425",
                    "label": "Visualization Requirements for Business Intelligence Analytics: A Goal-Based, Iterative Framework",
                    "doi": "10.1109/re.2019.00022",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "information visualization plays a key role in business intelligence analytics. with ever larger amounts of data that need to be interpreted, using the right visualizations is crucial in order to understand the underlying patterns and results obtained by analysis algorithms. despite its importance, defining the right visualization is still a challenging task. business users are rarely experts in information visualization, and they may not exactly know the most adequate visualization tools or patterns for their goals. consequently, misinterpreted graphs and wrong results can be obtained, leading to missed opportunities and significant losses for companies. the main problem underneath is a lack of tools and methodologies that allow non-expert users to define their visualization and data analysis goals in business terms. in order to tackle this problem, we present an iterative goal-oriented approach based on the i* language for the automatic derivation of data visualizations. our approach links non-expert user requirements to the data to be analyzed, choosing the most suited visualization techniques in a semi-automatic way. the great advantage of our proposal is that we provide non-expert users with the best suited visualizations according to their information needs and their data with little effort and without requiring expertise in information visualization."
                },
                {
                    "id": "R194428",
                    "label": "Predicting How to Test Requirements: An Automated Approach",
                    "doi": "10.1109/re.2019.00023",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "an important task in requirements engineering is to identify and determine how to verify a requirement (e.g., by manual review, testing, or simulation; also called potential verification method). this information is required to effectively create test cases and verification plans for requirements. [objective] in this paper, we propose an automatic approach to classify natural language requirements with respect to their potential verification methods (pvm). [method] our approach uses a convolutional neural network architecture to implement a multiclass and multilabel classifier that assigns probabilities to a predefined set of six possible verification methods, which we derived from an industrial guideline. additionally, we implemented a backtracing approach to analyze and visualize the reasons for the network\u2019s decisions. [results] in a 10-fold cross validation on a set of about 27,000 industrial requirements, our approach achieved a macro averaged f1 score of 0.79 across all labels. for the classification into test or non-test, the approach achieves an even higher f1 score of 0.94. [conclusions] the results show that our approach might help to increase the quality of requirements specifications with respect to the pvm attribute and guide engineers in effectively deriving test cases and verification plans."
                },
                {
                    "id": "R194431",
                    "label": "Do End-Users Want Explanations? Analyzing the Role of Explainability as an Emerging Aspect of Non-Functional Requirements",
                    "doi": "10.1109/re.2019.00032",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"software systems are getting more and more complex. their ubiquitous presence makes users more dependent on them and their correctness in many aspects of daily life. thus, there is a rising need to make software systems and their decisions more comprehensible. this seems to call for more transparency in software-supported decisions. therefore, transparency is gaining importance as a non-functional requirement. however, the abstract quality aspect of transparency needs to be better understood and related to mechanisms that can foster it. integrating explanations in software to leverage systems' opacity has been discussed often. yet, an important first step is to understand user requirements with respect to explainable software behavior: are users really interested in transparency, and are explanations considered an adequate mechanism to achieve it? we conducted a survey with 107 end-users to assess their opinion on the current status of transparency in software systems, and what they consider main advantages and disadvantages of explanations embedded in software. the overall attitude towards embedded explanations was positive. however, we also identified potential disadvantages. we assess the relation between explanations and transparency and analyze its possible impact on software quality.\""
                },
                {
                    "id": "R194445",
                    "label": "Automated Recommendation of Software Refactorings Based on Feature Requests",
                    "doi": "10.1109/re.2019.00029",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "during software evolution, developers often receive new requirements expressed as feature requests. to implement the requested features, developers have to perform necessary modifications (refactorings) to prepare for new adaptation that accommodates the new requirements. software refactoring is a well-known technique that has been extensively used to improve software quality such as maintainability and extensibility. however, it is often challenging to determine which kind of refactorings should be applied. consequently, several approaches based on various heuristics have been proposed to recommend refactorings. however, there is still lack of automated support to recommend refactorings given a feature request. to this end, in this paper, we propose a novel approach that recommends refactorings based on the history of the previously requested features and applied refactorings. first, we exploit the stateof-the-art refactoring detection tools to identify the previous refactorings applied to implement the past feature requests. second, we train a machine classifier with the history data of the feature requests and refactorings applied on the commits that implemented the corresponding feature requests. the machine classifier is then used to predict refactorings for new feature requests. we evaluate the proposed approach on the dataset of 43 open source java projects and the results suggest that the proposed approach can accurately recommend refactorings (average precision 73%)."
                },
                {
                    "id": "R194458",
                    "label": "Analysing Gender Differences in Building Social Goal Models: A Quasi-Experiment",
                    "doi": "10.1109/re.2019.00027",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"context: recent research has shown gender differences in problem-solving, and gender biases in how software supports it. gendermag has 5 problem-solving facets related to gender-inclusiveness: motivation for using the software, information processing style, computer self-efficacy, attitude towards risk, and ways of learning new technology. some facet values are more frequent in women, others in men. the role these facets may play when building social goal models is largely unexplored. objectives: we evaluated the impact of different levels of gendermag facets on creating and modifying istar 2.0 models. methods: we performed a quasi-experiment. we characterised 100 participants according to each gendermag facet. participants performed creation and modification tasks on istar 2.0. we measured their accuracy, speed, and ease, using metrics of task success, time, and effort, collected with eye-tracking, eeg and eda sensors, and participants' feedback. results: although participants with facet levels frequently seen in women had lower perceived performance and speed, their accuracy was higher. we also observed some statistically significant differences in visual effort, mental effort, and stress. conclusions: participants with a comprehensive information processing style and a more conservative attitude towards risk (characteristics more frequently seen in women) solved the tasks with a lower speed but higher accuracy.\""
                },
                {
                    "id": "R194474",
                    "label": "How do Practitioners Capture and Utilize User Feedback During Continuous Software Engineering?",
                    "doi": "10.1109/re.2019.00026",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"continuous software engineering (cse) evolved as a process for rapid software evolution. continuous delivery enables developers to frequently retrieve user feedback on the latest software increment. developers use these insights for requirements validation and verification. despite the importance of users, reports about user feedback in cse practice are sparse. we conducted 20 interviews with practitioners from 17 companies that apply cse. we asked practitioners how they capture and utilize user feedback. in this paper, we detail the practitioners' answers by posing three research questions. to improve continuous user feedback capture and utilization with respect to requirements engineering, we derived five recommendations: first, internal sources should be approached, as they provide a rich source of user feedback; second, existing tool support should be adapted and extended to automate user feedback processing; third, a concept of reference points should be established to relate user feedback to requirements; fourth, the utilization of user feedback for requirements validation should be increased; and last, the interaction with user feedback should be enabled and supported by increasing developer-user communication. we conclude that a continuous user understanding activity can improve requirements engineering by contributing to both the completeness and correctness of requirements.\""
                },
                {
                    "id": "R194607",
                    "label": "Extracting and Analyzing Context Information in User-Support Conversations on Twitter",
                    "doi": "10.1109/re.2019.00024",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "while many apps include built-in options to report bugs or request features, users still provide an increasing amount of feedback via social media, like twitter. compared to traditional issue trackers, the reporting process in social media is unstructured and the feedback often lacks basic context information, such as the app version or the device concerned when experiencing the issue. to make this feedback actionable to developers, support teams engage in recurring, effortful conversations with app users to clarify missing context items. this paper introduces a simple approach that accurately extracts basic context information from unstructured, informal user feedback on mobile apps, including the platform, device, app version, and system version. evaluated against a truthset of 3014 tweets from official twitter support accounts of the 3 popular apps netflix, snapchat, and spotify, our approach achieved precisions from 81% to 99% and recalls from 86% to 98% for the different context item types. combined with a chatbot that automatically requests missing context items from reporting users, our approach aims at auto-populating issue trackers with structured bug reports."
                },
                {
                    "id": "R194610",
                    "label": "Requirements Classification with Interpretable Machine Learning and Dependency Parsing",
                    "doi": "10.1109/re.2019.00025",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"requirements classification is a traditional application of machine learning (ml) to re that helps handle large requirements datasets. a prime example of an re classification problem is the distinction between functional and non-functional (quality) requirements. state-of-the-art classifiers build their effectiveness on a large set of word features like text n-grams or pos n-grams, which do not fully capture the essence of a requirement. as a result, it is arduous for human analysts to interpret the classification results by exploring the classifier's inner workings. we propose the use of more general linguistic features, such as dependency types, for the construction of interpretable ml classifiers for re. through a feature engineering effort, in which we are assisted by modern introspection tools that reveal the hidden inner workings of ml classifiers, we derive a set of 17 linguistic features. while classifiers that use our proposed features fit the training set slightly worse than those that use high-dimensional feature sets, our approach performs generally better on validation datasets and it is more interpretable.\""
                },
                {
                    "id": "R194641",
                    "label": "Digital Discrimination in Sharing Economy A Requirements Engineering Perspective",
                    "doi": "10.1109/re48521.2020.00031",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "recent evidence has revealed that sharing economy platforms such as uber, airbnb, and taskrabbit, have become active hubs for digital discrimination. this new form of discrimination refers to a phenomenon where a business transaction is influenced by race, gender, age, or any other non-business related characteristic of providers or consumers. existing research often tackles this problem from a socio-economic and regulatory points of view. however, the research on the design aspects of sharing economy software, which enable such complex sociotechnical problems to emerge online, is still underdeveloped. to bridge this gap, in this paper, we propose a new perspective on digital discrimination, tackling the problem from a requirements engineering point of view. specifically, we analyze a large dataset of online user feedback as well as synthesize existing literature to identify and classify pervasive discrimination concerns in the sharing economy market. based on this analysis, we devise a crowd-driven domain model to represent these concerns along with their relations to the functional features and user goals of sharing economy platforms. this model is intended to provide requirements engineers, working on sharing economy software, with systematic insights into the complex types of socio-technical problems that can emerge in the operational environments of their systems."
                },
                {
                    "id": "R194686",
                    "label": "The Manager Perspective on Requirements Impact on Automotive Systems Development Speed",
                    "doi": "10.1109/re.2018.00-55",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "context: historically, automotive manufacturers have adopted rigid requirements engineering processes, which allowed them to meet safety-critical requirements while integrating thousands of physical and software components into a highly complex and differentiated product. nowadays, needs of improving development speed are pushing companies in this domain towards new ways of developing software. objectives: we aim at obtaining a manager perspective on how the goal to increase development speed impacts how software intense automotive systems are developed and their requirements managed. methods: we used a qualitative multiple-case study, based on 20 semi-structured interviews, at two automotive manufacturers. our sampling strategy focuses on manager roles, complemented with technical specialists. results: we found that both a requirements style dominated by safety concerns, and decomposition of requirements over many levels of abstraction impact development speed negatively. furthermore, the use of requirements as part of legal contracts with suppliers hiders fast collaboration. suggestions for potential improvements include domain-specific tooling, model-based requirements, test automation, and a combination of lightweight pre-development requirements engineering with precise specifications post-development. conclusions: we offer an empirical account of expectations and needs for new requirements engineering approaches in the automotive domain, necessary to coordinate hundreds of collaborating organizations developing software-intensive and potentially safety-critical systems."
                },
                {
                    "id": "R194698",
                    "label": "A Qualitative Study on using GuideGen to Keep Requirements and Acceptance Tests Aligned",
                    "doi": "10.1109/re.2018.00-54",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "software requirements constantly change, thus impacting all other artifacts of an evolving system. in order to keep the system in a consistent state, changes in requirements should be documented and applied accordingly to all affected artifacts, including acceptance tests. in practice, however, changes in requirements are not always documented nor applied to the affected acceptance tests. this is mostly due to poor communication, lack of time or work overload, and eventually leads to project delays, unintended costs and unsatisfied customers. guidegen is a tool-supported approach for keeping requirements and acceptance tests aligned. when a requirement is changed, guidegen automatically generates guidance in natural language on how to modify impacted acceptance tests and communicates this information to the concerned parties. in this paper, we evaluate guidegen in terms of its perceived usefulness for practitioners and its applicability to real software projects. the evaluation was conducted via interviews with 23 industrial practitioners from ten companies based in europe. the results indicate that guidegen is a useful approach that facilitates requirements change management and the communication of changes between requirements and test engineers. the participants also identified potential for improvement, in particular for using guidegen in large projects."
                },
                {
                    "id": "R194870",
                    "label": "Requirements Reference Models Revisited: Accommodating Hierarchy in System Design",
                    "doi": "10.1109/re.2019.00028",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"reference models such as parnas' four-variable model, jackson's and zaves' world machine model, and gunther et al.'s wrspm model abstractly define and relate key artifacts in requirements engineering. such reference models are intended to serve as a frame of reference for engineers to understand and reason about the artifacts involved in requirements engineering. however, when discussing the requirements of modern systems that are developed in a hierarchical and middle-out manner, these reference models do not provide a framework in which the relationship between requirements and architecture is explicitly discussed. conceptual clarity about this relationship is crucial since the architecture and requirements for such systems become intrinsically intertwined as the architectural choices made during development influence the requirements and vice-versa. hence, to precisely determine the scope of specifying requirements, distinguish requirements from architecture details, reason about the requirements, and determine how the requirements are realized in the system, we argue that a requirements reference model intended as a reference for such systems must explicitly discuss the architecture - requirements relationship. to that end, we define a hierarchical reference model that formally, yet abstractly, captures the intertwined relationship between the architecture and requirements in a way that will serve the same purpose as other models, but be more suitable for modern systems where architecture and requirements co-evolve. to illustrate the concepts in this model, we use a generic patient-controlled analgesic infusion pump system as a case example.\""
                },
                {
                    "id": "R194875",
                    "label": "A Metamodeling Approach to Support the Engineering of Modeling Method Requirements",
                    "doi": "10.1109/re.2019.00030",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "the notion of \"modeling method requirements\" refers to a category typically neglected by re taxonomies and frameworks - i.e., those requirements that motivate the realization of (conceptual) modeling methods and tools. they can be considered domain-specific, in the sense that all modeling methods provide a knowledge schema for some selected application domain (narrow or broad). besides this inherent domain-specific nature, we are investigating how the characteristics of modeling methods inform the re perspective, and how in turn re can support the engineering of such artifacts. thus, the work at hand aims to raise awareness about modeling method requirements in the re community. the core contribution is the cochaco (concept-characteristic-connector) method for the representation and management of such requirements, as well as for streamlining with subsequent engineering phases. cochaco is itself a modeling method - i.e., it achieves its goals through diagrammatic modeling means for which a supporting tool was prototyped and evolved. the proposal originates in required support for the initial phase of the agile modeling method engineering (amme) methodology, which was successfully applied in developing a variety of project-specific modeling tools. from this accumulated experience, awareness of \"modeling method requirements\" emerged and informed the design decisions of cochaco."
                },
                {
                    "id": "R194884",
                    "label": "Extraction of System States from Natural Language Requirements",
                    "doi": "10.1109/re.2019.00031",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "in recent years, simulations have proven to be an important means to verify the behavior of complex software systems. the different states of a system are monitored in the simulations and are compared against the requirements specification. so far, system states in natural language requirements cannot be automatically linked to signals from the simulation. however, the manual mapping between requirements and simulation is a time-consuming task. named-entity recognition is a sub-task from the field of automated information retrieval and is used to classify parts of natural language texts into categories. in this paper, we use a self-trained named-entity recognition model with bidirectional lstms and cnns to extract states from requirements specifications. we present an almost entirely automated approach and an iterative semi-automated approach to train our model. the automated and iterative approach are compared and discussed with respect to the usual manual extraction. we show that the manual extraction of states in 2,000 requirements takes nine hours. our automated approach achieves an f1-score of 0.51 with 15 minutes of manual work and the iterative approach achieves an f1-score of 0.62 with 100 minutes of work."
                },
                {
                    "id": "R194895",
                    "label": "Scalable Analysis of Real-Time Requirements",
                    "doi": "10.1109/re.2019.00033",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "detecting issues in real-time requirements is usually a trade-off between flexibility and cost: the effort expended depends on how expensive it is to fix a defect introduced by faulty, ambiguous or incomplete requirements. the most rigorous techniques for real-time requirement analysis depend on the formalisation of these requirements. completely formalised real-time requirements allow the detection of issues that are hard to find through other means, like real-time inconsistency (i.e., \"do the requirements lead to deadlocks and starvation of the system?\") or vacuity (i.e., \"are some requirements trivially satisfied\"). current analysis techniques for real-time requirements suffer from scalability issues \u2013 larger sets of such requirements are usually intractable. we present a new technique to analyse formalised real-time requirements for various properties. our technique leverages recent advances in software model checking and automatic theorem proving by converting the analysis problem for real-time requirements to a program analysis task. we also report preliminary results from an ongoing, large scale application of our technique in the automotive domain at bosch."
                },
                {
                    "id": "R194905",
                    "label": "Arithmetic Semantics of Feature and Goal Models for Adaptive Cyber-Physical Systems",
                    "doi": "10.1109/re.2019.00034",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "many cyber-physical systems (cpss) today are self-adaptive, in order to handle frequent changes in environmental conditions and requirements. in cpss, goal-based reasoning is often used to include stakeholder and social concerns in decision making during design and runtime adaptation activities. to better support some of these activities, arithmetic semantics for goal models were proposed to enable the generation of mathematical functions usable by systems. however, goal models often allow invalid combinations of alternatives, which can be prevented by companion feature models. in this paper, to enable the generation of valid and optimal configurations for adaptive cpss, we propose new arithmetic semantics for feature models that enable their transformations to mathematical functions (in several programming languages) further restricting the ones generated from goal models. the composition of feature and goal functions results in a smaller design space, leading to fewer but valid solutions that can be generated (e.g., through optimization) and used in simulations and running adaptive cpss with social concerns. finally, a simulation model in sysml is proposed in this paper to demonstrate the feasibility and usefulness of this composition."
                },
                {
                    "id": "R194922",
                    "label": "The Next Release Problem Revisited: A New Avenue for Goal Models",
                    "doi": "10.1109/re.2018.00-56",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "context. goal models have long been critiqued for the time it takes to construct them as well as for their limited cognitive and visual scalability. is such criticism general or does it depend on the supported task? objectives. we advocate for the latter and the aim of this paper is to demonstrate that the next release problem is a suitable application domain for goal models. this hypothesis stems from the fact that product release management is a long-term investment, and software products are commonly managed in \"themes\" which are smaller focus areas of the product. methods. we employ a version of goal models that is tailored for the next release problem by capturing requirements, synergies among them, constraints, and release objectives. such goal model allows discovering optimal solutions considering multiple criteria for the next release. results. a retrospective case study confirms that goal models are easier to read and comprehend when organized in themes, and that the reasoning results help product managers decide for the next release. our scalability experiments show that, through reasoning based on optimization modulo theories, the discovery of the optimal solution is fast and scales sufficiently well with respect to the model size, connectivity, and number of alternative solutions."
                },
                {
                    "id": "R194932",
                    "label": "Enhancing Automated Requirements Traceability by Resolving Polysemy",
                    "doi": "10.1109/re.2018.00-53",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"requirements traceability provides critical support throughout all phases of software engineering. automated tracing based on information retrieval (ir) reduces the effort required to perform a manual trace. unfortunately, ir-based trace recovery suffers from low precision due to polysemy, which refers to the coexistence of multiple meanings for a term appearing in different requirements. latent semantic indexing (lsi) has been introduced as a method to tackle polysemy, as well as synonymy. however, little is known about the scope and significance of polysemous terms in requirements tracing. while quantifying the effect, we present a novel method based on artificial neural networks (ann) to enhance the capability of automatically resolving polysemous terms. the core idea is to build an ann model which leverages a term's highest-scoring coreferences in different requirements to learn whether this term has the same meaning in those requirements. experimental results based on 2 benchmark datasets and 6 long-lived open-source software projects show that our approach outperforms lsi on identifying polysemous terms and hence increasing the precision of automated tracing.\""
                },
                {
                    "id": "R194954",
                    "label": "Vetting Automatically Generated Trace Links: What Information is Useful to Human Analysts?",
                    "doi": "10.1109/re.2018.00-52",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"automated traceability has been investigated for over a decade with promising results. however, a human analyst is needed to vet the generated trace links to ensure their quality. the process of vetting trace links is not trivial and while previous studies have analyzed the performance of the human analyst, they have not focused on the analyst's information needs. the aim of this study is to investigate what context information the human analyst needs. we used design science research, in which we conducted interviews with ten practitioners in the traceability area to understand the information needed by human analysts. we then compared the information collected from the interviews with existing literature. we created a prototype tool that presents this information to the human analyst. to further understand the role of context information, we conducted a controlled experiment with 33 participants. our interviews reveal that human analysts need information from three different sources: 1) from the artifacts connected by the link, 2) from the traceability information model, and 3) from the tracing algorithm. the experiment results show that the content of the connected artifacts is more useful to the analyst than the contextual information of the artifacts.\""
                },
                {
                    "id": "R194974",
                    "label": "Modeling User Concerns in the App Store: A Case Study on the Rise and Fall of Yik Yak",
                    "doi": "10.1109/re.2018.00-51",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"mobile application (app) stores have lowered the barriers to app market entry, leading to an accelerated and unprecedented pace of mobile software production. to survive in such a highly competitive and vibrant market, release engineering decisions should be driven by a systematic analysis of the complex interplay between the user, system, and market components of the mobile app ecosystem. to demonstrate the feasibility and value of such analysis, in this paper, we present a case study on the rise and fall of yik yak, one of the most popular social networking apps at its peak. in particular, we identify and analyze the design decisions that led to the downfall of yik yak and track rival apps' attempts to take advantage of this failure. we further perform a systematic in-depth analysis to identify the main user concerns in the domain of anonymous social networking apps and model their relations to the core features of the domain. such a model can be utilized by app developers to devise sustainable release engineering strategies that can address urgent user concerns and maintain market viability.\""
                },
                {
                    "id": "R195005",
                    "label": "Catalog of Invisibility Requirements for UbiComp and IoT Applications",
                    "doi": "10.1109/re.2018.00019",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "a new set of non-functional requirements (nfrs) have appeared with the advent of ubiquitous computing (ubicomp) and more recently internet of things (iot). invisibility is one of these nfrs that means the ability to hide technology from users. although invisibility is long seen as an essential characteristic for achieving the goals of ubicomp, it has not been cataloged regarding its subcharacteristics and solutions, making its design and requirements specification in such applications a challenging task. considering the softgoal interdependency graph (sig), which is a well-known format to catalog nfrs, this work aims at capturing subcharacteristics and solutions for invisibility and cataloging them in a sig. since there is no systematic approach on how to build sigs, we also propose to systematize the definition of invisibility sig using the following well-defined research methods: snowballing, database search, grounded theory and questionnaires. as a result, we got an invisibility sig composed of two main subcharacteristics, twelve sub-subcharacteristics, ten general solutions and fifty-six specific solutions. this organized body of knowledge is useful for supporting software engineers to specify requirements and practical solutions for ubicomp and iot applications. furthermore, the proposed methodology used to capture and catalog requirements in a sig can be reused for other nfrs."
                },
                {
                    "id": "R195023",
                    "label": "RE and Society - A Perspective on RE in Times of Smart Cities and Smart Rural Areas",
                    "doi": "10.1109/re.2018.00020",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "our requirements engineering (re) community has known for decades that the success or failure of re methods heavily depends on the context in which they are applied. thus, many experiences have been gained and shared in the community that reflect which re methods are suitable for a specific context, such as embedded systems development (e.g., automotive or military domain) or information systems development (e.g., banking or flight control domain). nowadays, in times of smart cities and their counterpart smart rural areas, where newly introduced it systems have a strong effect on our society, a new and challenging context arises for re, which opens up new research questions. as a contribution to this situation and to foster discussions in our community about whether our re methods are appropriate in this new \"social context\", this perspective paper reflects on the state of the art and on our own experiences in applying re in the context of smart rural areas. these results might also pertain in the context of smart cities that pose similar challenges to re. in addition, we present a framework comprising both an initial classification of social contexts, particularly their end users, and a classification for re methods. example usage scenarios illustrate how this framework helps to reflect on the suitability of our re methods, and, if necessary, provides the basis for adapting them or creating new ones. finally, we outline a roadmap with research questions and related activities with which we want to encourage our community to perform the proposed research activities in order to enrich our body of experiences and adapt our methods to this highly relevant context."
                },
                {
                    "id": "R195107",
                    "label": "On Systems of Systems Engineering: A Requirements Engineering Perspective and Research Agenda",
                    "doi": "10.1109/re.2018.00021",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"the emergence of systems of systems (soss) and systems of systems engineering (sose) is largely driven by global societal needs including energy-water-food nexus, population demographics, global climate, integrated transport, security and social activity. however, due to their scale, structural and functional complexity and emergent properties, these global spanning cyber-physical systems of systems are becoming increasingly complex and more difficult for current requirements engineering (re) practices to handle. in this paper, we firstly introduce sose as an emerging discipline and key characteristics of soss. we then highlight the challenges that the re discipline must respond to. we discuss some weaknesses of current re techniques and approaches to cope with the complexity of soss. we then argue that there is a need for the global re community to evolve current re approaches and to develop new ways of thinking, new re capabilities and possibly a new re science as a key mechanism for addressing requirements engineering complexities posed by systems of systems. we then outline a requirements engineering perspective and research agenda that identifies 'top-10' research themes informed by a cluster of four systems of systems engineering projects funded by the european commission's horizon 2020 research programme.\""
                },
                {
                    "id": "R195113",
                    "label": "Automated Extraction of Semantic Legal Metadata using Natural Language Processing",
                    "doi": "10.1109/re.2018.00022",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "[context] semantic legal metadata provides information that helps with understanding and interpreting the meaning of legal provisions. such metadata is important for the systematic analysis of legal requirements. [objectives] our work is motivated by two observations: (1) the existing requirements engineering (re) literature does not provide a harmonized view on the semantic metadata types that are useful for legal requirements analysis. (2) automated support for the extraction of semantic legal metadata is scarce, and further does not exploit the full potential of natural language processing (nlp). our objective is to take steps toward addressing these limitations. [methods] we review and reconcile the semantic legal metadata types proposed in re. subsequently, we conduct a qualitative study aimed at investigating how the identified metadata types can be extracted automatically. [results and conclusions] we propose (1) a harmonized conceptual model for the semantic metadata types pertinent to legal requirements analysis, and (2) automated extraction rules for these metadata types based on nlp. we evaluate the extraction rules through a case study. our results indicate that the rules generate metadata annotations with high accuracy."
                },
                {
                    "id": "R195124",
                    "label": "The Grace Period Has Ended: An Approach to Operationalize GDPR Requirements",
                    "doi": "10.1109/re.2018.00023",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"the general data protection regulation (gdpr) aims to protect personal data of eu residents and can impose severe sanctions for non-compliance. organizations are currently implementing various measures to ensure their software systems fulfill gdpr obligations such as identifying a legal basis for data processing or enforcing data anonymization. however, as regulations are formulated vaguely, it is difficult for practitioners to extract and operationalize legal requirements from the gdpr. this paper aims to help organizations understand the data protection obligations imposed by the gdpr and identify measures to ensure compliance. to achieve this goal, we propose guideme, a 6-step systematic approach that supports elicitation of solution requirements that link gdpr data protection obligations with the privacy controls that fulfill these obligations and that should be implemented in an organization's software system. we illustrate and evaluate our approach using an example of a university information system. our results demonstrate that the solution requirements elicited using our approach are aligned with the recommendations of privacy experts and are expressed correctly.\""
                },
                {
                    "id": "R195129",
                    "label": "Semantic Incompleteness in Privacy Policy Goals",
                    "doi": "10.1109/re.2018.00025",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"companies that collect personal information online often maintain privacy policies that are required to accurately reflect their data practices and privacy goals. to be comprehensive and flexible for future practices, policies contain ambiguity that summarize practices over multiple types of products and business contexts. ambiguity in data practice descriptions undermines policies as an effective way to communicate system design choices to users, and as a reliable regulatory mechanism. in this paper, we report an investigation to identify incompleteness by representing data practice descriptions as semantic frames. the approach is a grounded analysis to discover which data actions and semantic roles correspond are needed to construct complete data practice descriptions. our results include 281 data action instances obtained from 202 manually annotated statements across five privacy policies. therein, we identified 878 instances of 17 types of semantic roles. incomplete data practice descriptions undermine user comprehension, and can affect the user's perceived privacy risk, which we measure using factorial vignette surveys. we observed that user perception of risk decreases when two roles are present in a statement: the condition under which a data action is performed, and the purpose for which the user's information is used.\""
                },
                {
                    "id": "R195144",
                    "label": "Learning from Mistakes: An Empirical Study of Elicitation Interviews Performed by Novices",
                    "doi": "10.1109/re.2018.00027",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "[context] interviews are the most widely used elicitation technique in requirements engineering. however, conducting effective requirements elicitation interviews is challenging, due to the combination of technical and soft skills that requirements analysts often acquire after a long period of professional practice. empirical evidence about training the novices on conducting effective requirements elicitation interviews is scarce. [objectives] we present a list of most common mistakes that novices make in requirements elicitation interviews. the objective is to assist the educators in teaching interviewing skills to student analysts. [re-search method] we conducted an empirical study involving role-playing and authentic assessment with 110 students, teamed up in 28 groups, to conduct interviews with a customer. one re-searcher made observation notes during the interview while two researchers reviewed the recordings. we qualitatively analyzed the data to identify the themes and classify the mistakes. [results and conclusion] we identified 34 unique mistakes classified into 7 high level themes. we also give examples of the mistakes made by the novices in each theme, to assist the educationists and trainers. our research design is a novel combination of well-known pedagogical approaches described in sufficient details to make it re-peatable for future requirements engineering education and training research."
                },
                {
                    "id": "R195153",
                    "label": "Efficiency and Effectiveness of Requirements Elicitation Techniques for Children",
                    "doi": "10.1109/re.2018.00028",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "[context] the market for software targeting children, both for education and entertainment, is growing. existing work, mainly from hci, has considered the effectiveness of elicitation techniques for eliciting requirements from children as part of a design process. [objective] however, we are lacking work which compares requirements elicitation techniques when used with children. [methods] this study compares five elicitation techniques, taking into consideration the effectiveness and efficiency of each technique. techniques were used with a total of 54 children aged 8-13, eliciting requirements for a museum flight simulator. we compare techniques by looking at the number and type of requirements discovered, perceived participant satisfaction, resources required, perceived usefulness, and requirements coverage of domain specific categories. [conclusions] we observed notable differences between the techniques, including the effectiveness of observations and relative ineffectiveness of questionnaires. we present a set of guidelines to aid industry in eliciting requirements for child-friendly software."
                },
                {
                    "id": "R195173",
                    "label": "Towards Ubiquitous RE: A Perspective on Requirements Engineering in the Era of Digital Transformation",
                    "doi": "10.1109/re.2018.00029",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "we are now living in the era of digital transformation: innovative and digital business models are transforming the global business world and society. however, the authors of this paper have perceived barriers that prevent requirements engineers from contributing properly to the development of the software systems that underpin the digital transformation. we also realized that breaking down each of these barriers would contribute to requirements engineering (re) becoming ubiquitous in certain dimensions: re everywhere, with everyone, for everything, automated, accepting openness, and cross-domain. in this paper, we analyze each dimension of ubiquity in the scope of the interaction between requirements engineers and end users. in particular, we point out the transformation that is required to break down each barrier, present the perspective of the scientific community and our own practical perspective, and discuss our vision on how to achieve this dimension of ubiquity. our goal is to raise the interest of the research community in providing approaches to address the barriers and move towards ubiquitous re."
                },
                {
                    "id": "R195179",
                    "label": "FAME: Supporting Continuous Requirements Elicitation by Combining User Feedback and Monitoring",
                    "doi": "10.1109/re.2018.00030",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "context: software evolution ensures that software systems in use stay up to date and provide value for end-users. however, it is challenging for requirements engineers to continuously elicit needs for systems used by heterogeneous end-users who are out of organisational reach. objective: we aim at supporting continuous requirements elicitation by combining user feedback and usage monitoring. online feedback mechanisms enable end-users to remotely communicate problems, experiences, and opinions, while monitoring provides valuable information about runtime events. it is argued that bringing both information sources together can help requirements engineers to understand end-user needs better. method/tool: we present fame, a framework for the combined and simultaneous collection of feedback and monitoring data in web and mobile contexts to support continuous requirements elicitation. in addition to a detailed discussion of our technical solution, we present the first evidence that fame can be successfully introduced in real-world contexts. therefore, we deployed fame in a web application of a german small and medium-sized enterprise (sme) to collect user feedback and usage data. results/conclusion: our results suggest that fame not only can be successfully used in industrial environments but that bringing feedback and monitoring data together helps the sme to improve their understanding of end-user needs, ultimately supporting continuous requirements elicitation."
                },
                {
                    "id": "R195182",
                    "label": "On the Impact of Semantic Transparency on Understanding and Reviewing Social Goal Models",
                    "doi": "10.1109/re.2018.00031",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"context: i* is one of the most influential languages in the requirements engineering research community. perhaps due to its complexity and low adoption in industry, it became a natural candidate for studies aiming at improving its concrete syntax and the stakeholders' ability to correctly interpret i* models. objectives: we evaluate the impact of semantic transparency on understanding and reviewing i* models, in the presence of a language key. methods: we performed a quasi-experiment comparing the standard i* concrete syntax with an alternative that has an increased semantic transparency. we asked 57 novice participants to perform understanding and reviewing tasks on i* models, and measured their accuracy, speed and ease, using metrics of task success, time and effort, collected with eye-tracking and participants' feedback. results: we found no evidence of improved accuracy or speed attributable to the alternative concrete syntax. although participants' perceived ease was similar, they devoted significantly less visual effort to the model and the provided language key, when using the alternative concrete syntax. conclusions: the context provided by the model and language key may mitigate the i* symbol recognition deficit reported in previous works. however, the alternative concrete syntax required a significantly lower visual effort.\""
                },
                {
                    "id": "R195201",
                    "label": "An Experimental Comparison of Two Navigation Techniques for Requirements Modeling Tools",
                    "doi": "10.1109/re.2018.00032",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"in requirements engineering, many modeling tasks require viewing different parts of a model concurrently. however, traditional zoom+scroll navigation uses a single focus zoom, i.e., at a given point in time, a user can zoom in on a single spot in the model only. therefore, new focus+context navigation techniques have been proposed that allow multiple foci at the same time. in this paper, we report on an experiment with students where we compare the participants' performance when using a requirements modeling tool with traditional zoom+scroll navigation vs. one with so-called flexiview navigation which is a focus+context technique with multiple foci. the participants had to perform typical modeling tasks such as searching, editing, and traversing a model. all tasks were performed on medium-sized tablets with a tool for manipulating so-called imitgraphs. imitgraphs are enriched node-and-edge diagrams that can mimic various diagram types such as class, activity, or goal decomposition diagrams. we found that navigation with flexiview outperformed zoom+scroll navigation with respect to task completion time, number of mistakes, cognitive load, and user satisfaction.\""
                },
                {
                    "id": "R195216",
                    "label": "Morse: Reducing the Feature Interaction Explosion Problem using Subject Matter Knowledge as Abstract Requirements",
                    "doi": "10.1109/re.2018.00033",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "the feature interaction problem appears in many different kinds of complex systems, especially systems whose elements are created or maintained by separate entities - for example, a modern automobile that incorporates electronic systems produced by different suppliers. cross-cutting concerns, such as safety and security, require a comprehensive analysis of the possible interactions. however, there is a combinatorial explosion in the number of feature combinations to be considered. our work approaches the feature interaction problem from a novel point of view: we seek to use the abstract subject matter knowledge of domain experts to deduce why some features will not interact, rather than trying to discover or resolve the interactions. in this paper, we present a method that can automatically reduce the required number of combinations and situations that have to be evaluated or resolved for feature interactions. our tool, called morse, rules out feature combinations that cannot have interactions based on traceable deductions from relatively simple abstract requirements that capture relevant subject matter knowledge. our method is useful as a means of focusing attention on particular situations where more detailed functional requirements may be needed to avoid unacceptable risk arising from unintended interactions between features. relatively simple abstract requirements that capture relevant subject matter knowledge. our method is useful as a means of focusing attention on particular situations where more detailed functional requirements may be needed to avoid unacceptable risk arising from unintended interactions between features."
                },
                {
                    "id": "R195218",
                    "label": "Discovering, Analyzing, and Managing Safety Stories in Agile Projects",
                    "doi": "10.1109/re.2018.00034",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "traditionally, safety-critical projects have been developed using the waterfall process. however, this makes it costly and challenging to incrementally introduce new features and to certify the modified product for use. as a result, there has been increasing interest in adopting agile development paradigms within the safety-critical domain. this in turn introduces numerous challenges. in this paper we address the specific problems of discovering, analyzing, specifying, and managing safety requirements within the agile scrum process. we propose safetyscrum, a methodology that augments the scrum lifecycle with incrementally applied safety-related activities and introduces the notion of \"safety debt\" for incrementally tracking the current safety status of a project. we demonstrate the viability of safetyscrum for managing safety stories in an agile development environment by applying it to a project in which our existing unmanned aerial vehicle system is enhanced to support a river-rescue scenario."
                },
                {
                    "id": "R195224",
                    "label": "Understanding Challenging Situations in Agile Quality Requirements Engineering and Their Solution strategies: Insights from a Case Study",
                    "doi": "10.1109/re.2018.00035",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "in the last few years, agile development methods are getting increasingly popular in large-scale distributed contexts. despite this popularity, empirical studies have reported several challenges that large-scale distributed agile projects face regarding the implementation of quality requirements. however, there is little known about the mechanisms behind those challenges and the practices currently used by agile practitioners to adequately assure the implementation of quality requirements in distributed context. to look deeper into this, we performed a qualitative multi-case study in six different organizations in the netherlands. our multi-case study included seventeen semi-structured open-ended in-depth interviews with agile practitioners of different background and expertise. the analysis of the collected data re-sulted in identifying eleven mechanisms that could be associated with the previously published list of challenges. moreover, the analysis uncovered nine practices used by agile practitioners as solutions to the challenges, in order to ensure the implementation of quality requirements. last, we have mapped the identified mechanisms and practices to the previously identified challenges to get insight into the possible cause and mitigation of those challenges."
                },
                {
                    "id": "R195495",
                    "label": "SAFE: A Simple Approach for Feature Extraction from App Descriptions and App Reviews",
                    "doi": "10.1109/re.2017.71",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "a main advantage of app stores is that they aggregate important information created by both developers and users. in the app store product pages, developers usually describe and maintain the features of their apps. in the app reviews, users comment these features. recent studies focused on mining app features either as described by developers or as reviewed by users. however, extracting and matching the features from the app descriptions and the reviews is essential to bear the app store advantages, e.g. allowing analysts to identify which app features are actually being reviewed and which are not. in this paper, we propose safe, a novel uniform approach to extract app features from the single app pages, the single reviews and to match them. we manually build 18 part-of-speech patterns and 5 sentence patterns that are frequently used in text referring to app features. we then apply these patterns with several text pre-and post-processing steps. a major advantage of our approach is that it does not require large training and configuration data. to evaluate its accuracy, we manually extracted the features mentioned in the pages and reviews of 10 apps. the extraction precision and recall outperformed two state-of-the-art approaches. for well-maintained app pages such as for google drive our approach has a precision of 87% and on average 56% for 10 evaluated apps. safe also matches 87% of the features extracted from user reviews to those extracted from the app descriptions."
                },
                {
                    "id": "R195513",
                    "label": "A Framework for Improving the Verifiability of Visual Notation Design Grounded in the Physics of Notations",
                    "doi": "10.1109/re.2017.37",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "this paper proposes a systematic framework for applying the physics of notations (pon), a theory for the design of cognitively effective visual notations. the pon consists of nine principles, but not all principles lend themselves equally to a clear and unambiguous operationalization. as a result, many visual notations designed according to the pon apply it in different ways. the proposed framework guides what information is required of a reported pon application to ensure that the application of each principle is verifiable. the framework utilizes an evidence-driven design rationale model to structure information needed to assess principles requiring user involvement or cognitive theories. this approach aims to reduce ambiguity in some of the principles by making design choices explicit, and highlighting the level of evidence presented to support it. we demonstrate the proposed framework in a showcase of a recently published visual notation which has been designed with the pon in mind."
                },
                {
                    "id": "R195519",
                    "label": "Using Argumentation to Explain Ambiguity in Requirements Elicitation Interviews",
                    "doi": "10.1109/re.2017.27",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"the requirements elicitation process often starts with an interview between a customer and a requirements analyst. during these interviews, ambiguities in the dialogic discourse may reveal the presence of tacit knowledge that needs to be made explicit. it is therefore important to understand the nature of ambiguities in interviews and to provide analysts with cognitive tools to identify and alleviate ambiguities. ambiguities perceived by analysts are sometimes triggered by specific categories of terms used by the customer such as pronouns, quantifiers, and vague or under-specified terms. however, many of the ambiguities that arise in practice cannot be rooted in single terms. rather, entire fragments of speech and their relation to the mental state of the analyst need to be considered.in this paper, we show that particular types of ambiguities can be characterised by means of argumentation theory. argumentation is the study of how conclusions can be reached through logical reasoning. in an argumentation theory, statements are represented as arguments, and conflict relations among statements are represented as attacks. based on a set of ambiguous fragments extracted from interviews, we define a model of the mental state of the analyst during an interview and translate it into an argumentation theory. then, we show that many of the ambiguities can be characterized in terms of 'attacks' on arguments. the main novelty of this work is in addressing the problem of explaining fragment-level ambiguities in requirements elicitation interviews through the formal modeling of the analyst's mental model using argumentation theory. our contribution provides a data-grounded, theoretical basis to have a more complete understanding of the ambiguity phenomenon, and lays the foundations to design intelligent computer-based agents that are able to automatically identify ambiguities.\""
                },
                {
                    "id": "R195551",
                    "label": "Feedback Gathering from an Industrial Point of View",
                    "doi": "10.1109/re.2017.9",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "feedback communication channels allow end-users to express their needs, which can be considered in software development and evolution. although feedback gathering and analysis have been identified as an important topic and several researchers have started their investigation, information is scarce on how software companies currently elicit end-user feedback. in this study, we explore the experiences of software companies with respect to feedback gathering. the results of a case study and online survey indicate two sides of the same coin: on the one hand, most software companies are aware of the relevance of end-user feedback for software evolution and provide feedback channels, which allow end-users to communicate their needs and problems. on the other hand, the quantity and quality of the feedback received varies. we conclude that software companies still do not fully exploit the potential of end-user feedback for software development and evolution."
                },
                {
                    "id": "R195578",
                    "label": "What Requirements Knowledge Do Developers Need to Manage Change in Safety-Critical Systems?",
                    "doi": "10.1109/re.2017.65",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"developers maintaining safety-critical systems need to assess the impact a proposed change would have upon existing safety controls. by leveraging the network of traceability links that are present in most safety-critical systems, we can push timely information about related hazards, environmental assumptions, and safety requirements to developers. in this work we take a design science approach to discover the informational needs of developers as they engage in software maintenance activities and then propose and evaluate techniques for presenting and visualizing this information. through a human-centered study involving five safety-critical system practitioners and 14 experienced developers, we analyze the way in which developers use requirements knowledge while maintaining safety-critical code, identify their informational needs, and propose and evaluate a supporting visualization technique. the insights proposed as a result of this study can be used to design requirements-based knowledge tools for supporting developers' maintenance tasks.\""
                },
                {
                    "id": "R195587",
                    "label": "What Questions do Requirements Engineers Ask?",
                    "doi": "10.1109/re.2017.76",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "requirements engineering (re) is comprised of various tasks related to discovering, documenting, and maintaining different kinds of requirements. to accomplish these tasks, a requirements engineer or business analyst needs to retrieve and combine information from multiple sources such as use case models, interview scripts, and business rules. however, collecting and analyzing all the required data can be tedious and the resulting data is often incomplete with inadequate trace links. analyzing real-world queries can shed light on the questions requirements professionals would like to ask and the artifacts needed to support such questions. we therefore conducted an online survey with requirements professionals in the it industry. our analysis included 29 survey responses and a total of 159 natural language queries. using open coding and grounded theory, we analyzed and grouped these queries into 9 different query purposes and 54 sub-purposes, and also identified frequently used artifacts. the results from the survey could help project-level planners identify important questions, proactively instrument their environments with supporting tools, and strategically collect data that is needed to answer the queries of interest to their project."
                },
                {
                    "id": "R195597",
                    "label": "Datasets from Fifteen Years of Automated Requirements Traceability Research: Current State, Characteristics, and Quality",
                    "doi": "10.1109/re.2017.80",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "software datasets play a crucial role in advancing automated software traceability research. they can be used by researchers in different ways to develop or validate new automated approaches. the diversity and quality of the datasets within a research community have a significant impact on the accuracy, generalizability, and reproducibility of the results and consequently on the usefulness and practicality of the techniques under study. collecting and assessing the quality of such datasets are not trivial tasks and have been reported as an obstacle by many researchers in the domain of software engineering. this paper presents a first-of-its-kind study to review and assess the datasets that have been used in software traceability research over the last fifteen years. it presents and articulates the current status of these datasets, their characteristics, and their threats to validity. furthermore, this paper introduces a traceability-dataset quality assessment (t-dqa) framework to categorize software traceability datasets and assist researchers to select appropriate datasets for their research based on different characteristics of the datasets and the context in which those datasets will be used."
                },
                {
                    "id": "R195664",
                    "label": "The Trouble with Security Requirements",
                    "doi": "10.1109/re.2017.13",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"manifold approaches to security requirements engineering have been proposed, yet there is no consensus how to elicit, analyze, or express security needs. this perspective paper systematizes the problem space of security requirements engineering. security needs result from the interplay of three dimensions: threats, security goals, and system design. elementary statements can be made in each dimension, but such one-dimensional requirements remain partial and insufficient. to understand security needs, one has to analyze their interaction. distinct analysis tasks arise for each pair of dimensions and are supported by different techniques: risk analysis, as in coras, between threats and security goals; security design, as exemplified by the framework of haley et al., between goals and design; and security design analysis, such as microsoft's threat modeling technique with data flow diagrams and stride, between design and threats. all three perspectives are necessary to develop secure systems. security requirements engineering must iterate through them, because threats determine the relevance of security goals, security design seeks ways to fulfill them, and design choices themselves influence threats and security goals.\""
                },
                {
                    "id": "R195669",
                    "label": "Safety-Focused Security Requirements Elicitation for Medical Device Software",
                    "doi": "10.1109/re.2017.21",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "security attacks on medical devices have been shown to have potential safety concerns. because of this, stakeholders (device makers, regulators, users, etc.) have increasing interest in enhancing security in medical devices. an effective means to approach this objective is to integrate systematic security requirements elicitation and analysis into the design and evaluation of medical device software. this paper extends the sequence-based enumeration approach, a systematic approach for defining the behavior of embedded software, to analyze the requirement documents of a medical device for the purpose of eliciting security requirements. as a proof of concept, we apply our approach on a concrete case study, which shows that the extended approach is useful for identifying sequences of medical device events that might be harmful to the patient, for example because the events are initiated by an active adversary trying to use the device in a malicious way. we then show how security requirements may be formulated based on the identified threats. by exploring these sequences systematically, the developers can reliably assess what, where, and how the security threats may manifest in their system, what the safety implications are, and finally they can evaluate the resulting requirements and mitigations."
                },
                {
                    "id": "R195679",
                    "label": "Reinforcing Security Requirements with Multifactor Quality Measurement",
                    "doi": "10.1109/re.2017.77",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "choosing how to write natural language scenarios is challenging, because stakeholders may over-generalize their descriptions or overlook or be unaware of alternate scenarios. in security, for example, this can result in weak security constraints that are too general, or missing constraints. another challenge is that analysts are unclear on where to stop generating new scenarios. in this paper, we introduce the multifactor quality method (mqm) to help requirements analysts to empirically collect system constraints in scenarios based on elicited expert preferences. the method combines quantitative statistical analysis to measure system quality with qualitative coding to extract new requirements. the method is bootstrapped with minimal analyst expertise in the domain affected by the quality area, and then guides an analyst toward selecting expert-recommended requirements to monotonically increase system quality. we report the results of applying the method to security. this include 550 requirements elicited from 69 security experts during a bootstrapping stage, and subsequent evaluation of these results in a verification stage with 45 security experts to measure the overall improvement of the new requirements. security experts in our studies have an average of 10 years of experience. our results show that using our method, we detect an increase in the security quality ratings collected in the verification stage. finally, we discuss how our proposed method helps to improve security requirements elicitation, analysis, and measurement."
                },
                {
                    "id": "R195692",
                    "label": "\u201cSHORT\u201der Reasoning About Larger Requirements Models",
                    "doi": "10.1109/re.2017.31",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "when requirements engineering(re) models are unreasonably complex, they cannot support efficient decision making. short is a tool to simplify that reasoning by exploiting the \"key\" decisions within re models. these \"keys\" have the property that once values are assigned to them, it is very fast to reason over the remaining decisions. using these \"keys\", reasoning about re models can be greatly shortened by focusing stakeholder discussion on just these key decisions.this paper evaluates the short tool on eight complex re models. we find that the number of keys are typically only 12% of all decisions. since they are so few in number, keys can be used to reason faster about models. for example, using keys, we can optimize over those models (to achieve the most goals at least cost) two to three orders of magnitude faster than standard methods. better yet, finding those keys is not difficult: short runs in low order polynomial time and terminates in a few minutes for the largest models."
                },
                {
                    "id": "R195713",
                    "label": "Modeling and Reasoning with Changing Intentions: An Experiment",
                    "doi": "10.1109/re.2017.19",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"existing modeling approaches in requirements engineering assume that stakeholder goals are static: once set, they remain the same throughout the lifecycle of the project. of course, such goals, like anything else, may change over time. in earlier work, we introduced evolving intentions: an approach that allows stakeholders to specify how evaluations of goal model elements change over time. simulation over evolving intentions enables stakeholders to ask a variety of 'what if' questions, and evaluate possible evolutions of a goal model. growingleaf is a web-based tool that implements both the modeling and analysis components of this approach. in this paper, we investigate the effectiveness and usability of evolving intentions, simulation over evolving intentions, and growingleaf. we report on a between-subjects experiment we conducted with fifteen graduate students familiar with requirements engineering. using qualitative, quantitative, and timing data, we show that evolving intentions were intuitive, that simulation over evolving intentions increased the subjects' understanding and produced meaningful results, and that growingleaf was found to be effective and usable.\""
                },
                {
                    "id": "R195728",
                    "label": "Does Goal-Oriented Requirements Engineering Achieve Its Goal?",
                    "doi": "10.1109/re.2017.40",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "the number of papers and articles on goals would suggest that goal-oriented requirements engineering is a well understood and mature area within the requirements engineering discipline. in particular, there is a wealth of published material on formal goal modelling approaches. however, the uptake of the goal approaches advocated by academics and researchers within real world settings appears to be quite low. where goals are used in industrial practice their use is mainly informal and the methods used are inconsistent. there appears to be a significant gap between research and practice in the use of goals within requirements engineering. a two-part study was undertaken to check whether there is evidence to support this view of a disconnection between research and industry. firstly, a literature survey of requirements engineering papers about goals reveals a large body of published material, but the majority has little industrial involvement. secondly, a questionnaire completed by experienced requirements engineering practitioners suggests that use of goals in practice is inconsistent, informal, and rarely utilises formal modelling approaches. this paper proposes future work that would close the gap between research and practice in the use of goals within requirements engineering."
                },
                {
                    "id": "R195743",
                    "label": "New Frontiers for Requirements Engineering",
                    "doi": "10.1109/re.2017.23",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "requirements engineering (re) has grown from its humble beginnings to embrace a wide variety of techniques, drawn from many disciplines, and the diversity of tasks currently performed under the label of re has grown beyond that encom-passed by software development. we briefly review how re has evolved and observe that re is now a collection of best practices for pragmatic, outcome-focused critical thinking \u2013 applicable to any domain. we discuss an alternative perspective on, and de-scription of, the discipline of re and advocate for the evolution of re toward a discipline that supports the application of re prac-tice to any domain. we call upon re practitioners to proactively engage in alternative domains and call upon researchers that adopt practices from other domains to actively engage with their inspiring domains. for both, we ask that they report upon their experience so that we can continue to expand re frontiers."
                },
                {
                    "id": "R195749",
                    "label": "How Much Undocumented Knowledge is there in Agile Software Development?: Case Study on Industrial Project Using Issue Tracking System and Version Control System",
                    "doi": "10.1109/re.2017.33",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "in agile software development projects, software engineers prioritize implementation over documentation to eliminate needless documentation. is the cost of missing documentation greater than the cost of producing unnecessary or unused documentation? even without these documents, software engineers maintain other software artifacts, such as tickets in an issue tracking system (its) or source code committed to a version control system (vcs). do these artifacts contain the necessary knowledge? in this paper, we examine undocumented knowledge in an agile software development project at ntt. for our study, we collected 159 commit logs in a vcs and 102 tickets in the its from the three-month period of the project. we propose a ticket-commit network chart (tcc) that visually represents time-series commit activities along with filed issue tickets. we also implement a tool to generate the tcc using both commit log and ticket data. our study revealed that in 16% of all commits, software engineers committed source code to the vcs without a corresponding issue ticket in the its. had these commits been based on individual issue tickets, these \"unissued\" tickets would have accounted for 20% of all tickets. software users and requirements engineers also evaluated the contents of these commits and found that 42% of the \"unissued\" tickets were required for software operation and 23% of those were required for requirements modification."
                },
                {
                    "id": "R195767",
                    "label": "Software Requirements Analyst Profile: A Descriptive Study of Brazil and Mexico",
                    "doi": "10.1109/re.2017.22",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "software requirements analyst work is considered crucial in the software development lifecycle. this paper presents a descriptive study on the software requirements analyst profile, considering brazilian and mexican markets, two countries that lead it investment ranking in latin america. to identify the competences expected by the brazilian and mexican markets for software requirements professionals was the study objective. the competency model considers a set of knowledge, skills and attitudes. content analysis and nvivo software were used to categorize 311 job ads for software requirements analysts between 2016 and 2017. in terms of knowledge, the importance of higher education for both countries was identified. \"using techniques and tools,\" \"methodological competence,\" \"good written communication,\" and \"good verbal communication\" were identified as relevant skills. the most important attitudes identified were: \"analytical thinking,\" \"organization,\" \"interpersonal relationship\" and \"information sharing.\" most of results obtained confirm requirements analyst skills pointed out by other authors, but there are some differences due to the geographic context. at the same time, findings can help improve the understanding of different approaches in the requirements engineering field in the two countries and professionals working particularly in the latin american market."
                },
                {
                    "id": "R195789",
                    "label": "Improving the Identification of Hedonic Quality in User Requirements \u2014 A Controlled Experiment",
                    "doi": "10.1109/re.2017.49",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "context and motivation systematically engineering a good user experience (ux) into a computer-based system under development demands that the user requirements of the system reflect all needs, including emotional, of all stakeholders. user requirements address two different types of qualities: pragmatic qualities (pqs), that address system functionality and usability, and hedonic qualities (hqs) that address the stakeholder\\'s psychological well-being. studies show that users tend to describe such satisfying uxes mainly with pqs, and that some users seem to believe that they are describing a hq when they are actually describing a pq. question/problem the problem is to see if classification of any user requirement as pq-related or hq-related is difficult, and if so, why. principal ideas/results we conducted a controlled experiment in which twelve requirements-engineering and ux professionals, hereinafter called \"classifiers\" classified each of 105 user requirements as pq-related or hq-related. the experiment shows that neither (1) a classifier\\'s involvement in the project from which the requirements came nor (2) the classifier\\'s use of a detailed model of the qualities in addition to the standard definitions of \"pq\" and \"hq\" has a positive effect on the consistency of the classifier\\'s classification with that of others. contribution the experiment revealed that classification of user requirements is a lot harder than initially assumed."
                },
                {
                    "id": "R195806",
                    "label": "Usability Insights for Requirements Engineering Tools: A User Study with Practitioners in Aeronautics",
                    "doi": "10.1109/re.2017.20",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "requirements engineering plays a crucial role in coordinating the different stakeholders needed for safe aeronautics systems engineering. we conducted a qualitative study, using interviews and mockups, with fifteen industrial practitioners from four aeronautics companies, in order to investigate what tasks are actually performed by requirements engineers and how current tools support these tasks. we found that re-specific tools constrain engineers to a rigid workflow, which is conflicting with the adaptive exploration of the problem. engineers often start by using general-purpose tools to foster exploration and collaborative work with suppliers, at the expense of traceability. when engineers shift to requirements refinement and verification, they must use re-specific tools to grant traceability. then, the lack of tool usability yields significant time loss and dissatisfaction. based on scenarios of observed re practices and walkthrough, we formulate usability insights for re-specific tools in order to conciliate flexibility and traceability throughout the re process."
                },
                {
                    "id": "R195849",
                    "label": "Detecting Vague Words &amp; Phrases in Requirements Documents in a Multilingual Environment",
                    "doi": "10.1109/re.2017.24",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "vagueness in software requirements documents can lead to several maintenance problems, especially when the customer and development team do not share the same language. currently, companies rely on human translators to maintain communication and limit vagueness by translating the requirement documents by hand. in this paper, we describe two approaches that automatically identify vagueness in requirements documents in a multilingual environment. we perform two studies for calibration purposes under strict industrial limitations, and describe the tool that we ultimately deploy. in the first study, six participants, two native portuguese speakers and four native spanish speakers, evaluated both approaches. then, we conducted a field study to test the performance of the best approach in real-world environments at two companies. we describe several lessons learned for research and industrial deployment."
                },
                {
                    "id": "R195857",
                    "label": "A Case Study on Evaluating the Relevance of Some Rules for Writing Requirements Through an Online Survey",
                    "doi": "10.1109/re.2017.11",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"as part of a research project that aims at proposing a new methodology for defining a series of rules for writing good requirements \u2013 often referred to as a controlled natural language (cnl) \u2013 for the french space agency (cnes, centre national d'\u00e9tudes spatiales), we asked both experienced engineers and non-experts to fill in an online questionnaire in order to gather their perception about requirements written according to recommendations commonly found in cnls, and to compare them with seemingly more natural and less restrictive formulations. the examples we used for this case study were adapted from genuine requirements in french, extracted from several specifications of a recent space project. our main goal is to evaluate whether (and to what extent) the writing rules we considered may be relevant for the engineers at cnes. in particular, we try to identify cases where the experts' opinions differ from the recommended use and where these rules could thus probably be adapted.\""
                },
                {
                    "id": "R195866",
                    "label": "A Case Study on a Specification Approach Using Activity Diagrams in Requirements Documents",
                    "doi": "10.1109/re.2017.28",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"rising complexity of systems has long been a major challenge in requirements engineering. this manifests in more extensive and harder to understand requirements documents. at the daimler ag, an approach is applied that combines the use of activity diagrams with natural language specifications to specify system functions. the approach starts with an activity diagram that is created to get an early overview. the contained information is then transferred to a textual requirements document, where details are added and the behavior is refined. while the approach aims to reduce efforts needed to understand a system's behavior, the application of the approach itself causes new challenges on its own. by examining existing specifications at daimler, we identified nine categories of inconsistencies and deviations between activity diagrams and their textual representations. in a case study, we examined one system in detail to assess how often these occur. in a follow-up survey, we presented instances of the categories to different stakeholders of the system and let them asses the categories regarding their severity. our analysis indicates that a coexistence of textual and graphical representations of models without proper tool support results in inconsistencies and deviations that may cause severe maintenance costs or even provoke faults in subsequent development steps.\""
                },
                {
                    "id": "R195982",
                    "label": "A Formalization Method to Process Structured Natural Language to Logic Expressions to Detect Redundant Specification and Test Statements",
                    "doi": "10.1109/re.2017.38",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "automotive systems are constantly increasing in complexity and size. beside the increase of requirements specifications and related test specification due to new systems and higher system interaction, we observe an increase of redundant specifications. as the predominant specification language (both for requirements and test cases) is still natural text, it is not easy to detect these redundancies. in principle, to detect these redundancies, each statement has to be compared to all others. this proves to be difficult because of number and informal expression of statements. in this paper we propose a solution to the problem of detecting redundant specification and test statements described in structured natural language. we propose a formalization process for requirements specification and test statements, allowing us to detect redundant statements and thus reduce the efforts for specification and validation. specification pattern systems and linear temporal logic provide the base for our process. we did evaluate the method in the context of mercedes-benz passenger car development. the results show that for the investigated sample set of test statements, we could detect about 30% of test steps as redundant. this indicates the savings potential of our approach."
                },
                {
                    "id": "R195986",
                    "label": "Do Words Make a Difference? An Empirical Study on the Impact of Taxonomies on the Classification of Requirements",
                    "doi": "10.1109/re.2017.57",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"requirements taxonomies help to classify and channel the requirements in a project. a very simple taxonomy is the distinction between functional and non-functional requirements. furthermore, a taxonomy helps to decide if a statement is a requirement at all or just something else (e.g., 'information'). the quality of a taxonomy is important as we do not want to put a statement in the wrong category.in this paper, we argue that we need to take cognitive psychology into account in this task of requirements classification. cognitive psychology focuses on the abilities and limitations of the human mind. we present a controlled experiment and a replication in which we compare three requirements taxonomies.the participants had to evaluate a set of requirements based on the given taxonomies. the results of these experiments show that there are differences between the taxonomies: interestingly, the question whether a statement is identified as a requirement or not depends on the taxonomy. these experiments present initial results, we assume that these results are related to phenomena of cognitive psychology.we conclude that the wording should be carefully taken into account in the definition of the categories of a high quality requirements taxonomy.\""
                },
                {
                    "id": "R195995",
                    "label": "Requirements Capture and Analysis in ASSERT(TM)",
                    "doi": "10.1109/re.2017.54",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "capturing high-level requirements in a human readable but formal representation suitable for analysis is an important goal for ge. to that end we have augmented an existing controlled-english modeling language with a new controlled-english requirements capture language to create the requirements capture frontend of the assert(tm) tool suite. requirements captured in assert can be analyzed for a number of possible shortcomings, both individually and collectively. once a set of requirements has reached a satisfactory level of completeness, consistency, etc., it can then be further used to generate test cases and test procedures. this paper will focus on the requirements capture and analysis functions of assert and will illustrate its capabilities with a sample problem previously used as a challenge problem for requirements specification."
                },
                {
                    "id": "R195998",
                    "label": "Mining Associations Between Quality Concerns and Functional Requirements",
                    "doi": "10.1109/re.2017.68",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "the cost and effort of developing software systems in a new technical area can be extensive. an organization must perform a domain analysis to discover competing products, analyze their architectures and features, and ultimately discover and specify product requirements. however, delivering high quality products, depends not only on gaining an understanding of functional requirements, but also of qualities such as performance, reliability, security, and usability. discovering such concerns early in the requirements process drives architectural design decisions. this paper extends our prior work on mining functional requirements from large collections of domain documents, by proposing and evaluating a new technique for discovering and specifying quality concerns related to specific functional components. we evaluate our approach against three domains of positive train control, electronic health records, and medical infusion pumps, and show that it significantly outperforms a basic information retrieval approach. finally we classified the forms of retrieved information, discussed the utility of different types, and conducted a small study with an experienced engineer to investigate the quality of requirements produced using our approach."
                },
                {
                    "id": "R196009",
                    "label": "Legal Markup Generation in the Large: An Experience Report",
                    "doi": "10.1109/re.2017.10",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "legal markup (metadata) is an important prerequisite for the elaboration of legal requirements. manually encoding legal texts into a markup representation is laborious, specially for large legal corpora amassed over decades and centuries. at the same time, automating the generation of markup in a fully accurate manner presents a challenge due to the flexibility of the natural-language content in legal texts and variations in how these texts are organized. following an action research method, we successfully collaborated with the government of luxembourg in transitioning five major legislative codes from plain-text to a legal markup format. our work focused on generating markup for the structural elements of the underlying codes. the technical basis for our work is an adaptation and enhancement of an academic markup generation tool developed in our prior research [1]. we reflect on the experience gained from applying automated markup generation at large scales. in particular, we elaborate the decisions we made in order to strike a cost-effective balance between automation and manual work for legal markup generation. we evaluate the quality of automatically-generated structural markup in real-world conditions and subject to the practical considerations of our collaborating partner."
                },
                {
                    "id": "R196014",
                    "label": "An Evaluation of Constituency-Based Hyponymy Extraction from Privacy Policies",
                    "doi": "10.1109/re.2017.87",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "requirements analysts can model regulated data practices to identify and reason about risks of non-compliance. if terminology is inconsistent or ambiguous, however, these models and their conclusions will be unreliable. to study this problem, we investigated an approach to automatically construct an information type ontology by identifying information type hyponymy in privacy policies using tregex patterns. tregex is a utility to match regular expressions against constituency parse trees, which are hierarchical expressions of natural language clauses, including noun and verb phrases. we discovered the tregex patterns by applying content analysis to 30 privacy policies from six domains (shopping, telecommunication, social networks, employment, health, and news.) from this dataset, three semantic and four lexical categories of hyponymy emerged based on category completeness and word-order. among these, we identified and empirically evaluated 72 tregex patterns to automate the extraction of hyponyms from privacy policies. the patterns match information type hyponyms with an average precision of 0.72 and recall of 0.74."
                },
                {
                    "id": "R196021",
                    "label": "Gamifying Collaborative Prioritization: Does Pointsification Work?",
                    "doi": "10.1109/re.2017.66",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "gamification has been applied in software engineering contexts, and more recently in requirements engineering with the purpose of improving the motivation and engagement of people performing specific engineering tasks. but often an objective evaluation that the resulting gamified tasks successfully meet the intended goal is missing. on the other hand, current practices in designing gamified processes seem to rest on a try, test and learn approach, rather than on first principles design methods. thus empirical evaluation should play an even more important role.we combined gamification and automated reasoning techniques to support collaborative requirements prioritization in software evolution. a first prototype has been evaluated in the context of three industrial use cases. to further investigate the impact of specific game elements, namely point-based elements, we performed a quasi-experiment comparing two versions of the tool, with and without pointsification. we present the results from these two empirical evaluations, and discuss lessons learned."
                },
                {
                    "id": "R196033",
                    "label": "Behind Points and Levels \u2014 The Influence of Gamification Algorithms on Requirements Prioritization",
                    "doi": "10.1109/re.2017.59",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"prioritizing requirements is a crucial ingredient of successful requirements engineering (re). the popular prioritization techniques assume that stakeholders are known and can be mandated to contribute to the prioritization process. this prerequisite no longer holds for many of today's systems where significant stakeholders (end-users, in particular) are outside organizational reach: they are neither known nor can they be identified among the members of the involved organizations. classic techniques for involving these stakeholders such as polls or questionnaires are neither interactive nor collaborative, which is detrimental for prioritization. social media enable collaborative prioritization, but fall short in motivating stakeholders outside organizational reach to participate voluntarily. in this light, we are developing the garuso platform, which combines social media with gamification for motivating stakeholders. while first approaches to employing gamification in re are promising, research is still in its infancy. especially, little is known about the influence of the gamification algorithms controlling single game elements on the stakeholders' activities. in this paper we report on a field experiment in which we investigated this influence with garuso. we found statistically significant differences between different algorithms controlling single game elements on the contributions of stakeholders to the prioritization of requirements.\""
                },
                {
                    "id": "R196046",
                    "label": "Task Interruptions in Requirements Engineering: Reality Versus Perceptions!",
                    "doi": "10.1109/re.2017.75",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"task switching and interruptions are a daily reality in software development projects: developers switch between requirements engineering (re), coding, testing, daily meetings, and other tasks. task switching may increase productivity through increased information flow and effective time management. however, it might also cause a cognitive load to reorient the primary task, which accounts for the decrease in developers' productivity and increases in errors. this cognitive load is even greater in cases of cognitively demanding tasks as the ones typical for re activities. in this paper, to compare the reality of task switching in re with the perception of developers, we conducted two studies: (i) a case study analysis on 5,076 recorded tasks of 19 developers and (ii) a survey of 25 developers. the results of our retrospective analysis show that in all of the cases that the disruptiveness of re interruptions is statistically different from other software development tasks, re related tasks are more vulnerable to interruptions compared to other task types. moreover, we found that context switching, the priority of the interrupting task, and the interruption source and timing are key factors that impact re interruptions. we also provided a set of re task switching patterns along with recommendations for both practitioners and researchers. while the results of our retrospective analysis show that self-interruptions are more disruptive than external interruptions, developers have different perceptions about the disruptiveness of various sources of interruptions.\""
                },
                {
                    "id": "R196065",
                    "label": "Piggybacking on an Autonomous Hauler: Business Models Enabling a System-of-Systems Approach to Mapping an Underground Mine",
                    "doi": "10.1109/re.2017.55",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"with ever-increasing productivity targets in mining operations, there is a growing interest in mining automation. in future mines, remote controlled and autonomous haulers will operate underground guided by lidar (light detection and ranging) sensors. we envision reusing lidar measurements to maintain accurate mine maps that would contribute to both safety and productivity. extrapolating from a pilot project on reliable wireless communication in boliden's kankberg mine, we propose establishing a system-of-systems (sos) with lidar-equipped haulers and existing mapping solutions as constituent systems. sos requirements engineering inevitably adds a political layer, as independent actors are stakeholders both on the system and sos levels. we present four sos scenarios representing different business models, discussing how development and operations could be distributed among boliden and external stakeholders, e.g., the vehicle suppliers, the hauling company, and the developers of the mapping software. based on eight key variation points, we compare the four scenarios from both technical and business perspectives. finally, we validate our findings in a seminar with participants from the relevant stakeholders. we conclude that to determine which scenario is the most promising for boliden, trade-offs regarding control, costs, risks, and innovation must be carefully evaluated.\""
                },
                {
                    "id": "R196070",
                    "label": "What do Support Analysts Know About Their Customers? On the Study and Prediction of Support Ticket Escalations in Large Software Organizations",
                    "doi": "10.1109/re.2017.61",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"understanding and keeping the customer happy is a central tenet of requirements engineering. strategies to gather, analyze, and negotiate requirements are complemented by efforts to manage customer input after products have been deployed. for the latter, support tickets are key in allowing customers to submit their issues, bug reports, and feature requests. whenever insufficient attention is given to support issues, however, their escalation to management is time-consuming and expensive, especially for large organizations managing hundreds of customers and thousands of support tickets. our work provides a step towards simplifying the job of support analysts and managers, particularly in predicting the risk of escalating support tickets. in a field study at our large industrial partner, ibm, we used a design science methodology to characterize the support process and data available to ibm analysts in managing escalations. through iterative cycles of design and evaluation, we translated our understanding of support analysts' expert knowledge of their customers into features of a support ticket model to be implemented into a machine learning model to predict support ticket escalations. we trained and evaluated our machine learning model on over 2.5 million support tickets and 10,000 escalations, obtaining a recall of 79.9% and an 80.8% reduction in the workload for support analysts looking to identify support tickets at risk of escalation. further on-site evaluations, through a prototype tool we developed to implement our machine learning techniques in practice, showed more efficient weekly support-ticket-management meetings. the features we developed in the support ticket model are designed to serve as a starting place for organizations interested in implementing our model to predict support ticket escalations, and for future researchers to build on to advance research in escalation prediction.\""
                },
                {
                    "id": "R197609",
                    "label": "Requirements Engineering Challenges in Large-Scale Agile System Development",
                    "doi": "10.1109/re.2017.60",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "motivated by their success in software development, companies implement agile methods and their practices increasingly for software-intense, large products, such as cars, telecommunication infrastructure, and embedded systems. such systems are usually subject to safety and regulative concerns as well as different development cycles of hardware and software. consequently, requirements engineering involves upfront and detailed analysis, which can be at odds with agile (software) development. in this paper, we present results from a multiple case study with two car manufacturers, a telecommunications company, and a technology company that are on the journey to introduce organization wide continuous integration and continuous delivery to customers. based on 20 qualitative interviews, 5 focus groups, and 2 cross-company workshops, we discuss possible scopes of agile methods within system development, the consequences this has on the role of requirements, and the challenges that arise from the interplay of requirements engineering and agile methods in large-scale system development. these relate in particular to communicating and managing knowledge about a) customer value and b) the system under development. we conclude that better alignment of a holistic requirements model with agile development practices promises rich gains in development speed, flexibility, and overall quality of software and systems."
                },
                {
                    "id": "R198663",
                    "label": "Requirements Engineering Visualization: A Systematic Literature Review",
                    "doi": "10.1109/re.2016.61",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "requirements engineering (re) is a decision-centric activity which is highly data-intensive. the results of this process are known to have key impact on the results of the project. as known from the experience in other fields and disciplines, visualization can potentially provide more insights into data, information and knowledge studied. while research in the area of information visualization and its application to software engineering has rapidly increased over the last decade, there is only a limited amount of studies addressing the usage and impact of visualization techniques for re activities. in this paper, we report on the results of a systematic literature review (slr) related to re visualization. extending the established slr process by the usage of grounded theory for the encoding of papers, we synthesize 18 usage patterns. even though there are punctual applications, there is a clear deficit on a holistic perspective across the different re activities. as another conclusion, we derive the clear need for more research on visualization support in particular for tackling requirements uncertainty, requirements verification, and modeling, as well as non-functional requirements (nfrs)."
                },
                {
                    "id": "R198673",
                    "label": "A Serious Game for Eliciting Social Engineering Security Requirements",
                    "doi": "10.1109/re.2016.39",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "social engineering is the acquisition of information about computer systems by methods that deeply include nontechnical means. while technical security of most critical systems is high, the systems remain vulnerable to attacks from social engineers. social engineering is a technique that: (i) does not require any (advanced) technical tools, (ii) can be used by anyone, (iii) is cheap. traditional security requirements elicitation approaches often focus on vulnerabilities in network or software systems. few approaches even consider the exploitation of humans via social engineering and none of them elicits personal behaviours of individual employees. while the amount of social engineering attacks and the damage they cause rise every year, the security awareness of these attacks and their consideration during requirements elicitation remains negligible. we propose to use a card game to elicit these requirements, which all employees of a company can play to understand the threat and document security requirements. the game considers the individual context of a company and presents underlying principles of human behaviour that social engineers exploit, as well as concrete attack patterns. we evaluated our approach with several groups of researchers, it administrators, and professionals from industry."
                },
                {
                    "id": "R198681",
                    "label": "A Theory of Vagueness and Privacy Risk Perception",
                    "doi": "10.1109/re.2016.20",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"ambiguity arises in requirements when a statement is unintentionally or otherwise incomplete, missing information, or when a word or phrase has more than one possible meaning. for web-based and mobile information systems, ambiguity, and vagueness inparticular, undermines the ability of organizations to align their privacy policies with their data practices, which can confuse or mislead users thus leading to an increase in privacy risk. in this paper, we introduce a theory of vagueness for privacy policy statements based on a taxonomy of vague terms derived from an empirical content analysis of 15 privacy policies. the taxonomy was evaluated in a paired comparison experiment and results were analyzed using the bradley-terry model to yield a rank order of vague terms in both isolation and composition. the theory predicts how vague modifiers to information actions and information types can be composed to increase or decrease overall vagueness. we further provide empirical evidence based on factorial vignette surveys to show how increases in vagueness will decrease users' acceptance of privacy risk and thus decrease users' willingness to share personal information.\""
                },
                {
                    "id": "R198689",
                    "label": "Stimulating Stakeholders' Imagination: New Creativity Triggers for Eliciting Novel Requirements",
                    "doi": "10.1109/re.2016.36",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "requirements engineering is a creative process in which stakeholders and engineers work together to create ideas for new products, services and systems. several techniques have proved to be effective for eliciting creative requirements. yet, most of these techniques are heavy to implement and require long periods of time to be applied correctly. few lightweight creativity techniques have been developed for use in requirements engineering. one such lightweight technique is the creativity trigger, which provides simple guidance to stakeholders and engineers to help produce creative requirements. while easy to apply, creativity triggers were derived informally from experience of practitioners and have not been validated in a systematic way. this paper reports design and preliminary validation research, that sought to provide empirical foundations for a more complete set of lightweight creativity triggers, to be used by stakeholders and engineers to quickly and simply generate new and useful requirements on products, services and systems."
                },
                {
                    "id": "R198706",
                    "label": "Challenging Incompleteness of Performance Requirements by Sentence Patterns",
                    "doi": "10.1109/re.2016.24",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "performance requirements play an important role in software development. they describe system behavior that directly impacts the user experience. specifying performance requirements in a way that all necessary content is contained, i.e., the completeness of the individual requirements, is challenging, yet project critical. furthermore, it is still an open question, what content is necessary to make a performance requirement complete. to address this problem, we introduce a framework for specifying performance requirements. this framework (i) consists of a unified model derived from existing performance classifications, (ii) denotes completeness through a content model, and (iii) is operationalized through sentence patterns. we evaluate both the applicability of the framework as well as its ability uncover incompleteness with performance requirements taken from 11 industrial specifications. in our study, we were able to specify 86% of the examined performance requirements by means of our framework. furthermore, we show that 68% of the specified performance requirements are incomplete with respect to our notion of completeness. we argue that our framework provides an actionable definition of completeness for performance requirements."
                },
                {
                    "id": "R198724",
                    "label": "Ambiguity Cues in Requirements Elicitation Interviews",
                    "doi": "10.1109/re.2016.25",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "customer-analyst interviews are considered among the most effective means to perform requirements elicitation. however, during these interviews, ambiguity can hamper communication between customer and requirements analyst. ambiguity is particularly dangerous in those cases in which the analyst misunderstands some linguistic expression of the customer, with-out being aware of the misunderstanding. on the other hand, if the analyst is able to detect ambiguous situations, this has been shown to help him/her in disclosing tacit knowledge. indeed, the occurrence of an ambiguity might reveal the presence of unexpressed, system-relevant knowledge that needs to be elicited. therefore, for the requirements elicitation interview to succeed, it is important for the analyst not to overlook ambiguities. to support the ambiguity-awareness of the requirements analyst, this paper aims to provide a set of cues that can be identified in the linguistic expressions of the customer, and that typically lead to ambiguity. to this end, we performed 34 customer-analyst interviews, and we isolated the speech fragments that caused the ambiguity. based on the analysis of these fragments, and leveraging the previous literature on ambiguity in written requirements, we identified a set of cues that can be used by requirements analysts as a reference handbook to detect ambiguities."
                },
                {
                    "id": "R198795",
                    "label": "Trace++: A Traceability Approach to Support Transitioning to Agile Software Engineering",
                    "doi": "10.1109/re.2016.47",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "agile methodologies have been introduced as an alternative to traditional software engineering methodologies. however, despite the advantages of using agile methodologies, the transition between traditional and agile methodologies is not an easy task. there are several problems associated with the use of agile methodologies. examples of these problems are related to (i) lack of metrics to measure the amount of rework that occurs per sprint, (ii) interruption of a project after several iterations, (iii) changes in the requirements, (iv) lack of documentation, and (v) lack of management control. in this paper we present trace++, a traceability technique that extends traditional traceability relationships with extra information in order to support the transition between traditional and agile software development. the use of trace++ has been evaluated in two real projects of different software development companies to measure the benefits of using trace++ to support agile software development."
                },
                {
                    "id": "R198797",
                    "label": "An Exploratory Study on User Interaction Challenges When Handling Interconnected Requirements Artifacts of Various Sizes",
                    "doi": "10.1109/re.2016.52",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "requirements documentation is essential for developing software systems of non-trivial size. the cost of creating and maintaining documentation artifacts in terms of time and effort is significantly influenced by the tools with which engineers view, navigate and edit documentation artifacts. however, there is not much evidence about how well documentation tools actually support engineers, particularly when dealing with artifacts that are larger than the available display screen and with multiple artifacts at the same time. therefore, we conducted an exploratory study based on 29 interviews with software practitioners to understand the current practice of presenting and manipulating artifacts in documentation tools, and how practitioners deal with the challenges encountered. our study shows that a significant number of artifacts cannot be viewed entirely, even on large screens. moreover, more than half of the participants use four or more artifacts concurrently. nevertheless, current tools only provide primitive capabilities for handling concurrent and large artifacts, thus forcing engineers to create, for example, mental images of the currently used artifacts or use workarounds such as hanging printouts to the wall. our results may trigger new research and help improve requirements engineering tools."
                },
                {
                    "id": "R198808",
                    "label": "Looking into the Crystal Ball: Requirements Evolution over Time",
                    "doi": "10.1109/re.2016.45",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"goal modeling has long been used in the literature to model and reason about system requirements, constraints within the domain and environment, and stakeholders' goals. goal model analysis helps stakeholders answer 'what if' questions enabling them to make tradeoff decisions about their project requirements. however, questions concerning the evolution over time of stakeholder requirements or changes in actor intentionality are not explicitly addressed by current approaches. in this paper, we tackle this problem by presenting a method for specifying changes in intentions over time, and a technique that uses simulation for asking a variety of 'what if' questions about such models. using the development of a web-based modeling tool as an example, we demonstrate that this technique is effective for debugging goal models and answering stakeholder questions.\""
                },
                {
                    "id": "R198833",
                    "label": "Goal-Oriented Requirements Engineering: A Systematic Literature Map",
                    "doi": "10.1109/re.2016.41",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "over the last two decades, much attention has been paid to the area of goal-oriented requirements engineering(gore), where goals are used as a useful conceptualization to elicit, model and analyze requirements, capturing alternatives and conflicts. goal modeling has been adapted and applied to many sub-topics within re and beyond, such as agent-orientation, aspect-orientation, business intelligence, model-driven development, security, and so on. despite extensive efforts in this field, the re community lacks a recent, general systematic literature review of the area. as a first step towards providing a gore overview, we present a systematic literature map, focusing on gore-related publications at a high-level, categorizing and analyzing paper information in order to answer several research questions, while omitting a detailed analysis of individual paper quality. our literature map covers the 246 top-cited gore-related conference and journal papers, according to scopus, classifying them into a number of descriptive paper types and topics, providing an analysis of the data, which is made publicly available. we use our analysis results to make recommendations concerning future gore research."
                },
                {
                    "id": "R198856",
                    "label": "An Exploratory Study on Handling Requirements and Acceptance Test Documentation in Industry",
                    "doi": "10.1109/re.2016.37",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"with the emergence and spread of agile processes, the practices of writing and maintaining documentation have drastically changed in the last decade. in this work, we performed a qualitative study to explore the current practices for managing two related types of software documentation: requirements and acceptance tests. we interviewed twenty practitioners from seventeen business units in fifteen companies to investigate the companies' practices for writing, maintaining and linking requirements and acceptance test documentation. the study yields interesting and partially unexpected results. for example, we had expected that tests would be more extensively documented than requirements, while we found a strong linear correlation between the number of requirements and tests in our sample. we also found that technical people are usually not involved in the requirements engineering activities, which often results in misunderstood or underestimated requirements. acceptance tests are written, in many cases, based on requirements that are not necessarily detailed enough. also, acceptance tests are not regularly maintained, which occasionally results in confusing features and bugs.\""
                },
                {
                    "id": "R198877",
                    "label": "Managing Requirements Change the Informal Way: When Saying \u2018No\u2019 is Not an Option",
                    "doi": "10.1109/re.2016.64",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "software has always been considered as malleable. changes to software requirements are inevitable during the development process. despite many software engineering advances over several decades, requirements changes are a source of project risk, particularly when businesses and technologies are evolving rapidly. although effectively managing requirements changes is a critical aspect of software engineering, conceptions of requirements change in the literature and approaches to their management in practice still seem rudimentary. the overall goal of this study is to better understand the process of requirements change management. we present findings from an exploratory case study of requirements change management in a globally distributed setting. in this context we noted a contrast with the traditional models of requirements change. in theory, change control policies and formal processes are considered as a natural strategy to deal with requirements changes. yet we observed that \"informal requirements changes\" (infrc) were pervasive and unavoidable. our results reveal an equally \\'natural\\' informal change management process that is required to handle infrc in parallel. we present a novel model of requirements change which, we argue, better represents the phenomenon and more realistically incorporates both the informal and formal types of change."
                },
                {
                    "id": "R198904",
                    "label": "NANE: Identifying Misuse Cases Using Temporal Norm Enactments",
                    "doi": "10.1109/re.2016.34",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "recent data breaches in domains such as healthcare where confidentiality of data is crucial indicate that breaches often originate from misuses, not only from vulnerabilities in the technical (software or hardware) architecture. current requirements engineering (re) approaches determine what access control mechanisms are needed to protect sensitive resources (assets). however, current re approaches inadequately characterize how a user is expected to interact with others in relation to the relevant assets. consequently, a requirements analyst cannot readily identify misuses by legitimate users. we adopt social norms as a natural, formal means of characterizing user interactions whereby potential misuses map to norm violations. our research goal is to help analysts identify misuse cases by formal reasoning about norm enactments. we propose nane, a formal framework for identifying such misuse cases using a semiautomated process. we demonstrate how nane enables monitoring of potential misuses on a healthcare scenario."
                },
                {
                    "id": "R198917",
                    "label": "Mining Requirements Knowledge from Collections of Domain Documents",
                    "doi": "10.1109/re.2016.50",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"when organizations enter domains that are entirely new to them, they need to invest significant time and effort to acquire domain knowledge. this typically involves searching through a broad set of domain documents, retrieving relevant ones, and analyzing the textual content in order to discover and specify pertinent requirements. depending on the nature of the domain and the availability of documentation, this task can be extremely time-consuming and may require non-trivial human effort. furthermore, the task must often be performed repeatedly throughout early phases of the project. in this paper we first explore the effort needed to manually build a high-level domain model capturing the functional components. we then present mark (mining requirements knowledge), which identifies and retrieves the documents containing descriptions of functional components in the domain model. domain analysts can use this information to to specify requirements. we introduce and evaluate an algorithm which ranks domain documents according to their relevance to a component and then highlights sections of text which are likely to contain requirements-related information. we describe our process within the context of the positive train control (ptc) domain with a repository of of 523 documents, representing 852mb of data. we empirically evaluate the mark relevance algorithm and its ability to retrieve relevant requirements knowledge for requirements related to ptc's on-board unit.\""
                },
                {
                    "id": "R198944",
                    "label": "Acquiring Creative Requirements from the Crowd: Understanding the Influences of Personality and Creative Potential in Crowd RE",
                    "doi": "10.1109/re.2016.68",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"as a creative discipline, requirements engineering (re), lends importance to understanding the associated human factors. crowd re, the approach of acquiring requirements from members of the public-the so-called crowd-emphasizes human factors further. we investigate how human personality and creative potential influence a requirement acquisition task. these factors are of specific importance to crowd re because (1) crowd workers are generally not trained in re, and (2) a key motivation in engaging them is to benefit from their creativity. we propose a sequential crowd re process, where workers in one stage review requirements from the previous stage and produce additional requirements. to reduce potential information overload in this process, we propose strategies for selecting requirements from one stage to expose to workers in later stages. we conducted a study on amazon mechanical turk tasking 300 workers with creating requirements via the above sequential process (in the domain of smart home applications for concreteness) and tasking an additional 300 workers to rate the creativity (novelty and usefulness) of those requirements. our findings offer insights on how to carry out crowd re effectively. first, we find that a crowd worker's (1) creative potential, and personality traits of openness and conscientiousness have significant positive influence on the novelty of the worker's ideas, and (2) personality traits of agreeableness and conscientiousness have significant positive influence, but extraversion has significant negative influence on the usefulness of the worker's ideas. second, we find that exposing a worker to ideas from previous workers cognitively stimulates the worker to produce creative ideas. third, we identify effective strategies based on personality traits and creative potential for selecting a few requirements from a pool of previous requirements to stimulate a worker.\""
                },
                {
                    "id": "R198964",
                    "label": "Advancing Repeated Research in Requirements Engineering: A Theoretical Replication of Viewpoint Merging",
                    "doi": "10.1109/re.2016.46",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"compared to building a single requirements view, modeling stakeholder viewpoints and then merging them is shown to improve the understanding of the problem domain, but also very time consuming. how has the situation changed? this paper reports our replication of a case study, where we take theoretical replication's advantage to mitigate the original study design's threat and to embrace an important evolving factor, namely automated tool support for producing i* models. our replicate case study verifies the rich domain understanding gained through viewpoint-based modeling, and updates the prior results by showing the time saving enabled by the tool. our work offers operational insights into independent, theoretical replications. these insights, we believe, can advance requirements engineering research toward an empirically backed body of knowledge.\""
                },
                {
                    "id": "R198972",
                    "label": "Automated Extraction of Conceptual Models from User Stories via NLP",
                    "doi": "10.1109/re.2016.40",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "natural language (nl) is still the predominant notation that practitioners use to represent software requirements. albeit easy to read, nl does not readily highlight key concepts and relationships such as dependencies and conflicts. this contrasts with the inherent capability of graphical conceptual models to visualize a given domain in a holistic fashion. in this paper, we propose to automatically derive conceptual models from a concise and widely adopted nl notation for requirements: user stories. due to their simplicity, we hypothesize that our approach can improve on the low accuracy of previous works. we present an algorithm that combines state-of-the-art heuristics and that is implemented in our visual narrator tool. we evaluate our work on two case studies wherein we obtained promising precision and recall results (between 80% and 92%). the creators of the user stories perceived the generated models as a useful artifact to communicate and discuss the requirements, especially for team members who are not yet familiar with the project."
                },
                {
                    "id": "R198979",
                    "label": "What is the Impact of Bad Layout in the Understandability of Social Goal Models?",
                    "doi": "10.1109/re.2016.51",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"the i* community has published guidelines, including model layout guidelines, for the construction of models. our goal is to evaluate the effect of the layout guidelines on the i* novice stakeholders' ability to understand and review i* models. we performed a quasi-experiment where participants were given two understanding and two reviewing tasks. both tasks involved a model with a bad layout and another model following the i* layout guidelines. we evaluated the impact of layouts by combining the success level in those tasks and the required effort to accomplish them. effort was assessed using time, perceived complexity (with nasa tlx), and eye-tracking data. participants were more successful in understanding than in reviewing tasks. however, we found no statistically significant difference in the success, time taken, or perceived complexity, between tasks conducted with models with a bad layout and models with a good layout. most participants had little to no prior knowledge in i*, making them more representative of stakeholders with no requirements engineering expertise. they were able to understand the models fairly well after a short tutorial, but struggled when reviewing models. in the end, adherence to the existing i* layout guidelines did not significantly impact i* model understanding and reviewing performance.\""
                },
                {
                    "id": "R198994",
                    "label": "Collaborative Traceability Management: Challenges and Opportunities",
                    "doi": "10.1109/re.2016.17",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "traceability and trace link management are important for various reasons, including managing knowledge about a complex software system, monitoring the progress of its development, and proving that it is developed in accordance to regulations. however, it is difficult to maintain and use trace links in real-world projects where artifacts undergo constant change and multiple stakeholders are involved. in this paper, we extend the current body of knowledge on traceability management by regarding its collaborative aspects in an industrial setting. based on 15 industrial cases and semi-structured interviews with 24 practitioners, we identify challenges involved in collaborative traceability management, and how traceability management can be used to enable collaboration. our findings show that main challenges are boundaries between organizations and tools, a lack of common goals and responsibilities, and the difficulty of collaboratively maintaining trace links. we also identify traceability as an important facilitator for communication and knowledge management across these boundaries."
                },
                {
                    "id": "R199005",
                    "label": "Change impact analysis for Natural Language requirements: An NLP approach",
                    "doi": "10.1109/re.2015.7320403",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "requirements are subject to frequent changes as a way to ensure that they reflect the current best understanding of a system, and to respond to factors such as new and evolving needs. changing one requirement in a requirements specification may warrant further changes to the specification, so that the overall correctness and consistency of the specification can be maintained. a manual analysis of how a change to one requirement impacts other requirements is time-consuming and presents a challenge for large requirements specifications. we propose an approach based on natural language processing (nlp) for analyzing the impact of change in natural language (nl) requirements. our focus on nl requirements is motivated by the prevalent use of these requirements, particularly in industry. our approach automatically detects and takes into account the phrasal structure of requirements statements. we argue about the importance of capturing the conditions under which change should propagate to enable more accurate change impact analysis. we propose a quantitative measure for calculating how likely a requirements statement is to be impacted by a change under given conditions. we conduct an evaluation of our approach by applying it to 14 change scenarios from two industrial case studies."
                },
                {
                    "id": "R199014",
                    "label": "Requirements engineering: The quest for the dependent variable",
                    "doi": "10.1109/re.2015.7320404",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"requirements engineering is a vibrant and broad research area. it covers a range of activities with different objectives. by reviewing experiments previously included in systematic literature reviews, this paper provides an overview of the dependent variables used in experimental requirements engineering research. this paper also identifies the theoretical motivation for the use of these variables in the experiments. the results show that a wide range of different variables has been applied in experiments and operationalized through both subjective assessments (e.g., subjects' perceived utility of a technique) and objective measurements (e.g., the number of defects found in a requirements specification). the theoretical basis for these variables and operationalizations are unclear in most cases. directions for theoretical work to identify suitable dependent variables are provided.\""
                },
                {
                    "id": "R199023",
                    "label": "Ambiguity as a resource to disclose tacit knowledge",
                    "doi": "10.1109/re.2015.7320405",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "interviews are the most common and effective means to perform requirements elicitation and support knowledge transfer between a customer and a requirements analyst. ambiguity in communication is often perceived as a major obstacle for knowledge transfer, which could lead to unclear and incomplete requirements documents. in this paper, we analyse the role of ambiguity in requirements elicitation interviews. to this end, we have performed a set of customer-analyst interviews to observe how ambiguity occurs during requirements elicitation. from this direct experience, we have observed that ambiguity is a multi-dimensional cognitive phenomenon with a dominant pragmatic facet, and we have defined a phenomenological framework to describe the different types of ambiguity in interviews. we have also discovered that, rather than an obstacle, the occurrence of an ambiguity is often a resource for discovering tacit knowledge. starting from this observation, we have envisioned the further steps needed in the research to exploit these findings."
                },
                {
                    "id": "R199028",
                    "label": "An information theoretic approach for extracting and tracing non-functional requirements",
                    "doi": "10.1109/re.2015.7320406",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"non-functional requirements (nfrs) are high-level quality constraints that a software system should exhibit. detecting such constraints early in the process is critical for the stability of software architectural design. however, due to their pervasive nature, and the lack of robust modeling and documentation techniques, nfrs are often overlooked during the requirements elicitation phase. realizing such constraints at later stages of the development process often leads to architecture erosion and poor traceability. motivated by these observations, we propose an unsupervised, computationally efficient, and scalable approach for extracting and tracing nfrs in software systems. based on main assumptions of the cluster hypothesis and information theory, the proposed approach exploits the semantic knowledge embedded in the textual content of requirements specifications to discover, classify, and trace high-level software quality constraints imposed by the system's functional features. three experimental systems are used to conduct the experimental analysis in this paper. results show that the proposed approach can discover software nfrs with an average accuracy of 73%, enabling these nfrs to be traced to their implementations with accuracy levels adequate for practical applications.\""
                },
                {
                    "id": "R199039",
                    "label": "From requirements elicitation to variability analysis using repertory grid: A cognitive approach",
                    "doi": "10.1109/re.2015.7320407",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"the growing complexity and dynamics of the execution environment have been major motivation for designing self-adaptive systems. although significant work can be found in the field of formalizing or modeling the requirements of adaptive system, not enough attention has been paid towards the requirements elicitation techniques for the same. it is still an open challenge to elicit the users' requirements in the light of various contexts and introduce the required flexibility in the system's behavior at an early phase of requirements engineering. we explore the idea of using a cognitive technique, repertory grid, to acquire the knowledge of various stakeholders along multiple dimensions of problem space and design space. we aim at discovering the scope of variations in the features of the system by capturing the intentional and technical variability in the problem space and design space respectively. a stepwise methodology for finding the right set of features in the changing context has also been provided in this work. we evaluate the proposed idea by a preliminary case study using smart home system domain.\""
                },
                {
                    "id": "R199113",
                    "label": "Goal and Preference Identification through natural language",
                    "doi": "10.1109/re.2015.7320408",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "goal models allow efficient representation of stakeholder goals and alternative ways by which these can be satisfied. preferences over goals in the goal model are then used to specify criteria for selecting alternatives that fit specific contexts, situations and strategies. given such preferences, automated reasoning tools allow for efficient exploration of such alternatives. nevertheless, to be amenable to such automated processing, goals and preferences need to be specified in a formal language, making automated processing inaccessible to the very bearers of goals and preferences, i.e., the stakeholders. we combine natural language processing techniques to allow specification of preferences through natural language statements. the natural language statement is first matched through regular expressions to distinguish between the preference component and the goal component. the former is then mapped to a preferential strength measure, while the latter is used to identify the relevant goal in the goal model through statistical semantic similarity techniques. the result constitutes a formal representation that can be used for alternatives analysis. in this way, stakeholders can access advanced goal reasoning techniques through simple natural language preference expressions, facilitating their decision making in various requirements analysis contexts. an experimental evaluation with human participants shows that the proposed system is of substantial precision and that a mapping from natural preferential verbalizations to predefined preferential strength labels is possible through sampling from crowds."
                },
                {
                    "id": "R199123",
                    "label": "Selecting creativity techniques for creative requirements: An evaluation of four techniques using creativity workshops",
                    "doi": "10.1109/re.2015.7320409",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "requirements engineering is recognized as a creative process where stakeholders jointly discover new creative ideas for innovative and novel products that eventually are expressed as requirements. this paper evaluates four different creativity techniques, namely hall of fame, constraint removal, brainstorming, and idea box, using creativity workshops with students and industry practitioners. in total, 34 creativity workshops were conducted with 90 students from two universities, and 86 industrial practitioners from six companies. the results from this study indicate that brainstorming can generate by far the most ideas, while hall of fame generates most creative ideas. idea box generates the least number of ideas, and the least number of creative ideas. finally, hall of fame was the technique that led to the most number of requirements that was included in future releases of the products."
                },
                {
                    "id": "R199136",
                    "label": "Feature lifecycles as they spread, migrate, remain, and die in App Stores",
                    "doi": "10.1109/re.2015.7320410",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "we introduce a theoretical characterisation of feature lifecycles in app stores, to help app developers to identify trends and to find undiscovered requirements. to illustrate and motivate app feature lifecycle analysis, we use our theory to empirically analyse the migratory and non-migratory behaviours of 4,053 non-free features from two app stores (samsung and blackberry). the results reveal that, in both stores, intransitive features (those that neither migrate nor die out) exhibit significantly different behaviours with regard to important properties, such as their price. further correlation analysis also highlights differences between trends relating price, rating, and popularity. our results indicate that feature lifecycle analysis can yield insights that may also help developers to understand feature behaviours and attribute relationships."
                },
                {
                    "id": "R199149",
                    "label": "What you ask is what you get: Understanding architecturally significant functional requirements",
                    "doi": "10.1109/re.2015.7320411",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"software architects are responsible for designing an architectural solution that satisfies the functional and non-functional requirements of the system to the fullest extent possible. however, the details they need to make informed architectural decisions are often missing from the requirements specification. an earlier study we conducted indicated that architects intuitively recognize architecturally significant requirements in a project, and often seek out relevant stakeholders in order to ask probing questions (pqs) that help them acquire the information they need. this paper presents results from a qualitative interview study aimed at identifying architecturally significant functional requirements' categories from various business domains, exploring relevant pqs for each category, and then grouping pqs by type. using interview data from 14 software architects in three countries, we identified 15 categories of architecturally significant functional requirements and 6 types of pqs. we found that the domain knowledge of the architect and her experience influence the choice of pqs significantly. a preliminary quantitative evaluation of the results against real-life software requirements specification documents indicated that software specifications in our sample largely do not contain the crucial architectural differentiators that may impact architectural choices and that pqs are a necessary mechanism to unearth them. further, our findings provide the initial list of pqs which could be used to prompt business analysts to elicit architecturally significant functional requirements that the architects need.\""
                },
                {
                    "id": "R199158",
                    "label": "A requirements monitoring model for systems of systems",
                    "doi": "10.1109/re.2015.7320412",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "many software systems today can be characterized as systems of systems (sos) comprising interrelated and heterogeneous systems developed by diverse teams over many years. due to their scale, complexity, and heterogeneity engineers face significant challenges when determining the compliance of sos with their requirements. requirements monitoring approaches are a viable solution for checking system properties at runtime. however, existing approaches do not adequately consider the characteristics of sos: different types of requirements exist at different levels and across different systems; requirements are maintained by different stakeholders; and systems are implemented using diverse technologies. this paper describes a three-dimensional requirements monitoring model (rmm) for sos providing the following contributions: (i) our approach allows modeling the monitoring scopes of requirements with respect to the sos architecture; (ii) it employs event models to abstract from different technologies and systems to be monitored; and (iii) it supports instantiating the rmm at runtime depending on the actual sos configuration. to evaluate the feasibility of our approach we created a rmm for a real-world sos from the automation software domain. we evaluated the model by instantiating it using an existing monitoring framework and a simulator running parts of this sos. the results indicate that the model is sufficiently expressive to support monitoring sos requirements of a directed sos. it further facilitates diagnosis by discovering violations of requirements across different levels and systems in realistic monitoring scenarios."
                },
                {
                    "id": "R199162",
                    "label": "Handling knowledge uncertainty in risk-based requirements engineering",
                    "doi": "10.1109/re.2015.7320413",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"requirements engineers are faced with multiple sources of uncertainty. in particular, the extent to which the identified software requirements and environment assumptions are adequate and sufficiently complete is uncertain; the extent to which they will be satisfied in the system-to-be is uncertain; and the extent to which obstacles to their satisfaction will occur is uncertain. the resolution of such domain-level uncertainty requires estimations of the likelihood that those different types of situations may or may not occur. however, the extent to which the resulting estimates are accurate is uncertain as well. this meta-level uncertainty limits current risk-based methods for requirements engineering. the paper introduces a quantitative approach for managing it. an earlier formal framework for probabilistic goals and obstacles is extended to explicitly cope with uncertainties about estimates of likelihoods of fine-grained obstacles to goal satisfaction. such estimates are elicited from multiple sources and combined in order to reduce their uncertainty margins. the combined estimates and their uncertainties are up-propagated through obstacle refinement trees and then through the system's goal model. two metrics are introduced for measuring problematic uncertainties. when applied to the probability distributions obtained by up-propagation to the top-level goals, the metrics allow critical leaf obstacles with most problematic uncertainty margins to be highlighted. the proposed approach is evaluated on excerpts from a real ambulance dispatching system.\""
                },
                {
                    "id": "R200042",
                    "label": "Forging high-quality User Stories: Towards a discipline for Agile Requirements",
                    "doi": "10.1109/re.2015.7320415",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "user stories are a widely used notation for formulating requirements in agile development projects. despite their popularity in industry, little to no academic work is available on assessing their quality. the few existing approaches are too generic or employ highly qualitative metrics. we propose the quality user story framework, consisting of 14 quality criteria that user story writers should strive to conform to. additionally, we introduce the conceptual model of a user story, which we rely on to design the aqusa software tool. aqusa aids requirements engineers in turning raw user stories into higher-quality ones by exposing defects and deviations from good practice in user stories. we evaluate our work by applying the framework and a prototype implementation to three user story sets from industry."
                },
                {
                    "id": "R200045",
                    "label": "Exposing the susceptibility of off-nominal behaviors in reactive system requirements",
                    "doi": "10.1109/re.2015.7320416",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"requirements are typically specified on the assumption that the system's operating environment will behave in what is considered to be an expected and nominal manner. when gathering requirements, one concern is whether the requirements are too incomplete to account for every possible, unintended, off-nominal behavior (onb) that the operating environment can create in the system. in this paper, we present a semi-automated approach, based on the causal component model (ccm), which can expose, within a set of requirements, whether onbs can result in undesired system states. we demonstrate how the ccm approach exposes and helps address potential off-nominal behavior problems in a set of requirements that represents a real-world product. our case study shows that the approach can expose susceptibility to onbs and can supply information in correcting requirements.\""
                },
                {
                    "id": "R200049",
                    "label": "Assessment of risk perception in security requirements composition",
                    "doi": "10.1109/re.2015.7320417",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"security requirements analysis depends on how well-trained analysts perceive security risk, understand the impact of various vulnerabilities, and mitigate threats. when systems are composed of multiple machines, configurations, and software components that interact with each other, risk perception must account for the composition of security requirements. in this paper, we report on how changes to security requirements affect analysts risk perceptions and their decisions about how to modify the requirements to reach adequate security levels. we conducted two user surveys of 174 participants wherein participants assess security levels across 64 factorial vignettes. we analyzed the survey results using multi-level modeling to test for the effect of security requirements composition on participants' overall security adequacy ratings and on their ratings of individual requirements. we accompanied this analysis with grounded analysis of elicited requirements aimed at lowering the security risk. our results suggest that requirements composition affects experts' adequacy ratings on security requirements. in addition, we identified three categories of requirements modifications, called refinements, replacements and reinforcements, and we measured how these categories compare with overall perceived security risk. finally, we discuss the future impact of our work in security requirements assessment practice.\""
                },
                {
                    "id": "R200061",
                    "label": "Resolving goal conflicts via argumentation-based analysis of competing hypotheses",
                    "doi": "10.1109/re.2015.7320418",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"a stakeholder's beliefs influence his or her goals. however, a stakeholder's beliefs may not be consistent with the goals of all stakeholders of a system being constructed. such belief-goal inconsistencies could manifest themselves as conflicting goals of the system to be. we propose arg-ach, a novel approach for capturing inconsistencies between stakeholders' goals and beliefs, and resolving goal conflicts. arg-ach employs a hybrid of (1) the analysis of competing hypotheses (ach), a structured analytic technique, for systematically eliciting stakeholders' goals and beliefs, and (2) rational argumentation for determining belief-goal inconsistencies to resolve conflicts. arg-ach treats conflicting goals as hypotheses that compete with each other and the winning hypothesis as a goal of the system to be. arg-ach systematically captures the trail of a requirements engineer's thought process in resolving conflicts. we evaluated arg-ach via a study in which 20 subjects applied arg-ach or ach to resolve goal conflicts in a sociotechnical system concerning national security. we found that arg-ach is superior to ach with respect to completeness and coverage of belief search; length of belief chaining; ease of use; explicitness of the assumptions made; and repeatability of conclusions across subjects. not surprisingly, arg-ach required more time than ach: although this is justified by improvements in quality, the gap could be reduced through better tooling.\""
                },
                {
                    "id": "R200098",
                    "label": "Detecting repurposing and over-collection in multi-party privacy requirements specifications",
                    "doi": "10.1109/re.2015.7320419",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "mobile and web applications increasingly leverage service-oriented architectures in which developers integrate third-party services into end user applications. this includes identity management, mapping and navigation, cloud storage, and advertising services, among others. while service reuse reduces development time, it introduces new privacy and security risks due to data repurposing and over-collection as data is shared among multiple parties who lack transparency into third-party data practices. to address this challenge, we propose new techniques based on description logic (dl) for modeling multiparty data flow requirements and verifying the purpose specification and collection and use limitation principles, which are prominent privacy properties found in international standards and guidelines. we evaluate our techniques in an empirical case study that examines the data practices of the waze mobile application and three of their service providers: facebook login, amazon web services (a cloud storage provider), and flurry.com (a popular mobile analytics and advertising platform). the study results include detected conflicts and violations of the principles as well as two patterns for balancing privacy and data use flexibility in requirements specifications. analysis of automation reasoning over the dl models show that reasoning over complex compositions of multi-party systems is feasible within exponential asymptotic timeframes proportional to the policy size, the number of expressed data, and orthogonal to the number of conflicts found."
                },
                {
                    "id": "R200106",
                    "label": "A quality model for the systematic assessment of requirements traceability",
                    "doi": "10.1109/re.2015.7320420",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "\"traceability is an important quality of software requirements and allows to describe and follow their life throughout a development project. the importance of traceable requirements is reflected by the fact that requirements standards, safety regulations, and maturity models explicitly demand for it. in practice, traceability is created and maintained by humans, which make mistakes. in result, existing traces are potentially of dubious quality but serve as the foundation for high impact development decisions. we found in previous studies that practitioners miss clear guidance on how to systematically assess the quality of existing traces. in this paper, we review the elements involved in establishing traceability in a development project and derive a quality model that specifies per element the acceptable state (traceability gate) and unacceptable deviations (traceability problem) from this state. we describe and formally define how both, the acceptable states and the unacceptable deviations can be detected in order to enable practitioners to systematically assess their project's traceability. we evaluated the proposed model through an expert survey. the participating experts considered the quality model to be complete and attested that its quality criteria are of high relevance. we further found that the experts weight the occurrence of different traceability problems with different criticality. this information can be used to quantify the impact of traceability problems and to prioritize the assessment of traceability elements.\""
                },
                {
                    "id": "R200119",
                    "label": "Sketching and notation creation with FlexiSketch Team: Evaluating a new means for collaborative requirements elicitation",
                    "doi": "10.1109/re.2015.7320421",
                    "research_field": {
                        "id": "R140",
                        "label": "Software Engineering"
                    },
                    "abstract": "whiteboards and paper allow for any kind of notations and are easy to use. requirements engineers love to use them in creative requirements elicitation and design sessions. however, the resulting diagram sketches cannot be interpreted by software modeling tools. we have developed flexisketch as an alternative to whiteboards in previous work. it is a mobile tool for model-based sketching of free-form diagrams that allows the definition and re-use of diagramming notations on the fly. the latest version of the tool, called flexisketch team, supports collaboration with multiple tablets and an electronic whiteboard, such that several users can work simultaneously on the same model sketch. in this paper we present an exploratory study about how novice and experienced engineers sketch and define ad-hoc notations collaboratively in early requirements elicitation sessions when supported by our tool. results show that participants incrementally build notations by defining language constructs the first time they use them. participants considered the option to re-use defined constructs to be a big motivational factor for providing type definitions. they found our approach useful for longer sketching sessions and situations where sketches are re-used later on."
                }
            ]
        },
        {
            "id": "R187648",
            "label": "OCO for control",
            "research_fields": [
                {
                    "id": "R109",
                    "label": "Control Theory"
                }
            ],
            "properties": [
                "research problem",
                "Theoretical guarantees",
                "Has application in",
                "data-driven approach",
                "has constraints",
                "Type of considered system",
                "Measurement noise",
                "Process noise"
            ],
            "papers": [
                {
                    "id": "R195918",
                    "label": "Online Stochastic Optimization for Unknown Linear Systems: Data-Driven Synthesis and Controller Analysis",
                    "doi": "",
                    "research_field": {
                        "id": "R109",
                        "label": "Control Theory"
                    },
                    "abstract": "this paper proposes a data-driven control framework to regulate an unknown, stochastic linear dynamical system to the solution of a (stochastic) convex optimization problem. despite the centrality of this problem, most of the available methods critically rely on a precise knowledge of the system dynamics (thus requiring off-line system identification and model refinement). to this aim, in this paper we first show that the steady-state transfer function of a linear system can be computed directly from control experiments, bypassing explicit model identification. then, we leverage the estimated transfer function to design a controller \u2013 which is inspired by stochastic gradient descent methods \u2013 that regulates the system to the solution of the prescribed optimization problem. a distinguishing feature of our methods is that they do not require any knowledge of the system dynamics, disturbance terms, or their distributions. our technical analysis combines concepts and tools from behavioral system theory, stochastic optimization with decision-dependent distributions, and stability analysis. we illustrate the applicability of the framework on a case study for mobility-on-demand ride service scheduling in manhattan, ny."
                },
                {
                    "id": "R187843",
                    "label": "Online Optimization as a Feedback Controller: Stability and Tracking",
                    "doi": "10.1109/tcns.2019.2906916",
                    "research_field": {
                        "id": "R109",
                        "label": "Control Theory"
                    },
                    "abstract": "this paper develops and analyzes feedback-based online optimization methods to regulate the output of a linear time invariant (lti) dynamical system to the optimal solution of a time-varying convex optimization problem. the design of the algorithm is based on continuous-time primal-dual dynamics, properly modified to incorporate feedback from the lti dynamical system, applied to a proximal augmented lagrangian function. the resultant closed-loop algorithm tracks the solution of the time-varying optimization problem without requiring knowledge of (time varying) disturbances in the dynamical system. the analysis leverages integral quadratic constraints to provide linear matrix inequality (lmi) conditions that guarantee global exponential stability and bounded tracking error. analytical results show that under a sufficient time-scale separation between the dynamics of the lti dynamical system and the algorithm, the lmi conditions can be always satisfied. this paper further proposes a modified algorithm that can track an approximate solution trajectory of the constrained optimization problem under less restrictive assumptions. as an illustrative example, the proposed algorithms are showcased for power transmission systems, to compress the time scales between secondary and tertiary control, and allow to simultaneously power rebalancing and tracking of the dc optimal power flow points."
                },
                {
                    "id": "R187671",
                    "label": "Data-driven online convex optimization for control of dynamical systems",
                    "doi": "10.1109/cdc45484.2021.9683550",
                    "research_field": {
                        "id": "R109",
                        "label": "Control Theory"
                    },
                    "abstract": "we propose a data-driven online convex optimization algorithm for controlling dynamical systems. in particular, the control scheme makes use of an initially measured input-output trajectory and behavioral systems theory which enable it to handle unknown discrete-time linear time-invariant systems as well as a priori unknown time-varying cost functions. further, only output feedback instead of full state measurements is required for the proposed approach. analysis of the closed loop\u2019s performance reveals that the algorithm achieves sublinear regret if the variation of the cost functions is sublinear. the effectiveness of the proposed algorithm, even in the case of noisy measurements, is illustrated by a simulation example."
                },
                {
                    "id": "R195925",
                    "label": "Model-Free Nonlinear Feedback Optimization",
                    "doi": "",
                    "research_field": {
                        "id": "R109",
                        "label": "Control Theory"
                    },
                    "abstract": "feedback optimization is a control paradigm that enables physical systems to autonomously reach efficient operating points. its central idea is to interconnect optimization iterations in closed-loop with the physical plant. since iterative gradient-based methods are extensively used to achieve optimality, feedback optimization controllers typically require the knowledge of the steady-state sensitivity of the plant, which may not be easily accessible in some applications. in contrast, in this paper we develop a model-free feedback controller for efficient steady-state operation of general dynamical systems. the proposed design consists in updating control inputs via gradient estimates constructed from evaluations of the nonconvex objective at the current input and at the measured output. we study the dynamic interconnection of the proposed iterative controller with a stable nonlinear discrete-time plant. for this setup, we characterize the optimality and the stability of the closed-loop behavior as functions of the problem dimension, the number of iterations, and the rate of convergence of the physical plant. to handle general constraints that affect multiple inputs, we enhance the controller with frank-wolfe type updates."
                }
            ]
        },
        {
            "id": "R194212",
            "label": "Ontology construction",
            "research_fields": [
                {
                    "id": "R133",
                    "label": "Artificial Intelligence"
                },
                {
                    "id": "R141823",
                    "label": "Semantic Web"
                }
            ],
            "properties": [
                "method",
                "Methodology",
                "Tool",
                "has URI",
                "data source",
                "research problem",
                "evaluation",
                "Process",
                "name",
                "people involved",
                "serialization language"
            ],
            "papers": [
                {
                    "id": "R194747",
                    "label": "A health consumer ontology of fast food information",
                    "doi": "10.1109/BIBM49941.2020.9313375",
                    "research_field": {
                        "id": "R141823",
                        "label": "Semantic Web"
                    },
                    "abstract": "a variety of severe health issues can be attributed to poor nutrition and poor eating behaviors. research has explored the impact of nutritional knowledge on an individual\u2019s inclination to purchase and consume certain foods. this paper introduces the ontology of fast food facts, a knowledge base that models consumer nutritional data from major fast food establishments. this artifact serves as an aggregate knowledge base to centralize nutritional information for consumers. as a semantically-linked data source, the ontology of fast food facts could engender methods and tools to further the research and impact the health consumers\u2019 diet and behavior, which is a factor in many severe health outcomes. we describe the initial development of this ontology and future directions we plan with this knowledge base."
                },
                {
                    "id": "R194910",
                    "label": "An example of food ontology for diabetes control",
                    "doi": "",
                    "research_field": {
                        "id": "R141823",
                        "label": "Semantic Web"
                    },
                    "abstract": "this paper describes our experience in the rapid prototyping of a food ontology oriented to the nutritional and health care domain that is used to share knowledge between the different stakeholders involved in the pips project."
                },
                {
                    "id": "R195387",
                    "label": "FOBI: an ontology to represent food intake data and associate it with metabolomic data",
                    "doi": "10.1093/databa/baaa033",
                    "research_field": {
                        "id": "R141823",
                        "label": "Semantic Web"
                    },
                    "abstract": "abstract \\n nutrition research can be conducted by using two complementary approaches: (i) traditional self-reporting methods or (ii) via metabolomics techniques to analyze food intake biomarkers in biofluids. however, the complexity and heterogeneity of these two very different types of data often hinder their analysis and integration. to manage this challenge, we have developed a novel ontology that describes food and their associated metabolite entities in a hierarchical way. this ontology uses a formal naming system, category definitions, properties and relations between both types of data. the ontology presented is called fobi (food-biomarker ontology) and it is composed of two interconnected sub-ontologies. one is a \u2019food ontology\u2019 consisting of raw foods and \u2018multi-component foods\u2019 while the second is a \u2018biomarker ontology\u2019 containing food intake biomarkers classified by their chemical classes. these two sub-ontologies are conceptually independent but interconnected by different properties. this allows data and information regarding foods and food biomarkers to be visualized in a bidirectional way, going from metabolomics to nutritional data or vice versa. potential applications of this ontology include the annotation of foods and biomarkers using a well-defined and consistent nomenclature, the standardized reporting of metabolomics workflows (e.g. metabolite identification, experimental design) or the application of different enrichment analysis approaches to analyze nutrimetabolomic data. availability: fobi is freely available in both owl (web ontology language) and obo (open biomedical ontologies) formats at the project\u2019s github repository (https://github.com/pcastellanoescuder/foodbiomarkerontology) and fobi visualization tool is available in https://polcastellano.shinyapps.io/fobi_visualization_tool/."
                },
                {
                    "id": "R195718",
                    "label": "NAct: The Nutrition &amp; Activity Ontology for Healthy Living",
                    "doi": "10.3233/faia210377",
                    "research_field": {
                        "id": "R141823",
                        "label": "Semantic Web"
                    },
                    "abstract": "this paper presents the nact (nutrition &amp; activity) ontology, designed to drive personalised nutritional and physical activity recommendations and effectively support healthy living, through a reasoning-based ai decision support system. nact coalesces nutritional, medical, behavioural and lifestyle indicators with potential dietary and physical activity directives. the paper presents the first version of the ontology, including its co-design and engineering methodology, along with usage examples in supporting healthy nutritional and physical activity choices. lastly, the plan for future improvements and extensions is discussed."
                },
                {
                    "id": "R198117",
                    "label": "FOODS: A Food-Oriented Ontology-Driven System",
                    "doi": "10.1109/dest.2008.4635195",
                    "research_field": {
                        "id": "R141823",
                        "label": "Semantic Web"
                    },
                    "abstract": "in this paper the authors present the design and development of a counseling system for food or menu planning in a restaurant, clinic/hospital, or at home, the food-oriented ontology-driven system (foods). foods comprises (a) a food ontology, (b) an expert system using the ontology, and some knowledge about cooking methods and prices, and (c) a user interface suitable for novices in computers and diets as well as for experts. the ontology contains specifications of ingredients, substances, nutrition facts, recommended daily intakes for different regions, dishes, and menus. the expert system assists in finding the appropriate dish or menu for the consumer, client or customer, who use foods by entering their favorite ingredients, ingredients to avoid, favorite flavors, and so on. in the health section users can provide their gender, age, height and weight, which will be used to calculate such data as the body mass index. with foods enterprises can assist customers through an appropriate suggestion of dishes and meals with the help of individual nutritional profiles. smes that might be interested in using foods are institutions for training and instruction of cooking, restaurants, clinics, hospitals, together with clinical and therapeutical dietitians and nutritional therapists. in the long run such systems might become part of the emerging consumer health informatics portfolio."
                },
                {
                    "id": "R198205",
                    "label": "FTTO: An example of Food Ontology for traceability purpose",
                    "doi": "10.1109/IDAACS.2013.6662689",
                    "research_field": {
                        "id": "R141823",
                        "label": "Semantic Web"
                    },
                    "abstract": "this paper describes our experience in the development of the food track&trace ontology (ftto), an ontology oriented to the domain of food traceability. ftto is used to share knowledge between agents involved in the food supply chain and intends to be a reference that permits to work with information obtained through the matching and merging of different controlled vocabularies. as part of a bigger research project, the ftto has been designed to be connected with a global traceability information systems obtained through the modelling of the food supply chain and of data required for internal and chain traceability. the paper presents the project milestones and the creation of the core ontology emphasizing on the development of the main classes."
                },
                {
                    "id": "R198620",
                    "label": "Ontology and semantic matching for diabetic food recommendations",
                    "doi": "10.1109/ICITEED.2013.6676233",
                    "research_field": {
                        "id": "R141823",
                        "label": "Semantic Web"
                    },
                    "abstract": "\"foods recommendation for diabetes patients is indispensable for controlling blood sugar levels. currently, the foods preparation is done by a nutrition expert. the patient's dependence on the nutrition experts is very high, thus the selection of foods could not be done independently. the automation system to determine foods combination for diabetic patients is needed to solve these problems. in this study, the automation system has been designed and implemented. the technologies used in this research are the owl and swrl. there are few researches that explore an automation process of foods recommendation for diabetes patients using the technology of owl and swrl. domain knowledge based on ontology is needed to process foods composition automatically. however, using swrl and owl technology is not enough, because the accuracy of the words required. a semantic ontology understanding was added using weighted tree similarity method to specify the composition of foods for diabetic patients. 73% data were able to be correctly predicted by this method.\""
                }
            ]
        },
        {
            "id": "R198658",
            "label": "Nacre mechanics template",
            "research_fields": [
                {
                    "id": "",
                    "label": ""
                }
            ],
            "properties": [
                "Has method",
                "Has result",
                "research problem",
                "has material"
            ],
            "papers": [
                {
                    "id": "R189685",
                    "label": "Deoxyguanosine Phosphate Mediated Sacrificial Bonds Promote Synergistic Mechanical Properties in Nacre-Mimetic Nanocomposites",
                    "doi": "10.1021/bm400056c",
                    "research_field": {
                        "id": "R254",
                        "label": "Materials Science and Engineering"
                    },
                    "abstract": "\"we show that functionalizing polymer-coated colloidal nanoplatelets with guanosine groups allows synergistic increase of mechanical properties in nacre-mimetic lamellar self-assemblies. anionic montmorillonite (mtm) was first coated using cationic poly(diallyldimethylammonium chloride) (pdadmac) to prepare core-shell colloidal platelets, and subsequently the remaining chloride counterions allowed exchange to functional anionic 2'-deoxyguanosine 5'-monophosphate (dgmp) counterions, containing hydrogen bonding donors and acceptors. the compositions were studied using elemental analysis, scanning and transmission electron microscopy, wide-angle x-ray scattering, and tensile testing. the lamellar spacing between the clays increases from 1.85 to 2.14 nm upon addition of the dgmp. adding dgmp increases the elastic modulus, tensile strength, and strain 33.0%, 40.9%, and 5.6%, respectively, to 13.5 gpa, 67 mpa, and 1.24%, at 50% relative humidity. this leads to an improved toughness seen as a ca. 50% increase of the work-to-failure. this is noteworthy, as previously it has been observed that connecting the core-shell nanoclay platelets covalently or ionically leads to increase of the stiffness but to reduced strain. we suggest that the dynamic supramolecular bonds allow slippage and sacrificial bonds between the self-assembling nanoplatelets, thus promoting toughness, still providing dynamic interactions between the platelets.\""
                },
                {
                    "id": "R189677",
                    "label": "Multifunctional Nanoclay Hybrids of High Toughness, Thermal, and Barrier Performances",
                    "doi": "10.1021/am401928d",
                    "research_field": {
                        "id": "R254",
                        "label": "Materials Science and Engineering"
                    },
                    "abstract": "to address brittleness of nanoclay hybrids of high inorganic content, ductile polymers (polyethylene oxide and hydroxyethyl cellulose) and montmorillonite (mtm) have been assembled into hybrid films using a water-based filtration process. nacre-mimetic layered films resulted and were characterized by fe-sem and xrd. mechanical properties at ambient condition were studied by tensile test, while performance at elevated temperature and moisture conditions were evaluated by tga, dynamic vapor sorption, and dynamic thermomechanical and hygromechanical analyses. antiflammability and barrier properties against oxygen and water vapor were also investigated. despite their high mtm content in the 60-85 wt % range, the hybrids exhibit remarkable ductility and a storage modulus above 2 gpa even in severe conditions (300\u00b0c or 94% rh). moreover, they present fire-shielding property and are amongst the best oxygen and water vapor barrier hybrids reported in the literature. this study thus demonstrates nanostructure property advantages for synergistic effects in hybrids combining inexpensive, available, and environmentally benign constituents."
                },
                {
                    "id": "R189682",
                    "label": "Artificial Nacre-like Bionanocomposite Films from the Self-Assembly of Chitosan-Montmorillonite Hybrid Building Blocks",
                    "doi": "10.1002/anie.201004748",
                    "research_field": {
                        "id": "R254",
                        "label": "Materials Science and Engineering"
                    },
                    "abstract": "in the last decade, there has been a trend in chemistry to reduce the human impact on the environment. special attention has been paid to the replacement of conventional petroleum-based plastics by materials based on biopolymers. however, the mechanical and thermal properties and functionalities of these biopolymers have to be enhanced to be competitive with the petroleum-based plastics from the viewpoint of practical applications. one of the most promising solutions to overcome these drawbacks is the elaboration of bionanocomposite, namely the dispersion of nanosized filler into a biopolymer matrix. because of their functional properties, bionanocomposites as green nanocomposites based on biopolymers and layered silicates (clays) have received intensive attention in materials science. 4] chitosan and montmorillonite (mtm), an abundant polysaccharide and a natural clay respectively, have been widely used as the constituents of bionanocomposites. the intercalation of chitosan into mtm and the dispersion of mtm nanosheets in the chitosan matrix have been systematically investigated. bionanocomposites based on chitosan intercalation into mtm can be used as a sensor applied in the potentiometric determination of several anions. bionanocomposite films formed through the dispersion of mtm nanosheets in the chitosan matrix have shown enhancement of the mechanical and thermal properties compared with the pure chitosan film. unfortunately, the enhancement of the tensile strength and thermal stability of the chitosan\u2013mtm bionanocomposite film is still low far from the expectations in industry. systematic studies are carried out in materials science on natural materials with the objective of duplicating their properties in artificial materials. natural nanocomposites provide prime design models of lightweight, strong, stiff, and tough materials due to the hierarchical organization of the micro and nanostructures. one attractive biological model for artificial material design is nacre (mother-of-pearl). the microscopic architecture of nacre has been classically illustrated as a \u201cbrick-and-mortar\u201d arrangement that plays an important role in the amazing mechanical properties of the nacre. this arrangement is constituted of highly aligned inorganic aragonite platelets surrounded by a protein matrix, which serves as a glue between the platelets. recently, the microstructure of the nacre has been mimicked by several innovative techniques to fabricate the artificial nacre-like materials with high mechanical performance. for example, layer-by-layer (lbl) deposition combining with cross-linking yielded poly(vinyl alcohol)/mtm nacre-like nanocomposites with a tensile strength of up to 400 mpa; the ice-crystal templates of the microscopic layers were designed to form a brick-and-mortar microstructured al2o3/poly(methyl methacrylate) composite that is 300 times tougher than its constituents; the assembly of al2o3 platelets on the air/water interface and sequent spincoating was developed into the fabrication of lamellar al2o3/ chitosan hybrid films with high flaw tolerance and ductility; the self-assembly of nanoclays with polymers coating by a paper-making method resulted in the nacre-mimetic films; and nacre-like structural mtm\u2013polyimide nanocomposites were fabricated by centrifugation deposition-assisted assembly. our group has also fabricated nacre-like chitosanlayered double hydroxide hybrid films with a tensile strength of up to 160mpa by sequential dipping coating and the lbl technique. the concept of mimicking nacre and recently developed innovative techniques inspired us to fabricate the highly sustainable artificial nacre-like chitosan\u2013mtm bionanocomposite film with high performance to seek a promising material for the replacement of conventional petroleumbased plastics. herein, we introduce a novel approach to fabricate artificial nacre-like chitosan\u2013mtm bionanocomposite films by self-assembly of chitosan\u2013mtm hybrid building blocks (scheme 1). the chitosan molecules are very easily coated onto exfoliated mtm nanosheets to yield the hybrid building blocks by strong electrostatic and hydrogen-bonding interactions. these hybrid building blocks can be dispersed in distilled water and then aligned to a nacre-like lamellar microstructure by vacuum-filtrationor water-evaporationinduced self-assembly because of the role that the orientation of the nanosheets and linking of the chitosan play. the fabrication process is simple, fast, time-saving, and easily scaled up compared with the lbl, ice-crystal-template, and other techniques. [*] h. b. yao, z. h. tan, h. y. fang, prof. dr. s. h. yu division of nanomaterials and chemistry hefei national laboratory for physical sciences at microscale department of chemistry national synchrotron radiation laboratory university of science and technology of china hefei, anhui 230026 (p.r. china) fax: (+ 86)551-360-3040 e-mail: shyu@ustc.edu.cn homepage: http://staff.ustc.edu.cn/~ yulab/"
                },
                {
                    "id": "R189679",
                    "label": "Thermochromic Artificial Nacre Based on Montmorillonite",
                    "doi": "10.1021/acsami.7b07953",
                    "research_field": {
                        "id": "R254",
                        "label": "Materials Science and Engineering"
                    },
                    "abstract": "nacre-inspired nanocomposites have attracted a great deal of attention in recent years because of their special mechanical properties and universality of the underlying principles of materials engineering. the ability to respond to external stimuli will augment the high toughness and high strength of artificial nacre-like composites and open new technological horizons for these materials. herein, we fabricated robust artificial nacre based on montmorillonite (mmt) that combines robustness with reversible thermochromism. our artificial nacre shows great potential in various fields such as aerospace and sensors and opens an avenue to fabricate artificial nacre responsive to other external stimuli in the future."
                },
                {
                    "id": "R189410",
                    "label": "Graphene Oxide Papers Modified by Divalent Ions\u2014Enhancing Mechanical Properties <i>via</i> Chemical Cross-Linking",
                    "doi": "10.1021/nn700349a",
                    "research_field": {
                        "id": "R254",
                        "label": "Materials Science and Engineering"
                    },
                    "abstract": "significant enhancement in mechanical stiffness (10-200%) and fracture strength (approximately 50%) of graphene oxide paper, a novel paperlike material made from individual graphene oxide sheets, can be achieved upon modification with a small amount (less than 1 wt %) of mg(2+) and ca(2+). these results can be readily rationalized in terms of the chemical interactions between the functional groups of the graphene oxide sheets and the divalent metals ions. while oxygen functional groups on the basal planes of the sheets and the carboxylate groups on the edges can both bond to mg(2+) and ca(2+), the main contribution to mechanical enhancement of the paper comes from the latter."
                },
                {
                    "id": "R201427",
                    "label": "Written-in Conductive Patterns on Robust Graphene Oxide Biopaper by Electrochemical Microstamping",
                    "doi": "10.1002/anie.201307830",
                    "research_field": {
                        "id": "R254",
                        "label": "Materials Science and Engineering"
                    },
                    "abstract": "the silk road: by employing silk fibroin as a binder between graphene oxide films and aluminum foil for a facile, highly localized reduction process, conductive paper is reinvented. the flexible, robust biographene papers have high toughness and electrical conductivity. this electrochemical written-in approach is readily applicable for the fabrication of conductive patterned papers with complex circuitries."
                },
                {
                    "id": "R203125",
                    "label": "Fatigue Resistant Bioinspired Composite from Synergistic Two-Dimensional Nanocomponents",
                    "doi": "10.1021/acsnano.7b02706",
                    "research_field": {
                        "id": "R254",
                        "label": "Materials Science and Engineering"
                    },
                    "abstract": "portable and wearable electronics require much more flexible graphene-based electrode with high fatigue life, which could repeatedly bend, fold, or stretch without sacrificing its mechanical properties and electrical conductivity. herein, a kind of ultrahigh fatigue resistant graphene-based nanocomposite via tungsten disulfide (ws2) nanosheets is synthesized by introducing a synergistic effect with covalently cross-linking inspired by the orderly layered structure and abundant interfacial interactions of nacre. the fatigue life of resultant graphene-based nanocomposites is more than one million times at the stress level of 270 mpa, and the electrical conductivity can be kept as high as 197.1 s/cm after 1.0 \u00d7 105 tensile testing cycles. these outstanding properties are attributed to the synergistic effect from lubrication of ws2 nanosheets for deflecting crack propagation, and covalent bonding between adjacent go nanosheets for bridging crack, which is verified by the molecular dynamics (md) simulations. the ws2 induced synergistic effect with covalent bonding offers a guidance for constructing graphene-based nanocomposites with high fatigue life, which have great potential for applications in flexible and wearable electronic devices, etc."
                },
                {
                    "id": "R201164",
                    "label": "A robust and conductive metal-impregnated graphene oxide membrane selectively separating organic vapors",
                    "doi": "10.1039/c4cc08896d",
                    "research_field": {
                        "id": "R254",
                        "label": "Materials Science and Engineering"
                    },
                    "abstract": "a robust and conductive graphene oxide membrane with selective separation properties can be easily prepared by the vapor phase metal-impregnation effect provided by an atomic layer deposition process."
                },
                {
                    "id": "R203394",
                    "label": "Robust Bioinspired Graphene Film via \u03c0\u2013\u03c0 Cross-linking",
                    "doi": "10.1021/acsami.7b07748",
                    "research_field": {
                        "id": "R254",
                        "label": "Materials Science and Engineering"
                    },
                    "abstract": "graphene composite films inspired by nacre are the subject of ongoing research efforts to optimize their properties for applications in flexible energy devices. noncovalent interactions do not cause interruption of the delocalized conjugated \u03c0-electron system, thus preserving graphene's excellent properties. herein, we synthesized a conjugated molecule with pyrene groups on both ends of a long linear chain (ap-dss) from 1-aminopyrene (ap) and disuccinimidyl suberate (dss). the ap-dss molecules are used to cross-link adjacent graphene nanosheets via \u03c0-\u03c0 interfacial interactions to improve properties of graphene films. the tensile strength and toughness of resultant graphene films were 4.1 and 6.4 times higher, respectively, than that of pure rgo film. more remarkably, the electrical conductivity showed a simultaneous improvement, which is rare to be achieved in other kinds of covalent or noncovalent functionalization. such integration demonstrates the advantage of this work to previously reported noncovalent functionalization of graphene."
                },
                {
                    "id": "R203400",
                    "label": "Strong, Conductive, Foldable Graphene Sheets by Sequential Ionic and \u03c0 Bridging",
                    "doi": "10.1002/adma.201802733",
                    "research_field": {
                        "id": "R254",
                        "label": "Materials Science and Engineering"
                    },
                    "abstract": "the goal of this work is to develop an inexpensive low\u2010temperature process that provides polymer\u2010free, high\u2010strength, high\u2010toughness, electrically conducting sheets of reduced graphene oxide (rgo). to develop this process, we have evaluated the mechanical and electrical properties resulting from the application of an ionic bonding agent (cr3+), a \u03c0\u2013\u03c0 bonding agent comprising pyrene end groups, and their combinations for enhancing the performance of rgo sheets. when only one bonding agent was used, the \u03c0\u2013\u03c0 bonding agent is much more effective than the ionic bonding agent for improving both the mechanical and electrical properties of rgo sheets. however, the successive application of ionic bonding and \u03c0\u2013\u03c0 bonding agents maximizes tensile strength, toughness, long\u2010term electrical stability in various corrosive solutions, and resistance to mechanical abuse and ultrasonic dissolution. using a combination of ionic bonding and \u03c0\u2013\u03c0 bonding agents, high tensile strength (821 mpa), high toughness (20 mj m\u22123), and electrical conductivity (416 s cm\u22121) were obtained, as well as remarkable retention of mechanical and electrical properties during ultrasonication and mechanical cycling by both sheet stretch and sheet folding, suggesting high potential for applications in aerospace and flexible electronics."
                },
                {
                    "id": "R201158",
                    "label": "Realizing Ultrahigh Modulus and High Strength of Macroscopic Graphene Oxide Papers Through Crosslinking of Mussel-Inspired Polymers",
                    "doi": "10.1002/adma.201300118",
                    "research_field": {
                        "id": "R254",
                        "label": "Materials Science and Engineering"
                    },
                    "abstract": "covalently crosslinked graphene oxide papers (gops) with enhanced mechanical properties are prepared by a strategy involving crosslinking by means of intercalated polymers. the strength and modulus of the crosslinked gops increase by 115% and 550%, respectively, compared to the pristine gops. these results broaden the potential applications of graphene, and the crosslinking strategy will open the door to the assembly of other nanometer-scale materials."
                },
                {
                    "id": "R203397",
                    "label": "Superior Fatigue Resistant Bioinspired Graphene-Based Nanocomposite via Synergistic Interfacial Interactions",
                    "doi": "10.1002/adfm.201605636",
                    "research_field": {
                        "id": "R254",
                        "label": "Materials Science and Engineering"
                    },
                    "abstract": "excellent fatigue resistance is a prerequisite for flexible energy devices to achieve high and stable performance under repeated deformation state. inspired by the sophisticated interfacial architecture of nacre, herein a super fatigue\u2010resistant graphene\u2010based nanocomposite with integrated high tensile strength and toughness through poly(dopamine)\u2010nickel ion (ni2+) chelate architecture that mimics byssal threads is demonstrated. these kind of synergistic interfacial interactions of covalent and ionic bonding effectively suppress the crack propagation in the process of fatigue testing, resulting in superhigh fatigue life of this bioinspired graphene\u2010based nanocomposite (bgbn). in addition, the electrical conductivity is well kept after fatigue testing. the proposed synergistic interfacial interactions could serve as a guideline for fabricating high\u2010performance multifunctional bgbns with promising applications in flexible energy devices, such as flexible electrodes for supercapacitors and lithium batteries, etc."
                },
                {
                    "id": "R178358",
                    "label": "Large Scale Self-Assembly of Smectic Nanocomposite Films by Doctor Blading versus Spray Coating: Impact of Crystal Quality on Barrier Properties",
                    "doi": "10.1021/acs.macromol.7b00701",
                    "research_field": {
                        "id": "R137665",
                        "label": "Coating and Surface Technology"
                    },
                    "abstract": "flexible transparent barrier films are required in various fields of application ranging from flexible, transparent food packaging to display encapsulation. environmentally friendly, waterborne polymer\u2013clay nanocomposites would be preferred but fail to meet in particular requirements for ultra high water vapor barriers. here we show that self-assembly of nanocomposite films into one-dimensional crystalline (smectic) polymer\u2013clay domains is a so-far overlooked key-factor capable of suppressing water vapor diffusivity despite appreciable swelling at elevated temperatures and relative humidity (r.h.). moreover, barrier performance was shown to improve with quality of the crystalline order. in this respect, spray coating is superior to doctor blading because it yields significantly better ordered structures. for spray-coated waterborne nanocomposite films (21.4 \u03bcm) ultra high barrier specifications are met at 23 \u00b0c and 50% r.h. with oxygen transmission rates (otr) < 0.0005 cm3 m\u20132 day\u20131 bar\u20131 and water vapor ..."
                },
                {
                    "id": "R178362",
                    "label": "Clay-Based Nanocomposite Coating for Flexible Optoelectronics Applying Commercial Polymers",
                    "doi": "10.1021/nn400713e",
                    "research_field": {
                        "id": "R137665",
                        "label": "Coating and Surface Technology"
                    },
                    "abstract": "transparency, flexibility, and especially ultralow oxygen (otr) and water vapor (wvtr) transmission rates are the key issues to be addressed for packaging of flexible organic photovoltaics and organic light-emitting diodes. concomitant optimization of all essential features is still a big challenge. here we present a thin (1.5 \u03bcm), highly transparent, and at the same time flexible nanocomposite coating with an exceptionally low otr and wvtr (1.0 \u00d7 10(-2) cm(3) m(-2) day(-1) bar(-1) and <0.05 g m(-2) day(-1) at 50% rh, respectively). a commercially available polyurethane (desmodur n 3600 and desmophen 670 ba, bayer materialscience ag) was filled with a delaminated synthetic layered silicate exhibiting huge aspect ratios of about 25,000. functional films were prepared by simple doctor-blading a suspension of the matrix and the organophilized clay. this preparation procedure is technically benign, is easy to scale up, and may readily be applied for encapsulation of sensitive flexible electronics."
                }
            ]
        },
        {
            "id": "R40006",
            "label": "Basic reproduction number estimate",
            "research_fields": [
                {
                    "id": "R12",
                    "label": "Life Sciences"
                }
            ],
            "properties": [
                "Time period",
                "Basic reproduction number",
                "location"
            ],
            "papers": [
                {
                    "id": "R44731",
                    "label": "Transmission interval estimates suggest pre-symptomatic spread of COVID-19",
                    "doi": "10.1101/2020.03.03.20029983",
                    "research_field": {
                        "id": "R57",
                        "label": "Virology"
                    },
                    "abstract": "abstract background as the covid-19 epidemic is spreading, incoming data allows us to quantify values of key variables that determine the transmission and the effort required to control the epidemic. we determine the incubation period and serial interval distribution for transmission clusters in singapore and in tianjin. we infer the basic reproduction number and identify the extent of pre-symptomatic transmission. methods we collected outbreak information from singapore and tianjin, china, reported from jan.19-feb.26 and jan.21-feb.27, respectively. we estimated incubation periods and serial intervals in both populations. results the mean incubation period was 7.1 (6.13, 8.25) days for singapore and 9 (7.92, 10.2) days for tianjin. both datasets had shorter incubation periods for earlier-occurring cases. the mean serial interval was 4.56 (2.69, 6.42) days for singapore and 4.22 (3.43, 5.01) for tianjin. we inferred that early in the outbreaks, infection was transmitted on average 2.55 and 2.89 days before symptom onset (singapore, tianjin). the estimated basic reproduction number for singapore was 1.97 (1.45, 2.48) secondary cases per infective; for tianjin it was 1.87 (1.65, 2.09) secondary cases per infective. conclusions estimated serial intervals are shorter than incubation periods in both singapore and tianjin, suggesting that pre-symptomatic transmission is occurring. shorter serial intervals lead to lower estimates of r0, which suggest that half of all secondary infections should be prevented to control spread."
                },
                {
                    "id": "R44743",
                    "label": "Estimation of the epidemic properties of the 2019 novel coronavirus: A mathematical modeling study",
                    "doi": "10.1101/2020.02.18.20024315",
                    "research_field": {
                        "id": "R57",
                        "label": "Virology"
                    },
                    "abstract": "abstract background the 2019 novel coronavirus (covid-19) emerged in wuhan, china in december 2019 and has been spreading rapidly in china. decisions about its pandemic threat and the appropriate level of public health response depend heavily on estimates of its basic reproduction number and assessments of interventions conducted in the early stages of the epidemic. methods we conducted a mathematical modeling study using five independent methods to assess the basic reproduction number (r0) of covid-19, using data on confirmed cases obtained from the china national health commission for the period 10 th january \u2013 8 th february. we analyzed the data for the period before the closure of wuhan city (10 th january \u2013 23 rd january) and the post-closure period (23 rd january \u2013 8 th february) and for the whole period, to assess both the epidemic risk of the virus and the effectiveness of the closure of wuhan city on spread of covid-19. findings before the closure of wuhan city the basic reproduction number of covid-19 was 4.38 (95% ci: 3.63 \u2013 5.13), dropping to 3.41 (95% ci: 3.16 \u2013 3.65) after the closure of wuhan city. over the entire epidemic period covid-19 had a basic reproduction number of 3.39 (95% ci: 3.09 \u2013 3.70), indicating it has a very high transmissibility. interpretation covid-19 is a highly transmissible virus with a very high risk of epidemic outbreak once it emerges in metropolitan areas. the closure of wuhan city was effective in reducing the severity of the epidemic, but even after closure of the city and the subsequent expansion of that closure to other parts of hubei the virus remained extremely infectious. emergency planners in other cities should consider this high infectiousness when considering responses to this virus. funding national natural science foundation of china, china medical board, national science and technology major project of china"
                },
                {
                    "id": "R44759",
                    "label": "Transmission potential of COVID-19 in Iran",
                    "doi": "10.1101/2020.03.08.20030643",
                    "research_field": {
                        "id": "R57",
                        "label": "Virology"
                    },
                    "abstract": "abstract we estimated the reproduction number of 2020 iranian covid-19 epidemic using two different methods: r 0 was estimated at 4.4 (95% ci, 3.9, 4.9) (generalized growth model) and 3.50 (1.28, 8.14) (epidemic doubling time) (february 19 - march 1) while the effective r was estimated at 1.55 (1.06, 2.57) (march 6-19)."
                },
                {
                    "id": "R44776",
                    "label": "Estimating the generation interval for COVID-19 based on symptom onset data",
                    "doi": "10.1101/2020.03.05.20031815",
                    "research_field": {
                        "id": "R57",
                        "label": "Virology"
                    },
                    "abstract": "abstract background estimating key infectious disease parameters from the covid-19 outbreak is quintessential for modelling studies and guiding intervention strategies. whereas different estimates for the incubation period distribution and the serial interval distribution have been reported, estimates of the generation interval for covid-19 have not been provided. methods we used outbreak data from clusters in singapore and tianjin, china to estimate the generation interval from symptom onset data while acknowledging uncertainty about the incubation period distribution and the underlying transmission network. from those estimates we obtained the proportions pre-symptomatic transmission and reproduction numbers. results the mean generation interval was 5.20 (95%ci 3.78-6.78) days for singapore and 3.95 (95%ci 3.01-4.91) days for tianjin, china when relying on a previously reported incubation period with mean 5.2 and sd 2.8 days. the proportion of pre-symptomatic transmission was 48% (95%ci 32-67%) for singapore and 62% (95%ci 50-76%) for tianjin, china. estimates of the reproduction number based on the generation interval distribution were slightly higher than those based on the serial interval distribution. conclusions estimating generation and serial interval distributions from outbreak data requires careful investigation of the underlying transmission network. detailed contact tracing information is essential for correctly estimating these quantities."
                },
                {
                    "id": "R44793",
                    "label": "Effects of voluntary event cancellation and school closure as countermeasures against COVID\u221219 outbreak in Japan",
                    "doi": "10.1101/2020.03.19.20037945",
                    "research_field": {
                        "id": "R57",
                        "label": "Virology"
                    },
                    "abstract": "abstract background to control the covid-19 outbreak in japan, sports and entertainment events were canceled and schools were closed throughout japan from february 26 through march 19. that policy has been designated as voluntary event cancellation and school closure (vecsc). object this study assesses vecsc effectiveness based on predicted outcomes. method: a simple susceptible\u2013infected\u2013recovery model was applied to data of patients with symptoms in japan during january 14 through march 25. the respective reproduction numbers were estimated before vecsc (r), during vecsc (r e ), and after vecsc (r a ). results results suggest r before vecsc as 1.987 [1.908, 2.055], r e during vecsc as 1.122 [0.980, 1.260], and r a after vecsc as 3.086 [2.529, 3.739]. discussion and conclusion results demonstrated that vecsc can reduce covid-19 infectiousness considerably, but the value of r rose to exceed 2.5 after vecsc."
                },
                {
                    "id": "R44799",
                    "label": "Early Transmission Dynamics in Wuhan, China, of Novel Coronavirus\u2013Infected Pneumonia",
                    "doi": "10.1056/nejmoa2001316",
                    "research_field": {
                        "id": "R57",
                        "label": "Virology"
                    },
                    "abstract": "abstract background the initial cases of novel coronavirus (2019-ncov)\u2013infected pneumonia (ncip) occurred in wuhan, hubei province, china, in december 2019 and january 2020. we analyzed data on the first 425 confirmed cases in wuhan to determine the epidemiologic characteristics of ncip. methods we collected information on demographic characteristics, exposure history, and illness timelines of laboratory-confirmed cases of ncip that had been reported by january 22, 2020. we described characteristics of the cases and estimated the key epidemiologic time-delay distributions. in the early period of exponential growth, we estimated the epidemic doubling time and the basic reproductive number. results among the first 425 patients with confirmed ncip, the median age was 59 years and 56% were male. the majority of cases (55%) with onset before january 1, 2020, were linked to the huanan seafood wholesale market, as compared with 8.6% of the subsequent cases. the mean incubation period was 5.2 days (95% confidence interval [ci], 4.1 to 7.0), with the 95th percentile of the distribution at 12.5 days. in its early stages, the epidemic doubled in size every 7.4 days. with a mean serial interval of 7.5 days (95% ci, 5.3 to 19), the basic reproductive number was estimated to be 2.2 (95% ci, 1.4 to 3.9). conclusions on the basis of this information, there is evidence that human-to-human transmission has occurred among close contacts since the middle of december 2019. considerable efforts to reduce transmission will be required to control outbreaks if similar dynamics apply elsewhere. measures to prevent or reduce transmission should be implemented in populations at risk. (funded by the ministry of science and technology of china and others.)"
                },
                {
                    "id": "R44806",
                    "label": "Estimation of the Transmission Risk of 2019-nCov and Its Implication for Public Health Interventions",
                    "doi": "10.2139/ssrn.3525558",
                    "research_field": {
                        "id": "R57",
                        "label": "Virology"
                    },
                    "abstract": "english abstract: background: since the emergence of the first pneumonia cases in wuhan, china, the novel coronavirus (2019-ncov) infection has been quickly spreading out to other provinces and neighbouring countries. estimation of the basic reproduction number by means of mathematical modelling can be helpful for determining the potential and severity of an outbreak, and providing critical information for identifying the type of disease interventions and intensity.\\r\\n\\r\\nmethods: a deterministic compartmental model was devised based on the clinical progression of the disease, epidemiological status of the individuals, and the intervention measures.\\r\\n\\r\\nfindings: the estimation results based on likelihood and model analysis reveal that the control reproduction number may be as high as 6.47 (95% ci 5.71-7.23). sensitivity analyses reveal that interventions, such as intensive contact tracing followed by quarantine and isolation, can effectively reduce the control reproduction number and transmission risk, with the effect of travel restriction of wuhan on 2019-ncov infection in beijing being almost equivalent to increasing quarantine by 100-thousand baseline value.\\r\\n\\r\\ninterpretation: it is essential to assess how the expensive, resource-intensive measures implemented by the chinese authorities can contribute to the prevention and control of the 2019-ncov infection, and how long should be maintained. under the most restrictive measures, the outbreak is expected to peak within two weeks (since january 23rd 2020) with significant low peak value. with travel restriction (no imported exposed individuals to beijing), the number of infected individuals in 7 days will decrease by 91.14% in beijing, compared with the scenario of no travel restriction.\\r\\n\\r\\nmandarin abstract: \u80cc\u666f\uff1a\u81ea\u4ece\u4e2d\u56fd\u6b66\u6c49\u51fa\u73b0\u7b2c\u4e00\u4f8b\u80ba\u708e\u75c5\u4f8b\u4ee5\u6765\uff0c\u65b0\u578b\u51a0\u72b6\u75c5\u6bd2\uff082019-ncov\uff09\u611f\u67d3\u5df2\u8fc5\u901f\u4f20\u64ad\u5230\u5176\u4ed6\u7701\u4efd\u548c\u5468\u8fb9\u56fd\u5bb6\u3002\u901a\u8fc7\u6570\u5b66\u6a21\u578b\u4f30\u8ba1\u57fa\u672c\u518d\u751f\u6570\uff0c\u6709\u52a9\u4e8e\u786e\u5b9a\u75ab\u60c5\u7206\u53d1\u7684\u53ef\u80fd\u6027\u548c\u4e25\u91cd\u6027\uff0c\u5e76\u4e3a\u786e\u5b9a\u75be\u75c5\u5e72\u9884\u7c7b\u578b\u548c\u5f3a\u5ea6\u63d0\u4f9b\u5173\u952e\u4fe1\u606f\u3002\\r\\n\\r\\n\u65b9\u6cd5\uff1a\u6839\u636e\u75be\u75c5\u7684\u4e34\u5e8a\u8fdb\u5c55\uff0c\u4e2a\u4f53\u7684\u6d41\u884c\u75c5\u5b66\u72b6\u51b5\u548c\u5e72\u9884\u63aa\u65bd\uff0c\u8bbe\u8ba1\u786e\u5b9a\u6027\u7684\u4ed3\u5ba4\u6a21\u578b\u3002\\r\\n\\r\\n\u7ed3\u679c\uff1a\u57fa\u4e8e\u4f3c\u7136\u51fd\u6570\u548c\u6a21\u578b\u5206\u6790\u7684\u4f30\u8ba1\u7ed3\u679c\u8868\u660e\uff0c\u63a7\u5236\u518d\u751f\u6570\u53ef\u80fd\u9ad8\u8fbe6.47\uff0895\uff05ci 5.71-7.23\uff09\u3002\u654f\u611f\u6027\u5206\u6790\u663e\u793a\uff0c\u5bc6\u96c6\u63a5\u89e6\u8ffd\u8e2a\u548c\u9694\u79bb\u7b49\u5e72\u9884\u63aa\u65bd\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u63a7\u5236\u518d\u751f\u6570\u548c\u4f20\u64ad\u98ce\u9669\uff0c\u6b66\u6c49\u5c01\u57ce\u63aa\u65bd\u5bf9\u5317\u4eac2019-ncov\u611f\u67d3\u7684\u5f71\u54cd\u51e0\u4e4e\u7b49\u540c\u4e8e\u589e\u52a0\u9694\u79bb\u63aa\u65bd10\u4e07\u7684\u57fa\u7ebf\u503c\u3002\\r\\n\\r\\n\u89e3\u91ca\uff1a\u5fc5\u987b\u8bc4\u4f30\u4e2d\u56fd\u5f53\u5c40\u5b9e\u65bd\u7684\u6602\u8d35\uff0c\u8d44\u6e90\u5bc6\u96c6\u578b\u63aa\u65bd\u5982\u4f55\u6709\u52a9\u4e8e\u9884\u9632\u548c\u63a7\u52362019-ncov\u611f\u67d3\uff0c\u4ee5\u53ca\u5e94\u7ef4\u6301\u591a\u957f\u65f6\u95f4\u3002\u5728\u6700\u4e25\u683c\u7684\u63aa\u65bd\u4e0b\uff0c\u9884\u8ba1\u75ab\u60c5\u5c06\u5728\u4e24\u5468\u5185\uff08\u81ea2020\u5e741\u670823\u65e5\u8d77\uff09\u8fbe\u5230\u5cf0\u503c\uff0c\u5cf0\u503c\u8f83\u4f4e\u3002\u4e0e\u6ca1\u6709\u51fa\u884c\u9650\u5236\u7684\u60c5\u51b5\u76f8\u6bd4\uff0c\u6709\u4e86\u51fa\u884c\u9650\u5236\uff08\u5373\u6ca1\u6709\u8f93\u5165\u7684\u6f5c\u4f0f\u7c7b\u4e2a\u4f53\u8fdb\u5165\u5317\u4eac\uff09\uff0c\u5317\u4eac\u76847\u5929\u611f\u67d3\u8005\u6570\u91cf\u5c06\u51cf\u5c1191.14\uff05\u3002"
                },
                {
                    "id": "R44812",
                    "label": "Pattern of early human-to-human transmission of Wuhan 2019-nCoV",
                    "doi": "10.1101/2020.01.23.917351",
                    "research_field": {
                        "id": "R57",
                        "label": "Virology"
                    },
                    "abstract": "abstract on december 31, 2019, the world health organization was notified about a cluster of pneumonia of unknown aetiology in the city of wuhan, china. chinese authorities later identified a new coronavirus (2019-ncov) as the causative agent of the outbreak. as of january 23, 2020, 655 cases have been confirmed in china and several other countries. understanding the transmission characteristics and the potential for sustained human-to-human transmission of 2019-ncov is critically important for coordinating current screening and containment strategies, and determining whether the outbreak constitutes a public health emergency of international concern (pheic). we performed stochastic simulations of early outbreak trajectories that are consistent with the epidemiological findings to date. we found the basic reproduction number, r 0 , to be around 2.2 (90% high density interval 1.4\u20143.8), indicating the potential for sustained human-to-human transmission. transmission characteristics appear to be of a similar magnitude to severe acute respiratory syndrome-related coronavirus (sars-cov) and the 1918 pandemic influenza. these findings underline the importance of heightened screening, surveillance and control efforts, particularly at airports and other travel hubs, in order to prevent further international spread of 2019-ncov."
                },
                {
                    "id": "R44819",
                    "label": "Report 3: Transmissibility of 2019-nCoV. 2020. WHO Collaborating Centre for Infectious Disease Modelling, MRC Centre for Global Infectious Disease Analysis",
                    "doi": "10.25561/77148",
                    "research_field": {
                        "id": "R57",
                        "label": "Virology"
                    },
                    "abstract": "self-sustaining human-to-human transmission of the novel coronavirus (2019-ncov) is the only plausible explanation of the scale of the outbreak in wuhan. we estimate that, on average, each case infected 2.6 (uncertainty range: 1.5-3.5) other people up to 18 january 2020, based on an analysis combining our past estimates of the size of the outbreak in wuhan with computational modelling of potential epidemic trajectories. this implies that control measures need to block well over 60% of transmission to be effective in controlling the outbreak. it is likely, based on the experience of sars and mers-cov, that the number of secondary cases caused by a case of 2019-ncov is highly variable \u2013 with many cases causing no secondary infections, and a few causing many. whether transmission is continuing at the same rate currently depends on the effectiveness of current control measures implemented in china and the extent to which the populations of affected areas have adopted risk-reducing behaviours. in the absence of antiviral drugs or vaccines, control relies upon the prompt detection and isolation of symptomatic cases. it is unclear at the current time whether this outbreak can be contained within china; uncertainties include the severity spectrum of the disease caused by this virus and whether cases with relatively mild symptoms are able to transmit the virus efficiently. identification and testing of potential cases need to be as extensive as is permitted by healthcare and diagnostic testing capacity \u2013 including the identification, testing and isolation of suspected cases with only mild to moderate disease (e.g. influenza-like illness), when logistically feasible."
                },
                {
                    "id": "R44825",
                    "label": "Preliminary estimation of the basic reproduction number of novel coronavirus (2019-nCoV) in China, from 2019 to 2020: A data-driven analysis in the early phase of the outbreak",
                    "doi": "10.1016/j.ijid.2020.01.050",
                    "research_field": {
                        "id": "R57",
                        "label": "Virology"
                    },
                    "abstract": "backgrounds an ongoing outbreak of a novel coronavirus (2019-ncov) pneumonia hit a major city of china, wuhan, december 2019 and subsequently reached other provinces/regions of china and countries. we present estimates of the basic reproduction number, r0, of 2019-ncov in the early phase of the outbreak. methods accounting for the impact of the variations in disease reporting rate, we modelled the epidemic curve of 2019-ncov cases time series, in mainland china from january 10 to january 24, 2020, through the exponential growth. with the estimated intrinsic growth rate (\u03b3), we estimated r0 by using the serial intervals (si) of two other well-known coronavirus diseases, mers and sars, as approximations for the true unknown si. findings the early outbreak data largely follows the exponential growth. we estimated that the mean r0 ranges from 2.24 (95%ci: 1.96-2.55) to 3.58 (95%ci: 2.89-4.39) associated with 8-fold to 2-fold increase in the reporting rate. we demonstrated that changes in reporting rate substantially affect estimates of r0. conclusion the mean estimate of r0 for the 2019-ncov ranges from 2.24 to 3.58, and significantly larger than 1. our findings indicate the potential of 2019-ncov to cause outbreaks."
                },
                {
                    "id": "R44836",
                    "label": "Estimating the effective reproduction number of the 2019-nCoV in China",
                    "doi": "10.1101/2020.01.27.20018952",
                    "research_field": {
                        "id": "R57",
                        "label": "Virology"
                    },
                    "abstract": "abstract we estimate the effective reproduction number for 2019-ncov based on the daily reported cases from china cdc. the results indicate that 2019-ncov has a higher effective reproduction number than sars with a comparable fatality rate. article summary line this modeling study indicates that 2019-ncov has a higher effective reproduction number than sars with a comparable fatality rate."
                },
                {
                    "id": "R44842",
                    "label": "Early Transmissibility Assessment of a Novel Coronavirus in Wuhan, China",
                    "doi": "10.2139/ssrn.3524675",
                    "research_field": {
                        "id": "R57",
                        "label": "Virology"
                    },
                    "abstract": "between december 1, 2019 and january 26, 2020, nearly 3000 cases of respiratory illness caused by a novel coronavirus originating in wuhan, china have been reported. in this short analysis, we combine publicly available cumulative case data from the ongoing outbreak with phenomenological modeling methods to conduct an early transmissibility assessment. our model suggests that the basic reproduction number associated with the outbreak (at time of writing) may range from 2.0 to 3.1. though these estimates are preliminary and subject to change, they are consistent with previous findings regarding the transmissibility of the related sars-coronavirus and indicate the possibility of epidemic potential."
                },
                {
                    "id": "R44847",
                    "label": "Novel coronavirus 2019-nCoV: early estimation of epidemiological parameters and epidemic predictions",
                    "doi": "10.1101/2020.01.23.20018549",
                    "research_field": {
                        "id": "R57",
                        "label": "Virology"
                    },
                    "abstract": "abstract since first identified, the epidemic scale of the recently emerged novel coronavirus (2019-ncov) in wuhan, china, has increased rapidly, with cases arising across china and other countries and regions. using a transmission model, we estimate a basic reproductive number of 3.11 (95%ci, 2.39\u20134.13); 58\u201376% of transmissions must be prevented to stop increasing; wuhan case ascertainment of 5.0% (3.6\u20137.4); 21022 (11090\u201333490) total infections in wuhan 1 to 22 january. changes to previous version case data updated to include 22 jan 2020; we did not use cases reported after this period as cases were reported at the province level hereafter, and large-scale control interventions were initiated on 23 jan 2020; improved likelihood function, better accounting for first 41 confirmed cases, and now using all infections (rather than just cases detected) in wuhan for prediction of infection in international travellers; improved characterization of uncertainty in parameters, and calculation of epidemic trajectory confidence intervals using a more statistically rigorous method; extended range of latent period in sensitivity analysis to reflect reports of up to 6 day incubation period in household clusters; removed travel restriction analysis, as different modelling approaches (e.g. stochastic transmission, rather than deterministic transmission) are more appropriate to such analyses."
                },
                {
                    "id": "R44856",
                    "label": "Time-varying transmission dynamics of Novel Coronavirus Pneumonia in China",
                    "doi": "10.1101/2020.01.25.919787",
                    "research_field": {
                        "id": "R57",
                        "label": "Virology"
                    },
                    "abstract": "abstract rationale several studies have estimated basic production number of novel coronavirus pneumonia (ncp). however, the time-varying transmission dynamics of ncp during the outbreak remain unclear. objectives we aimed to estimate the basic and time-varying transmission dynamics of ncp across china, and compared them with sars. methods data on ncp cases by february 7, 2020 were collected from epidemiological investigations or official websites. data on severe acute respiratory syndrome (sars) cases in guangdong province, beijing and hong kong during 2002-2003 were also obtained. we estimated the doubling time, basic reproduction number ( r 0 ) and time-varying reproduction number ( r t ) of ncp and sars. measurements and main results as of february 7, 2020, 34,598 ncp cases were identified in china, and daily confirmed cases decreased after february 4. the doubling time of ncp nationwide was 2.4 days which was shorter than that of sars in guangdong (14.3 days), hong kong (5.7 days) and beijing (12.4 days). the r 0 of ncp cases nationwide and in wuhan were 4.5 and 4.4 respectively, which were higher than r 0 of sars in guangdong ( r 0 =2.3), hongkong ( r 0 =2.3), and beijing ( r 0 =2.6). the r t for ncp continuously decreased especially after january 16 nationwide and in wuhan. the r 0 for secondary ncp cases in guangdong was 0.6, and the r t values were less than 1 during the epidemic. conclusions ncp may have a higher transmissibility than sars, and the efforts of containing the outbreak are effective. however, the efforts are needed to persist in for reducing time-varying reproduction number below one. at a glance commentary scientific knowledge on the subject since december 29, 2019, pneumonia infection with 2019-ncov, now named as novel coronavirus pneumonia (ncp), occurred in wuhan, hubei province, china. the disease has rapidly spread from wuhan to other areas. as a novel virus, the time-varying transmission dynamics of ncp remain unclear, and it is also important to compare it with sars. what this study adds to the field we compared the transmission dynamics of ncp with sars, and found that ncp has a higher transmissibility than sars. time-varying production number indicates that rigorous control measures taken by governments are effective across china, and persistent efforts are needed to be taken for reducing instantaneous reproduction number below one."
                },
                {
                    "id": "R44865",
                    "label": "Modelling the epidemic trend of the 2019 novel coronavirus outbreak in China",
                    "doi": "10.1101/2020.01.23.916726",
                    "research_field": {
                        "id": "R57",
                        "label": "Virology"
                    },
                    "abstract": "we present a timely evaluation of the chinese 2019-ncov epidemic in its initial phase, where 2019-ncov demonstrates comparable transmissibility but lower fatality rates than sars and mers. a quick diagnosis that leads to case isolation and integrated interventions will have a major impact on its future trend. nevertheless, as china is facing its spring festival travel rush and the epidemic has spread beyond its borders, further investigation on its potential spatiotemporal transmission pattern and novel intervention strategies are warranted."
                },
                {
                    "id": "R44901",
                    "label": "Real-Time Estimation of the Risk of Death from Novel Coronavirus (COVID-19) Infection: Inference Using Exported Cases",
                    "doi": "10.3390/jcm9020523",
                    "research_field": {
                        "id": "R57",
                        "label": "Virology"
                    },
                    "abstract": "the exported cases of 2019 novel coronavirus (covid-19) infection that were confirmed outside china provide an opportunity to estimate the cumulative incidence and confirmed case fatality risk (ccfr) in mainland china. knowledge of the ccfr is critical to characterize the severity and understand the pandemic potential of covid-19 in the early stage of the epidemic. using the exponential growth rate of the incidence, the present study statistically estimated the ccfr and the basic reproduction number\u2014the average number of secondary cases generated by a single primary case in a na\u00efve population. we modeled epidemic growth either from a single index case with illness onset on 8 december 2019 (scenario 1), or using the growth rate fitted along with the other parameters (scenario 2) based on data from 20 exported cases reported by 24 january 2020. the cumulative incidence in china by 24 january was estimated at 6924 cases (95% confidence interval [ci]: 4885, 9211) and 19,289 cases (95% ci: 10,901, 30,158), respectively. the latest estimated values of the ccfr were 5.3% (95% ci: 3.5%, 7.5%) for scenario 1 and 8.4% (95% ci: 5.3%, 12.3%) for scenario 2. the basic reproduction number was estimated to be 2.1 (95% ci: 2.0, 2.2) and 3.2 (95% ci: 2.7, 3.7) for scenarios 1 and 2, respectively. based on these results, we argued that the current covid-19 epidemic has a substantial potential for causing a pandemic. the proposed approach provides insights in early risk assessment using publicly available data."
                },
                {
                    "id": "R44910",
                    "label": "Estimating the Unreported Number of Novel Coronavirus (2019-nCoV) Cases in China in the First Half of January 2020: A Data-Driven Modelling Analysis of the Early Outbreak",
                    "doi": "10.3390/jcm9020388",
                    "research_field": {
                        "id": "R57",
                        "label": "Virology"
                    },
                    "abstract": "background: in december 2019, an outbreak of respiratory illness caused by a novel coronavirus (2019-ncov) emerged in wuhan, china and has swiftly spread to other parts of china and a number of foreign countries. the 2019-ncov cases might have been under-reported roughly from 1 to 15 january 2020, and thus we estimated the number of unreported cases and the basic reproduction number, r0, of 2019-ncov. methods: we modelled the epidemic curve of 2019-ncov cases, in mainland china from 1 december 2019 to 24 january 2020 through the exponential growth. the number of unreported cases was determined by the maximum likelihood estimation. we used the serial intervals (si) of infection caused by two other well-known coronaviruses (cov), severe acute respiratory syndrome (sars) and middle east respiratory syndrome (mers) covs, as approximations of the unknown si for 2019-ncov to estimate r0. results: we confirmed that the initial growth phase followed an exponential growth pattern. the under-reporting was likely to have resulted in 469 (95% ci: 403\u2013540) unreported cases from 1 to 15 january 2020. the reporting rate after 17 january 2020 was likely to have increased 21-fold (95% ci: 18\u201325) in comparison to the situation from 1 to 17 january 2020 on average. we estimated the r0 of 2019-ncov at 2.56 (95% ci: 2.49\u20132.63). conclusion: the under-reporting was likely to have occurred during the first half of january 2020 and should be considered in future investigation."
                },
                {
                    "id": "R44918",
                    "label": "Estimation of the Transmission Risk of the 2019-nCoV and Its Implication for Public Health Interventions",
                    "doi": "10.3390/jcm9020462",
                    "research_field": {
                        "id": "R57",
                        "label": "Virology"
                    },
                    "abstract": "since the emergence of the first cases in wuhan, china, the novel coronavirus (2019-ncov) infection has been quickly spreading out to other provinces and neighboring countries. estimation of the basic reproduction number by means of mathematical modeling can be helpful for determining the potential and severity of an outbreak and providing critical information for identifying the type of disease interventions and intensity. a deterministic compartmental model was devised based on the clinical progression of the disease, epidemiological status of the individuals, and intervention measures. the estimations based on likelihood and model analysis show that the control reproduction number may be as high as 6.47 (95% ci 5.71\u20137.23). sensitivity analyses show that interventions, such as intensive contact tracing followed by quarantine and isolation, can effectively reduce the control reproduction number and transmission risk, with the effect of travel restriction adopted by wuhan on 2019-ncov infection in beijing being almost equivalent to increasing quarantine by a 100 thousand baseline value. it is essential to assess how the expensive, resource-intensive measures implemented by the chinese authorities can contribute to the prevention and control of the 2019-ncov infection, and how long they should be maintained. under the most restrictive measures, the outbreak is expected to peak within two weeks (since 23 january 2020) with a significant low peak value. with travel restriction (no imported exposed individuals to beijing), the number of infected individuals in seven days will decrease by 91.14% in beijing, compared with the scenario of no travel restriction."
                },
                {
                    "id": "R44726",
                    "label": "The early phase of the COVID-19 outbreak in Lombardy, Italy",
                    "doi": "",
                    "research_field": {
                        "id": "R57",
                        "label": "Virology"
                    },
                    "abstract": "in the night of february 20, 2020, the first case of novel coronavirus disease (covid-19) was confirmed in the lombardy region, italy. in the week that followed, lombardy experienced a very rapid increase in the number of cases. we analyzed the first 5,830 laboratory-confirmed cases to provide the first epidemiological characterization of a covid-19 outbreak in a western country. epidemiological data were collected through standardized interviews of confirmed cases and their close contacts. we collected demographic backgrounds, dates of symptom onset, clinical features, respiratory tract specimen results, hospitalization, contact tracing. we provide estimates of the reproduction number and serial interval. the epidemic in italy started much earlier than february 20, 2020. at the time of detection of the first covid-19 case, the epidemic had already spread in most municipalities of southern-lombardy. the median age for of cases is 69 years (range, 1 month to 101 years). 47% of positive subjects were hospitalized. among these, 18% required intensive care. the mean serial interval is estimated to be 6.6 days (95% ci, 0.7 to 19). we estimate the basic reproduction number at 3.1 (95% ci, 2.9 to 3.2). we estimated a decreasing trend in the net reproduction number starting around february 20, 2020. we did not observe significantly different viral loads in nasal swabs between symptomatic and asymptomatic. the transmission potential of covid-19 is very high and the number of critical cases may become largely unsustainable for the healthcare system in a very short-time horizon. we observed a slight decrease of the reproduction number, possibly connected with an increased population awareness and early effect of interventions. aggressive containment strategies are required to control covid-19 spread and catastrophic outcomes for the healthcare system."
                }
            ]
        },
        {
            "id": "R52190",
            "label": "Transient absorption spectroscopy",
            "research_fields": [
                {
                    "id": "R12",
                    "label": "Life Sciences"
                },
                {
                    "id": "R122",
                    "label": "Chemistry"
                }
            ],
            "properties": [
                "Has participating device",
                "Has participating person",
                "Has gaseous sample input",
                "Has solution sample input",
                "Has solid sample input",
                "Has radiant energy input",
                "Has output"
            ],
            "papers": [
                {
                    "id": "R45098",
                    "label": "Flash photolysis observation of the absorption spectra of trapped positive holes and electrons in colloidal titanium dioxide",
                    "doi": "10.1021/j150648a018",
                    "research_field": {
                        "id": "R122",
                        "label": "Chemistry"
                    },
                    "abstract": "\"photolyse laser eclair a 347 nm d'un sol de tio 2 contenant un intercepteur d'electron adsorbe (pt ou mv 2+ ). etude par spectres d'absorption des especes piegees. a \u03bb max =475 nm observation de \u00abtrous\u00bb h + . vitesses de declin de h + en solutions acide et alcaline. trous h + en exces. avec un sol de tio 2 contenant un intercepteur de trous (alcool polyvinylique ou thiocyanate), observation d'un spectre a \u03bb max =650 nm attribue aux electrons pieges en exces, proches de la surface des particules colloidales\""
                },
                {
                    "id": "R45100",
                    "label": "Charge carrier trapping and recombination dynamics in small semiconductor particles",
                    "doi": "10.1021/ja00312a043",
                    "research_field": {
                        "id": "R122",
                        "label": "Chemistry"
                    },
                    "abstract": "reference lpi-article-1985-033doi:10.1021/ja00312a043view record in web of science record created on 2006-02-21, modified on 2017-05-12"
                },
                {
                    "id": "R45102",
                    "label": "Picosecond flash spectroscopy of titania colloids with adsorbed dyes",
                    "doi": "10.1021/j100364a056",
                    "research_field": {
                        "id": "R122",
                        "label": "Chemistry"
                    },
                    "abstract": "spectra for electrons trapped in tio{sub 2} have been reported. in this study, kinetic analysis of processes taking place when tio{sub 2} colloids are flashed in the presence of three dyes leads to assignment of the spectrum of a trapped hole. within the duration of a 20-ps pulse at 350 nm, a transient is formed in tio{sub 2} which decays with a second-order rate constant of 2.4 {times} 10{sup {minus}10} n{sub e} s{sup {minus}1}, where n{sub e} is the number of electrons. the absorbance is probably attributable to electrons in the conduction band (a term that must be used cautiously for these very amorphous systems), and the rate constant measures the rate of hole-electron recombination. upon addition of a dye that may scavenge carriers, a new transient grown with a rate constant of 5 {times} 10{sup 8} s{sup {minus}1}. this feature, with an absorption maximum at 630 nm, is attributed to a trapped hole. the mechanism proposed for the results of this intense pulse experiment involves two photons and excitation of both dye and colloid. the evidence includes observation of spectra of reduced dyes and quantitative consistency not achieved with any other model."
                },
                {
                    "id": "R45104",
                    "label": "Charge Carrier Dynamics at TiO2 Particles:\u2009 Reactivity of Free and Trapped Holes",
                    "doi": "10.1021/jp9639915",
                    "research_field": {
                        "id": "R122",
                        "label": "Chemistry"
                    },
                    "abstract": "details of the mechanism of the photocatalytic oxidation of the model compounds dichloroacetate, dca-, and thiocyanate, scn-, have been investigated employing time-resolved laser flash photolysis. nanosized colloidal titanium dioxide (tio2, anatase) particles with a mean diameter of 24 a were used as photocatalysts in optically transparent aqueous suspensions. detailed spectroscopic investigations of the processes occurring upon band gap irradiation in these colloidal aqueous tio2 suspensions in the absence of any hole scavengers showed that while electrons are trapped instantaneously, i.e., within the duration of the laser flash (20 ns), at least two different types of traps have to be considered for the remaining holes. deeply trapped holes, h+tr, are rather long-lived and unreactive, i.e., they are transferred neither to dca- nor to scn- ions. shallowly trapped holes, h+tr*, on the other hand, are in a thermally activated equilibrium with free holes which exhibit a very high oxidation potential. the ov..."
                },
                {
                    "id": "R45106",
                    "label": "How fast is interfacial hole transfer? In situ monitoring of carrier dynamics in anatase TiO 2 nanoparticles by femtosecond laser spectroscopy",
                    "doi": "10.1039/B101721G",
                    "research_field": {
                        "id": "R122",
                        "label": "Chemistry"
                    },
                    "abstract": "by comparing the transient absorption spectra of nanosized anatase tio2 colloidal systems with and without scn\u2212, the broad absorption band around 520 nm observed immediately after band-gap excitation for the system without scn\u2212 has been assigned to shallowly trapped holes. in the presence of scn\u2212, the absorption from the trapped holes at 520 nm cannot be observed because of the ultrafast interfacial hole transfer between tio2 nanoparticles and scn\u2212. the hole and electron trapping times were estimated to be <50 and 260 fs, respectively, by the analysis of rise and decay dynamics of transient absorption spectra. the rate of the hole transfer from nanosized tio2 colloid to scn\u2212 is comparable to that of the hole trapping and the time of formation of a weakly coupled (scn\u00b7\u00b7\u00b7scn)\u2022\u2212 is estimated to be \u223d2.3 ps with 0.3 m kscn. a further \\n structural change to form a stable (scn)2\u2022\u2212 is observed in a timescale of 100\u223d150 ps, which is almost independent of the concentration of scn\u2212."
                },
                {
                    "id": "R45108",
                    "label": "Identification of Reactive Species in Photoexcited Nanocrystalline TiO2 Films by Wide-Wavelength-Range (400\u22122500 nm) Transient Absorption Spectroscopy",
                    "doi": "10.1021/jp031305d",
                    "research_field": {
                        "id": "R122",
                        "label": "Chemistry"
                    },
                    "abstract": "reactive species, holes, and electrons in photoexcited nanocrystalline tio2 films were studied by transient absorption spectroscopy in the wavelength range from 400 to 2500 nm. the electron spectrum was obtained through a hole-scavenging reaction under steady-state light irradiation. the spectrum can be analyzed by a superposition of the free-electron and trapped-electron spectra. by subtracting the electron spectrum from the transient absorption spectrum, the spectrum of trapped holes was obtained. as a result, three reactive speciestrapped holes and free and trapped electronswere identified in the transient absorption spectrum. the reactivity of these species was evaluated through transient absorption spectroscopy in the presence of hole- and electron-scavenger molecules. the spectra indicate that trapped holes and electrons are localized at the surface of the particles and free electrons are distributed in the bulk."
                }
            ]
        }
    ]
}