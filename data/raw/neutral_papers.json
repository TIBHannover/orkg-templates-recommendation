{
    "neutral_papers": [
        {
            "id": "R175117",
            "label": "Toward Altmetric-Driven Research-Paper Recommender System Framework",
            "doi": "10.1109/sitis.2017.21",
            "research_field": {
                "id": "R141823",
                "label": "Semantic Web"
            },
            "abstract": "\"the volume of literature and more particularly research-oriented publications is growing at an exponential rate, and better tools and methodologies are required to efficiently and effectively retrieve desired documents. the development of academic search engines, digital libraries and archives has led to better information filtering mechanisms that has resulted to improved search results. however, the state-of-the art research-paper recommender systems are still retrieving research articles without explicitly defining the domain of interest of the researchers. also, a rich set of research output (research objects) and their associated metrics are also not being utilized in the process of searching, querying, retrieving and recommending articles. consequently, a lot of irrelevant and unrelated information is being presented to the user. then again, the use of citation counts to rank and recommend research-paper to users is still disputed. recommendation metrics like citation counts, ratings in collaborative filtering, and keyword analysis' cannot be fully relied on as the only techniques through which similarity between documents can be computed, and this is because recommendations based on such metrics are not accurate and have lots of biasness. henceforth, altmetric-based techniques and methodologies are expected to give better recommendations of research papers since the circumstances surrounding a research papers are taken into consideration. this paper proposes a research paper recommender system framework that utilizes paper ontology and altmetric from research papers, to enhance the performance of research paper recommender systems.\""
        },
        {
            "id": "R175297",
            "label": "Ontology learning from text: A look back and into the future",
            "doi": "10.1145/2333112.2333115",
            "research_field": {
                "id": "R141823",
                "label": "Semantic Web"
            },
            "abstract": "ontologies are often viewed as the answer to the need for interoperable semantics in modern information systems. the explosion of textual information on the read/write web coupled with the increasing demand for ontologies to power the semantic web have made (semi-)automatic ontology learning from text a very promising research area. this together with the advanced state in related areas, such as natural language processing, have fueled research into ontology learning over the past decade. this survey looks at how far we have come since the turn of the millennium and discusses the remaining challenges that will define the research directions in this area in the near future."
        },
        {
            "id": "R178192",
            "label": "IoT-Lite: A Lightweight Semantic Model for the Internet of Things",
            "doi": "10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0035",
            "research_field": {
                "id": "R141823",
                "label": "Semantic Web"
            },
            "abstract": "over the past few years the semantics community has developed ontologies to describe concepts, relationships between different entities in various application domains, including internet of things (iot) applications. a key problem is that most of the iot related semantic descriptions are not as widely adopted as expected. one of the main concerns of users, developers is that semantic techniques increase the complexity, processing time, therefore they are unsuitable for dynamic, responsive environments such as the iot. to address this concern, we propose iot-lite, an instantiation of the semantic sensor network (ssn) ontology to describe key iot concepts allowing interoperability, discovery of sensory data in heterogeneous iot platforms by a lightweight semantics. we propose 10 rules for good, scalable semantic model design, follow them to create iot-lite. we also demonstrate the scalability of iot-lite by providing some experimental analysis,, assess iot-lite against another solution in terms of round time trip (rtt) performance for query-response times."
        },
        {
            "id": "R178198",
            "label": "A High-Level Approach Towards End User Development in the IoT",
            "doi": "10.1145/3027063.3053157",
            "research_field": {
                "id": "R141823",
                "label": "Semantic Web"
            },
            "abstract": "\"programming environments for end-user personalization in the internet of things (iot) are becoming increasingly common. they allow users to define simple iot applications, i.e., connections between different iot devices and services. unfortunately, the adopted representation models are highly technology-dependent, e.g., they often categorize devices and services by manufacturer or brand. such an approach is not suitable to face the expected growth of the iot, nor it allows to adapt to yet undiscovered iot services. in this paper, we present a generic and technology-independent representation for iot end-user programming environments. the aim of this ``high-level' representation is to allow end-users to create abstract iot applications that adapt to different contextual situations. we preliminary evaluated the representation by comparing it with the one used by existing programming environments in a user study with 10 participants. results show that the representation is understandable, and it allows users to create iot applications more correctly and quickly.\""
        },
        {
            "id": "R182024",
            "label": "DistSim - Scalable Distributed in-Memory Semantic Similarity Estimation for RDF Knowledge Graphs",
            "doi": "10.1109/icsc50631.2021.00062",
            "research_field": {
                "id": "R141823",
                "label": "Semantic Web"
            },
            "abstract": "in this paper, we present distsim, a scalable distributed in-memory semantic similarity estimation framework for knowledge graphs. distsim provides a multitude of state-of-the-art similarity estimators. we have developed the similarity estimation pipeline by combining generic software modules. for large scale rdf data, distsim proposes minhash with locality sensitivity hashing to achieve better scalability over all-pair similarity estimations. the modules of distsim can be set up using a multitude of (hyper)-parameters allowing to adjust the tradeoff between information taken into account, and processing time. furthermore, the output of the similarity estimation pipeline is native rdf. distsim is integrated into the sansa stack, documented in scala-docs, and covered by unit tests. additionally, the variables and provided methods follow the apache spark mllib name-space conventions. the performance of distsim was tested over a distributed cluster, for the dimensions of data set size and processing power versus processing time, which shows the scalability of distsim w.r.t. increasing data set sizes and processing power. distsim is already in use for solving several rdf data analytics related use cases. additionally, distsim is available and integrated into the open-source github project sansa."
        },
        {
            "id": "R185271",
            "label": "Multimedia ontology learning for automatic annotation and video browsing",
            "doi": "10.1145/1460096.1460159",
            "research_field": {
                "id": "R141823",
                "label": "Semantic Web"
            },
            "abstract": "in this work, we offer an approach to combine standard multimedia analysis techniques with knowledge drawn from conceptual metadata provided by domain experts of a specialized scholarly domain, to learn a domain-specific multimedia ontology from a set of annotated examples. a standard bayesian network learning algorithm that learns structure and parameters of a bayesian network is extended to include media observables in the learning. an expert group provides domain knowledge to construct a basic ontology of the domain as well as to annotate a set of training videos. these annotations help derive the associations between high-level semantic concepts of the domain and low-level mpeg-7 based features representing audio-visual content of the videos. we construct a more robust and refined version of this ontology by learning from this set of conceptually annotated videos. to encode this knowledge, we use mowl, a multimedia extension of web ontology language (owl) which is capable of describing domain concepts in terms of their media properties and of capturing the inherent uncertainties involved. we use the ontology specified knowledge for recognizing concepts relevant to a video to annotate fresh addition to the video database with relevant concepts in the ontology. these conceptual annotations are used to create hyperlinks in the video collection, to provide an effective video browsing interface to the user."
        },
        {
            "id": "R185300",
            "label": "Automatic Product Ontology Extraction from Textual Reviews",
            "doi": "",
            "research_field": {
                "id": "R141823",
                "label": "Semantic Web"
            },
            "abstract": "ontologies have proven beneficial in different settings that make use of textual reviews. however, manually constructing ontologies is a laborious and time-consuming process in need of automation. we propose a novel methodology for automatically extracting ontologies, in the form of meronomies, from product reviews, using a very limited amount of hand-annotated training data. we show that the ontologies generated by our method outperform hand-crafted ontologies (wordnet) and ontologies extracted by existing methods (text2onto and comet) in several, diverse settings. specifically, our generated ontologies outperform the others when evaluated by human annotators as well as on an existing q&a dataset from amazon. moreover, our method is better able to generalise, in capturing knowledge about unseen products. finally, we consider a real-world setting, showing that our method is better able to determine recommended products based on their reviews, in alternative to using amazon\u2019s standard score aggregations."
        },
        {
            "id": "R185335",
            "label": "Ontology Learning Process as a Bottom-up Strategy for Building Domain-specific Ontology from Legal Texts",
            "doi": "10.5220/0006188004730480",
            "research_field": {
                "id": "R141823",
                "label": "Semantic Web"
            },
            "abstract": "the objective of this paper is to present the role of ontology learning process in supporting an ontology engineer for creating and maintaining ontologies from textual resources. the knowledge structures that interest us are legal domain-specific ontologies. we will use these ontologies to build legal domain ontology for a lebanese legal knowledge based system. the domain application of this work is the lebanese criminal system. ontologies can be learnt from various sources, such as databases, structured and unstructured documents. here, the focus is on the acquisition of ontologies from unstructured text, provided as input. in this work, the ontology learning process represents a knowledge extraction phase using natural language processing techniques. the resulted ontology is considered as inexpressive ontology. there is a need to reengineer it in order to build a complete, correct and more expressive domain-specific ontology."
        },
        {
            "id": "R185349",
            "label": "The Ontology Extraction & Maintenance Framework Text-To-Onto",
            "doi": "",
            "research_field": {
                "id": "R141823",
                "label": "Semantic Web"
            },
            "abstract": "ontologies play an increasingly important role in knowledge management. one of the main problems associated with ontologies is that they need to be constructed and maintained. manual construction of larger ontologies is usually not feasible within companies because of the effort and costs required. therefore, a semi-automatic approach to ontology construction and maintenance is what everybody is wishing for. the paper presents a framework for semi-automatically learning ontologies from domainspecific texts by applying machine learning techniques. the text-to-onto framework integrates manual engineering facilities to follow a balanced cooperative modelling paradigm."
        },
        {
            "id": "R186134",
            "label": "Exploiting Declarative Mapping Rules for Generating GraphQL Servers with Morph-GraphQL",
            "doi": "10.1142/s0218194020400070",
            "research_field": {
                "id": "R141823",
                "label": "Semantic Web"
            },
            "abstract": "in the last decade, rest has become the most common approach to provide web services, yet it was not originally designed to handle typical modern applications (e.g. mobile apps). graphql was proposed to reduce the number of queries and data exchanged in comparison with rest. since its release in 2015, it has gained momentum as an alternative approach to rest. however, generating and maintaining graphql resolvers is not a simple task. first, a domain expert has to analyze a dataset, design the corresponding graphql schema and map the dataset to the schema. then, a software engineer (e.g. graphql developer) implements the corresponding graphql resolvers in a specific programming language. in this paper, we present an approach to exploit the information from mappings rules (relation between target and source schema) and generate a graphql server. these mapping rules construct a virtual knowledge graph which is accessed by the generated graphql resolvers. these resolvers translate the input graphql queries into the queries supported by the underlying dataset. domain experts or software developers may benefit from our approach: a domain expert does not need to involve software developers to implement the resolvers, and software developers can generate the initial version of the resolvers to be implemented. we implemented our approach in the morph-graphql framework and evaluated it using the lingbm benchmark."
        },
        {
            "id": "R186148",
            "label": "Learning Sequence Encoders for Temporal Knowledge Graph Completion",
            "doi": "10.18653/v1/d18-1516",
            "research_field": {
                "id": "R141823",
                "label": "Semantic Web"
            },
            "abstract": "research on link prediction in knowledge graphs has mainly focused on static multi-relational data. in this work we consider temporal knowledge graphs where relations between entities may only hold for a time interval or a specific point in time. in line with previous work on static knowledge graphs, we propose to address this problem by learning latent entity and relation type representations. to incorporate temporal information, we utilize recurrent neural networks to learn time-aware representations of relation types which can be used in conjunction with existing latent factorization methods. the proposed approach is shown to be robust to common challenges in real-world kgs: the sparsity and heterogeneity of temporal expressions. experiments show the benefits of our approach on four temporal kgs. the data sets are available under a permissive bsd-3 license."
        },
        {
            "id": "R187232",
            "label": "Explainable cyber-physical energy systems based on knowledge graph",
            "doi": "10.1145/3470481.3472704",
            "research_field": {
                "id": "R141823",
                "label": "Semantic Web"
            },
            "abstract": "explainability can help cyber-physical systems alleviating risk in automating decisions that are affecting our life. building an explainable cyber-physical system requires deriving explanations from system events and causality between the system elements. cyber-physical energy systems such as smart grids involve cyber and physical aspects of energy systems and other elements, namely social and economic. moreover, a smart-grid scale can range from a small village to a large region across countries. therefore, integrating these varieties of data and knowledge is a fundamental challenge to build an explainable cyber-physical energy system. this paper aims to use knowledge graph based framework to solve this challenge. the framework consists of an ontology to model and link data from various sources and graph-based algorithm to derive explanations from the events. a simulated demand response scenario covering the above aspects further demonstrates the applicability of this framework."
        },
        {
            "id": "R188804",
            "label": "Traveling Light \u2014 A Low-Overhead Approach for SPARQL Query Optimization",
            "doi": "10.1109/icsc50631.2021.00014",
            "research_field": {
                "id": "R141823",
                "label": "Semantic Web"
            },
            "abstract": "\"sparql query processing in triplestores has to deal with many of the same problems as query processing in relational databases, and additional problems due to the schema relaxed nature of rdf. the flexible pattern matching capabilities of sparql queries entail performance challenges for complex queries. most modern query optimizers produce a significant overhead as they use an exhaustive statistics generation and storage approach. currently, there is no pure online cost-based optimizer for sparql queries. in this paper, we explore the hypothesis that just storing selectivity statistics for predicates enables effective optimization of typical queries. based on this, we introduce a pure online optimizer for triplestores, the online join order optimizer (ojoo), which learns from query executions. ojoo's overhead in creating and persisting statistics is very low, and it provides an easily extendable storage architecture for statistics. we implemented the ojoo in a main-memory triplestore, pdstore (parsimonious data store), and evaluated its performance experimentally using the lehigh university benchmark (lubm). our experimental results revealed that the ojoo is competitive, efficient, scalable, and has a negligible runtime overhead.\""
        },
        {
            "id": "R142305",
            "label": "Mapping ER Schemas to OWL Ontologies",
            "doi": "10.1109/icsc.2009.61",
            "research_field": {
                "id": "R141823",
                "label": "Semantic Web"
            },
            "abstract": "as the semantic web initiative gains momentum, a fundamental problem of integrating existing data-intensive www applications into the semantic web emerges. in order for today\u2019s relational database supported web applications to transparently participate in the semantic web, their associated database schemas need to be converted into semantically equivalent ontologies. in this paper we present a solution to an important special case of the automatic mapping problem with wide applicability: mapping well-formed entity-relationship (er) schemas to semantically equivalent owl lite ontologies. we present a set of mapping rules that fully capture the er schema semantics, along with an overview of an implementation of the complete mapping algorithm integrated into the current sfsu er design tools software."
        },
        {
            "id": "R142311",
            "label": "ERONTO: a tool for extracting ontologies from extended E/R diagrams",
            "doi": "10.1145/1066677.1066828",
            "research_field": {
                "id": "R141823",
                "label": "Semantic Web"
            },
            "abstract": "realization of semantic web requires structuring of web data using domain ontologies. most data intensive websites are powered by relational databases whose design process involves developing conceptual model using e/r or extended e/r diagrams. this paper discusses the implementation details of a tool that builds domain ontologies in owl (ontology web language) from extended e/r diagrams. ontology development being a knowledge intensive task, our tool would be helpful in reducing the developmental efforts by automating the process. we bring out the differences and the similarities between the expressive capabilities of the two conceptual modeling methods, namely owl and extended e/r diagrams."
        },
        {
            "id": "R176039",
            "label": "Attention is All you Need",
            "doi": "",
            "research_field": {
                "id": "R112125",
                "label": "Machine Learning"
            },
            "abstract": "the dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. the best performing models also connect the encoder and decoder through an attention mechanism. we propose a new simple network architecture, the transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. our model achieves 28.4 bleu on the wmt 2014 english-to-german translation task, improving over the existing best results, including ensembles by over 2 bleu. on the wmt 2014 english-to-french translation task, our model establishes a new single-model state-of-the-art bleu score of 41.8 after training for 3.5 days on eight gpus, a small fraction of the training costs of the best models from the literature. we show that the transformer generalizes well to other tasks by applying it successfully to english constituency parsing both with large and limited training data."
        },
        {
            "id": "R41138",
            "label": "Electrodeposition of crystalline and photoactive silicon directly from silicon dioxide nanoparticles in molten CaCl 2",
            "doi": "10.1002/anie.201206789",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            },
            "abstract": "silicon is a widely used semiconductor for electronic and photovoltaic devices because of its earth-abundance, chemical stability, and the tunable electrical properties by doping. therefore, the production of pure silicon films by simple and inexpensive methods has been the subject of many investigations. the desire for lower-cost silicon-based solar photovoltaic devices has encouraged the quest for solar-grade silicon production through processes alternative to the currently used czochralski process or other processes. electrodeposition is one of the least expensive methods for fabricating films of metals and semiconductors. electrodeposition of silicon has been studied for over 30 years, in various solution media such as molten salts (lif-kf-k2sif6 at 745 8c and bao-sio2-baf2 at 1465 8c ), organic solvents (acetonitrile, tetrahydrofuran), and room-temperature ionic liquids. recently, the direct electrochemical reduction of bulk solid silicon dioxide in a cacl2 melt was reported. [7] a key factor for silicon electrodeposition is the purity of silicon deposit because si for the use in photovoltaic devices is solargrade silicon (> 99.9999% or 6n) and its grade is even higher in electronic devices (electronic-grade silicon or 11n). in most cases, the electrodeposited silicon does not meet these requirements without further purification and, to our knowledge, none have been shown to exhibit a photoresponse. in fact, silicon electrodeposition is not as straightforward as metal deposition, since the deposited semiconductor layer is resistive at room temperature, which complicates electron transfer through the deposit. in many cases, for example in room-temperature aprotic solvents, the deposited silicon acts as an insulating layer and prevents a continuous deposition reaction. in some cases, the silicon deposit contains a high level of impurities (> 2%). moreover, the nucleation and growth of silicon requires a large amount of energy. the deposition is made even more challenging if the si precursor is sio2, which is a very resistive material. we reported previously the electrochemical formation of silicon on molybdenum from a cacl2 molten salt (850 8c) containing a sio2 nanoparticle (np with a diameter of 5\u2013 15 nm) suspension by applying a constant reduction current. however this si film did not show photoactivity. here we show the electrodeposition of photoactive crystalline silicon directly from sio2 nps from cacl2 molten salt on a silver electrode that shows a clear photoresponse. to the best of our knowledge, this is a first report of the direct electrodeposition of photoactive silicon. the electrochemical reduction and the cyclic voltammetry (cv) of sio2 were investigated as described previously. [8] in this study, we found that the replacement of the mo substrate by silver leads to a dramatic change in the properties of the silicon deposit. the silver substrate exhibited essentially the same electrochemical and cv behavior as other metal substrates, that is, a high reduction current for sio2 at negative potentials of 1.0 v with the development of a new redox couple near 0.65 v vs. a graphite quasireference electrode (qre) (figure 1a). figure 1b shows a change in the reduction current as a function of the reduction potential, and the optical images of silver electrodes before and after the electrolysis, which displays a dark gray-colored deposit after the reduction. figure 2 shows sem images of silicon deposits grown potentiostatically ( 1.25 v vs. graphite qre) on silver. the amount of silicon deposit increased with the deposition time, and the deposit finally covered the whole silver surface (figure 2). high-magnification images show that the silicon deposit is not a film but rather platelets or clusters of silicon crystals of domain sizes in the range of tens of micrometers. the average height of the platelets was around 25 mm after a 10000 s deposition (figure 2b), and 45 mm after a 20000s deposition (figure 2c), respectively. the edges of the silicon crystals were clearly observed. contrary to other substrates, silver enhanced the crystallization of silicon produced from silicon dioxide reduction and it is known that silver induces the crystallization of amorphous silicon. energy-dispersive spectrometry (eds) elemental mapping (images shown in the bottom row of figure 2) revealed that small silver islands exist on the top of the silicon deposits, which we think is closely related to the growth mechanism of silicon on silver. the eds spectrum of the silicon deposit (figure 3a) suggested that the deposited silicon was quite pure and the amounts of other elements such as c, ca, and cl were below the detection limit (about 0.1 atom%). since the oxygen signal was probably from the native oxide formed on exposure of the deposit to air and silicon does not form an alloy with silver, the purity of silicon was estimated to be at least 99.9 atom%. the successful reduction of si(4+) in silicon dioxide to elemental silicon (si) was confirmed by xray photoelectron spectroscopy (xps) of the silicon deposit [*] dr. s. k. cho, dr. f.-r. f. fan, prof. a. j. bard center for electrochemistry, department of chemistry and biochemistry, the university of texas at austin austin, tx 78712 (usa) e-mail: ajbard@mail.utexas.edu"
        },
        {
            "id": "R41144",
            "label": "Up-scalable and controllable electrolytic production of photo-responsive nanostructured silicon",
            "doi": "10.1039/C3TA11823A",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            },
            "abstract": "the electrochemical reduction of solid silica has been investigated in molten cacl2 at 900 \u00b0c for the one-step, up-scalable, controllable and affordable production of nanostructured silicon with promising photo-responsive properties. cyclic voltammetry of the metallic cavity electrode loaded with fine silica powder was performed to elaborate the electrochemical reduction mechanism. potentiostatic electrolysis of porous and dense silica pellets was carried out at different potentials, focusing on the influences of the electrolysis potential and the microstructure of the precursory silica on the product purity and microstructure. the findings suggest a potential range between \u22120.60 and \u22120.95 v (vs. ag/agcl) for the production of nanostructured silicon with high purity (>99 wt%). according to the elucidated mechanism on the electro-growth of the silicon nanostructures, optimal process parameters for the controllable preparation of high-purity silicon nanoparticles and nanowires were identified. scaling-up the optimal electrolysis was successful at the gram-scale for the preparation of high-purity silicon nanowires which exhibited promising photo-responsive properties."
        },
        {
            "id": "R141661",
            "label": "Fluorescent N-Doped Carbon Dots as in Vitro and in Vivo Nanothermometer",
            "doi": "10.1021/acsami.5b08782",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            },
            "abstract": "the fluorescent n-doped carbon dots (n-cds) obtained from c3n4 emit strong blue fluorescence, which is stable with different ionic strengths and time. the fluorescence intensity of n-cds decreases with the temperature increasing, while it can recover to the initial one with the temperature decreasing. it is an accurate linear response of fluorescence intensity to temperature, which may be attributed to the synergistic effect of abundant oxygen-containing functional groups and hydrogen bonds. further experiments also demonstrate that n-cds can serve as effective in vitro and in vivo fluorescence-based nanothermometer."
        },
        {
            "id": "R141701",
            "label": "Carbon Dot Nanothermometry: Intracellular Photoluminescence Lifetime Thermal Sensing",
            "doi": "10.1021/acsnano.6b06670",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            },
            "abstract": "nanoscale biocompatible photoluminescence (pl) thermometers that can be used to accurately and reliably monitor intracellular temperatures have many potential applications in biology and medicine. ideally, such nanothermometers should be functional at physiological ph across a wide range of ionic strengths, probe concentrations, and local environments. here, we show that water-soluble n,s-co-doped carbon dots (cds) exhibit temperature-dependent photoluminescence lifetimes and can serve as highly sensitive and reliable intracellular nanothermometers. pl intensity measurements indicate that these cds have many advantages over alternative semiconductor- and cd-based nanoscale temperature sensors. importantly, their pl lifetimes remain constant over wide ranges of ph values (5-12), cd concentrations (1.5 \u00d7 10-5 to 0.5 mg/ml), and environmental ionic strengths (up to 0.7 mol\u00b7l-1 nacl). moreover, they are biocompatible and nontoxic, as demonstrated by cell viability and flow cytometry analyses using nih/3t3 and hela cell lines. n,s-cd thermal sensors also exhibit good water dispersibility, superior photo- and thermostability, extraordinary environment and concentration independence, high storage stability, and reusability-their pl decay curves at temperatures between 15 and 45 \u00b0c remained unchanged over seven sequential experiments. in vitro pl lifetime-based temperature sensing performed with human cervical cancer hela cells demonstrated the great potential of these nanosensors in biomedicine. overall, n,s-doped cds exhibit excitation-independent emission with strongly temperature-dependent monoexponential decay, making them suitable for both in vitro and in vivo luminescence lifetime thermometry."
        },
        {
            "id": "R141708",
            "label": "N,S co-doped carbon dots as a stable bio-imaging probe for detection of intracellular temperature and tetracycline",
            "doi": "10.1039/c7tb00810d",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            },
            "abstract": "n,s-cds display an unambiguous bioimaging ability in the detection of intracellular temperature and tetracycline with satisfactory results."
        },
        {
            "id": "R141724",
            "label": "Intracellular ratiometric temperature sensing using fluorescent carbon dots",
            "doi": "10.1039/c8na00255j",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            },
            "abstract": "a self-referencing dual fluorescing carbon dot-based nanothermometer can ratiometrically sense thermal events in hela cells with very high sensitivity."
        },
        {
            "id": "R141748",
            "label": "Dual functional highly luminescence B, N Co-doped carbon nanodots as nanothermometer and Fe3+/Fe2+ sensor",
            "doi": "10.1038/s41598-020-59958-5",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            },
            "abstract": "abstract dual functional fluorescence nanosensors have many potential applications in biology and medicine. monitoring temperature with higher precision at localized small length scales or in a nanocavity is a necessity in various applications. as well as the detection of biologically interesting metal ions using low-cost and sensitive approach is of great importance in bioanalysis. in this paper, we describe the preparation of dual-function highly fluorescent b, n-co-doped carbon nanodots (cds) that work as chemical and thermal sensors. the cds emit blue fluorescence peaked at 450\\u2009nm and exhibit up to 70% photoluminescence quantum yield with showing excitation-independent fluorescence. we also show that water-soluble cds display temperature-dependent fluorescence and can serve as highly sensitive and reliable nanothermometers with a thermo-sensitivity 1.8% \u00b0c \u22121 , and wide range thermo-sensing between 0\u201390\\u2009\u00b0c with excellent recovery. moreover, the fluorescence emission of cds are selectively quenched after the addition of fe 2+ and fe 3+ ions while show no quenching with adding other common metal cations and anions. the fluorescence emission shows a good linear correlation with concentration of fe 2+ and fe 3+ (r 2 \\u2009=\\u20090.9908 for fe 2+ and r 2 \\u2009=\\u20090.9892 for fe 3+ ) with a detection limit of of 80.0\\u2009\u00b1\\u20090.5\\u2009nm for fe 2+ and 110.0\\u2009\u00b1\\u20090.5\\u2009nm for fe 3+ . considering the high quantum yield and selectivity, cds are exploited to design a nanoprobe towards iron detection in a biological sample. the fluorimetric assay is used to detect fe 2+ in iron capsules and total iron in serum samples successfully."
        },
        {
            "id": "R142138",
            "label": "Mapping Intracellular Temperature Using Green Fluorescent Protein",
            "doi": "10.1021/nl300389y",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            },
            "abstract": "heat is of fundamental importance in many cellular processes such as cell metabolism, cell division and gene expression. (1-3) accurate and noninvasive monitoring of temperature changes in individual cells could thus help clarify intricate cellular processes and develop new applications in biology and medicine. here we report the use of green fluorescent proteins (gfp) as thermal nanoprobes suited for intracellular temperature mapping. temperature probing is achieved by monitoring the fluorescence polarization anisotropy of gfp. the method is tested on gfp-transfected hela and u-87 mg cancer cell lines where we monitored the heat delivery by photothermal heating of gold nanorods surrounding the cells. a spatial resolution of 300 nm and a temperature accuracy of about 0.4 \u00b0c are achieved. benefiting from its full compatibility with widely used gfp-transfected cells, this approach provides a noninvasive tool for fundamental and applied research in areas ranging from molecular biology to therapeutic and diagnostic studies."
        },
        {
            "id": "R142147",
            "label": "2D surface thermal imaging using rise-time analysis from laser-induced luminescence phosphor thermometry",
            "doi": "10.1088/0957-0233/20/2/025305",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            },
            "abstract": "\"the purpose of this paper is to demonstrate a novel technique for imaging 2d temperature distributions using rise-time analysis from luminescence exhibited from a y2o3:eu thermographic phosphor. in phosphor thermometry, it is usually the lifetime-decay temporal response that is used to determine temperature; the rise component is usually ignored. we claim to be the first to obtain 2d thermal imaging using the rise-time response. this was demonstrated using flame impingement experiments. a 1 mfps state-of-the-art high-speed shiamadzu hypervision camera was used to capture the phosphors' temporal response, and was later processed in matlab. the resulting thermal map clearly indicated a variation in temperature and showed an uncertainty of 20% at 400 \u00b0c. this is relatively high, and suggestions to improve this are proposed. a calibration of rise time versus temperature is taken between 200 and 700 \u00b0c. this paper builds on previous work in the field, and the results presented in this paper confirm the extended temperature sensing capability of y2o3:eu using rise-time characteristics.\""
        },
        {
            "id": "R142153",
            "label": "CdSe Quantum Dots for Two-Photon Fluorescence Thermal Imaging",
            "doi": "10.1021/nl1036098",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            },
            "abstract": "the technological development of quantum dots has ushered in a new era in fluorescence bioimaging, which was propelled with the advent of novel multiphoton fluorescence microscopes. here, the potential use of cdse quantum dots has been evaluated as fluorescent nanothermometers for two-photon fluorescence microscopy. in addition to the enhancement in spatial resolution inherent to any multiphoton excitation processes, two-photon (near-infrared) excitation leads to a temperature sensitivity of the emission intensity much higher than that achieved under one-photon (visible) excitation. the peak emission wavelength is also temperature sensitive, providing an additional approach for thermal imaging, which is particularly interesting for systems where nanoparticles are not homogeneously dispersed. on the basis of these superior thermal sensitivity properties of the two-photon excited fluorescence, we have demonstrated the ability of cdse quantum dots to image a temperature gradient artificially created in a biocompatible fluid (phosphate-buffered saline) and also their ability to measure an intracellular temperature increase externally induced in a single living cell."
        },
        {
            "id": "R142171",
            "label": "Temperature dependence of luminescent spectra and dynamics in nanocrystalline Y2O3:Eu3+",
            "doi": "10.1063/1.1538181",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            },
            "abstract": "a temperature dependence for emission of eu3+ in cubic nanocrystalline y2o3:eu3+ was studied in contrast with the polycrystalline powders. the emission intensity of eu3+ decreased solely with elevated temperature under the excitation of a 580 nm light, while it had a maximum at a certain temperature under a 488 nm light. the experimental data were well fitted based on a theory considering both the thermal activated distribution of electrons among 7fj and the thermal quenching effect. the results indicated that the thermal quenching rate in nanocrystals (ncs) was faster than that in the polycrystals. the nonradiative decay rate, wnr, the radiative transition rate, w0r, and the luminescent quantum efficiency (qe) were obtained according to the temperature dependence of fluorescence lifetime. it can be concluded that wnr and w0r both increase in ncs, and that qe decreases."
        },
        {
            "id": "R142209",
            "label": "Effect of Annealing on Upconversion Luminescence of ZnO:Er3+ Nanocrystals and High Thermal Sensitivity",
            "doi": "10.1021/jp0686689",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            },
            "abstract": "the effect of annealing on the upconversion luminescence of zno:er3+ nanocrystals was investigated in detail. the green and the red upconverted emissions under infrared 978-nm light excitation were remarkably enhanced with an increase of annealing temperature. moreover, for the sample annealed at 500 \u00b0c, the ratio of the intensity of 2h11/2 \u2192 4i15/2 emission to that of 4s3/2 \u2192 4i15/2 emission increased from less than to more than unity with an increase of the excitation density. however, the same case did not occur to the sample annealed at 700 \u00b0c, where the ratio was independent of excitation density except when the excitation density was higher than 42\\u2009700 w/cm2. this distinction was attributed mainly to the difference in energy gap between the 2h11/2 and 4s3/2 states in the two samples, originating from the local microstructure variation around er3+ ions. in addition, a high thermal sensitivity of 0.0062/\u00b0c was obtained in the zno:er3+ nanocrystals based on the temperature-dependent fluorescence intens..."
        },
        {
            "id": "R142214",
            "label": "Nanoscale thermometry via the fluorescence of YAG:Ce phosphor particles: measurements from 7 to 77\u00a0C",
            "doi": "10.1088/0957-4484/14/8/304",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            },
            "abstract": "the laser-induced fluorescence lifetime of 30 nm particles of yag:ce was measured as a function of temperature from 7 to 77\u00b0c. the fluorescence decay lifetimes for the nanoparticles of this phosphor varied from \u224818 to 27 ns, i.e. \u224833% relative to the longest lifetime measured. this large variation in lifetime, coupled with the high signal strength that was observed, suggest that yag:ce nanoparticles will be useful thermographic phosphors. we describe the material and the apparatus used to characterize its fluorescence, present the results of measurements made over the range of temperatures tested and comment on some possible applications for this novel material."
        },
        {
            "id": "R142218",
            "label": "Scanning thermal imaging of microelectronic circuits with a fluorescent nanoprobe",
            "doi": "10.1063/1.2123384",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            },
            "abstract": "we have developed a scanning thermal imaging method that uses a fluorescent particle as a temperature sensor. the particle, which contains rare-earth ions, is glued at the end of an atomic force microscope tip and allows the determination of the temperature of its surrounding medium. the measurement is performed by comparing the relative integrated intensity of two fluorescence lines that have a well-defined temperature dependence. as an example of application, we show the temperature map on an operating complementary metal-oxide-semiconductor integrated circuit."
        },
        {
            "id": "R142221",
            "label": "AC thermal imaging of a microwire with a fluorescent nanocrystal: Influence of the near field on the thermal contrast",
            "doi": "10.1063/1.3233940",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            },
            "abstract": "we have studied the temperature dependence of the visible fluorescence lines of 250 nm large pbf2 nanocrystals codoped with er3+ and yb3+ ions. by gluing such a particle at the end of a sharp atomic force microscope tip, we have developed a scanning thermal microscope able to observe the heating of electrically excited micro- and nanowires. by modulating the electrical current that flows in the structure, the resulting temperature variations modulate the particle fluorescence giving rise to the thermal contrast. we will show that the fluorescence is affected both by the near-field optical distribution and by temperature variations. we will show that it is possible to get rid of these optical effects and to keep the thermal contribution by comparing the images to reference images obtained when the device is not driven by a current. the determination of the temperature of the devices is performed by analyzing the thermal quenching of the fluorescent particle and is in good agreement with numerical simulatio..."
        },
        {
            "id": "R146779",
            "label": "A Solution-Processable Electron Acceptor Based on Dibenzosilole and Diketopyrrolopyrrole for Organic Solar Cells",
            "doi": "10.1002/aenm.201200911",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            },
            "abstract": "organic solar cells (oscs) are a promising cost-effective alternative for utility of solar energy, and possess low-cost, light-weight, and fl exibility advantages. [ 1\u20137 ] much attention has been focused on the development of oscs which have seen a dramatic rise in effi ciency over the last decade, and the encouraging power conversion effi ciency (pce) over 9% has been achieved from bulk heterojunction (bhj) oscs. [ 8 ] with regard to photoactive materials, fullerenes and their derivatives, such as [6,6]-phenyl c 61 butyric acid methyl ester (pc 61 bm), have been the dominant electron-acceptor materials in bhj oscs, owing to their high electron mobility, large electron affi nity and isotropy of charge transport. [ 9 ] however, fullerenes have a few disadvantages, such as restricted electronic tuning and weak absorption in the visible region. furthermore, in typical bhj system of poly(3-hexylthiophene) (p3ht):pc 61 bm, mismatching energy levels between donor and acceptor leads to energy loss and low open-circuit voltages ( v oc ). to solve these problems, novel electron acceptor materials with strong and broad absorption spectra and appropriate energy levels are necessary for oscs. recently, non-fullerene small molecule acceptors have been developed. [ 10 , 11 ] however, rare reports on the devices based on solution-processed non-fullerene small molecule acceptors have shown pces approaching or exceeding 1.5%, [ 12\u201319 ] and only one paper reported pces over 2%. [ 16 ]"
        },
        {
            "id": "R146794",
            "label": "A Rhodanine Flanked Nonfullerene Acceptor for Solution-Processed Organic Photovoltaics",
            "doi": "10.1021/ja5110602",
            "research_field": {
                "id": "R126",
                "label": "Materials Chemistry"
            },
            "abstract": "a novel small molecule, fbr, bearing 3-ethylrhodanine flanking groups was synthesized as a nonfullerene electron acceptor for solution-processed bulk heterojunction organic photovoltaics (opv). a straightforward synthesis route was employed, offering the potential for large scale preparation of this material. inverted opv devices employing poly(3-hexylthiophene) (p3ht) as the donor polymer and fbr as the acceptor gave power conversion efficiencies (pce) up to 4.1%. transient and steady state optical spectroscopies indicated efficient, ultrafast charge generation and efficient photocurrent generation from both donor and acceptor. ultrafast transient absorption spectroscopy was used to investigate polaron generation efficiency as well as recombination dynamics. it was determined that the p3ht:fbr blend is highly intermixed, leading to increased charge generation relative to comparative devices with p3ht:pc60bm, but also faster recombination due to a nonideal morphology in which, in contrast to p3ht:pc60bm devices, the acceptor does not aggregate enough to create appropriate percolation pathways that prevent fast nongeminate recombination. despite this nonoptimal morphology the p3ht:fbr devices exhibit better performance than p3ht:pc60bm devices, used as control, demonstrating that this acceptor shows great promise for further optimization."
        },
        {
            "id": "R108960",
            "label": "Use of species delimitation approaches to tackle the cryptic diversity of an assemblage of high Andean butterflies (Lepidoptera: Papilionoidea)",
            "doi": "10.1139/gen-2020-0100",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "cryptic biological diversity has generated ambiguity in taxonomic and evolutionary studies. single-locus methods and other approaches for species delimitation are useful for addressing this challenge, enabling the practical processing of large numbers of samples for identification and inventory purposes. this study analyzed an assemblage of high andean butterflies using dna barcoding and compared the identifications based on the current morphological taxonomy with three methods of species delimitation (automatic barcode gap discovery, generalized mixed yule coalescent model, and poisson tree processes). sixteen potential cryptic species were recognized using these three methods, representing a net richness increase of 11.3% in the assemblage. a well-studied taxon of the genus vanessa, which has a wide geographical distribution, appeared with the potential cryptic species that had a higher genetic differentiation at the local level than at the continental level. the analyses were useful for identifying the potential cryptic species in pedaliodes and forsterinaria complexes, which also show differentiation along altitudinal and latitudinal gradients. this genetic assessment of an entire assemblage of high andean butterflies (papilionoidea) provides baseline information for future research in a region characterized by high rates of endemism and population isolation."
        },
        {
            "id": "R108983",
            "label": "Barcoding the butterflies of southern South America: Species delimitation efficacy, cryptic diversity and geographic patterns of divergence",
            "doi": "10.1371/journal.pone.0186845",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "because the tropical regions of america harbor the highest concentration of butterfly species, its fauna has attracted considerable attention. much less is known about the butterflies of southern south america, particularly argentina, where over 1,200 species occur. to advance understanding of this fauna, we assembled a dna barcode reference library for 417 butterfly species of argentina, focusing on the atlantic forest, a biodiversity hotspot. we tested the efficacy of this library for specimen identification, used it to assess the frequency of cryptic species, and examined geographic patterns of genetic variation, making this study the first large-scale genetic assessment of the butterflies of southern south america. the average sequence divergence to the nearest neighbor (i.e. minimum interspecific distance) was 6.91%, ten times larger than the mean distance to the furthest conspecific (0.69%), with a clear barcode gap present in all but four of the species represented by two or more specimens. as a consequence, the dna barcode library was extremely effective in the discrimination of these species, allowing a correct identification in more than 95% of the cases. singletons (i.e. species represented by a single sequence) were also distinguishable in the gene trees since they all had unique dna barcodes, divergent from those of the closest non-conspecific. the clustering algorithms implemented recognized from 416 to 444 barcode clusters, suggesting that the actual diversity of butterflies in argentina is 3%\u20139% higher than currently recognized. furthermore, our survey added three new records of butterflies for the country (eurema agave, mithras hannelore, melanis hillapana). in summary, this study not only supported the utility of dna barcoding for the identification of the butterfly species of argentina, but also highlighted several cases of both deep intraspecific and shallow interspecific divergence that should be studied in more detail."
        },
        {
            "id": "R109043",
            "label": "A DNA barcode library for the butterflies of North America",
            "doi": "10.7717/peerj.11157",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "although the butterflies of north america have received considerable taxonomic attention, overlooked species and instances of hybridization continue to be revealed. the present study assembles a dna barcode reference library for this fauna to identify groups whose patterns of sequence variation suggest the need for further taxonomic study. based on 14,626 records from 814 species, dna barcodes were obtained for 96% of the fauna. the maximum intraspecific distance averaged 1/4 the minimum distance to the nearest neighbor, producing a barcode gap in 76% of the species. most species (80%) were monophyletic, the others were para- or polyphyletic. although 15% of currently recognized species shared barcodes, the incidence of such taxa was far higher in regions exposed to pleistocene glaciations than in those that were ice-free. nearly 10% of species displayed high intraspecific variation (&gt;2.5%), suggesting the need for further investigation to assess potential cryptic diversity. aside from aiding the identification of all life stages of north american butterflies, the reference library has provided new perspectives on the incidence of both cryptic and potentially over-split species, setting the stage for future studies that can further explore the evolutionary dynamics of this group."
        },
        {
            "id": "R135750",
            "label": "Characterization and comparison of poorly known moth communities through DNA barcoding in two Afrotropical environments in Gabon",
            "doi": "10.1139/gen-2018-0063",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "biodiversity research in tropical ecosystems\u2014popularized as the most biodiverse habitats on earth\u2014often neglects invertebrates, yet invertebrates represent the bulk of local species richness. insect communities in particular remain strongly impeded by both linnaean and wallacean shortfalls, and identifying species often remains a formidable challenge inhibiting the use of these organisms as indicators for ecological and conservation studies. here we use dna barcoding as an alternative to the traditional taxonomic approach for characterizing and comparing the diversity of moth communities in two different ecosystems in gabon. though sampling remains very incomplete, as evidenced by the high proportion (59%) of species represented by singletons, our results reveal an outstanding diversity. with about 3500 specimens sequenced and representing 1385 bins (barcode index numbers, used as a proxy to species) in 23 families, the diversity of moths in the two sites sampled is higher than the current number of species listed for the entire country, highlighting the huge gap in biodiversity knowledge for this country. both seasonal and spatial turnovers are strikingly high (18.3% of bins shared between seasons, and 13.3% between sites) and draw attention to the need to account for these when running regional surveys. our results also highlight the richness and singularity of savannah environments and emphasize the status of central african ecosystems as hotspots of biodiversity."
        },
        {
            "id": "R136193",
            "label": "Complete DNA barcode reference library for a country's butterfly fauna reveals high performance for temperate Europe",
            "doi": "10.1098/rspb.2010.1089",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "dna barcoding aims to accelerate species identification and discovery, but performance tests have shown marked differences in identification success. as a consequence, there remains a great need for comprehensive studies which objectively test the method in groups with a solid taxonomic framework. this study focuses on the 180 species of butterflies in romania, accounting for about one third of the european butterfly fauna. this country includes five eco-regions, the highest of any in the european union, and is a good representative for temperate areas. morphology and dna barcodes of more than 1300 specimens were carefully studied and compared. our results indicate that 90 per cent of the species form barcode clusters allowing their reliable identification. the remaining cases involve nine closely related species pairs, some whose taxonomic status is controversial or that hybridize regularly. interestingly, dna barcoding was found to be the most effective identification tool, outperforming external morphology, and being slightly better than male genitalia. romania is now the first country to have a comprehensive dna barcode reference database for butterflies. similar barcoding efforts based on comprehensive sampling of specific geographical regions can act as functional modules that will foster the early application of dna barcoding while a global system is under development."
        },
        {
            "id": "R138551",
            "label": "Probing planetary biodiversity with DNA barcodes: The Noctuoidea of North America",
            "doi": "10.1371/journal.pone.0178548",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "this study reports the assembly of a dna barcode reference library for species in the lepidopteran superfamily noctuoidea from canada and the usa. based on the analysis of 69,378 specimens, the library provides coverage for 97.3% of the noctuoid fauna (3565 of 3664 species). in addition to verifying the strong performance of dna barcodes in the discrimination of these species, the results indicate close congruence between the number of species analyzed (3565) and the number of sequence clusters (3816) recognized by the barcode index number (bin) system. distributional patterns across 12 north american ecoregions are examined for the 3251 species that have gps data while bin analysis is used to quantify overlap between the noctuoid faunas of north america and other zoogeographic regions. this analysis reveals that 90% of north american noctuoids are endemic and that just 7.5% and 1.8% of bins are shared with the neotropics and with the palearctic, respectively. one third (29) of the latter species are recent introductions and, as expected, they possess low intraspecific divergences."
        },
        {
            "id": "R138562",
            "label": "Fast Census of Moth Diversity in the Neotropics: A Comparison of Field-Assigned Morphospecies and DNA Barcoding in Tiger Moths",
            "doi": "10.1371/journal.pone.0148423",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "the morphological species delimitations (i.e. morphospecies) have long been the best way to avoid the taxonomic impediment and compare insect taxa biodiversity in highly diverse tropical and subtropical regions. the development of dna barcoding, however, has shown great potential to replace (or at least complement) the morphospecies approach, with the advantage of relying on automated methods implemented in computer programs or even online rather than in often subjective morphological features. we sampled moths extensively for two years using light traps in a patch of the highly endangered atlantic forest of brazil to produce a nearly complete census of arctiines (noctuoidea: erebidae), whose species richness was compared using different morphological and molecular approaches (dna barcoding). a total of 1,075 barcode sequences of 286 morphospecies were analyzed. based on the clustering method barcode index number (bin) we found a taxonomic bias of approximately 30% in our initial morphological assessment. however, a morphological reassessment revealed that the correspondence between morphospecies and molecular operational taxonomic units (motus) can be up to 94% if differences in genitalia morphology are evaluated in individuals of different motus originated from the same morphospecies (putative cases of cryptic species), and by recording if individuals of different genders in different morphospecies merge together in the same motu (putative cases of sexual dimorphism). the results of two other clustering methods (i.e. automatic barcode gap discovery and 2% threshold) were very similar to those of the bin approach. using empirical data we have shown that dna barcoding performed substantially better than the morphospecies approach, based on superficial morphology, to delimit species of a highly diverse moth taxon, and thus should be used in species inventories."
        },
        {
            "id": "R139497",
            "label": "Congruence between morphology-based species and Barcode Index Numbers (BINs) in Neotropical Eumaeini (Lycaenidae)",
            "doi": "10.7717/peerj.11843",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "\\n background \\n with about 1,000 species in the neotropics, the eumaeini (theclinae) are one of the most diverse butterfly tribes. correct morphology-based identifications are challenging in many genera due to relatively little interspecific differences in wing patterns. geographic infraspecific variation is sometimes more substantial than variation between species. in this paper we present a large dna barcode dataset of south american lycaenidae. we analyze how well dna barcode bins match morphologically delimited species. \\n \\n \\n methods \\n we compare morphology-based species identifications with the clustering of molecular operational taxonomic units (motus) delimitated by the resl algorithm in bold, which assigns barcode index numbers (bins). we examine intra- and interspecific divergences for genera represented by at least four morphospecies. we discuss the existence of local barcode gaps in a genus by genus analysis. we also note differences in the percentage of species with barcode gaps in groups of lowland and high mountain genera. \\n \\n \\n results \\n we identified 2,213 specimens and obtained 1,839 sequences of 512 species in 90 genera. overall, the mean intraspecific divergence value of co1 sequences was 1.20%, while the mean interspecific divergence between nearest congeneric neighbors was 4.89%, demonstrating the presence of a barcode gap. however, the gap seemed to disappear from the entire set when comparing the maximum intraspecific distance (8.40%) with the minimum interspecific distance (0.40%). clear barcode gaps are present in many genera but absent in others. from the set of specimens that yielded coi fragment lengths of at least 650 bp, 75% of the a priori morphology-based identifications were unambiguously assigned to a single barcode index number (bin). however, after a taxonomic a posteriori review, the percentage of matched identifications rose to 85%. bin splitting was observed for 17% of the species and bin sharing for 9%. we found that genera that contain primarily lowland species show higher percentages of local barcode gaps and congruence between bins and morphology than genera that contain exclusively high montane species. the divergence values to the nearest neighbors were significantly lower in high andean species while the intra-specific divergence values were significantly lower in the lowland species. these results raise questions regarding the causes of observed low inter and high intraspecific genetic variation. we discuss incomplete lineage sorting and hybridization as most likely causes of this phenomenon, as the montane species concerned are relatively young and hybridization is probable. the release of our data set represents an essential baseline for a reference library for biological assessment studies of butterflies in mega diverse countries using modern high-throughput technologies an highlights the necessity of taxonomic revisions for various genera combining both molecular and morphological data. \\n"
        },
        {
            "id": "R139508",
            "label": "Close congruence between Barcode Index Numbers (bins) and species boundaries in the Erebidae (Lepidoptera: Noctuoidea) of the Iberian Peninsula",
            "doi": "10.3897/bdj.5.e19840",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "abstract the dna barcode reference library for lepidoptera holds much promise as a tool for taxonomic research and for providing the reliable identifications needed for conservation assessment programs. we gathered sequences for the barcode region of the mitochondrial cytochrome c oxidase subunit i gene from 160 of the 176 nominal species of erebidae moths (insecta: lepidoptera) known from the iberian peninsula. these results arise from a research project which constructing a dna barcode library for the insect species of spain. new records for 271 specimens (122 species) are coupled with preexisting data for 38 species from the iberian fauna. mean interspecific distance was 12.1%, while the mean nearest neighbour divergence was 6.4%. all 160 species possessed diagnostic barcode sequences, but one pair of congeneric taxa (eublemma rosea and eublemma rietzi) were assigned to the same bin. as well, intraspecific sequence divergences higher than 1.5% were detected in four species which likely represent species complexes. this study reinforces the effectiveness of dna barcoding as a tool for monitoring biodiversity in particular geographical areas and the strong correspondence between sequence clusters delineated by bins and species recognized through detailed taxonomic analysis."
        },
        {
            "id": "R139538",
            "label": "High resolution DNA barcode library for European butterflies reveals continental patterns of mitochondrial genetic diversity",
            "doi": "10.1038/s42003-021-01834-7",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "abstract the study of global biodiversity will greatly benefit from access to comprehensive dna barcode libraries at continental scale, but such datasets are still very rare. here, we assemble the first high-resolution reference library for european butterflies that provides 97% taxon coverage (459 species) and 22,306 coi sequences. we estimate that we captured 62% of the total haplotype diversity and show that most species possess a few very common haplotypes and many rare ones. specimens in the dataset have an average 95.3% probability of being correctly identified. mitochondrial diversity displayed elevated haplotype richness in southern european refugia, establishing the generality of this key biogeographic pattern for an entire taxonomic group. fifteen percent of the species are involved in barcode sharing, but two thirds of these cases may reflect the need for further taxonomic research. this dataset provides a unique resource for conservation and for studying evolutionary processes, cryptic species, phylogeography, and ecology."
        },
        {
            "id": "R139546",
            "label": "A DNA barcode reference library for Swiss butterflies and forester moths as a tool for species identification, systematics and conservation",
            "doi": "10.1371/journal.pone.0208639",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "butterfly monitoring and red list programs in switzerland rely on a combination of observations and collection records to document changes in species distributions through time. while most butterflies can be identified using morphology, some taxa remain challenging, making it difficult to accurately map their distributions and develop appropriate conservation measures. in this paper, we explore the use of the dna barcode (a fragment of the mitochondrial gene coi) as a tool for the identification of swiss butterflies and forester moths (rhopalocera and zygaenidae). we present a national dna barcode reference library including 868 sequences representing 217 out of 224 resident species, or 96.9% of swiss fauna. dna barcodes were diagnostic for nearly 90% of swiss species. the remaining 10% represent cases of para- and polyphyly likely involving introgression or incomplete lineage sorting among closely related taxa. we demonstrate that integrative taxonomic methods incorporating a combination of morphological and genetic techniques result in a rate of species identification of over 96% in females and over 98% in males, higher than either morphology or dna barcodes alone. we explore the use of the dna barcode for exploring boundaries among taxa, understanding the geographical distribution of cryptic diversity and evaluating the status of purportedly endemic taxa. finally, we discuss how dna barcodes may be used to improve field practices and ultimately enhance conservation strategies."
        },
        {
            "id": "R140187",
            "label": "DNA Barcoding the Geometrid Fauna of Bavaria (Lepidoptera): Successes, Surprises, and Questions",
            "doi": "10.1371/journal.pone.0017134",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "background the state of bavaria is involved in a research program that will lead to the construction of a dna barcode library for all animal species within its territorial boundaries. the present study provides a comprehensive dna barcode library for the geometridae, one of the most diverse of insect families. methodology/principal findings this study reports dna barcodes for 400 bavarian geometrid species, 98 per cent of the known fauna, and approximately one per cent of all bavarian animal species. although 98.5% of these species possess diagnostic barcode sequences in bavaria, records from neighbouring countries suggest that species-level resolution may be compromised in up to 3.5% of cases. all taxa which apparently share barcodes are discussed in detail. one case of modest divergence (1.4%) revealed a species overlooked by the current taxonomic system: eupithecia goossensiata mabille, 1869 stat.n. is raised from synonymy with eupithecia absinthiata (clerck, 1759) to species rank. deep intraspecific sequence divergences (>2%) were detected in 20 traditionally recognized species. conclusions/significance the study emphasizes the effectiveness of dna barcoding as a tool for monitoring biodiversity. open access is provided to a data set that includes records for 1,395 geometrid specimens (331 species) from bavaria, with 69 additional species from neighbouring regions. taxa with deep intraspecific sequence divergences are undergoing more detailed analysis to ascertain if they represent cases of cryptic diversity."
        },
        {
            "id": "R140197",
            "label": "DNA barcodes distinguish species of tropical Lepidoptera",
            "doi": "10.1073/pnas.0510466103",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "although central to much biological research, the identification of species is often difficult. the use of dna barcodes, short dna sequences from a standardized region of the genome, has recently been proposed as a tool to facilitate species identification and discovery. however, the effectiveness of dna barcoding for identifying specimens in species-rich tropical biotas is unknown. here we show that cytochrome c oxidase i dna barcodes effectively discriminate among species in three lepidoptera families from area de conservaci\u00f3n guanacaste in northwestern costa rica. we found that 97.9% of the 521 species recognized by prior taxonomic work possess distinctive cytochrome c oxidase i barcodes and that the few instances of interspecific sequence overlap involve very similar species. we also found two or more barcode clusters within each of 13 supposedly single species. covariation between these clusters and morphological and/or ecological traits indicates overlooked species complexes. if these results are general, dna barcoding will significantly aid species identification and discovery in tropical settings."
        },
        {
            "id": "R140252",
            "label": "Species-Level Para- and Polyphyly in DNA Barcode Gene Trees: Strong Operational Bias in European Lepidoptera",
            "doi": "10.1093/sysbio/syw044",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "the proliferation of dna data is revolutionizing all fields of systematic research. dna barcode sequences, now available for millions of specimens and several hundred thousand species, are increasingly used in algorithmic species delimitations. this is complicated by occasional incongruences between species and gene genealogies, as indicated by situations where conspecific individuals do not form a monophyletic cluster in a gene tree. in two previous reviews, non-monophyly has been reported as being common in mitochondrial dna gene trees. we developed a novel web service \u201cmonophylizer\u201d to detect non-monophyly in phylogenetic trees and used it to ascertain the incidence of species non-monophyly in coi (a.k.a. cox1) barcode sequence data from 4977 species and 41,583 specimens of european lepidoptera, the largest data set of dna barcodes analyzed from this regard. particular attention was paid to accurate species identification to ensure data integrity. we investigated the effects of tree-building method, sampling effort, and other methodological issues, all of which can influence estimates of non-monophyly. we found a 12% incidence of non-monophyly, a value significantly lower than that observed in previous studies. neighbor joining (nj) and maximum likelihood (ml) methods yielded almost equal numbers of non-monophyletic species, but 24.1% of these cases of non-monophyly were only found by one of these methods. non-monophyletic species tend to show either low genetic distances to their nearest neighbors or exceptionally high levels of intraspecific variability. cases of polyphyly in coi trees arising as a result of deep intraspecific divergence are negligible, as the detected cases reflected misidentifications or methodological errors. taking into consideration variation in sampling effort, we estimate that the true incidence of non-monophyly is \u223c23%, but with operational factors still being included. within the operational factors, we separately assessed the frequency of taxonomic limitations (presence of overlooked cryptic and oversplit species) and identification uncertainties. we observed that operational factors are potentially present in more than half (58.6%) of the detected cases of non-monophyly. furthermore, we observed that in about 20% of non-monophyletic species and entangled species, the lineages involved are either allopatric or parapatric\u2014conditions where species delimitation is inherently subjective and particularly dependent on the species concept that has been adopted. these observations suggest that species-level non-monophyly in coi gene trees is less common than previously supposed, with many cases reflecting misidentifications, the subjectivity of species delimitation or other operational factors."
        },
        {
            "id": "R142471",
            "label": "DNA barcoding of Northern Nearctic Muscidae (Diptera) reveals high correspondence between morphological and molecular species limits",
            "doi": "10.1186/1472-6785-12-24",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "abstract \\n \\n background \\n various methods have been proposed to assign unknown specimens to known species using their dna barcodes, while others have focused on using genetic divergence thresholds to estimate \u201cspecies\u201d diversity for a taxon, without a well-developed taxonomy and/or an extensive reference library of dna barcodes. the major goals of the present work were to: a) conduct the largest species-level barcoding study of the muscidae to date and characterize the range of genetic divergence values in the northern nearctic fauna; b) evaluate the correspondence between morphospecies and barcode groupings defined using both clustering-based and threshold-based approaches; and c) use the reference library produced to address taxonomic issues. \\n \\n \\n results \\n our data set included 1114 individuals and their coi sequences (951 from churchill, manitoba), representing 160 morphologically-determined species from 25 genera, covering 89% of the known fauna of churchill and 23% of the nearctic fauna. following an iterative process through which all specimens belonging to taxa with anomalous divergence values and/or monophyly issues were re-examined, identity was modified for 9 taxa, including the reinstatement of phaonia luteva (walker) stat. nov. as a species distinct from phaonia errans (meigen). in the post-reassessment data set, no distinct gap was found between maximum pairwise intraspecific distances (range 0.00-3.01%) and minimum interspecific distances (range: 0.77-11.33%). nevertheless, using a clustering-based approach, all individuals within 98% of species grouped with their conspecifics with high (&gt;95%) bootstrap support; in contrast, a maximum species discrimination rate of 90% was obtained at the optimal threshold of 1.2%. dna barcoding enabled the determination of females from 5 ambiguous species pairs and confirmed that 16 morphospecies were genetically distinct from named taxa. there were morphological differences among all distinct genetic clusters; thus, no cases of cryptic species were detected. \\n \\n \\n conclusions \\n our findings reveal the great utility of building a well-populated, species-level reference barcode database against which to compare unknowns. when such a library is unavailable, it is still possible to obtain a fairly accurate (within ~10%) rapid assessment of species richness based upon a barcode divergence threshold alone, but this approach is most accurate when the threshold is tuned to a particular taxon. \\n"
        },
        {
            "id": "R142517",
            "label": "A DNA barcode library for 5,200 German flies and midges (Insecta: Diptera) and its implications for metabarcoding\u2010based biomonitoring",
            "doi": "10.1111/1755-0998.13022",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "this study summarizes results of a dna barcoding campaign on german diptera, involving analysis of 45,040 specimens. the resultant dna barcode library includes records for 2,453 named species comprising a total of 5,200 barcode index numbers (bins), including 2,700 coi haplotype clusters without species\u2010level assignment, so called \u201cdark taxa.\u201d overall, 88 out of 117 families (75%) recorded from germany were covered, representing more than 50% of the 9,544 known species of german diptera. until now, most of these families, especially the most diverse, have been taxonomically inaccessible. by contrast, within a few years this study provided an intermediate taxonomic system for half of the german dipteran fauna, which will provide a useful foundation for subsequent detailed, integrative taxonomic studies. using dna extracts derived from bulk collections made by malaise traps, we further demonstrate that species delineation using bins and operational taxonomic units (otus) constitutes an effective method for biodiversity studies using dna metabarcoding. as the reference libraries continue to grow, and gaps in the species catalogue are filled, bin lists assembled by metabarcoding will provide greater taxonomic resolution. the present study has three main goals: (a) to provide a dna barcode library for 5,200 bins of diptera; (b) to demonstrate, based on the example of bulk extractions from a malaise trap experiment, that dna barcode clusters, labelled with globally unique identifiers (such as otus and/or bins), provide a pragmatic, accurate solution to the \u201ctaxonomic impediment\u201d; and (c) to demonstrate that interim names based on bins and otus obtained through metabarcoding provide an effective method for studies on species\u2010rich groups that are usually neglected in biodiversity research projects because of their unresolved taxonomy."
        },
        {
            "id": "R142535",
            "label": "DNA Barcodes for the Northern European Tachinid Flies (Diptera: Tachinidae)",
            "doi": "10.1371/journal.pone.0164933",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "this data release provides coi barcodes for 366 species of parasitic flies (diptera: tachinidae), enabling the dna based identification of the majority of northern european species and a large proportion of palearctic genera, regardless of the developmental stage. the data will provide a tool for taxonomists and ecologists studying this ecologically important but challenging parasitoid family. a comparison of minimum distances between the nearest neighbors revealed the mean divergence of 5.52% that is approximately the same as observed earlier with comparable sampling in lepidoptera, but clearly less than in coleoptera. full barcode-sharing was observed between 13 species pairs or triplets, equaling to 7.36% of all species. delimitation based on barcode index number (bin) system was compared with traditional classification of species and interesting cases of possible species oversplits and cryptic diversity are discussed. overall, dna barcodes are effective in separating tachinid species and provide novel insight into the taxonomy of several genera."
        },
        {
            "id": "R145296",
            "label": "Molecular identification of mosquitoes (Diptera: Culicidae) in southeastern Australia",
            "doi": "10.1002/ece3.2095",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "abstract dna barcoding is a modern species identification technique that can be used to distinguish morphologically similar species, and is particularly useful when using small amounts of starting material from partial specimens or from immature stages. in order to use dna barcoding in a surveillance program, a database containing mosquito barcode sequences is required. this study obtained cytochrome oxidase i (coi) sequences for 113 morphologically identified specimens, representing 29 species, six tribes and 12 genera; 17 of these species have not been previously barcoded. three of the 29 species \u2500 culex palpalis, macleaya macmillani, and an unknown species originally identified as tripteroides atripes \u2500 were initially misidentified as they are difficult to separate morphologically, highlighting the utility of dna barcoding. while most species grouped separately (reciprocally monophyletic), the cx. pipiens subgroup could not be genetically separated using coi. the average conspecific and congeneric p\u2010distance was 0.8% and 7.6%, respectively. in our study, we also demonstrate the utility of dna barcoding in distinguishing exotics from endemic mosquitoes by identifying a single intercepted stegomyia aegypti egg at an international airport. the use of dna barcoding dramatically reduced the identification time required compared with rearing specimens through to adults, thereby demonstrating the value of this technique in biosecurity surveillance. the dna barcodes produced by this study have been uploaded to the \u2018mosquitoes of australia\u2013victoria\u2019 project on the barcode of life database (bold), which will serve as a resource for the victorian arbovirus disease control program and other national and international mosquito surveillance programs."
        },
        {
            "id": "R145304",
            "label": "Analyzing Mosquito (Diptera: Culicidae) Diversity in Pakistan by DNA Barcoding",
            "doi": "10.1371/journal.pone.0097268",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "background although they are important disease vectors mosquito biodiversity in pakistan is poorly known. recent epidemics of dengue fever have revealed the need for more detailed understanding of the diversity and distributions of mosquito species in this region. dna barcoding improves the accuracy of mosquito inventories because morphological differences between many species are subtle, leading to misidentifications. methodology/principal findings sequence variation in the barcode region of the mitochondrial coi gene was used to identify mosquito species, reveal genetic diversity, and map the distribution of the dengue-vector species in pakistan. analysis of 1684 mosquitoes from 491 sites in punjab and khyber pakhtunkhwa during 2010\u20132013 revealed 32 species with the assemblage dominated by culex quinquefasciatus (61% of the collection). the genus aedes (stegomyia) comprised 15% of the specimens, and was represented by six taxa with the two dengue vector species, ae. albopictus and ae. aegypti, dominant and broadly distributed. anopheles made up another 6% of the catch with an. subpictus dominating. barcode sequence divergence in conspecific specimens ranged from 0\u20132.4%, while congeneric species showed from 2.3\u201317.8% divergence. a global haplotype analysis of disease-vectors showed the presence of multiple haplotypes, although a single haplotype of each dengue-vector species was dominant in most countries. geographic distribution of ae. aegypti and ae. albopictus showed the later species was dominant and found in both rural and urban environments. conclusions as the first dna-based analysis of mosquitoes in pakistan, this study has begun the construction of a barcode reference library for the mosquitoes of this region. levels of genetic diversity varied among species. because of its capacity to differentiate species, even those with subtle morphological differences, dna barcoding aids accurate tracking of vector populations."
        },
        {
            "id": "R145434",
            "label": "DNA Barcoding of Neotropical Sand Flies (Diptera, Psychodidae, Phlebotominae): Species Identification and Discovery within Brazil",
            "doi": "10.1371/journal.pone.0140636",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "dna barcoding has been an effective tool for species identification in several animal groups. here, we used dna barcoding to discriminate between 47 morphologically distinct species of brazilian sand flies. dna barcodes correctly identified approximately 90% of the sampled taxa (42 morphologically distinct species) using clustering based on neighbor-joining distance, of which four species showed comparatively higher maximum values of divergence (range 4.23\u201319.04%), indicating cryptic diversity. the dna barcodes also corroborated the resurrection of two species within the shannoni complex and provided an efficient tool to differentiate between morphologically indistinguishable females of closely related species. taken together, our results validate the effectiveness of dna barcoding for species identification and the discovery of cryptic diversity in sand flies from brazil."
        },
        {
            "id": "R145437",
            "label": "DNA Barcoding to Improve the Taxonomy of the Afrotropical Hoverflies (Insecta: Diptera: Syrphidae)",
            "doi": "10.1371/journal.pone.0140264",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "the identification of afrotropical hoverflies is very difficult because of limited recent taxonomic revisions and the lack of comprehensive identification keys. in order to assist in their identification, and to improve the taxonomy of this group, we constructed a reference dataset of 513 coi barcodes of 90 of the more common nominal species from ghana, togo, benin and nigeria (w africa) and added ten publically available coi barcodes from nine nominal afrotropical species to this (total: 523 coi barcodes; 98 nominal species; 26 genera). the identification accuracy of this dataset was evaluated with three methods (k2p distance-based, neighbor-joining (nj) / maximum likelihood (ml) analysis, and using speciesidentifier). results of the three methods were highly congruent and showed a high identification success. nine species pairs showed a low ( 0.03) maximum intraspecific k2p distance was observed in eight species and barcodes of these species not always formed single clusters in the nj / ml analayses which may indicate the occurrence of cryptic species. optimal k2p thresholds to differentiate intra- from interspecific k2p divergence were highly different among the three subfamilies (eristalinae: 0.037, syrphinae: 0.06, microdontinae: 0.007\u20130.02), and among the different general suggesting that optimal thresholds are better defined at the genus level. in addition to providing an alternative identification tool, our study indicates that dna barcoding improves the taxonomy of afrotropical hoverflies by selecting (groups of) taxa that deserve further taxonomic study, and by attributing the unknown sex to species for which only one of the sexes is known."
        },
        {
            "id": "R145468",
            "label": "DNA barcoding of Neotropical black flies (Diptera: Simuliidae): Species identification and discovery of cryptic diversity in Mesoamerica",
            "doi": "10.11646/zootaxa.3936.1.5",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "although correct taxonomy is paramount for disease control programs and epidemiological studies, morphology-based taxonomy of black flies is extremely difficult. in the present study, the utility of a partial sequence of the coi gene, the dna barcoding region, for the identification of species of black flies from mesoamerica was assessed. a total of 32 morphospecies were analyzed, one belonging to the genus gigantodax and 31 species to the genus simulium and six of its subgenera (aspathia, eusimulium, notolepria, psaroniocompsa, psilopelmia, trichodagmia). the neighbour joining tree (nj) derived from the dna barcodes grouped most specimens according to species or species groups recognized by morphotaxonomic studies. intraspecific sequence divergences within morphologically distinct species ranged from 0.07% to 1.65%, while higher divergences (2.05%-6.13%) in species complexes suggested the presence of cryptic diversity. the existence of well-defined groups within s. callidum (dyar & shannon), s. quadrivittatum loew, and s. samboni jennings revealed the likely inclusion of cryptic species within these taxa. in addition, the suspected presence of sibling species within s. paynei vargas and s. tarsatum macquart was supported. dna barcodes also showed that specimens of species that are difficult to delimit morphologically such as s. callidum, s. pseudocallidum d\u00edaz n\u00e1jera, s. travisi vargas, vargas & ram\u00edrez-p\u00e9rez, relatives of the species complexes such as s. metallicum bellardi s.l. (e.g., s. horacioi okazawa & onishi, s. jobbinsi vargas, mart\u00ednez palacios, d\u00edaz n\u00e1jera, and s. puigi vargas, mart\u00ednez palacios & d\u00edaz n\u00e1jera), and s. virgatum coquillett complex (e.g., s. paynei and s. tarsatum) grouped together in the nj analysis, suggesting they represent valid species. dna barcoding combined with a sound morphotaxonomic framework provided an effective approach for the identification of medically important black flies species in mesoamerica and for the discovery of hidden diversity within this group."
        },
        {
            "id": "R145491",
            "label": "DNA barcoding of tropical black flies (Diptera: Simuliidae) of Thailand",
            "doi": "10.1111/1755-0998.12174",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "the ecological and medical importance of black flies drives the need for rapid and reliable identification of these minute, structurally uniform insects. we assessed the efficiency of dna barcoding for species identification of tropical black flies. a total of 351 cytochrome c oxidase subunit 1 sequences were obtained from 41 species in six subgenera of the genus simulium in thailand. despite high intraspecific genetic divergence (mean = 2.00%, maximum = 9.27%), dna barcodes provided 96% correct identification. barcodes also differentiated cytoforms of selected species complexes, albeit with varying levels of success. perfect differentiation was achieved for two cytoforms of simulium feuerborni, and 91% correct identification was obtained for the simulium angulistylum complex. low success (33%), however, was obtained for the simulium siamense complex. the differential efficiency of dna barcodes to discriminate cytoforms was attributed to different levels of genetic structure and demographic histories of the taxa. dna barcode trees were largely congruent with phylogenies based on previous molecular, chromosomal and morphological analyses, but revealed inconsistencies that will require further evaluation."
        },
        {
            "id": "R145495",
            "label": "DNA Barcoding for the Identification of Sand Fly Species (Diptera, Psychodidae, Phlebotominae) in Colombia",
            "doi": "10.1371/journal.pone.0085496",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "sand flies include a group of insects that are of medical importance and that vary in geographic distribution, ecology, and pathogen transmission. approximately 163 species of sand flies have been reported in colombia. surveillance of the presence of sand fly species and the actualization of species distribution are important for predicting risks for and monitoring the expansion of diseases which sand flies can transmit. currently, the identification of phlebotomine sand flies is based on morphological characters. however, morphological identification requires considerable skills and taxonomic expertise. in addition, significant morphological similarity between some species, especially among females, may cause difficulties during the identification process. dna-based approaches have become increasingly useful and promising tools for estimating sand fly diversity and for ensuring the rapid and accurate identification of species. a partial sequence of the mitochondrial cytochrome oxidase gene subunit i (coi) is currently being used to differentiate species in different animal taxa, including insects, and it is referred as a barcoding sequence. the present study explored the utility of the dna barcode approach for the identification of phlebotomine sand flies in colombia. we sequenced 700 bp of the coi gene from 36 species collected from different geographic localities. the coi barcode sequence divergence within a single species was <2% in most cases, whereas this divergence ranged from 9% to 26.6% among different species. these results indicated that the barcoding gene correctly discriminated among the previously morphologically identified species with an efficacy of nearly 100%. analyses of the generated sequences indicated that the observed species groupings were consistent with the morphological identifications. in conclusion, the barcoding gene was useful for species discrimination in sand flies from colombia."
        },
        {
            "id": "R145497",
            "label": "Half of the European fruit fly species barcoded (Diptera, Tephritidae); a feasibility test for molecular identification",
            "doi": "10.3897/zookeys.365.5819",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "abstract a feasibility test of molecular identification of european fruit flies (diptera: tephritidae) based on coi barcode sequences has been executed. a dataset containing 555 sequences of 135 ingroup species from three subfamilies and 42 genera and one single outgroup species has been analysed. 73.3% of all included species could be identified based on their coi barcode gene, based on similarity and distances. the low success rate is caused by singletons as well as some problematic groups: several species groups within the genus terellia and especially the genus urophora. with slightly more than 100 sequences \u2013 almost 20% of the total \u2013 this genus alone constitutes the larger part of the failure for molecular identification for this dataset. deleting the singletons and urophora results in a success-rate of 87.1% of all queries and 93.23% of the not discarded queries as correctly identified. urophora is of special interest due to its economic importance as beneficial species for weed control, therefore it is desirable to have alternative markers for molecular identification. we demonstrate that the success of dna barcoding for identification purposes strongly depends on the contents of the database used to blast against. especially the necessity of including multiple specimens per species of geographically distinct populations and different ecologies for the understanding of the intra- versus interspecific variation is demonstrated. furthermore thresholds and the distinction between true and false positives and negatives should not only be used to increase the reliability of the success of molecular identification but also to point out problematic groups, which should then be flagged in the reference database suggesting alternative methods for identification."
        },
        {
            "id": "R145502",
            "label": "Barcoding of biting midges in the genus Culicoides: a tool for species determination",
            "doi": "10.1111/j.1365-2915.2012.01050.x",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "biting midges of the genus culicoides (diptera: ceratopogonidae) are insect vectors of economically important veterinary diseases such as african horse sickness virus and bluetongue virus. however, the identification of culicoides based on morphological features is difficult. the sequencing of mitochondrial cytochrome oxidase subunit i (coi), referred to as dna barcoding, has been proposed as a tool for rapid identification to species. hence, a study was undertaken to establish dna barcodes for all morphologically determined culicoides species in swedish collections. in total, 237 specimens of culicoides representing 37 morphologically distinct species were used. the barcoding generated 37 supported clusters, 31 of which were in agreement with the morphological determination. however, two pairs of closely related species could not be separated using the dna barcode approach. moreover, culicoides obsoletus meigen and culicoides newsteadi austen showed relatively deep intraspecific divergence (more than 10 times the average), which led to the creation of two cryptic species within each of c. obsoletus and c. newsteadi. the use of coi barcodes as a tool for the species identification of biting midges can differentiate 95% of species studied. identification of some closely related species should employ a less conserved region, such as a ribosomal internal transcribed spacer."
        },
        {
            "id": "R145506",
            "label": "Identification of Nearctic black flies using DNA barcodes (Diptera: Simuliidae)",
            "doi": "10.1111/j.1755-0998.2009.02648.x",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "dna barcoding has gained increased recognition as a molecular tool for species identification in various groups of organisms. in this preliminary study, we tested the efficacy of a 615\u2010bp fragment of the cytochrome c oxidase i (coi) as a dna barcode in the medically important family simuliidae, or black flies. a total of 65 (25%) morphologically distinct species and sibling species in species complexes of the 255 recognized nearctic black fly species were used to create a preliminary barcode profile for the family. genetic divergence among congeners averaged 14.93% (range 2.83\u201315.33%), whereas intraspecific genetic divergence between morphologically distinct species averaged 0.72% (range 0\u20133.84%). dna barcodes correctly identified nearly 100% of the morphologically distinct species (87% of the total sampled taxa), whereas in species complexes (13% of the sampled taxa) maximum values of divergence were comparatively higher (max. 4.58\u20136.5%), indicating cryptic diversity. the existence of sibling species in prosimulium travisi and p. neomacropyga was also demonstrated, thus confirming previous cytological evidence about the existence of such cryptic diversity in these two taxa. we conclude that dna barcoding is an effective method for species identification and discovery of cryptic diversity in black flies."
        },
        {
            "id": "R145509",
            "label": "Identifying Canadian mosquito species through DNA barcodes",
            "doi": "10.1111/j.1365-2915.2006.00653.x",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "abstract a short fragment of mt dna from the cytochrome c oxidase 1 (co1) region was used to provide the first co1 barcodes for 37 species of canadian mosquitoes (diptera: culicidae) from the provinces ontario and new brunswick. sequence variation was analysed in a 617\u2010bp fragment from the 5\u2032 end of the co1 region. sequences of each mosquito species formed barcode clusters with tight cohesion that were usually clearly distinct from those of allied species. co1 sequence divergences were, on average, nearly 20 times higher for congeneric species than for members of a species; divergences between congeneric species averaged 10.4% (range 0.2\u201317.2%), whereas those for conspecific individuals averaged 0.5% (range 0.0\u20133.9%)."
        },
        {
            "id": "R145554",
            "label": "Identifying the Main Mosquito Species in China Based on DNA Barcoding",
            "doi": "10.1371/journal.pone.0047051",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "mosquitoes are insects of the diptera, nematocera, and culicidae families, some species of which are important disease vectors. identifying mosquito species based on morphological characteristics is difficult, particularly the identification of specimens collected in the field as part of disease surveillance programs. because of this difficulty, we constructed dna barcodes of the cytochrome c oxidase subunit 1, the coi gene, for the more common mosquito species in china, including the major disease vectors. a total of 404 mosquito specimens were collected and assigned to 15 genera and 122 species and subspecies on the basis of morphological characteristics. individuals of the same species grouped closely together in a neighborhood-joining tree based on coi sequence similarity, regardless of collection site. coi gene sequence divergence was approximately 30 times higher for species in the same genus than for members of the same species. divergence in over 98% of congeneric species ranged from 2.3% to 21.8%, whereas divergence in conspecific individuals ranged from 0% to 1.67%. cryptic species may be common and a few pseudogenes were detected."
        },
        {
            "id": "R146639",
            "label": "DNA barcodes for species delimitation in Chironomidae (Diptera): a case study on the genus Labrundinia",
            "doi": "10.4039/tce.2013.44",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "abstract in this study, we analysed the applicability of dna barcodes for delimitation of 79 specimens of 13 species of nonbiting midges in the subfamily tanypodinae (diptera: chironomidae) from s\u00e3o paulo state, brazil. our results support dna barcoding as an excellent tool for species identification and for solving taxonomic conflicts in genus labrundinia. molecular analysis of cytochrome c oxidase subunit i (coi) gene sequences yielded taxon identification trees, supporting 13 cohesive species clusters, of which three similar groups were subsequently linked to morphological variation at the larval and pupal stage. additionally, another cluster previously described by means of morphology was linked to molecular markers. we found a distinct barcode gap, and in some species substantial interspecific pairwise divergences (up to 19.3%) were observed, which permitted identification of all analysed species. the results also indicated that barcodes can be used to associate life stages of chironomids since coi was easily amplified and sequenced from different life stages with universal barcode primers."
        },
        {
            "id": "R146643",
            "label": "Revision of Nearctic Dasysyrphus Enderlein (Diptera: Syrphidae)",
            "doi": "10.11646/zootaxa.3660.1.1",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "dasysyrphus enderlein (diptera: syrphidae) has posed taxonomic challenges to researchers in the past, primarily due to their lack of interspecific diagnostic characters. in the present study, dna data (mitochondrial cytochrome c oxidase sub-unit i\u2014coi) were combined with morphology to help delimit species. this led to two species being resurrected from synonymy (d. laticaudus and d. pacificus) and the discovery of one new species (d. occidualis sp. nov.). an additional new species was described based on morphology alone (d. richardi sp. nov.), as the specimens were too old to obtain coi. part of the taxonomic challenge presented by this group arises from missing type specimens. neotypes are designated here for d. pauxillus and d. pinastri to bring stability to these names. an illustrated key to 13 nearctic species is presented, along with descriptions, maps and supplementary data. a phylogeny based on coi is also presented and discussed."
        },
        {
            "id": "R146646",
            "label": "Comprehensive evaluation of DNA barcoding for the molecular species identification of forensically important Australian Sarcophagidae (Diptera)",
            "doi": "10.1071/is12008",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "carrion-breeding sarcophagidae (diptera) can be used to estimate the post-mortem interval in forensic cases. difficulties with accurate morphological identifications at any life stage and a lack of documented thermobiological profiles have limited their current usefulness. the molecular-based approach of dna barcoding, which utilises a 648-bp fragment of the mitochondrial cytochrome oxidase subuniti gene, was evaluated in a pilot study for discrimination between 16 australian sarcophagids. the current study comprehensively evaluated barcoding for a larger taxon set of 588 australian sarcophagids. in total, 39 of the 84 known australian species were represented by 580 specimens, which includes 92% of potentially forensically important species. a further eight specimens could not be identified, but were included nonetheless as six unidentifiable taxa. a neighbour-joining tree was generated and nucleotide sequence divergences were calculated. all species except sarcophaga (fergusonimyia) bancroftorum, known for high morphological variability, were resolved as monophyletic (99.2% of cases), with bootstrap support of 100. excluding s. bancroftorum, the mean intraspecific and interspecific variation ranged from 1.12% and 2.81\u201311.23%, respectively, allowing for species discrimination. dna barcoding was therefore validated as a suitable method for molecular identification of australian sarcophagidae, which will aid in the implementation of this fauna in forensic entomology."
        },
        {
            "id": "R146932",
            "label": "DNA barcodes reveal cryptic genetic diversity within the blackfly subgenus Trichodagmia Enderlein (Diptera: Simuliidae: Simulium) and related taxa in the New World",
            "doi": "10.11646/zootaxa.3514.1.3",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "in this paper we investigate the utility of the coi dna barcoding region for species identification and for revealing hidden diversity within the subgenus trichodagmia and related taxa in the new world. in total, 24 morphospecies within the current expanded taxonomic concept of trichodagmia were analyzed. three species in the subgenus aspathia and 10 species in the subgenus simulium s.str. were also included in the analysis because of their putative phylogenetic relationship with trichodagmia. in the neighbour joining analysis tree (nj) derived from the dna barcodes most of the specimens grouped together according to species or species groups as recognized by other morphotaxonomic studies. the interspecific genetic divergence averaged 11.2% (range 2.8\u201319.5%), whereas intraspecific genetic divergence within morphologically distinct species averaged 0.5% (range 0\u20131.2%). higher values of genetic divergence (3.2\u20133.7%) in species complexes suggest the presence of cryptic diversity. the existence of well defined groups within s. piperi, s. duodenicornium, s. canadense and s. rostratum indicate the possible presence of cryptic species within these taxa. also, the suspected presence of a sibling species in s. tarsatum and s. paynei is supported. dna barcodes also showed that specimens from species that were taxonomically difficult to delimit such as s. hippovorum, s. rubrithorax, s. paynei, and other related taxa (s. solarii), grouped together in the nj analysis, confirming the validity of their species status. the recovery of partial barcodes from specimens in collections was time consuming and pcr success was low from specimens more than 10 years old. however, when a sequence was obtained, it provided good resolution for species identification. larvae preserved in \u2018weak\u2019 carnoy\u2019s solution (9:1 ethanol:acetic acid) provided full dna barcodes. adding legs directly to the pcr mix from recently collected and preserved adults was an inexpensive, fast methodology to obtain full barcodes. in summary, dna barcoding combined with a sound morphotaxonomic framework provides an effective approach for the delineation of species and for the discovery of hidden diversity in the subgenus trichodagmia."
        },
        {
            "id": "R146938",
            "label": "Evaluation of DNA barcoding and identification of new haplomorphs in Canadian deerflies and horseflies",
            "doi": "10.1111/j.1365-2915.2010.00896.x",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "this paper reports the first tests of the suitability of the standardized mitochondrial cytochrome c oxidase subunit i (coi) barcoding system for the identification of canadian deerflies and horseflies. two additional mitochondrial molecular markers were used to determine whether unambiguous species recognition in tabanids can be achieved. our 332 canadian tabanid samples yielded 650 sequences from five genera and 42 species. standard coi barcodes demonstrated a strong a + t bias (mean 68.1%), especially at third codon positions (mean 93.0%). our preliminary test of this system showed that the standard coi barcode worked well for canadian tabanidae: the target dna can be easily recovered from small amounts of insect tissue and aligned for all tabanid taxa. each tabanid species possessed distinctive sets of coi haplotypes which discriminated well among species. average conspecific kimura two\u2010parameter (k2p) divergence (0.49%) was 12 times lower than the average divergence within species. both the neighbour\u2010joining and the bayesian methods produced trees with identical monophyletic species groups. two species, chrysops dawsoni philip and chrysops montanus osten sacken (diptera: tabanidae), showed relatively deep intraspecific sequence divergences (\u223c10 times the average) for all three mitochondrial gene regions analysed. we suggest provisional differentiation of ch. montanus into two haplotypes, namely, ch. montanus haplomorph 1 and ch. montanus haplomorph 2, both defined by their molecular sequences and by newly discovered differences in structural features near their ocelli."
        },
        {
            "id": "R157039",
            "label": "DNA barcode library for European Gelechiidae (Lepidoptera) suggests greatly underestimated species diversity",
            "doi": "10.3897/zookeys.921.49199",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "for the first time, a nearly complete barcode library for european gelechiidae is provided. dna barcode sequences (coi gene \u2013 cytochrome c oxidase 1) from 751 out of 865 nominal species, belonging to 105 genera, were successfully recovered. a total of 741 species represented by specimens with sequences \u2265 500bp and an additional ten species represented by specimens with shorter sequences were used to produce 53 nj trees. intraspecific barcode divergence averaged only 0.54% whereas distance to the nearest-neighbour species averaged 5.58%. of these, 710 species possessed unique dna barcodes, but 31 species could not be reliably discriminated because of barcode sharing or partial barcode overlap. species discrimination based on the barcode index system (bin) was successful for 668 out of 723 species which clustered from minimum one to maximum 22 unique bins. fifty-five species shared a bin with up to four species and identification from dna barcode data is uncertain. finally, 65 clusters with a unique bin remained unidentified to species level. these putative taxa, as well as 114 nominal species with more than one bin, suggest the presence of considerable cryptic diversity, cases which should be examined in future revisionary studies."
        },
        {
            "id": "R157051",
            "label": "A Transcontinental Challenge \u2014 A Test of DNA Barcode Performance for 1,541 Species of Canadian Noctuoidea (Lepidoptera)",
            "doi": "10.1371/journal.pone.0092797",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "this study provides a first, comprehensive, diagnostic use of dna barcodes for the canadian fauna of noctuoids or \u201cowlet\u201d moths (lepidoptera: noctuoidea) based on vouchered records for 1,541 species (99.1% species coverage), and more than 30,000 sequences. when viewed from a canada-wide perspective, dna barcodes unambiguously discriminate 90% of the noctuoid species recognized through prior taxonomic study, and resolution reaches 95.6% when considered at a provincial scale. barcode sharing is concentrated in certain lineages with 54% of the cases involving 1.8% of the genera. deep intraspecific divergence exists in 7.7% of the species, but further studies are required to clarify whether these cases reflect an overlooked species complex or phylogeographic variation in a single species. non-native species possess higher nearest-neighbour (nn) distances than native taxa, whereas generalist feeders have lower nn distances than those with more specialized feeding habits. we found high concordance between taxonomic names and sequence clusters delineated by the barcode index number (bin) system with 1,082 species (70%) assigned to a unique bin. the cases of discordance involve both bin mergers and bin splits with 38 species falling into both categories, most likely reflecting bidirectional introgression. one fifth of the species are involved in a bin merger reflecting the presence of 158 species sharing their barcode sequence with at least one other taxon, and 189 species with low, but diagnostic coi divergence. a very few cases (13) involved species whose members fell into both categories. most of the remaining 140 species show a split into two or three bins per species, while virbia ferruginosa was divided into 16. the overall results confirm that dna barcodes are effective for the identification of canadian noctuoids. this study also affirms that bins are a strong proxy for species, providing a pathway for a rapid, accurate estimation of animal diversity."
        },
        {
            "id": "R157056",
            "label": "A DNA Barcode Library for North American Pyraustinae (Lepidoptera: Pyraloidea: Crambidae)",
            "doi": "10.1371/journal.pone.0161449",
            "research_field": {
                "id": "R136127",
                "label": "Ecology and Biodiversity of Animals and Ecosystems, Organismic Interactions"
            },
            "abstract": "although members of the crambid subfamily pyraustinae are frequently important crop pests, their identification is often difficult because many species lack conspicuous diagnostic morphological characters. dna barcoding employs sequence diversity in a short standardized gene region to facilitate specimen identifications and species discovery. this study provides a dna barcode reference library for north american pyraustines based upon the analysis of 1589 sequences recovered from 137 nominal species, 87% of the fauna. data from 125 species were barcode compliant (>500bp, <1% n), and 99 of these taxa formed a distinct cluster that was assigned to a single bin. the other 26 species were assigned to 56 bins, reflecting frequent cases of deep intraspecific sequence divergence and a few instances of barcode sharing, creating a total of 155 bins. two systems for otu designation, abgd and bin, were examined to check the correspondence between current taxonomy and sequence clusters. the bin system performed better than abgd in delimiting closely related species, while otu counts with abgd were influenced by the value employed for relative gap width. different species with low or no interspecific divergence may represent cases of unrecognized synonymy, whereas those with high intraspecific divergence require further taxonomic scrutiny as they may involve cryptic diversity. the barcode library developed in this study will also help to advance understanding of relationships among species of pyraustinae."
        },
        {
            "id": "R3000",
            "label": "A model for contextual data sharing in smartphone applications",
            "doi": "10.1108/ijpcc-06-2016-0030",
            "research_field": {
                "id": "R278",
                "label": "Information Science"
            },
            "abstract": "\\n purpose \\n the purpose of this paper is to introduce a model for identifying, storing and sharing contextual information across smartphone apps that uses the native device services. the authors present the idea of using user input and interaction within an app as contextual information, and how each app can identify and store contextual information. \\n \\n \\n design/methodology/approach \\n contexts are modeled as hierarchical objects that can be stored and shared by applications using native mechanisms. a proof-of-concept implementation of the model for the android platform demonstrates contexts modelled as hierarchical objects stored and shared by applications using native mechanisms. \\n \\n \\n findings \\n the model was found to be practically viable by implemented sample apps that share context and through a performance analysis of the system. \\n \\n \\n practical implications \\n the contextual data-sharing model enables the creation of smart apps and services without being tied to any vendor\u2019s cloud services. \\n \\n \\n originality/value \\n this paper introduces a new approach for sharing context in smartphone applications that does not require cloud services. \\n"
        },
        {
            "id": "R3021",
            "label": "TIB's Portal for Audiovisual Media: Combining Manual and Automatic Indexing",
            "doi": "10.1080/01639374.2014.917135",
            "research_field": {
                "id": "R278",
                "label": "Information Science"
            },
            "abstract": "\"the german national library of science and technology (tib) developed a web-based platform for audiovisual media. the audiovisual portal optimizes access to scientific videos such as computer animations and lecture and conference recordings. tib's av-portal combines traditional cataloging and automatic indexing of audiovisual media. the article describes metadata standards for audiovisual media and introduces the tib's metadata schema in comparison to other metadata standards for non-textual materials. additionally, we give an overview of multimedia retrieval technologies used for the portal and present the av-portal in detail as well as the additional value for libraries and their users.\""
        },
        {
            "id": "R34944",
            "label": "A Comprehensive Survey of Graph Embedding: Problems, Techniques and Applications",
            "doi": "10.1109/TKDE.2018.2807452",
            "research_field": {
                "id": "R278",
                "label": "Information Science"
            },
            "abstract": "graph is an important data representation which appears in a wide diversity of real-world scenarios. effective graph analytics provides users a deeper understanding of what is behind the data, and thus can benefit a lot of useful applications such as node classification, node recommendation, link prediction, etc. however, most graph analytics methods suffer the high computation and space cost. graph embedding is an effective yet efficient way to solve the graph analytics problem. it converts the graph data into a low dimensional space in which the graph structural information and graph properties are maximumly preserved. in this survey, we conduct a comprehensive review of the literature in graph embedding. we first introduce the formal definition of graph embedding as well as the related concepts. after that, we propose two taxonomies of graph embedding which correspond to what challenges exist in different graph embedding problem settings and how the existing work addresses these challenges in their solutions. finally, we summarize the applications that graph embedding enables and suggest four promising future research directions in terms of computation efficiency, problem settings, techniques, and application scenarios."
        },
        {
            "id": "R36001",
            "label": "SemEval-2018 Task 7: Semantic Relation Extraction and Classification\n            in Scientific Papers",
            "doi": "10.18653/v1/s18-1111",
            "research_field": {
                "id": "R278",
                "label": "Information Science"
            },
            "abstract": "this paper describes the first task on semantic relation extraction and classification in scientific paper abstracts at semeval 2018. the challenge focuses on domain-specific semantic relations and includes three different subtasks. the subtasks were designed so as to compare and quantify the effect of different pre-processing steps on the relation classification results. we expect the task to be relevant for a broad range of researchers working on extracting specialized knowledge from domain corpora, for example but not limited to scientific or bio-medical information extraction. the task attracted a total of 32 participants, with 158 submissions across different scenarios."
        },
        {
            "id": "R36010",
            "label": "Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction",
            "doi": "10.18653/v1/d18-1360",
            "research_field": {
                "id": "R278",
                "label": "Information Science"
            },
            "abstract": "we introduce a multi-task setup of identifying entities, relations, and coreference clusters in scientific articles. we create scierc, a dataset that includes annotations for all three tasks and develop a unified framework called sciie with shared span representations. the multi-task setup reduces cascading errors between tasks and leverages cross-sentence relations through coreference links. experiments show that our multi-task model outperforms previous models in scientific information extraction without using any domain-specific features. we further show that the framework supports construction of a scientific knowledge graph, which we use to analyze information in scientific literature."
        },
        {
            "id": "R38016",
            "label": "SMDM: enhancing enterprise-wide master data management using semantic web technologies",
            "doi": "",
            "research_field": {
                "id": "R278",
                "label": "Information Science"
            },
            "abstract": "\"motivated by evolving business requirements and novel enterprise applications, we propose and implement the semantic master data management (smdm), a semantics-level enhancement to the existing mdm solutions. the smdm system publishes relational-based master data as virtual rdf store, and injects instantaneous reasoning capabilities into semantic queries. two kinds of ontologies are introduced to the system, the core mdm ontology and the external imported domain ontology. smdm enables data linking among multi-domains, implicit relationship discovery, and declarative definition and extension of business policies and entities. based on these functions, modern companies can customize their applications and services on demand within the mdm hub. in the demonstration, we build the system environment based on ibm's mdm solution, and run the use cases on the master data of an insurance company.\""
        },
        {
            "id": "R38043",
            "label": "Semantic federation of product information from structured and unstructured sources",
            "doi": "",
            "research_field": {
                "id": "R278",
                "label": "Information Science"
            },
            "abstract": "product-related information can be found in various data sources and formats across the product lifecycle. effectively exploiting this information requires the federation of these sources, the extraction of implicit information, and the efficient access to this comprehensive knowledge base. existing solutions for product information management (pim) are usually restricted to structured information, but most of the business-critical information resides in unstructured documents. we present a generic architecture for federating heterogeneous information from various sources, including the internet of things, and argue how this process benefits from using semantic representations. a reference implementation tailor-made to business users is explained and evaluated. we also discuss several issues we experienced that we believe to be valuable for researchers and implementers of semantic information systems, as well as the information retrieval community."
        },
        {
            "id": "R38049",
            "label": "Explorations in the use of semantic web technologies for product information management",
            "doi": "",
            "research_field": {
                "id": "R278",
                "label": "Information Science"
            },
            "abstract": "master data refers to core business entities a company uses repeatedly across many business processes and systems (such as lists or hierarchies of customers, suppliers, accounts, products, or organizational units). product information is the most important kind of master data and product information management (pim) is becoming critical for modern enterprises because it provides a rich business context for various applications. existing pim systems are less flexible and scalable for on-demand business, as well as too weak to completely capture and use the semantics of master data. this paper explores how to use semantic web technologies to enhance a collaborative pim system by simplifying modeling and representation while preserving enough dynamic flexibility. furthermore, we build a semantic pim system using one of the state-of-art ontology repositories and summarize the challenges we encountered based on our experimental results, especially on performance and scalability. we believe that our study and experiences are valuable for both semantic web community and master data management community."
        },
        {
            "id": "R38066",
            "label": "Ontology-based exchange of product data semantics",
            "doi": "",
            "research_field": {
                "id": "R278",
                "label": "Information Science"
            },
            "abstract": "an increasing trend toward product development in a collaborative environment has resulted in the use of various software tools to enhance the product design. this requires a meaningful representation and exchange of product data semantics across different application domains. this paper proposes an ontology-based framework to enable such semantic interoperability. a standards-based approach is used to develop a product semantic representation language (psrl). formal description logic (daml+oil) is used to encode the psrl. mathematical logic and corresponding reasoning is used to determine semantic equivalences between an application ontology and the psrl. the semantic equivalence matrix enables resolution of ambiguities created due to differences in syntaxes and meanings associated with terminologies in different application domains. successful semantic interoperability will form the basis of seamless communication and thereby enable better integration of product development systems. note to practitioners-semantic interoperability of product information refers to automating the exchange of meaning associated with the data, among information resources throughout the product development. this research is motivated by the problems in enabling such semantic interoperability. first, product information is formalized into an explicit, extensible, and comprehensive product semantics representation language (psrl). the psrl is open and based on standard w3c constructs. next, in order to enable semantic translation, the paper describes a procedure to semi-automatically determine mappings between exactly equivalent concepts across representations of the interacting applications. the paper demonstrates that this approach to translation is feasible, but it has not yet been implemented commercially. current limitations and the directions for further research are discussed. future research addresses the determination of semantic similarities (not exact equivalences) between the interacting information resources."
        },
        {
            "id": "R38074",
            "label": "OntoIMM: An Ontology for Product Intelligent Master Model",
            "doi": "",
            "research_field": {
                "id": "R278",
                "label": "Information Science"
            },
            "abstract": "information organizing principle is one of the key issues of intelligent master model (imm), which is an enhancement of the master model (mm) based on kbe (knowledge-based engineering). despite the fact that the core product model (cpm) has been confirmed to be an organizing mechanism for product master model, the key issue of supporting the information organizing for imm is not yet well addressed, mainly due to the following two reasons; (1) lack of representation of complete information and knowledge with regard to product and process, including the know-why, know-how, and know-what information and knowledge, and (2) lack of semantic richness. therefore, a multiaspect extension to cpm was first defined, and then an ontology was constructed to represent the information and design knowledge. the extension refers to adding a design process model, context model, product control structure model, and design rationale model to cpm concerning the enhancement of master model, which is to comprehensively represent the reason, process, and result information and knowledge of theproduct. the ontology construction refers to representing the concepts, relationships among these concepts and consistency rules of imm information structure. finally, an example of barrel design and analysis process is illustrated to verify the effectiveness of proposed method."
        },
        {
            "id": "R38493",
            "label": "Current Challenges for Studying Search as Learning Processes",
            "doi": "10.2352/issn.2470-1173.2017.6.mobmu-302",
            "research_field": {
                "id": "R278",
                "label": "Information Science"
            },
            "abstract": "search of resources and information is among the most frequent activities on the web. while established information retrieval approaches address the relevance of search results to an information need, the actual learning scope of a user is normally disregarded. recent research in the search as learning (sal) area has recognized the importance of learning scopes and focused on observing and detecting learning needs. the article at hand takes a critical look at existing works in sal and related research disciplines. it aims to give a concise, interdisplinary overview which allows for the deduction of possible directions and necessary actions for prospective research works. it becomes apparent that past research employs a strong emphasis on textual resources, neglecting the diversity of online multimedia contents for learning and the impact of multimodal features on the learning process. we argue that exploring multimodal learning resources should be one focus of future sal projects."
        },
        {
            "id": "R38544",
            "label": "Estimating relative depth in single images via rankboost",
            "doi": "10.1109/icme.2017.8019434",
            "research_field": {
                "id": "R278",
                "label": "Information Science"
            },
            "abstract": "in this paper, we present a novel approach to estimate the relative depth of regions in monocular images. there are several contributions. first, the task of monocular depth estimation is considered as a learning-to-rank problem which offers several advantages compared to regression approaches. second, monocular depth clues of human perception are modeled in a systematic manner. third, we show that these depth clues can be modeled and integrated appropriately in a rankboost framework. for this purpose, a space-efficient version of rankboost is derived that makes it applicable to rank a large number of objects, as posed by the given problem. finally, the monocular depth clues are combined with results from a deep learning approach. experimental results show that the error rate is reduced by adding the monocular features while outperforming state-of-the-art systems."
        },
        {
            "id": "R171846",
            "label": "Investigation of the material combination 20MnCr5 and X45CrSi9-3 in the Tailored Forming of shafts with bearing seats",
            "doi": "10.1007/s11740-022-01119-w",
            "research_field": {
                "id": "R137654",
                "label": "Mechanical Process Engineering"
            },
            "abstract": "abstract the tailored forming process chain is used to manufacture hybrid components and consists of a joining process or additive manufacturing for various materials (e.g. deposition welding), subsequent hot forming, machining and heat treatment. in this way, components can be produced with materials adapted to the load case. for this paper, hybrid shafts are produced by deposition welding of a cladding made of x45crsi9-3 onto a workpiece made from 20mncr5. the hybrid shafts are then formed by means of cross-wedge rolling. it is investigated, how the thickness of the cladding and the type of cooling after hot forming (in air or in water) affect the properties of the cladding. the hybrid shafts are formed without layer separation. however, slight core loosening occurres in the area of the bearing seat due to the mannesmann effect. the microhardness of the cladding is only slightly effected by the cooling strategy, while the microhardness of the base material is significantly higher in water cooled shafts. the microstructure of the cladding after both cooling strategies consists mainly of martensite. in the base material, air cooling results in a mainly ferritic microstructure with grains of ferrite-pearlite. quenching in water results in a microstructure containing mainly martensite."
        },
        {
            "id": "R145720",
            "label": "Investigations on Tailored Forming of AISI 52100 as Rolling Bearing Raceway",
            "doi": "10.3390/met10101363",
            "research_field": {
                "id": "R137654",
                "label": "Mechanical Process Engineering"
            },
            "abstract": "hybrid cylindrical roller thrust bearing washers of type 81212 were manufactured by tailored forming. an aisi 1022m base material, featuring a sufficient strength for structural loads, was cladded with the bearing steel aisi 52100 by plasma transferred arc welding (pta). though aisi 52100 is generally regarded as non-weldable, it could be applied as a cladding material by adjusting pta parameters. the cladded parts were investigated after each individual process step and subsequently tested under rolling contact load. welding defects that could not be completely eliminated by the subsequent hot forming were characterized by means of scanning acoustic microscopy and micrographs. below the surface, pores with a typical size of ten \u00b5m were found to a depth of about 0.45 mm. in the material transition zone and between individual weld seams, larger voids were observed. grinding of the surface after heat treatment caused compressive residual stresses near the surface with a relatively small depth. fatigue tests were carried out on an fe8 test rig. eighty-two percent of the calculated rating life for conventional bearings was achieved. a high failure slope of the weibull regression was determined. a relationship between the weld defects and the fatigue behavior is likely."
        },
        {
            "id": "R145729",
            "label": "Manufacturing and Evaluation of Multi-Material Axial-Bearing Washers by Tailored Forming",
            "doi": "10.3390/met9020232",
            "research_field": {
                "id": "R137654",
                "label": "Mechanical Process Engineering"
            },
            "abstract": "components subject to rolling contact fatigue, such as gears and rolling bearings, are among the fundamental machine elements in mechanical and vehicle engineering. rolling bearings are generally not designed to be fatigue-resistant, as the necessary oversizing is not technically and economically marketable. in order to improve the load-bearing capacity, resource efficiency and application possibilities of rolling bearings and other possible multi-material solid components, a new process chain was developed at leibniz university hannover as a part of the collaborative research centre 1153 \u201ctailored forming\u201d. semi-finished products, already joined before the forming process, are used here to allow a further optimisation of joint quality by forming and finishing. in this paper, a plasma-powder-deposition welding process is presented, which enables precise material deposition and control of the welding depth. for this study, bearing washers (serving as rolling bearing raceways) of a cylindrical roller thrust bearing, similar to type 81212 with a multi-layer structure, were manufactured. a previously non-weldable high-performance material, steel aisi 5140, was used as the cladding layer. depending on the degree of forming, grain-refinement within the welded material was achieved by thermo-mechanical treatment of the joining zone during the forming process. this grain-refinements lead to an improvement of the mechanical properties and thus, to a higher lifetime for washers of an axial cylindrical roller bearing, which were examined as an exemplary component on a fatigue test bench. to evaluate the bearing washers, the results of the bearing tests were compared with industrial bearings and deposition welded axial-bearing washers without subsequent forming. in addition, the bearing washers were analysed micro-tribologically and by scanning acoustic microscopy both after welding and after the forming process. nano-scratch tests were carried out on the bearing washers to analyse the layer properties. together with the results of additional microscopic images of the surface and cross-sections, the causes of failure due to fatigue and wear were identified."
        },
        {
            "id": "R145732",
            "label": "Tribological Study on Tailored-Formed Axial Bearing Washers",
            "doi": "10.2474/trol.13.320",
            "research_field": {
                "id": "R137654",
                "label": "Mechanical Process Engineering"
            },
            "abstract": "to enhance tribological contacts under cyclic load, high performance materials are required. utilizing the same high-strength material for the whole machine element is not resource-efficient. in order to manufacture machine elements with extended functionality and specific properties, a combination of different materials can be used in a single component for a more efficient material utilization. by combining different joining techniques with subsequent forming, multi-material or tailored components can be manufactured. to reduce material costs and energy consumption during the component service life, a less expensive lightweight material should be used for regions remote from the highly stressed zones. the scope is not only to obtain the desired shape and dimensions for the finishing process, but also to improve properties like the bond strength between different materials and the microscopic structure of the material. the multi-material approach can be applied to all components requiring different properties in separate component regions such as shafts, bearings or bushes. the current study exemplarily presents the process route for the production of an axial bearing washer by means of tailored forming technology. the bearing washers were chosen to fit axial roller bearings (type 81212). the manufacturing process starts with the laser wire cladding of a hard facing made of martensitic chromium silicon steel (1.4718) on a base substrate of s235 (1.0038) steel. subsequently, the bearing washers are forged. after finishing, the surfaces of the bearing washers were tested in thrust bearings on an fe-8 test rig. the operational test of the bearings consists in a run-in phase at 250 rpm. a bearing failure is determined by a condition monitoring system. before and after this, the bearings were inspected by optical and ultrasonic microscopy in order to examine whether the bond of the coat is resistant against rolling contact fatigue. the feasibility of the approach could be proven by endurance test. the joining zone was able to withstand the rolling contact stresses and the bearing failed due to material-induced fatigue with high cycle stability."
        },
        {
            "id": "R162731",
            "label": "Cross-wedge rolling of PTA-welded hybrid steel billets with rolling bearing steel and hard material coatings",
            "doi": "10.1063/1.5112553",
            "research_field": {
                "id": "R137654",
                "label": "Mechanical Process Engineering"
            },
            "abstract": "within the collaborative research centre 1153 \u201ctailored forming\u201c a process chain for the manufacturing of hybrid high performance components is developed. exemplary process steps consist of deposit welding of high performance steel on low-cost steel, pre-shaping by cross-wedge rolling and finishing by milling.hard material coatings such as stellite 6 or delcrome 253 are used as wear or corrosion protection coatings in industrial applications. scientists of the institute of material science welded these hard material alloys onto a base material, in this case c22.8, to create a hybrid workpiece. scientists of the institut fur integrierte produktion hannover have shown that these hybrid workpieces can be formed without defects (e.g. detachment of the coating) by cross-wedge rolling. after forming, the properties of the coatings are retained or in some cases even improved (e.g. the transition zone between base material and coating). by adjustments in the welding process, it was possible to apply the 100cr6 rolling bearing steel, as of now declared as non-weldable, on the low-cost steel c22.8. 100cr6 was formed afterwards in its hybrid bonding state with c22.8 by cross-wedge rolling, thus a component-integrated bearing seat was produced. even after welding and forming, the rolling bearing steel coating could still be quench-hardened to a hardness of over 60 hrc. this paper shows the potential of forming hybrid billets to tailored parts. since industrially available standard materials can be used for hard material coatings by this approach, even though they are not weldable by conventional methods, it is not necessary to use expensive, for welding designed materials to implement a hybrid component concept.within the collaborative research centre 1153 \u201ctailored forming\u201c a process chain for the manufacturing of hybrid high performance components is developed. exemplary process steps consist of deposit welding of high performance steel on low-cost steel, pre-shaping by cross-wedge rolling and finishing by milling.hard material coatings such as stellite 6 or delcrome 253 are used as wear or corrosion protection coatings in industrial applications. scientists of the institute of material science welded these hard material alloys onto a base material, in this case c22.8, to create a hybrid workpiece. scientists of the institut fur integrierte produktion hannover have shown that these hybrid workpieces can be formed without defects (e.g. detachment of the coating) by cross-wedge rolling. after forming, the properties of the coatings are retained or in some cases even improved (e.g. the transition zone between base material and coating). by adjustments in the welding process, it was possible to apply the 100cr6 ro..."
        },
        {
            "id": "R139109",
            "label": "Cold Atmospheric Pressure Plasma VUV Interactions With Surfaces: Effect of Local Gas Environment and Source Design",
            "doi": "10.1002/ppap.201600043",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            },
            "abstract": "this study uses photoresist materials in combination with several optical filters as a diagnostic to examine the relative importance of vuv-induced surface modifications for different cold atmospheric pressure plasma (capp) sources. the argon fed khz-driven ring-appj showed the largest ratio of vuv surface modification relative to the total modification introduced, whereas the mhz appj showed the largest overall surface modification. the mhz appj shows increased total thickness reduction and reduced vuv effect as oxygen is added to the feed gas, a condition that is often used for practical applications. we examine the influence of noble gas flow from the appj on the local environment. the local environment has a decisive impact on polymer modification from vuv emission as o2 readily absorbs vuv photons."
        },
        {
            "id": "R139065",
            "label": "Etching materials with an atmospheric-pressure plasma jet",
            "doi": "10.1088/0963-0252/7/3/005",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            },
            "abstract": "a plasma jet has been developed for etching materials at atmospheric pressure and between 100 and c. gas mixtures containing helium, oxygen and carbon tetrafluoride were passed between an outer, grounded electrode and a centre electrode, which was driven by 13.56 mhz radio frequency power at 50 to 500 w. at a flow rate of , a stable, arc-free discharge was produced. this discharge extended out through a nozzle at the end of the electrodes, forming a plasma jet. materials placed 0.5 cm downstream from the nozzle were etched at the following maximum rates: for kapton ( and he only), for silicon dioxide, for tantalum and for tungsten. optical emission spectroscopy was used to identify the electronically excited species inside the plasma and outside in the jet effluent."
        },
        {
            "id": "R139068",
            "label": "An atmospheric pressure plasma source",
            "doi": "10.1063/1.125724",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            },
            "abstract": "an atmospheric pressure plasma source operated by radio frequency power has been developed. this source produces a unique discharge that is volumetric and homogeneous at atmospheric pressure with a gas temperature below 300\\u200a\u00b0c. it also produces a large quantity of oxygen atoms, \u223c5\u00d71015\\u200acm\u22123, which has important value for materials applications. a theoretical model shows electron densities of 0.2\u20132\u00d71011\\u200acm\u22123 and characteristic electron energies of 2\u20134 ev for helium discharges at a power level of 3\u201330 w\\u200acm\u22123."
        },
        {
            "id": "R139071",
            "label": "On the Vacuum Ultraviolet Radiation of a Miniaturized Non-thermal Atmospheric Pressure Plasma Jet",
            "doi": "10.1002/ppap.200731207",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            },
            "abstract": "the suitability of a miniaturized non-thermal appj operating with ar at ambient atmosphere for applications related to surface treatment is demonstrated. the vuv emission is measured and the dependence of selected line intensities over the radius of the plasma jet is presented. the ar discharge is characterized by an intense vuv radiation, attributed to n, h, and o atomic lines along with an ar2* excimer continuum, which is drastically reduced after adding up to 5% n2 to the ar working gas. two absorption dips are found in the vuv spectrum. the surface energy enhancement of substrates at temperatures as low as 35\\u2009\u00b0c along with chemical reactivity originating from abundant no and oh free radicals and uv/vuv radiation in the plasma give rise to numerous applications, e.g., in the medical and biological field."
        },
        {
            "id": "R139074",
            "label": "RF Capillary Jet - a Tool for Localized Surface Treatment",
            "doi": "10.1002/ctpp.200710017",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            },
            "abstract": "the uv/vuv spectrum of a non\u2010thermal capillary plasma jet operating with ar at ambient atmosphere and the temperature load of a substrate exposed to the jet have been measured. the vuv radiation is assigned to n, h, and o atomic lines along with an ar*2 excimer continuum. the absolute radiance (115\u2010200 nm) of the source has been determined. maximum values of 880 \u03bcw/mm2sr are obtained. substrate temperatures range between 35 \u00b0c for low powers and high gas flow conditions and 95 \u00b0c for high powers and reduced gas flow. the plasma source (13.56, 27.12 or 40.78 mhz) can be operated in ar and in n2. the further addition of a low percentage of silicon containing reactive admixtures has been demonstrated for thin film deposition. several further applications related to surface modification have been successfully applied. (\u00a9 2007 wiley\u2010vch verlag gmbh & co. kgaa, weinheim)"
        },
        {
            "id": "R139077",
            "label": "Spatially resolved diagnostics on a microscale atmospheric pressure plasma jet",
            "doi": "10.1088/0022-3727/41/19/194004",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            },
            "abstract": "despite enormous potential for technological applications, fundamentals of stable non-equilibrium micro-plasmas at ambient pressure are still only partly understood. micro-plasma jets are one sub-group of these plasma sources. for an understanding it is particularly important to analyse transport phenomena of energy and particles within and between the core and effluent of the discharge. the complexity of the problem requires the combination and correlation of various highly sophisticated diagnostics yielding different information with an extremely high temporal and spatial resolution. a specially designed rf microscale atmospheric pressure plasma jet (\u03bc-appj) provides excellent access for optical diagnostics to the discharge volume and the effluent region. this allows detailed investigations of the discharge dynamics and energy transport mechanisms from the discharge to the effluent. here we present examples for diagnostics applicable to different regions and combine the results. the diagnostics applied are optical emission spectroscopy (oes) in the visible and ultraviolet and two-photon absorption laser-induced fluorescence spectroscopy. by the latter spatially resolved absolutely calibrated density maps of atomic oxygen have been determined for the effluent. oes yields an insight into energy transport mechanisms from the core into the effluent. the first results of spatially and phase-resolved oes measurements of the discharge dynamics of the core are presented."
        },
        {
            "id": "R139080",
            "label": "Diagnostics on an atmospheric pressure plasma jet",
            "doi": "10.1088/1742-6596/71/1/012012",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            },
            "abstract": "the atmospheric pressure plasma jet (appj) is a homogeneous non-equilibrium discharge at ambient pressure. it operates with a noble base gas and a percentage-volume admixture of a molecular gas. applications of the discharge are mainly based on reactive species in the effluent. the effluent region of a discharge operated in helium with an oxygen admixture has been investigated. the optical emission from atomic oxygen decreases with distance from the discharge but can still be observed several centimetres in the effluent. ground state atomic oxygen, measured using absolutely calibrated two-photon laser induced fluorescence spectroscopy, shows a similar behaviour. detailed understanding of energy transport mechanisms requires investigations of the discharge volume and the effluent region. an atmospheric pressure plasma jet has been designed providing excellent diagnostics access and a simple geometry ideally suited for modelling and simulation. laser spectroscopy and optical emission spectroscopy can be applied in the discharge volume and the effluent region."
        },
        {
            "id": "R139083",
            "label": "Vacuum UV Radiation of a Plasma Jet Operated With Rare Gases at Atmospheric Pressure",
            "doi": "10.1109/TPS.2009.2019982",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            },
            "abstract": "the vacuum ultraviolet (vuv) emissions from 115 to 200 nm from the effluent of an rf (1.2 mhz) capillary jet fed with pure argon and binary mixtures of argon and xenon or krypton (up to 20%) are analyzed. the feed gas mixture is emanating into air at normal pressure. the ar2 excimer second continuum, observed in the region of 120-135 nm, prevails in the pure ar discharge. it decreases when small amounts (as low as 0.5%) of xe or kr are added. in that case, the resonant emission of xe at 147 nm (or 124 nm for kr, respectively) becomes dominant. the xe2 second continuum at 172 nm appears for higher admixtures of xe (10%). furthermore, several n i emission lines, the o i resonance line, and h i line appear due to ambient air. two absorption bands (120.6 and 124.6 nm) are present in the spectra. their origin could be unequivocally associated to o2 and o3. the radiance is determined end-on at varying axial distance in absolute units for various mixtures of ar/xe and ar/kr and compared to pure ar. integration over the entire vuv wavelength region provides the integrated spectral distribution. maximum values of 2.2 mw middotmm-2middotsr-1 are attained in pure ar and at a distance of 4 mm from the outlet nozzle of the discharge. by adding diminutive admixtures of kr or xe, the intensity and spectral distribution is effectively changed."
        },
        {
            "id": "R139086",
            "label": "Generation of atomic oxygen in the effluent of an atmospheric pressure plasma jet",
            "doi": "10.1088/0963-0252/18/1/015006",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            },
            "abstract": "\"the planar 13.56\\u2009mhz rf-excited low temperature atmospheric pressure plasma jet (appj) investigated in this study is operated with helium feed gas and a small molecular oxygen admixture. the effluent leaving the discharge through the jet's nozzle contains very few charged particles and a high reactive oxygen species' density. as its main reactive radical, essential for numerous applications, the ground state atomic oxygen density in the appj's effluent is measured spatially resolved with two-photon absorption laser induced fluorescence spectroscopy. the atomic oxygen density at the nozzle reaches a value of \u223c1016\\u2009cm\u22123. even at several centimetres distance still 1% of this initial atomic oxygen density can be detected. optical emission spectroscopy (oes) reveals the presence of short living excited oxygen atoms up to 10\\u2009cm distance from the jet's nozzle. the measured high ground state atomic oxygen density and the unaccounted for presence of excited atomic oxygen require further investigations on a possible energy transfer from the appj's discharge region into the effluent: energetic vacuum ultraviolet radiation, measured by oes down to 110\\u2009nm, reaches far into the effluent where it is presumed to be responsible for the generation of atomic oxygen.4 this study forms part of: s reuter 2007 formation mechanisms of atomic oxygen in an atmospheric pressure plasma jet characterised by spectroscopic methods dissertation duisburg-essen university and was presented at the 59th gaseous electronics conference (gec) (columbus, oh).\""
        },
        {
            "id": "R139089",
            "label": "The dynamics of radio-frequency driven atmospheric pressure plasma jets",
            "doi": "10.1088/1742-6596/162/1/012013",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            },
            "abstract": "the complex dynamics of radio-frequency driven atmospheric pressure plasma jets is investigated using various optical diagnostic techniques and numerical simulations. absolute number densities of ground state atomic oxygen radicals in the plasma effluent are measured by two-photon absorption laser induced fluorescence spectroscopy (talif). spatial profiles are compared with (vacuum) ultra-violet radiation from excited states of atomic oxygen and molecular oxygen, respectively. the excitation and ionization dynamics in the plasma core are dominated by electron impact and observed by space and phase resolved optical emission spectroscopy (proes). the electron dynamics is governed through the motion of the plasma boundary sheaths in front of the electrodes as illustrated in numerical simulations using a hybrid code based on fluid equations and kinetic treatment of electrons."
        },
        {
            "id": "R139094",
            "label": "Characterization of transient discharges under atmospheric-pressure conditions applying nitrogen photoemission and current measurements",
            "doi": "10.1088/0022-3727/45/12/125202",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            },
            "abstract": "the plasma parameters such as electron distribution function and electron density of three atmospheric-pressure transient discharges namely filamentary and homogeneous dielectric barrier discharges in air, and the spark discharge of an argon plasma coagulation (apc) system are determined. a combination of numerical simulation as well as diagnostic methods including current measurement and optical emission spectroscopy (oes) based on nitrogen emissions is used. the applied methods supplement each other and resolve problems, which arise when these methods are used individually. nitrogen is used as a sensor gas and is admixed in low amount to argon for characterizing the apc discharge. both direct and stepwise electron-impact excitation of nitrogen emissions are included in the plasma-chemical model applied for characterization of these transient discharges using oes where ambiguity arises in the determination of plasma parameters under specific discharge conditions. it is shown that the measured current solves this problem by providing additional information useful for the determination of discharge-specific plasma parameters."
        },
        {
            "id": "R139097",
            "label": "Absolute atomic oxygen and nitrogen densities in radio-frequency driven atmospheric pressure cold plasmas: Synchrotron vacuum ultra-violet high-resolution Fourier-transform absorption measurements",
            "doi": "10.1063/1.4813817",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            },
            "abstract": "reactive atomic species play a key role in emerging cold atmospheric pressure plasma applications, in particular, in plasma medicine. absolute densities of atomic oxygen and atomic nitrogen were measured in a radio-frequency driven non-equilibrium plasma operated at atmospheric pressure using vacuum ultra-violet (vuv) absorption spectroscopy. the experiment was conducted on the desirs synchrotron beamline using a unique vuv fourier-transform spectrometer. measurements were carried out in plasmas operated in helium with air-like n2/o2 (4:1) admixtures. a maximum in the o-atom concentration of (9.1\\u2009\u00b1\\u20090.7)\u00d71020\\u2009m\u22123 was found at admixtures of 0.35\\u2009vol.\\u2009%, while the n-atom concentration exhibits a maximum of (5.7\\u2009\u00b1\\u20090.4)\u00d71019\\u2009m\u22123 at 0.1\\u2009vol.\\u2009%."
        },
        {
            "id": "R139100",
            "label": "Impact of plasma jet vacuum ultraviolet radiation on reactive oxygen species generation in bio-relevant liquids",
            "doi": "10.1063/1.4934989",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            },
            "abstract": "plasma medicine utilizes the combined interaction of plasma produced reactive components. these are reactive atoms, molecules, ions, metastable species, and radiation. here, ultraviolet (uv, 100\u2013400\\u2009nm) and, in particular, vacuum ultraviolet (vuv, 10\u2013200\\u2009nm) radiation generated by an atmospheric pressure argon plasma jet were investigated regarding plasma emission, absorption in a humidified atmosphere and in solutions relevant for plasma medicine. the energy absorption was obtained for simple solutions like distilled water (dh2o) or ultrapure water and sodium chloride (nacl) solution as well as for more complex ones, for example, rosewell park memorial institute (rpmi 1640) cell culture media. as moderate stable reactive oxygen species, hydrogen peroxide (h2o2) was studied. highly reactive oxygen radicals, namely, superoxide anion (o2\u2022\u2212) and hydroxyl radicals (\u2022oh), were investigated by the use of electron paramagnetic resonance spectroscopy. all species amounts were detected for three different treatmen..."
        },
        {
            "id": "R139103",
            "label": "Summarizing results on the performance of a selective set of atmospheric plasma jets for separation of photons and reactive particles",
            "doi": "10.1088/0022-3727/48/44/444001",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            },
            "abstract": "a microscale atmospheric-pressure plasma jet is a remote plasma jet, where plasma-generated reactive particles and photons are involved in substrate treatment. here, we summarize our efforts to develop and characterize a particle- or photon-selective set of otherwise identical jets. in that way, the reactive species or photons can be used separately or in combination to study their isolated or combined effects to test whether the effects are additive or synergistic. the final version of the set of three jets\u2014particle-jet, photon-jet and combined jet\u2014is introduced. this final set realizes the highest reproducibility of the photon and particle fluxes, avoids turbulent gas flow, and the fluxes of the selected plasma-emitted components are almost identical in the case of all jets, while the other component is effectively blocked, which was verified by optical emission spectroscopy and mass spectrometry. schlieren-imaging and a fluid dynamics simulation show the stability of the gas flow. the performance of these selective jets is demonstrated with the example of the treatment of e. coli bacteria with the different components emitted by a he-only, a he/n2 and a he/o2 plasma. additionally, measurements of the vacuum uv photon spectra down to the wavelength of 50\\u2009nm can be made with the photon-jet and the relative comparison of spectral intensities among different gas mixtures is reported here. the results will show that the vacuum uv photons can lead to the inactivation of the e.coli bacteria."
        },
        {
            "id": "R139106",
            "label": "Concepts and characteristics of the \u2018COST Reference Microplasma Jet\u2019",
            "doi": "10.1088/0022-3727/49/8/084003",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            },
            "abstract": "biomedical applications of non-equilibrium atmospheric pressure plasmas have attracted intense interest in the past few years. many plasma sources of diverse design have been proposed for these applications, but the relationship between source characteristics and application performance is not well-understood, and indeed many sources are poorly characterized. this circumstance is an impediment to progress in application development. a reference source with well-understood and highly reproducible characteristics may be an important tool in this context. researchers around the world should be able to compare the characteristics of their own sources and also their results with this device. in this paper, we describe such a reference source, developed from the simple and robust micro-scaled atmospheric pressure plasma jet (\u03bc-appj) concept. this development occurred under the auspices of cost action mp1101 \u2018biomedical applications of atmospheric pressure plasmas\u2019. gas contamination and power measurement are shown to be major causes of irreproducible results in earlier source designs. these problems are resolved in the reference source by refinement of the mechanical and electrical design and by specifying an operating protocol. these measures are shown to be absolutely necessary for reproducible operation. they include the integration of current and voltage probes into the jet. the usual combination of matching unit and power supply is replaced by an integrated lc power coupling circuit and a 5\\u2009w single frequency generator. the design specification and operating protocol for the reference source are being made freely available."
        },
        {
            "id": "R139112",
            "label": "Absolute ozone densities in a radio-frequency driven atmospheric pressure plasma using two-beam UV-LED absorption spectroscopy and numerical simulations",
            "doi": "10.1088/1361-6595/aa8ebb",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            },
            "abstract": "the efficient generation of reactive oxygen species (ros) in cold atmospheric pressure plasma jets (appjs) is an increasingly important topic, e.g. for the treatment of temperature sensitive biological samples in the field of plasma medicine. a 13.56 mhz radio-frequency (rf) driven appj device operated with helium feed gas and small admixtures of oxygen (up to 1%), generating a homogeneous glow-mode plasma at low gas temperatures, was investigated. absolute densities of ozone, one of the most prominent ros, were measured across the 11 mm wide discharge channel by means of broadband absorption spectroscopy using the hartley band centred at \u03bb = 255 nm. a two-beam setup with a reference beam in mach\u2013zehnder configuration is employed for improved signal-to-noise ratio allowing high-sensitivity measurements in the investigated single-pass weak-absorbance regime. the results are correlated to gas temperature measurements, deduced from the rotational temperature of the n2 (c 3 \u03c0 u + \u2192 b 3 \u03c0 g + , \u03c5 = 0 \u2192 2) optical emission from introduced air impurities. the observed opposing trends of both quantities as a function of rf power input and oxygen admixture are analysed and explained in terms of a zero-dimensional plasma-chemical kinetics simulation. it is found that the gas temperature as well as the densities of o and o2(b 1 \u03c3 g + ) influence the absolute o3 densities when the rf power is varied."
        },
        {
            "id": "R139115",
            "label": "Chemical kinetics in an atmospheric pressure helium plasma containing humidity",
            "doi": "10.1039/c8cp02473a",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            },
            "abstract": "investigating the formation and kinetics of o and oh in a he\u2013h 2 o plasma jet using absorption spectroscopy and 0d modelling."
        },
        {
            "id": "R139118",
            "label": "Determination of NO densities in a surface dielectric barrier discharge using optical emission spectroscopy",
            "doi": "10.1063/1.5094894",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            },
            "abstract": "a new computationally assisted diagnostic to measure no densities in atmospheric-pressure microplasmas by optical emission spectroscopy (oes) is developed and validated against absorption spectroscopy in a volume dielectric barrier discharge (dbd). the oes method is then applied to a twin surface dbd operated in n 2 to measure the no density as a function of the o 2 admixture ( 0.1%\u2013 1%). the underlying rate equation model reveals that no ( a 2 \u03c3 + ) is primarily excited by reactions of the ground state no ( x 2 \u03c0 ) with metastables n 2 ( a 3 \u03c3 u + ).a new computationally assisted diagnostic to measure no densities in atmospheric-pressure microplasmas by optical emission spectroscopy (oes) is developed and validated against absorption spectroscopy in a volume dielectric barrier discharge (dbd). the oes method is then applied to a twin surface dbd operated in n 2 to measure the no density as a function of the o 2 admixture ( 0.1%\u2013 1%). the underlying rate equation model reveals that no ( a 2 \u03c3 + ) is primarily excited by reactions of the ground state no ( x 2 \u03c0 ) with metastables n 2 ( a 3 \u03c3 u + )."
        },
        {
            "id": "R139124",
            "label": "Comparison of electron heating and energy loss mechanisms in an RF plasma jet operated in argon and helium",
            "doi": "10.1088/1361-6595/ab6c81",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            },
            "abstract": "the \u03bc-appj is a well-investigated atmospheric pressure rf plasma jet. up to now, it has mainly been operated using helium as feed gas due to stability restrictions. however, the cost-jet design including precise electrical probes now offers the stability and reproducibility to create equi-operational plasmas in helium as well as in argon. in this publication, we compare fundamental plasma parameters and physical processes inside the cost reference microplasma jet, a capacitively coupled rf atmospheric pressure plasma jet, under operation in argon and in helium. differences already observable by the naked eye are reflected in differences in the power-voltage characteristic for both gases. using an electrical model and a power balance, we calculated the electron density and temperature at 0.6 w to be 9 \u00d7 10 17 m \u2212 3 , 1.2 ev and 7.8 \u00d7 10 16 m \u2212 3 , 1.7 ev for argon and helium, respectively. in case of helium, a considerable part of the discharge power is dissipated in elastic electron-atom collisions, while for argon most of the input power is used for ionization. phase-resolved optical emission spectroscopy reveals differently pronounced heating mechanisms. whereas bulk heating is more prominent in argon compared to helium, the opposite trend is observed for sheath heating. this also explains the different behavior observed in the power-voltage characteristics."
        },
        {
            "id": "R139130",
            "label": "Atmospheric plasma VUV photon emission",
            "doi": "10.1088/1361-6595/ab8e4d",
            "research_field": {
                "id": "R185",
                "label": "Plasma and Beam Physics"
            },
            "abstract": "owing to its distinctive photon energy range, vacuum ultraviolet (vuv) emission plays a key role in diverse photo-induced natural and technological processes. atmospheric-pressure plasma produced vuv is central to resolve long-held issues in dynamics of natural (e.g., lightning) and laboratory (e.g., streamer) plasmas. challenging the seemingly unavoidable vacuum systems used to prevent vuv emission quenching by ambient gases, here we report the first observation of vacuum-free generation of stable sub-110 nm vuv emission from atmospheric-pressure plasmas jetted into open air and atmospheric air plasma. emission from atomic helium at 58.4 nm is observed from a nonequilibrium atmospheric pressure plasma jet (n-appj), jetted directly into ambient air. in a similar experiment, we also report vuv emission from excited nitrogen species in an atmospheric pressure discharge in ambient air. the photon emissions detected expand the window of photo-induced processes beyond \u223c10 ev commonly achievable by existing non-excimer vuv plasma sources, and enables direct photo-excitation and ionization of molecular species such as co2 and many others. the thus-enabled direct photoionization of o2, o, and n species further justifies the role of direct photoionization in the dynamics of natural and laboratory atmospheric-pressure plasmas and informs the development of the relevant plasma photoionization models, which currently largely sidestep the sub-110 nm domain. these findings can make contribution to the complement of photoionization model of lightning, streamer, and other plasmas, open new avenues to quantify the yet elusive role of photoionization in the plasma dynamics."
        },
        {
            "id": "R110941",
            "label": "Microwave-Assisted Cobinamide Synthesis",
            "doi": "10.1021/jo501364b",
            "research_field": {
                "id": "R129",
                "label": "Organic Chemistry"
            },
            "abstract": "we present a new method for the preparation of cobinamide (cn)2cbi, a vitamin b12 precursor, that should allow its broader utility. treatment of vitamin b12 with only nacn and heating in a microwave reactor affords (cn)2cbi as the sole product. the purification procedure was greatly simplified, allowing for easy isolation of the product in 94% yield. the use of microwave heating proved beneficial also for (cn)2cbi(c-lactone) synthesis. treatment of (cn)2cbi with triethanolamine led to (cn)2cbi(c-lactam)."
        },
        {
            "id": "R111072",
            "label": "One-step synthesis of \u03b1/\u03b2 cyano-aqua cobinamides from vitamin B12 with Zn(II) or Cu(II) salts in methanol",
            "doi": "10.1142/s1088424611003446",
            "research_field": {
                "id": "R129",
                "label": "Organic Chemistry"
            },
            "abstract": "this short communication describes the screening of various metal salts for the preparation of cyano-aqua cobinamides from vitamin b12 in methanol. zncl 2 and cu(no 3 ) 2 \u00b73h 2 o have been identified as most active for this purpose and represent useful alternatives to the widely applied ce(iii) method that requires excess cyanide."
        },
        {
            "id": "R111101",
            "label": "Preparation of Dicyano- and\nMethylcobinamide from Vitamin B12a",
            "doi": "10.1055/s-0028-1083187",
            "research_field": {
                "id": "R129",
                "label": "Organic Chemistry"
            },
            "abstract": "treatment of vitamin b 12a 1 (hydroxycobalamin hydrochloride, aquocobalamin) with nabh 4 and zncl 2 leads to the selective cleavage of the nucleotide loop and gives dicyanocobinamide 2a in good yield. methylcobinamide 4 was prepared from 2 via aquocyanocobinamide 3. the glutathione-mediated methylation of 3 in a ph 3.5 buffer solution proceeded with mel, but not with meots."
        },
        {
            "id": "R111111",
            "label": "Beitr\u00e4ge zur Chemie und Biochemie der \u201eCobalamine\u201d, II. Mitteil.: \u00dcber den Abbau der \u201eCobalamine\u201d mit Cer (III)-hydroxyd. 7-[D-Ribofuranosido]-adenin, ein Abbauprodukt des Pseudovitamins B12",
            "doi": "10.1002/cber.19560891104",
            "research_field": {
                "id": "R129",
                "label": "Organic Chemistry"
            },
            "abstract": "unter der katalytischen wirkung des cer(iii)-hydroxyds in wasrigem medium bei 95\u00b0 und neutralem ph werden die meisten vitamine der b12-gruppe rasch zu atiocobalamin, nucleosid und phosphorsaure abgebaut. ein zusatz von cn\u2296 beschleunigt den abbau und beseitigt weitgehend die unterschiede in der abbaugeschwindigkeit der einzelnen b12-arten. der cer-abbau ist eine sehr schonende methode zur gewinnung von reinstem atiocobalamin und vor allem von schwer zuganglichen nucleosiden der b12-faktoren. das nucleosid des pseudovitamins b12 wird in kristallisiertem zustand gewonnen und als 7-[d-ribofuranosido]-adenin charakterisiert."
        },
        {
            "id": "R137059",
            "label": "Nickel-Catalyzed Cross-Coupling of Aryl Methyl Ethers with Aryl Boronic Esters",
            "doi": "10.1002/anie.200801447",
            "research_field": {
                "id": "R129",
                "label": "Organic Chemistry"
            },
            "abstract": "palladiumand nickel-catalyzed cross-coupling reactions have been recognized as an indispensable tool for current organic synthesis. among these reactions, suzuki\u2013miyaura coupling is, arguably, of the greatest practical importance of these methods because of the attractive features of organoboronic acids: widespread availability, stability to air and moisture, and low toxicity. recently, tremendous progress has been made in the development of more elaborate catalyst systems that allow the couplings to be conducted at room temperature, to use unreactive chlorides, and to use alkyl electrophiles. despite these significant advances, the electrophilic coupling partner for use in suzuki\u2013miyaura coupling remains limited, for the most part, to organic halides and sulfonates; although the use of less available electrophiles, including diazonium salts, ammonium salts, aryltriazene/bf3, [7c] azoles, and phosphonium salts has been reported. aryl methyl ethers, which are as readily available as aryl halides, have never been used in the suzuki\u2013miyaura coupling reaction, except for the ruthenium-catalyzed system, which requires a ligating group at the ortho position for the reaction to proceed. herein, we describe a method for the nickel-catalyzed cross-coupling of aryl methyl ethers with boronic esters [eq. (1)]. the advantages of using aryl alkyl ethers in the metalcatalyzed cross-coupling reaction have been documented by wenkert and dankwardt in the nickel-catalyzed reaction with grignard reagents (i.e., kumada\u2013tamao\u2013corriutype coupling). although functional-group compatibility and availability of the starting grignard reagents for these initial methods are rather limited, these pioneering studies offer a starting point for the development of cross-coupling reactions between aryl methyl ethers and organoboron reagents. thus, we investigated the reaction of 2-methoxynaphthalene (1a) with organoboron compounds in the presence of a catalytic amount of [ni(cod)2] (cod= cycloocta-1,5-diene) and pcy3 (table 1). whereas attempts with boronic acid (table 1, entry 1) and borates (table 1, entries 2 and 3) were unsuccessful, cross-coupling with boronic ester 2a furnished the product in modest yield (table 1, entry 4). although the"
        },
        {
            "id": "R137062",
            "label": "Cross-Coupling Reactions of Aryl Pivalates with Boronic Acids",
            "doi": "10.1021/ja806244b",
            "research_field": {
                "id": "R129",
                "label": "Organic Chemistry"
            },
            "abstract": "the first cross-coupling of acylated phenol derivatives has been achieved. in the presence of an air-stable ni(ii) complex, readily accessible aryl pivalates participate in the suzuki-miyaura coupling with arylboronic acids. the process is tolerant of considerable variation in each of the cross-coupling components. in addition, a one-pot acylation/cross-coupling sequence has been developed. the potential to utilize an aryl pivalate as a directing group has also been demonstrated, along with the ability to sequentially cross-couple an aryl bromide followed by an aryl pivalate, using palladium and nickel catalysis, respectively."
        },
        {
            "id": "R135704",
            "label": "Counterion-Mediated Crossing of the Cyanine Limit in Crystals and Fluid Solution: Bond Length Alternation and Spectral Broadening Unveiled by Quantum Chemistry",
            "doi": "10.1021/jacs.9b10686",
            "research_field": {
                "id": "R130",
                "label": "Physical Chemistry"
            },
            "abstract": "absorption spectra of cyanine+br- salts show a remarkable solvent dependence in non-/polar solvents, exhibiting a narrow, sharp band shapes in dichloromethane but broad features in toluene; this change was attributed to ion pair association, breaking the symmetry of the cyanine, similar to the situation in the crystals (p.-a. bouit et al, j. am. chem. soc. 2010, 132, 4328). our density functional theory (dft) based quantum mechanics/molecular mechanics (qm/mm) calculations of the crystals evidence the crucial role of specific asymmetric anion positioning on the symmetry breaking. molecular dynamics (md) simulations prove the ion pair association in non-polar solvents. time-dependent dft vibronic calculations in toluene show that ion pairing, controlled by steric demands, induces symmetry breaking in the electronic ground state. this largely broadens the spectrum in very reasonable agreement with experiment, while the principal pattern of vibrational modes is retained. the current findings allow to establish a unified picture on symmetry breaking of polymethine dyes in fluid solution."
        },
        {
            "id": "R135710",
            "label": "Continuous Symmetry Breaking Induced by Ion Pairing Effect in Heptamethine Cyanine Dyes: Beyond the Cyanine Limit",
            "doi": "10.1021/ja9100886",
            "research_field": {
                "id": "R130",
                "label": "Physical Chemistry"
            },
            "abstract": "the association of heptamethine cyanine cation 1(+) with various counterions a (a = br(-), i(-), pf(6)(-), sbf(6)(-), b(c(6)f(5))(4)(-), trisphat) was realized. the six different ion pairs have been characterized by x-ray diffraction, and their absorption properties were studied in polar (dcm) and apolar (toluene) solvents. a small, hard anion (br(-)) is able to strongly polarize the polymethine chain, resulting in the stabilization of an asymmetric dipolar-like structure in the crystal and in nondissociating solvents. on the contrary, in more polar solvents or when it is associated with a bulky soft anion (trisphat or b(c(6)f(5))(4)(-)), the same cyanine dye adopts preferentially the ideal polymethine state. the solid-state and solution absorption properties of heptamethine dyes are therefore strongly correlated to the nature of the counterion."
        },
        {
            "id": "R141814",
            "label": "Resonance-Enhanced Charge Delocalization in Carbazole\u2013Oligoyne\u2013Oxadiazole Conjugates",
            "doi": "10.1021/jacs.0c04003",
            "research_field": {
                "id": "R130",
                "label": "Physical Chemistry"
            },
            "abstract": "there are notably few literature reports of electron donor-acceptor oligoynes, even though they offer unique opportunities for studying charge transport through \"all-carbon\" molecular bridges. in this context, the current study focuses on a series of carbazole-(c\u2261c)n-2,5-diphenyl-1,3,4-oxadiazoles (n = 1-4) as conjugated \u03c0-systems in general and explores their photophysical properties in particular. contrary to the behavior of typical electron donor-acceptor systems, for these oligoynes, the rates of charge recombination after photoexcitation increase with increasing electron donor-acceptor distance. to elucidate this unusual performance, we conducted detailed photophysical and time-dependent density functional theory investigations. significant delocalization of the molecular orbitals along the bridge indicates that the bridging states come into resonance with either the electron donor or acceptor, thereby accelerating the charge transfer. moreover, the calculated bond lengths reveal a reduction in bond-length alternation upon photoexcitation, indicating significant cumulenic character of the bridge in the excited state. in short, strong vibronic coupling between the electron-donating n-arylcarbazoles and the electron-accepting 1,3,4-oxadiazoles accelerates the charge recombination as the oligoyne becomes longer."
        },
        {
            "id": "R144060",
            "label": "Organic Fluorescent Thermometers Based on Borylated Arylisoquinoline Dyes",
            "doi": "10.1002/chem.201402027",
            "research_field": {
                "id": "R130",
                "label": "Physical Chemistry"
            },
            "abstract": "borylated arylisoquinolines with redshifted internal charge-transfer (ict) emission were prepared and characterized. upon heating, significant fluorescence quenching was observed, which forms the basis for a molecular thermometer. in the investigated temperature range (283-323\\u2005k) an average sensitivity of -1.2 to -1.8%\\u2009k(-1) was found for the variations in fluorescence quantum yield and lifetime. in the physiological temperature window (298-318\\u2005k) the average sensitivity even reaches values of up to -2.4%\\u2009k(-1). the thermometer function is interpreted as the interplay between excited ict states of different geometry. in addition, the formation of an intramolecular lewis pair can be followed by (11)b\\u2005nmr spectroscopy. this provides a handle to monitor temperature-dependent ground-state geometry changes of the dyes. the role of steric hindrance is addressed by the inclusion of a derivative that lacks the lewis pair formation."
        },
        {
            "id": "R144074",
            "label": "Mitochondria-targeted fluorescent thermometer monitors intracellular temperature gradient",
            "doi": "10.1039/c5cc01088h",
            "research_field": {
                "id": "R130",
                "label": "Physical Chemistry"
            },
            "abstract": "a small molecule fluorescent thermometer targeting mitochondria (mito thermo yellow) enables us to monitor the intracellular temperature gradient, generated by exogenous heating in various cells."
        },
        {
            "id": "R144081",
            "label": "A soluble cryogenic thermometer with high sensitivity based on excited-state configuration transformations",
            "doi": "10.1039/c5cp04400f",
            "research_field": {
                "id": "R130",
                "label": "Physical Chemistry"
            },
            "abstract": "cryogenic temperature detection plays an irreplaceable role in exploring nature. developing high sensitivity, accurate, observable and convenient measurements of cryogenic temperature is not only a challenge but also an opportunity for the thermometer field. the small molecule 9-(9,9-dimethyl-9h-fluoren-3yl)-14-phenyl-9,14-dihydrodibenzo[a,c]phenazine (fipac) in 2-methyl-tetrahydrofuran (methf) solution is utilized for the detection of cryogenic temperature with a wide range from 138 k to 343 k. this system possesses significantly high sensitivity at low temperature, which reaches as high as 19.4% k(-1) at 138 k. the temperature-dependent ratio of the dual emission intensity can be fitted as a single-exponential curve as a function of temperature. this single-exponential curve can be explained by the mechanism that the dual emission feature of fipac results from the excited-state configuration transformations upon heating or cooling, which is very different from the previously reported mechanisms. here, our work gives an overall interpretation for this mechanism. therefore, application of fipac as a cryogenic thermometer is experimentally and theoretically feasible."
        },
        {
            "id": "R160244",
            "label": "Hybrid Automaton Implementation For Intelligent Agents\u2019 Behavior Modelling",
            "doi": "10.1109/icisct47635.2019.9011955",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "it is predicted that the future of our world might be considered as an urban future, and this future should be happy, or, at least, livable. to describe ways and conditions for creating this future, the \u201csmart city\u201d concept has been worked out. \u201csmart city\u201d in this concept is under consideration as a cyber-physical system. digital twins have become the main elements of these systems, while computer simulation is to be the main technology for these systems investigation. in this paper, the process of a hybrid (continuous-discrete) automaton implementation for the aim of intelligent agents\u2019 behavior modelling is under discussion."
        },
        {
            "id": "R175113",
            "label": "Toward Altmetric-Driven Research-Paper Recommender System Framework",
            "doi": "10.1109/sitis.2017.21",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "\"the volume of literature and more particularly research-oriented publications is growing at an exponential rate, and better tools and methodologies are required to efficiently and effectively retrieve desired documents. the development of academic search engines, digital libraries and archives has led to better information filtering mechanisms that has resulted to improved search results. however, the state-of-the art research-paper recommender systems are still retrieving research articles without explicitly defining the domain of interest of the researchers. also, a rich set of research output (research objects) and their associated metrics are also not being utilized in the process of searching, querying, retrieving and recommending articles. consequently, a lot of irrelevant and unrelated information is being presented to the user. then again, the use of citation counts to rank and recommend research-paper to users is still disputed. recommendation metrics like citation counts, ratings in collaborative filtering, and keyword analysis' cannot be fully relied on as the only techniques through which similarity between documents can be computed, and this is because recommendations based on such metrics are not accurate and have lots of biasness. henceforth, altmetric-based techniques and methodologies are expected to give better recommendations of research papers since the circumstances surrounding a research papers are taken into consideration. this paper proposes a research paper recommender system framework that utilizes paper ontology and altmetric from research papers, to enhance the performance of research paper recommender systems.\""
        },
        {
            "id": "R178149",
            "label": "A Similarity-Inclusive Link Prediction Based Recommender System Approach",
            "doi": "10.5755/j01.eie.25.6.24828",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "despite being a challenging research field with many unresolved problems, recommender systems are getting more popular in recent years. these systems rely on the personal preferences of users on items given in the form of ratings and return the preferable items based on choices of like-minded users. in this study, a graph-based recommender system using link prediction techniques incorporating similarity metrics is proposed. a graph-based recommender system that has ratings of users on items can be represented as a bipartite graph, where vertices correspond to users and items and edges to ratings. recommendation generation in a bipartite graph is a link prediction problem. in current literature, modified link prediction approaches are used to distinguish between fundamental relational dualities of like vs. dislike and similar vs. dissimilar. however, the similarity relationship between users/items is mostly disregarded in the complex domain. the proposed model utilizes user-user and item-item cosine similarity value with the relational dualities in order to improve coverage and hits rate of the system by carefully incorporating similarities. on the standard movielens hetrec and movielens datasets, the proposed similarity-inclusive link prediction method performed empirically well compared to other methods operating in the complex domain. the experimental results show that the proposed recommender system can be a plausible alternative to overcome the deficiencies in recommender systems."
        },
        {
            "id": "R178155",
            "label": "A link prediction approach for item recommendation with complex number",
            "doi": "10.1016/j.knosys.2015.02.013",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "recommendation can be reduced to a sub-problem of link prediction, with specific nodes (users and items) and links (similar relations among users/items, and interactions between users and items). however, the previous link prediction algorithms need to be modified to suit the recommendation cases since they do not consider the separation of these two fundamental relations: similar or dissimilar and like or dislike. in this paper, we propose a novel and unified way to solve this problem, which models the relation duality using complex number. under this representation, the previous works can directly reuse. in experiments with the movie lens dataset and the android software website appchina.com, the presented approach achieves significant performance improvement comparing with other popular recommendation algorithms both in accuracy and coverage. besides, our results revealed some new findings. first, it is observed that the performance is improved when the user and item popularities are taken into account. second, the item popularity plays a more important role than the user popularity does in final recommendation. since its notable performance, we are working to apply it in a commercial setting, appchina.com website, for application recommendation."
        },
        {
            "id": "R140043",
            "label": "Unleashing innovation through internal hackathons",
            "doi": "10.1109/innotek.2014.6877369",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "hackathons have become an increasingly popular approach for organizations to both test their new products and services as well as to generate new ideas. most events either focus on attracting external developers or requesting employees of the organization to focus on a specific problem. in this paper we describe extensions to this paradigm that open up the event to internal employees and preserve the open-ended nature of the hackathon itself. in this paper we describe our initial motivation and objectives for conducting an internal hackathon, our experience in pioneering an internal hackathon at at&t including specific things we did to make the internal hackathon successful. we conclude with the benefits (both expected and unexpected) we achieved from the internal hackathon approach, and recommendations for continuing the use of this valuable tool within at&t."
        },
        {
            "id": "R140059",
            "label": "Open data hackathons: an innovative strategy to enhance entrepreneurial intention",
            "doi": "10.1108/ijis-06-2017-0055",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "\\n purpose \\n in terms of entrepreneurship, open data benefits include economic growth, innovation, empowerment and new or improved products and services. hackathons encourage the development of new applications using open data and the creation of startups based on these applications. researchers focus on factors that affect nascent entrepreneurs\u2019 decision to create a startup but researches in the field of open data hackathons have not been fully investigated yet. this paper aims to suggest a model that incorporates factors that affect the decision of establishing a startup by developers who have participated in open data hackathons. \\n \\n \\n design/methodology/approach \\n in total, 70 papers were examined and analyzed using a three-phased literature review methodology, which was suggested by webster and watson (2002). these surveys investigated several factors that affect a nascent entrepreneur to create a startup. \\n \\n \\n findings \\n eventually, by identifying the motivations for developers to participate in a hackathon, and understanding the benefits of the use of open data, researchers will be able to elaborate the proposed model and evaluate if the contest has contributed to the decision of establish a startup and what factors affect the decision to establish a startup apply to open data developers, and if the participants of the contest agree with these factors. \\n \\n \\n originality/value \\n the paper expands the scope of open data research on entrepreneurship field, stating the need for more research to be conducted regarding the open data in entrepreneurship through hackathons. \\n"
        },
        {
            "id": "R140070",
            "label": "Hackathons as Co-optation Ritual: Socializing Workers and Institutionalizing Innovation in the \u201cNew\u201d Economy",
            "doi": "10.1108/s0277-283320170000031005",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "abstract \\nhackathons, time-bounded events where participants write computer code and build apps, have become a popular means of socializing tech students and workers to produce \u201cinnovation\u201d despite little promise of material reward. although they offer participants opportunities for learning new skills and face-to-face networking and set up interaction rituals that create an emotional \u201chigh,\u201d potential advantage is even greater for the events\u2019 corporate sponsors, who use them to outsource work, crowdsource innovation, and enhance their reputation. ethnographic observations and informal interviews at seven hackathons held in new york during the course of a single school year show how the format of the event and sponsors\u2019 discursive tropes, within a dominant cultural frame reflecting the appeal of silicon valley, reshape unpaid and precarious work as an extraordinary opportunity, a ritual of ecstatic labor, and a collective imaginary for fictional expectations of innovation that benefits all, a powerful strategy for manufacturing workers\u2019 consent in the \u201cnew\u201d economy."
        },
        {
            "id": "R140080",
            "label": "Interdisciplinary Online Hackathons as an Approach to Combat the COVID-19 Pandemic: Case Study",
            "doi": "10.2196/25283",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "\\n background \\n the covid-19 outbreak has affected the lives of millions of people by causing a dramatic impact on many health care systems and the global economy. this devastating pandemic has brought together communities across the globe to work on this issue in an unprecedented manner. \\n \\n \\n objective \\n this case study describes the steps and methods employed in the conduction of a remote online health hackathon centered on challenges posed by the covid-19 pandemic. it aims to deliver a clear implementation road map for other organizations to follow. \\n \\n \\n methods \\n this 4-day hackathon was conducted in april 2020, based on six covid-19\u2013related challenges defined by frontline clinicians and researchers from various disciplines. an online survey was structured to assess: (1) individual experience satisfaction, (2) level of interprofessional skills exchange, (3) maturity of the projects realized, and (4) overall quality of the event. at the end of the event, participants were invited to take part in an online survey with 17 (+5 optional) items, including multiple-choice and open-ended questions that assessed their experience regarding the remote nature of the event and their individual project, interprofessional skills exchange, and their confidence in working on a digital health project before and after the hackathon. mentors, who guided the participants through the event, also provided feedback to the organizers through an online survey. \\n \\n \\n results \\n a total of 48 participants and 52 mentors based in 8 different countries participated and developed 14 projects. a total of 75 mentorship video sessions were held. participants reported increased confidence in starting a digital health venture or a research project after successfully participating in the hackathon, and stated that they were likely to continue working on their projects. of the participants who provided feedback, 60% (n=18) would not have started their project without this particular hackathon and indicated that the hackathon encouraged and enabled them to progress faster, for example, by building interdisciplinary teams, gaining new insights and feedback provided by their mentors, and creating a functional prototype. \\n \\n \\n conclusions \\n this study provides insights into how online hackathons can contribute to solving the challenges and effects of a pandemic in several regions of the world. the online format fosters team diversity, increases cross-regional collaboration, and can be executed much faster and at lower costs compared to in-person events. results on preparation, organization, and evaluation of this online hackathon are useful for other institutions and initiatives that are willing to introduce similar event formats in the fight against covid-19. \\n"
        },
        {
            "id": "R140106",
            "label": "Smart Cities in Europe",
            "doi": "10.1080/10630732.2011.601117",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "\"urban performance currently depends not only on a city's endowment of hard infrastructure (physical capital), but also, and increasingly so, on the availability and quality of knowledge communication and social infrastructure (human and social capital). the latter form of capital is decisive for urban competitiveness. against this background, the concept of the \u201csmart city\u201d has recently been introduced as a strategic device to encompass modern urban production factors in a common framework and, in particular, to highlight the importance of information and communication technologies (icts) in the last 20 years for enhancing the competitive profile of a city. the present paper aims to shed light on the often elusive definition of the concept of the \u201csmart city.\u201d we provide a focused and operational definition of this construct and present consistent evidence on the geography of smart cities in the eu27. our statistical and graphical analyses exploit in depth, for the first time to our knowledge, the most recent version of the urban audit data set in order to analyze the factors determining the performance of smart cities. we find that the presence of a creative class, the quality of and dedicated attention to the urban environment, the level of education, and the accessibility to and use of icts for public administration are all positively correlated with urban wealth. this result prompts the formulation of a new strategic agenda for european cities that will allow them to achieve sustainable urban development and a better urban landscape.\""
        },
        {
            "id": "R148013",
            "label": "Google Dataset Search: Building a search engine for datasets in an open Web ecosystem",
            "doi": "",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "\"there are thousands of data repositories on the web, providing access to millions of datasets. national and regional governments, scientific publishers and consortia, commercial data providers, and others publish data for fields ranging from social science to life science to high-energy physics to climate science and more. access to this data is critical to facilitating reproducibility of research results, enabling scientists to build on others' work, and providing data journalists easier access to information and its provenance. in this paper, we discuss google dataset search, a dataset-discovery tool that provides search capabilities over potentially all datasets published on the web. the approach relies on an open ecosystem, where dataset owners and providers publish semantically enhanced metadata on their own sites. we then aggregate, normalize, and reconcile this metadata, providing a search engine that lets users find datasets in the \u201clong tail\u201d of the web. in this paper, we discuss both social and technical challenges in building this type of tool, and the lessons that we learned from this experience.\""
        },
        {
            "id": "R156129",
            "label": "DIGITAL MANUFACTURING: REQUIREMENTS AND CHALLENGES FOR IMPLEMENTING DIGITAL SURROGATES",
            "doi": "10.1109/wsc.2018.8632242",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "a key challenge for manufacturers today is efficiently producing and delivering products on time. issues include demand for customized products, changes in orders, and equipment status change, complicating the decision-making process. a real-time digital representation of the manufacturing operation would help address these challenges. recent technology advancements of smart sensors, iot, and cloud computing make it possible to realize a \"digital twin\" of a manufacturing system or process. digital twins or surrogates are data-driven virtual representations that replicate, connect, and synchronize the operation of a manufacturing system or process. they utilize dynamically collected data to track system behaviors, analyze performance, and help make decisions without interrupting production. in this paper, we define digital surrogate, explore their relationships to simulation, digital thread, artificial intelligence, and iot. we identify the technology and standard requirements and challenges for implementing digital surrogates. a production planning case is used to exemplify the digital surrogate concept."
        },
        {
            "id": "R159487",
            "label": "Urban Intelligence: a Modular, Fully Integrated, and Evolving Model for Cities Digital Twinning",
            "doi": "10.1109/honet.2019.8907962",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "the urban intelligence (ui) paradigm proposes an ecosystem of technologies to improve urban environment, wellbeing, quality of life and smart city systems. it fosters the definition of a digital twin of the city, namely a cyber-physical counterpart of all the city systems and sub-systems. here we propose a novel approach to ui that extends available frameworks combining advanced multidisciplinary modelling of the city, simulation and learning tools with numerical optimization techniques, each of them specialized for the digital representation of city systems and subsystems, including not only city infrastructures, but also city users and their interactions. ui provides sets of candidate policies in complex scenarios and supports policy makers and stakeholders in designing sustainable and personalized solutions. the main characteristics of the proposed ui architecture are (a) fully multidisciplinary integration of city layers, (b) connection and evolution with the city, (c) integration of participative strategies to include \u201chuman-oriented\u201d information, and (d) modularity of application."
        },
        {
            "id": "R159450",
            "label": "Urban Digital Twins for Smart Cities and Citizens: The Case Study of Herrenberg, Germany",
            "doi": "10.3390/su12062307",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "cities are complex systems connected to economic, ecological, and demographic conditions and change. they are also characterized by diverging perceptions and interests of citizens and stakeholders. thus, in the arena of urban planning, we are in need of approaches that are able to cope not only with urban complexity but also allow for participatory and collaborative processes to empower citizens. this to create democratic cities. connected to the field of smart cities and citizens, we present in this paper, the prototype of an urban digital twin for the 30,000-people town of herrenberg in germany. urban digital twins are sophisticated data models allowing for collaborative processes. the herein presented prototype comprises (1) a 3d model of the built environment, (2) a street network model using the theory and method of space syntax, (3) an urban mobility simulation, (4) a wind flow simulation, and (5) a number of empirical quantitative and qualitative data using volunteered geographic information (vgi). in addition, the urban digital twin was implemented in a visualization platform for virtual reality and was presented to the general public during diverse public participatory processes, as well as in the framework of the \u201cmorgenstadt werkstatt\u201d (tomorrow\u2019s cities workshop). the results of a survey indicated that this method and technology could significantly aid in participatory and collaborative processes. further understanding of how urban digital twins support urban planners, urban designers, and the general public as a collaboration and communication tool and for decision support allows us to be more intentional when creating smart cities and sustainable cities with the help of digital twins. we conclude the paper with a discussion of the presented results and further research directions."
        },
        {
            "id": "R159456",
            "label": "Geospatial Artificial Intelligence: Potentials of Machine Learning for 3D Point Clouds and Geospatial Digital Twins",
            "doi": "10.1007/s41064-020-00102-3",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "abstract artificial intelligence (ai) is changing fundamentally the way how it solutions are implemented and operated across all application domains, including the geospatial domain. this contribution outlines ai-based techniques for 3d point clouds and geospatial digital twins as generic components of geospatial ai. first, we briefly reflect on the term \u201cai\u201d and outline technology developments needed to apply ai to it solutions, seen from a software engineering perspective. next, we characterize 3d point clouds as key category of geodata and their role for creating the basis for geospatial digital twins; we explain the feasibility of machine learning (ml) and deep learning (dl) approaches for 3d point clouds. in particular, we argue that 3d point clouds can be seen as a corpus with similar properties as natural language corpora and formulate a \u201cnaturalness hypothesis\u201d for 3d point clouds. in the main part, we introduce a workflow for interpreting 3d point clouds based on ml/dl approaches that derive domain-specific and application-specific semantics for 3d point clouds without having to create explicit spatial 3d models or explicit rule sets. finally, examples are shown how ml/dl enables us to efficiently build and maintain base data for geospatial digital twins such as virtual 3d city models, indoor models, or building information models."
        },
        {
            "id": "R159459",
            "label": "RESEARCH ON CONSTRUCTION OF SPATIO-TEMPORAL DATA VISUALIZATION PLATFORM FOR GIS AND BIM FUSION",
            "doi": "10.5194/isprs-archives-xlii-3-w10-555-2020",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "abstract. the visualization model of gis and bim fusion can provide data bearing platform and main technical support for future urban operation centers, digital twin cities, and smart cities. based on the analysis of the features and advantages of gis and bim fusion, this paper proposes a construction method of the spatio-temporal data visualization platform for gis and bim fusion. it expounds and analyzes the overall architecture design of platform, multi-dimensional and multi-spatial scales visualization, space analysis for gis and bim fusion, and platform applications and so on. the urban virtual simulation spatio-temporal data platform project of teda new district in tianjin has verified and demonstrated that the effect of application is good. this provides a feasible solution for the construction of spatio-temporal data visualization platform.\\n"
        },
        {
            "id": "R159465",
            "label": "A Socio-Technical Perspective on Urban Analytics: The Case of City-Scale Digital Twins",
            "doi": "10.1080/10630732.2020.1798177",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "abstract this paper demonstrates that a shift from a purely technical to a more socio-technical perspective has significant implications for the conceptualization, design, and implementation of smart city technologies. such implications are discussed and illustrated through the case of an emerging urban analytics tool, the city-scale digital twin. based on interdisciplinary insights and a participatory knowledge co-production and tool co-development process, including both researchers and prospective users, we conclude that in order to move beyond a mere \u201chype technology,\u201d city-scale digital twins must reflect the specifics of the urban and socio-political context."
        },
        {
            "id": "R159473",
            "label": "Participatory Sensing and Digital Twin City: Updating Virtual City Models for Enhanced Risk-Informed Decision-Making",
            "doi": "10.1061/(asce)me.1943-5479.0000748",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "abstractthe benefits of a digital twin city have been assessed based on real-time data collected from preinstalled internet of things (iot) sensors (e.g.,\\xa0traffic, energy use, air pollution, water ..."
        },
        {
            "id": "R159481",
            "label": "The Digital Twin of the City of Zurich for Urban Planning",
            "doi": "10.1007/s41064-020-00092-2",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "abstract population growth will confront the city of zurich with a variety of challenges in the coming years, as the increase in the number of inhabitants and jobs will lead to densification and competing land uses. the tasks for the city administration have become more complex, whereas tools and methods are often based on traditional, static approaches while involving a limited number of citizens and stakeholders in relevant decisions. the digital transformation of more and more\\xa0pieces of the planning and decision-making process will make both increasingly more illustrative, easier to understand and more comprehensible. an important data basis for these processes is the digital twin of the city of zurich. 3d spatial data and their models transform themes of the city, such as buildings, bridges, vegetation, etc., to the digital world, are being updated when required, and create advantages in digital space. these benefits need to be highlighted and published. an important step in public awareness is the release of 3d spatial data under open government data. this allows the development of applications, the promotion of understanding, and the simplification of the creation of different collaborative platforms. by\\xa0visualization and analysis of digital prototypes and the demonstration of interactions with the built environment, scenarios can be digitally developed and discussed in decision-making bodies. questions about the urban climate can be simulated with the help of the digital twin and results can be linked to the existing 3d spatial data. thus, the 3d spatial data set, the models and their descriptions through\\xa0metadata become the reference and must be updated according to the requirements. depending on requirements and questions, further 3d spatial data must be added. the description of the 3d spatial data and their models or the lifecycle management of the digital twin must be carried out with great care. only in this way, decision processes can be supported in a comprehensible way."
        },
        {
            "id": "R159484",
            "label": "Smart city dvelopment with digital twin technology",
            "doi": "10.18690/978-961-286-362-3.20",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "growing urban areas are major consumers of natural resources, energy and raw materials. understanding cities\u00b4 urban metabolism is salient when developing sustainable and resilient cities. this paper addresses concepts of smart city and digital twin technology as means to foster more sustainable urban development. smart city has globally been well adopted concept in urban development. with smart city development cities aim to optimize overall performance of the city, its infrastructures, processes and services, but also to improve socio-economic wellbeing. dynamic digital twins are constituted to form real-time connectivity between virtual and physical objects. digital twin combines virtual objects to its physical counterparts. this conceptual paper provides additionally examples from dynamic digital twin platforms and digital twin of helsinki, finland."
        },
        {
            "id": "R160211",
            "label": "Architecting Smart City Digital Twins: Combined Semantic Model and Machine Learning Approach",
            "doi": "10.1061/(asce)me.1943-5479.0000774",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "abstractthis work was motivated by the premise that next-generation smart city systems will be enabled by widespread adoption of sensing and communication technologies deeply embedded within the ph..."
        },
        {
            "id": "R160217",
            "label": "Methodological Framework for Digital Transition and Performance Assessment of Smart Cities",
            "doi": "10.23919/splitech.2019.8783170",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "the ultimate goal of smart cities is to improve citizens\u2019 quality of life in a scenario where technological solutions challenge urban governance. however, the knowledge and framework for data use for smart cities remain relatively unknown. the actual translation of city problems into diverse actions requires specific methodologies to guide digital transitions of cities and to assess to what extent the smart cities\u2019 initiatives pursue sustainable development goals. this paper proposes a methodological framework for digital modelling of cities allowing assessment of their performance and supporting decision making. the city model adopts the concept of digital twin as a powerful tool for discussion between stakeholders, as well as citizens to find the smartest solutions and get valuable insight after their deployment. the methodological framework is presented as a set of digital twin concept, stages of digital twinning and implementation strategy. furthermore, the most common city information models, suitable for implementation of digital twins are summarized."
        },
        {
            "id": "R160231",
            "label": "Using big data analytics and IoT principles to keep an eye on underground infrastructure",
            "doi": "10.1109/bigdata.2017.8258503",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "a concept development study by the open geospatial consortium (ogc) has highlighted the importance of high-quality feature data for underground urban infrastructure (ugi). analysis of large survey datasets, including both visual and non-visual methods, is essential for creating and maintaining ugi geodata. connecting hidden features with diverse, high-velocity sensing streams and realistic predictive models that effectively characterize them is key to lower construction costs, efficient infrastructure operation, sound disaster preparedness, and new smart city services. iot principles that combine ogc geodata and sensor web observation standards may offer the best chance for working towards functional \u201cdigital twins\u201d of such hidden infrastructure that are both cost effective and scalable with the increasing complexity and instrumentation of the underground built environment. technical and policy challenges remain, however, before this can be achieved."
        },
        {
            "id": "R160241",
            "label": "AUTOMATIC 3D BUILDINGS COMPACT RECONSTRUCTION FROM LIDAR POINT CLOUDS",
            "doi": "10.5194/isprs-archives-xliii-b2-2020-473-2020",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "abstract. point clouds generated from aerial lidar and photogrammetric techniques are great ways to obtain valuable spatial insights over large scale. however, their nature hinders the direct extraction and sharing of underlying information. the generation of consistent large-scale 3d city models from this real-world data is a major challenge. specifically, the integration in workflows usable by decision-making scenarios demands that the data is structured, rich and exchangeable. citygml permits new advances in terms of interoperable endeavour to use city models in a collaborative way. efforts have led to render good-looking digital twins of cities but few of them take into account their potential use in finite elements simulations (wind, floods, heat radiation model, etc.). in this paper, we target the automatic reconstruction of consistent 3d city buildings highlighting closed solids, coherent surface junctions, perfect snapping of vertices, etc. it specifically investigates the topological and geometrical consistency of generated models from aerial lidar point cloud, formatted following the cityjson specifications. these models are then usable to store relevant information and provides geometries usable within complex computations such as computational fluid dynamics, free of local inconsistencies (e.g. holes and unclosed solids).\\n"
        },
        {
            "id": "R160247",
            "label": "Smart city digital twins",
            "doi": "10.1109/ssci.2017.8285439",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "\"driven by the challenges of rapid urbanization, cities are determined to implement advanced socio-technological changes and transform into smarter cities. the success of such transformation, however, greatly relies on a thorough understanding of the city's states of spatiotemporal flux. the ability to understand such fluctuations in context and in terms of interdependencies that exist among various entities across time and space is crucial, if cities are to maintain their smart growth. here, we introduce a smart city digital twin paradigm that can enable increased visibility into cities' human-infrastructure-technology interactions, in which spatiotemporal fluctuations of the city are integrated into an analytics platform at the real-time intersection of reality-virtuality. through learning and exchange of spatiotemporal information with the city, enabled through virtualization and the connectivity offered by internet of things (iot), this digital twin of the city becomes smarter over time, able to provide predictive insights into the city's smarter performance and growth.\""
        },
        {
            "id": "R160253",
            "label": "The Potential of Digital Twin Model Integrated With Artificial Intelligence Systems",
            "doi": "10.1109/eeeic/icpseurope49358.2020.9160810",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "the paper explores the use of a \u201cdigital twin model\u201d applied to the case study of a residential district, and organized as a three-dimensional data system able to participate to the intelligent optimization and automation of the energy management and efficiency of the building system. the case study focuses on the area called rinascimento iii in rome, consisting of 16 eight-floor building hosting 216 apartment units with an overall percentage of self-renewable energy produced by the building complex equal to 70%. this already quite high percentage means that the building complex can be defined as a near zero energy building (nzeb), i.e. a building that has a very high energy performance, and the nearly-zero or very low amount of energy required should be covered to a very significant extent by energy from renewable sources, including energy from renewable source produced on-site or nearby."
        },
        {
            "id": "R160256",
            "label": "Devising a Game Theoretic Approach to Enable Smart City Digital Twin Analytics",
            "doi": "10.24251/hicss.2019.241",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "despite investments in advancing information and communications technology (ict)-integrated infrastructure systems toward becoming smarter cities, cities often face a large gap between smart sustainable supply and demand. here, we review the core concepts of ict-integrated infrastructure systems as they pertain to developing smart and sustainable cities, and describe how a game theoretic-based digital twin of a city can enable more visibility and insight into the successful implementation of such systems. this study is a foundational step toward enabling participation of all city stakeholders (i.e., government, industry, and citizens) in the decision making process and the creation of smart sustainable cities. engaging city stakeholders in such a manner allows for collective participation in changes, which can enable continuous adaptation toward more sustaining growth and prosperity. 1. smart sustainable cities 1.1. urbanization, growth of supply and demand, and urge for efficiency between 1950 and 2018, the world\u2019s urban population have grown from 751 to more than 4.2 billion. projections anticipate that, by 2050, they will constitute nearly 70% of the world population [1]. in the united states, currently the most urbanized region in the world, this percentage is expected to increase to"
        },
        {
            "id": "R160263",
            "label": "Evaluation of Urban-Scale Building Energy-Use Models and Tools\u2014Application for the City of Fribourg, Switzerland",
            "doi": "10.3390/su13041595",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "building energy-use models and tools can simulate and represent the distribution of energy consumption of buildings located in an urban area. the aim of these models is to simulate the energy performance of buildings at multiple temporal and spatial scales, taking into account both the building shape and the surrounding urban context. this paper investigates existing models by simulating the hourly space heating consumption of residential buildings in an urban environment. existing bottom-up urban-energy models were applied to the city of fribourg in order to evaluate the accuracy and flexibility of energy simulations. two common energy-use models\u2014a machine learning model and a gis-based engineering model\u2014were compared and evaluated against anonymized monitoring data. the study shows that the simulations were quite precise with an annual mean absolute percentage error of 12.8 and 19.3% for the machine learning and the gis-based engineering model, respectively, on residential buildings built in different periods of construction. moreover, a sensitivity analysis using the morris method was carried out on the gis-based engineering model in order to assess the impact of input variables on space heating consumption and to identify possible optimization opportunities of the existing model."
        },
        {
            "id": "R160274",
            "label": "Smart City Digital Twin\u2013Enabled Energy Management: Toward Real-Time Urban Building Energy Benchmarking",
            "doi": "10.1061/(asce)me.1943-5479.0000741",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "abstractto meet energy-reduction goals, cities are challenged with assessing building energy performance and prioritizing efficiency upgrades across existing buildings. although current top-down bu..."
        },
        {
            "id": "R160291",
            "label": "A Smart Campus\u2019 Digital Twin for Sustainable Comfort Monitoring",
            "doi": "10.3390/su12219196",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "interdisciplinary cross-cultural and cross-organizational research offers great opportunities for innovative breakthroughs in the field of smart cities, yet it also presents organizational and knowledge development hurdles. smart cities must be large towns able to sustain the needs of their citizens while promoting environmental sustainability. smart cities foment the widespread use of novel information and communication technologies (icts); however, experimenting with these technologies in such a large geographical area is unfeasible. consequently, smart campuses (scs), which are universities where technological devices and applications create new experiences or services and facilitate operational efficiency, allow experimentation on a smaller scale, the concept of scs as a testbed for a smart city is gaining momentum in the research community. nevertheless, while universities acknowledge the academic role of a smart and sustainable approach to higher education, campus life and other student activities remain a mystery, which have never been universally solved. this paper proposes a sc concept to investigate the integration of building information modeling tools with internet of things- (iot)-based wireless sensor networks in the fields of environmental monitoring and emotion detection to provide insights into the level of comfort. additionally, it explores the ability of universities to contribute to local sustainability projects by sharing knowledge and experience across a multi-disciplinary team. preliminary results highlight the significance of monitoring workspaces because productivity has been proven to be directly influenced by environment parameters. the comfort-monitoring infrastructure could also be reused to monitor physical parameters from educational premises to increase energy efficiency."
        },
        {
            "id": "R160298",
            "label": "A Digital Twin of Bridges for Structural Health Monitoring",
            "doi": "10.12783/shm2019/32287",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "\"\u00a9 international workshop on structural health monitoring. all rights reserved. bridges are critical infrastructure systems connecting different regions and providing widespread social and economic benefits. it is therefore essential that they are designed, constructed and maintained properly to adapt to changing conditions of use and climate-driven events. with the rapid development in capability of collecting bridge monitoring data, a data challenge emerges due to insufficient capability in managing, processing and interpreting large monitoring datasets to extract useful information which is of practical value to the industry. one emerging area of research which focuses on addressing this challenge is the creation of 'digital twins' for bridges. a digital twin serves as a virtual representation of the physical infrastructure (i.e. the physical twin), which can be updated in near real time as new data is collected, provide feedback into the physical twin and perform 'what-if scenarios for assessing asset risks and predicting asset performance. this paper presents and broadly discusses two years of exploratory study towards creating a digital twin of bridges for structural health monitoring purposes. in particular, it has involved an interdisciplinary collaboration between civil engineers at the cambridge centre for smart infrastructure and construction (csic) and statisticians at the alan turing institute (ati), using two monitored railway bridges in staffordshire, uk as a case study. four areas of research were investigated: (i) real-time data management using bim, (ii) physics-based approaches, (iii) data-driven approaches, and (iv) data-centric engineering approaches (i.e. synthesis of physics-based and data-driven approaches). a framework for creating a digital twin of bridges, particularly for structural health monitoring purposes, is proposed and briefly discussed.\""
        },
        {
            "id": "R160301",
            "label": "Digital Twin Aided Vulnerability Assessment and Risk-Based Maintenance Planning of Bridge Infrastructures Exposed to Extreme Conditions",
            "doi": "10.3390/su13042051",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "over the past centuries, millions of bridge infrastructures have been constructed globally. many of those bridges are ageing and exhibit significant potential risks. frequent risk-based inspection and maintenance management of highway bridges is particularly essential for public safety. at present, most bridges rely on manual inspection methods for management. the efficiency is extremely low, causing the risk of bridge deterioration and defects to increase day by day, reducing the load-bearing capacity of bridges, and restricting the normal and safe use of them. at present, the applications of digital twins in the construction industry have gained significant momentum and the industry has gradually entered the information age. in order to obtain and share relevant information, engineers and decision makers have adopted digital twins over the entire life cycle of a project, but their applications are still limited to data sharing and visualization. this study has further demonstrated the unprecedented applications of digital twins to sustainability and vulnerability assessments, which can enable the next generation risk-based inspection and maintenance framework. this study adopts the data obtained from a constructor of zhongcheng village bridge in zhejiang province, china as a case study. the applications of digital twins to bridge model establishment, information collection and sharing, data processing, inspection and maintenance planning have been highlighted. then, the integration of \u201cdigital twins (or building information modelling, bim) + bridge risk inspection model\u201d has been established, which will become a more effective information platform for all stakeholders to mitigate risks and uncertainties of exposure to extreme weather conditions over the entire life cycle."
        },
        {
            "id": "R160306",
            "label": "Mobile Mapping, Machine Learning and Digital Twin for Road Infrastructure Monitoring and Maintenance: Case Study of Mohammed VI Bridge in Morocco",
            "doi": "10.1109/morgeo49228.2020.9121882",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "the concepts of digital twin has been recently introduced, it refers to functional connections between a complex physical system and its high-fidelity digital replica. digital twin process workflow is proposed in case of mohammed vi bridge modeling in morocco. the current maintenance of a road infrastructure is based on a manual inspection and a system based on traditional tools. aging infrastructures require a new approach to maintenance in terms of inspection, bridge maintenance system, simulation and systematic evaluation. this system now exists and is called the digital twin. digital twin can be thought of as a virtual prototype in service that changes dynamically in near real time as its physical twin changes. an urban infrastructure digital twin is a virtual instance of his physical twin that is continuously updated with multisource, multisensor and multitemporal data that can be used for monitoring, simulating and forecasting any potential problem that may appear in the structure and proposing planning for repair and maintenance of health status throughout the life cycle of this infrastructure. this work presents a general vision and a justification for integrating dt technology with geospatial data. the paper examines the benefits of integrating 3d gis data acquired by automated mobile mapping (mms) workflows for modeling the reality of a major bridge infrastructure in morocco. this allowed to study the future performance of this bridge structure on virtual twin structures under different environmental conditions. cloud point data are acquired by a mobile mapping system on mohammed vi bridge and converted in bim model by a scan to bim process and is integrated in a gis and bim virtual environment and shows the efficiency of volumetric auscultation in terms of surface flatness and distortion inspection. this project provides a new bridge maintenance system using the concept of a digital twin. this digital model is a platform that allows to collect, organize and share the maintenance history of this important road infrastructure in morocco."
        },
        {
            "id": "R160311",
            "label": "Digital Twin and CyberGIS for Improving Connectivity and Measuring the Impact of Infrastructure Construction Planning in Smart Cities",
            "doi": "10.3390/ijgi9040240",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "smart technologies are advancing, and smart cities can be made smarter by increasing the connectivity and interactions of humans, the environment, and smart devices. this paper discusses selective technologies that can potentially contribute to developing an intelligent environment and smarter cities. while the connectivity and efficiency of smart cities is important, the analysis of the impact of construction development and large projects in the city is crucial to decision and policy makers, before the project is approved. this raises the question of assessing the impact of a new infrastructure project on the community prior to its commencement\u2014what type of technologies can potentially be used for creating a virtual representation of the city? how can a smart city be improved by utilizing these technologies? there are a wide range of technologies and applications available but understanding their function, interoperability, and compatibility with the community requires more discussion around system designs and architecture. these questions can be the basis of developing an agenda for further investigations. in particular, the need for advanced tools such as mobile scanners, geospatial artificial intelligence, unmanned aerial vehicles, geospatial augmented reality apps, light detection, and ranging in smart cities is discussed. in line with smart city technology development, this special issue includes eight accepted articles covering trending topics, which are briefly reviewed."
        },
        {
            "id": "R160319",
            "label": "The Circular Economy: A New Development Strategy in China",
            "doi": "10.1162/108819806775545321",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "activities over the past several years, however, clearly show that ce is emerging as an economic strategy rather than a purely environmental strategy. the major objective of the government is to promote the sustainable development of economy and society, while it also helps to achieve sustainable environmental protection. powers, increasing the wealth of the population and providing employment and business opportunities. the rapid economic growth, however, has engendered serious natural resource depletion and environmental pollution, and the continuing increase of population has exacerbated this situation greatly. recent research has pointed out that growth of the gross domestic product (gdp) in china has significantly reduced the opportunities of future generations to enjoy natural and environmental resources.1 the central government promised in 2002 to build a prosperous society in a comprehensive way by 2020. by then, gdp per capita is anticipated to reach u.s. $3,000 and the total gdp to quadruple. obviously, it is unrealistic for china to expect to realize this ambitious objective in terms of natural resource use if it continues its current development pathway, with population increasing to 1.45 billion in 2020 (qu 2004), low productivity, and the absence of eco-efficiency."
        },
        {
            "id": "R160323",
            "label": "Circular cities",
            "doi": "10.1177/0042098018806133",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "a circular approach to the way in which we manage the resources consumed and produced in cities \u2013 materials, energy, water and land \u2013 will significantly reduce the consumption of finite resources globally. it will also help to address urban problems including resource security, waste disposal, greenhouse gas emissions, pollution, heating, drought and flooding. taking a circular approach can also tackle many other socio-economic problems afflicting cities, for example, providing access to affordable accommodation, expanding and diversifying the economic base, building more engaged and collaborative communities in cities. thus it has great potential to improve our urban living environments. to date, the industrial ecologists and economists have tended to dominate the circularity debate, focusing on closed-loop industrial systems and circular economy (circular businesses and systems of provision). in this paper i investigate why the current state-of-the-art conceptualisation for circular economy (resolve) is inadequate when applied to a city. through this critique and a broader review of the literature i identify the principles and components which are lacking from the circular economy (ce) conceptualisation when applied to a city. i then use this to develop my own definition and conceptualisation of a circular approach to urban resource management."
        },
        {
            "id": "R160331",
            "label": "An Architecture for Blockchain over Edge-enabled IoT for Smart Circular Cities",
            "doi": "10.1109/dcoss.2019.00092",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "\"circular economy is a novel economic model, where every 'asset' is not wasted but reused and upscaled. the internet of things-iot paradigm can underpin the transition to a circular economy by enabling fine-grained and continuous asset tracking. however, there are issues related to security and privacy of iot devices that generate and handle sensitive and personal data. the use of blockchain technology provides an answer to this issue, however, its application raises issues related to the highly-constrained nature of these networks. in this paper, edge computing is presented as a solution to this issue, providing a way in which blockchain and edge computing can be used together to address the constrained nature of iot. furthermore, we present the challenges that this combination poses and the opportunities that it brings. we propose an architecture that decreases the iot devices requirements for memory capacity and increases the overall performance. we also discuss the architecture design and the challenges that it has, comparing it to the traditional blockchain architecture as well as an edge computing architecture for mobile blockchain. the paper closes with a discussion and future extensions of our work are presented, as well.\""
        },
        {
            "id": "R160337",
            "label": "Digital Twin in Circular Economy: Remanufacturing in Construction",
            "doi": "10.1088/1755-1315/588/3/032014",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "abstract \\n global warming attracts increasing public attention. however, in the past few decades, the contribution of construction to greenhouse gas emissions is around 40% of total emissions. the promotion of construction waste remanufacturing faces challenges. the application of digital twins in the remanufacturing of construction waste contributes to the tracking, recycling and management of construction waste. this article reviews the current research on construction waste remanufacturing and the application of digital twin in construction and remanufacturing, aiming at finding the current challenge of construction waste remanufacturing and the opportunity of digital twin to solve it. then, the digital twin platform concept for construction waste remanufacturing is provided as a solution for the current challenges. theoretically, this paper points out the shortcomings of the current research in construction waste remanufacturing based on literature review. meanwhile, this article proposes the application of digital twin in construction waste remanufacturing, which expands the research scope of circular economy in construction. in fact, this research has driven the digital twin application in more industries. besides, this research proposes a concept of potential solutions for the current challenges of construction waste in circular economy."
        },
        {
            "id": "R160340",
            "label": "Integrating Virtual Reality and Digital Twin in Circular Economy Practices: A Laboratory Application Case",
            "doi": "10.3390/su12062286",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "the increasing awareness of customers toward climate change effects, the high demand instability affecting several industrial sectors, and the fast automation and digitalization of production systems are forcing companies to re-think their business strategies and models in view of both the circular economy (ce) and industry 4.0 (i4.0) paradigms. some studies have already assessed the relations between ce and i4.0, their benefits, and barriers. however, a practical demonstration of their potential impact in real contexts is still lacking. the aim of this paper is to present a laboratory application case showing how i4.0-based technologies can support ce practices by virtually testing a waste from electrical and electronic equipment (weee) disassembly plant configuration through a set of dedicated simulation tools. our results highlight that service-oriented, event-driven processing and information models can support the integration of smart and digital solutions in current ce practices at the factory level."
        },
        {
            "id": "R160349",
            "label": "Leveraging Digital Twin for Sustainability Assessment of an Educational Building",
            "doi": "10.3390/su13020480",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "the eu green deal, beginning in 2019, promoted a roadmap for operating the transition to a sustainable eu economy by turning climate issues and environmental challenges into opportunities in all policy areas and making the transition fair and inclusive for all. focusing on the built environment, the voluntary adoption of rating systems for sustainability assessment is growing, with an increasing market value, and is perceived as a social responsibility both by public administration and by private companies. this paper proposes a framework for shifting from a static sustainability assessment to a digital twin (dt)-based and internet of things (iot)-enabled dynamic approach. this new approach allows for a real-time evaluation and control of a wide range of sustainability criteria with a user-centered point of view. a pilot building, namely, the elux lab cognitive building in the university of brescia, was used to test the framework with some sample applications. the educational building accommodates the daily activities of the engineering students by constantly interacting with the sensorized asset monitoring indoor comfort and air quality conditions as well as the energy behavior of the building in order to optimize the trade-off with renewable energy production. the framework is the cornerstone of a methodology exploiting the digital twin approach to support the decision processes related to sustainability through the whole building\u2019s life cycle."
        },
        {
            "id": "R160363",
            "label": "Beyond the State of the Art of Electric Vehicles: A Fact-Based Paper of the Current and Prospective Electric Vehicle Technologies",
            "doi": "10.3390/wevj12010020",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "today, there are many recent developments that focus on improving the electric vehicles and their components, particularly regarding advances in batteries, energy management systems, autonomous features and charging infrastructure. this plays an important role in developing next electric vehicle generations, and encourages more efficient and sustainable eco-system. this paper not only provides insights in the latest knowledge and developments of electric vehicles (evs), but also the new promising and novel ev technologies based on scientific facts and figures\u2014which could be from a technological point of view feasible by 2030. in this paper, potential design and modelling tools, such as digital twin with connected internet-of-things (iot), are addressed. furthermore, the potential technological challenges and research gaps in all ev aspects from hard-core battery material sciences, power electronics and powertrain engineering up to environmental assessments and market considerations are addressed. the paper is based on the knowledge of the 140+ fte counting multidisciplinary research centre mobi-vub, that has a 40-year track record in the field of electric vehicles and e-mobility."
        },
        {
            "id": "R160374",
            "label": "A Digital Twin Paradigm: Vehicle-to-Cloud Based Advanced Driver Assistance Systems",
            "doi": "10.1109/vtc2020-spring48590.2020.9128938",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "digital twin, an emerging representation of cyberphysical systems, has attracted increasing attentions very recently. it opens the way to real-time monitoring and synchronization of real-world activities with the virtual counterparts. in this study, we develop a digital twin paradigm using an advanced driver assistance system (adas) for connected vehicles. by leveraging vehicle-to-cloud (v2c) communication, on-board devices can upload the data to the server through cellular network. the server creates a virtual world based on the received data, processes them with the proposed models, and sends them back to the connected vehicles. drivers can benefit from this v2c based adas, even if all computations are conducted on the cloud. the cooperative ramp merging case study is conducted, and the field implementation results show the proposed digital twin framework can benefit the transportation systems regarding mobility and environmental sustainability with acceptable communication delays and packet losses."
        },
        {
            "id": "R160377",
            "label": "Time series behavior modeling with digital twin for Internet of Vehicles",
            "doi": "10.1186/s13638-019-1589-8",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "abstract electric vehicle (ev) is considered eco-friendly with low carbon emission and maintenance costs. given the current battery and charging technology, driving experience of evs relies heavily on the availability and reachability of ev charging infrastructure. as the number of charging piles increases, carefully designed arrangement of resources and efficient utilization of the infrastructure is essential to the future development of ev industry. the mobility and distribution of evs determine the charging demand and the load of power distribution grid. then, dynamic traffic pattern of numerous interconnected evs poses great impact on charging plans and charging infrastructure. in this paper, we introduce the digital twin of a real-world ev by modeling the mobility based on a time series behaviors of evs to evaluate the charging algorithm and pile arrangement policy. the introduced digital twin ev is a virtually simulated equivalence with same traffic behaviors and charging activities as the ev in real world. the behavior and route choice of evs is dynamically simulated base on the time-varying driving operations, travel intent, and charging plan in a simulated large-scale charging scenario composed of concurrently moving evs and correspondingly equipped charging piles. different ev navigation algorithms and charging algorithms of internet of vehicle can be exactly evaluated in the dynamic simulation of the digital twins of the moving evs and charging infrastructure. then we analyze the collected data such as energy consumption, charging capacity, charging frequency, and waiting time in queue on both the ev side and the charging pile side to evaluate the charging efficiency. the simulation is used to study the relations between the scheduled charging operation of evs and the deployment of piles. the proposed model helps evaluate and validate the design of the charging recommendation and the deployment plan regarding to the arrangement and distribution of charging piles."
        },
        {
            "id": "R160381",
            "label": "Roads Infrastructure Digital Twin: A Step Toward Smarter Cities Realization",
            "doi": "10.1109/mnet.011.2000398",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "digital twin is a new concept that consists of creating an up-to-date virtual asset in cyberspace which mimics the original physical asset in most of its aspects, ultimately to monitor, analyze, test, and optimize the physical asset. in this article, we investigate and discuss the use of the digital twin concept of the roads as a step toward realizing the dream of smart cities. to this end, we propose the deployment of a digital twin box to the roads that is composed of a 360\u00b0 camera and a set of iot devices connected to a single onboard computer. the digital twin box creates a digital twin of the physical road asset by constantly sending real-time data to the edge/cloud, including the 360\u00b0 live stream, gps location, and measurements of the temperature and humidity. this data will be used for realtime monitoring and other purposes by displaying the live stream via head-mounted devices or using a 360\u00b0 web-based player. additionally, we perform an object detection process to extract all possible objects from the captured stream. for some specific objects (person and vehicle), an identification module and a tracking module are employed to identify the corresponding objects and keep track of all video frames where these objects appeared. the outcome of the latter step would be of utmost importance to many other services and domains such as national security. to show the viability of the proposed solution, we have implemented and conducted real-world experiments where we focus more on the detection and recognition processes. the achieved results show the effectiveness of the proposed solution in creating a digital twin of the roads, a step forward to enable self-driving vehicles as a crucial component of smart mobility, using the digital twin box."
        },
        {
            "id": "R160384",
            "label": "A Digital Twin-based Privacy Enhancement Mechanism for the Automotive Industry",
            "doi": "10.1109/is.2018.8710526",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "this paper discusses a digital twin demonstrator for privacy enhancement in the automotive industry. here, the digital twin demonstrator is presented as a method for the design and implementation of privacy enhancement mechanisms, and is used to detect privacy concerns and minimize breaches and associated risks to which smart car drivers can be exposed through connected infotainment applications and services. the digital twin-based privacy enhancement demonstrator is designed to simulate variety of conditions that can occur in the smart car ecosystem. we firstly identify the core stakeholders (actors) in the smart car ecosystem, their roles and exposure to privacy vulnerabilities and associated risks. secondly, we identify assets that consume and generate sensitive privacy data in smart cars, their functionalities, and relevant privacy concerns and risks. thirdly, we design an infrastructure for collecting (i) real-time sensor data from smart cars and their assets, and (ii) environmental data, road and traffic data, generated through operational driving lifecycle. in order to ensure compliance of the collected data with privacy policies and regulations, e.g. with gdpr requirements for enforcement of the data subject\u2019s rights, we design methods for the digital twin-based privacy enhancement demonstrator that are based on behavioural analytics informed by gdpr. we also perform data anonymization to minimize privacy risks and enable actions such as sending an automatic informed consent to the stakeholders."
        },
        {
            "id": "R160390",
            "label": "Collaborative city digital twin for the COVID-19 pandemic: A federated learning solution",
            "doi": "10.26599/tst.2021.9010026",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "\"in this work, we propose a collaborative city digital twin based on fl, a novel paradigm that allowing multiple city dt to share the local strategy and status in a timely manner. in particular, an fl central server manages the local updates of multiple collaborators (city dt), provides a global model which is trained in multiple iterations at different city dt systems, until the model gains the correlations between various response plan and infection trend. that means, a collaborative city dt paradigm based on fl techniques can obtain knowledge and patterns from multiple dts, and eventually establish a `global view' for city crisis management. meanwhile, it also helps to improve each city digital twin selves by consolidating other dt's respective data without violating privacy rules. to validate the proposed solution, we take covid-19 pandemic as a case study. the experimental results on the real dataset with various response plan validate our proposed solution and demonstrate the superior performance.\""
        },
        {
            "id": "R160395",
            "label": "Building and exploiting a Digital Twin for the management of drinking water distribution networks",
            "doi": "10.1080/1573062x.2020.1771382",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "abstract digital twins (dts) are starting to be exploited to improve the management of water distribution systems (wdss) and, in the future, they will be crucial for decision making. in this paper, the authors propose several requirements that a dt of a water distribution system should accomplish. developing a dt is a challenge, and a continuous process of adjustments and learning is required. due to the advantages of having a dt of the wds always available, during the last years a strategy to build and maintain a dt of the water distribution network of valencia (spain) and its metropolitan area (1.6 million inhabitants) was developed. this is one of the first dts built of a water utility, being currently in operation. the great benefits of their use in the daily operation of the system ensure that they will begin to be usual in the most advanced smart cities."
        },
        {
            "id": "R160402",
            "label": "BIM and IoT: A Synopsis from GIS Perspective",
            "doi": "10.5194/isprsarchives-xl-2-w4-33-2015",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "abstract. internet-of-things (iot) focuses on enabling communication between all devices, things that are existent in real life or that are virtual. building information models (bims) and building information modelling is a hype that has been the buzzword of the construction industry for last 15 years. bims emerged as a result of a push by the software companies, to tackle the problems of inefficient information exchange between different software and to enable true interoperability. in bim approach most up-to-date an accurate models of a building are stored in shared central databases during the design and the construction of a project and at post-construction stages. gis based city monitoring / city management applications require the fusion of information acquired from multiple resources, bims, city models and sensors. this paper focuses on providing a method for facilitating the gis based fusion of information residing in digital building \u201cmodels\u201d and information acquired from the city objects i.e. \u201cthings\u201d. once this information fusion is accomplished, many fields ranging from emergency response, urban surveillance, urban monitoring to smart buildings will have potential benefits.\\n"
        },
        {
            "id": "R160408",
            "label": "Using Smart City Technology to Make Healthcare Smarter",
            "doi": "10.1109/jproc.2017.2787688",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "smart cities use information and communication technologies (icts) to scale services include utilities and transportation to a growing population. in this paper, we discuss how smart city icts can also improve healthcare effectiveness and lower healthcare cost for smart city residents. we survey current literature and introduce original research to offer an overview of how smart city infrastructure supports strategic healthcare using both mobile and ambient sensors combined with machine learning. finally, we consider challenges that will be faced as healthcare providers make use of these opportunities."
        },
        {
            "id": "R160415",
            "label": "Digital Twins: From Personalised Medicine to Precision Public Health",
            "doi": "10.3390/jpm11080745",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "a digital twin is a virtual model of a physical entity, with dynamic, bi-directional links between the physical entity and its corresponding twin in the digital domain. digital twins are increasingly used today in different industry sectors. applied to medicine and public health, digital twin technology can drive a much-needed radical transformation of traditional electronic health/medical records (focusing on individuals) and their aggregates (covering populations) to make them ready for a new era of precision (and accuracy) medicine and public health. digital twins enable learning and discovering new knowledge, new hypothesis generation and testing, and in silico experiments and comparisons. they are poised to play a key role in formulating highly personalised treatments and interventions in the future. this paper provides an overview of the technology\u2019s history and main concepts. a number of application examples of digital twins for personalised medicine, public health, and smart healthy cities are presented, followed by a brief discussion of the key technical and other challenges involved in such applications, including ethical issues that arise when digital twins are applied to model humans."
        },
        {
            "id": "R160422",
            "label": "A Novel Cloud-Based Framework for the Elderly Healthcare Services Using Digital Twin",
            "doi": "10.1109/access.2019.2909828",
            "research_field": {
                "id": "R137681",
                "label": "Information Systems, Process and Knowledge Management"
            },
            "abstract": "with the development of technologies, such as big data, cloud computing, and the internet of things (iot), digital twin is being applied in industry as a precision simulation technology from concept to practice. further, simulation plays a very important role in the healthcare field, especially in research on medical pathway planning, medical resource allocation, medical activity prediction, etc. by combining digital twin and healthcare, there will be a new and efficient way to provide more accurate and fast services for elderly healthcare. however, how to achieve personal health management throughout the entire lifecycle of elderly patients, and how to converge the medical physical world and the virtual world to realize real smart healthcare, are still two key challenges in the era of precision medicine. in this paper, a framework of the cloud healthcare system is proposed based on digital twin healthcare (clouddth). this is a novel, generalized, and extensible framework in the cloud environment for monitoring, diagnosing and predicting aspects of the health of individuals using, for example, wearable medical devices, toward the goal of personal health management, especially for the elderly. clouddth aims to achieve interaction and convergence between medical physical and virtual spaces. accordingly, a novel concept of digital twin healthcare (dth) is proposed and discussed, and a dth model is implemented. next, a reference framework of clouddth based on dth is constructed, and its key enabling technologies are explored. finally, the feasibility of some application scenarios and a case study for real-time supervision are demonstrated."
        },
        {
            "id": "R161108",
            "label": "Polymers and the Environment",
            "doi": "10.5772/51057",
            "research_field": {
                "id": "R131",
                "label": "Polymer Chemistry"
            },
            "abstract": "the traditional polymer materials available today, especially the plastics, are the result of decades of evolution. their production is extremely efficient in terms of utilization of raw materials and energy, as well as of waste release. the products present a series of excellent properties such as impermeability to water and microorganisms, high mechanical strength, low density (useful for transporting goods), and low cost due to manufacturing scale and process optimization [1]. however, some of their most useful features, the chemical, physical and biological inertness, and durability resulted in their accumulation in the environment if not recycled. unfortunately, the accumulation of plastics, along with other materials, is becoming a serious problem for all countries in the world. these materials occupy significant volume in landfills and dumps today. recently, the presence of huge amounts of plastic fragments on the oceans has been observed, considerable part of them coming from the streets, going through the drains with the rain, and then going into the rivers and lakes, and then to the oceans [1]. as a result, there is a very strong and irreversible movement, in all countries of the world, to use materials that do not harm the planet, that is, low environmental impact materials."
        },
        {
            "id": "R161372",
            "label": "Biocatalytic Degradation Efficiency of Postconsumer Polyethylene Terephthalate Packaging Determined by Their Polymer Microstructures",
            "doi": "10.1002/advs.201900491",
            "research_field": {
                "id": "R131",
                "label": "Polymer Chemistry"
            },
            "abstract": "polyethylene terephthalate (pet) is the most important mass\u2010produced thermoplastic polyester used as a packaging material. recently, thermophilic polyester hydrolases such as tfcut2 from thermobifida fusca have emerged as promising biocatalysts for an eco\u2010friendly pet recycling process. in this study, postconsumer pet food packaging containers are treated with tfcut2 and show weight losses of more than 50% after 96 h of incubation at 70 \u00b0c. differential scanning calorimetry analysis indicates that the high linear degradation rates observed in the first 72 h of incubation is due to the high hydrolysis susceptibility of the mobile amorphous fraction (maf) of pet. the physical aging process of pet occurring at 70 \u00b0c is shown to gradually convert maf to polymer microstructures with limited accessibility to enzymatic hydrolysis. analysis of the chain\u2010length distribution of degraded pet by nuclear magnetic resonance spectroscopy reveals that maf is rapidly hydrolyzed via a combinatorial exo\u2010 and endo\u2010type degradation mechanism whereas the remaining pet microstructures are slowly degraded only by endo\u2010type chain scission causing no detectable weight loss. hence, efficient thermostable biocatalysts are required to overcome the competitive physical aging process for the complete degradation of postconsumer pet materials close to the glass transition temperature of pet."
        },
        {
            "id": "R161549",
            "label": "Mechanical Recycling of Packaging Plastics: A Review",
            "doi": "10.1002/marc.202000415",
            "research_field": {
                "id": "R131",
                "label": "Polymer Chemistry"
            },
            "abstract": "the current global plastics economy is highly linear, with the exceptional performance and low carbon footprint of polymeric materials at odds with dramatic increases in plastic waste. transitioning to a circular economy that retains plastic in its highest value condition is essential to reduce environmental impacts, promoting reduction, reuse, and recycling. mechanical recycling is an essential tool in an environmentally and economically sustainable economy of plastics, but current mechanical recycling processes are limited by cost, degradation of mechanical properties, and inconsistent quality products. this review covers the current methods and challenges for the mechanical recycling of the five main packaging plastics: poly(ethylene terephthalate), polyethylene, polypropylene, polystyrene, and poly(vinyl chloride) through the lens of a circular economy. their reprocessing induced degradation mechanisms are introduced and strategies to improve their recycling are discussed. additionally, this review briefly examines approaches to improve polymer blending in mixed plastic waste streams and applications of lower quality recyclate."
        },
        {
            "id": "R137377",
            "label": "A dc non-thermal atmospheric-pressure plasma microjet",
            "doi": "10.1088/0963-0252/21/3/034018",
            "research_field": {
                "id": "R114008",
                "label": "Applied Physics"
            },
            "abstract": "a direct current (dc), non-thermal, atmospheric-pressure plasma microjet is generated with helium/oxygen gas mixture as working gas. the electrical property is characterized as a function of the oxygen concentration and show distinctive regions of operation. side-on images of the jet were taken to analyze the mode of operation as well as the jet length. a self-pulsed mode is observed before the transition of the discharge to normal glow mode. optical emission spectroscopy is employed from both end-on and side-on along the jet to analyze the reactive species generated in the plasma. line emissions from atomic oxygen (at 777.4 nm) and helium (at 706.5 nm) were studied with respect to the oxygen volume percentage in the working gas, flow rate and discharge current. optical emission intensities of cu and oh are found to depend heavily on the oxygen concentration in the working gas. ozone concentration measured in a semi-confined zone in front of the plasma jet is found to be from tens to \u223c120 ppm. the results presented here demonstrate potential pathways for the adjustment and tuning of various plasma parameters such as reactive species selectivity and quantities or even ultraviolet emission intensities manipulation in an atmospheric-pressure non-thermal plasma source. the possibilities of fine tuning these plasma species allows for enhanced applications in health and medical related areas."
        },
        {
            "id": "R137380",
            "label": "Deposition of a TMDSO-Based Film by a Non-Equilibrium Atmospheric Pressure DC Plasma Jet: Deposition of a TMDSO-Based Film\u2026",
            "doi": "10.1002/ppap.201200166",
            "research_field": {
                "id": "R114008",
                "label": "Applied Physics"
            },
            "abstract": "this work deals with the deposition of thin films using an atmospheric pressure direct current nitrogen plasma jet with tetramethyldisiloxane as precursor. the effect of o-2 flow and plasma discharge power on film deposition rate and film chemical characteristics is investigated in detail by surface profilometry, fourier transform infrared spectroscopy, and x-ray photoelectron spectroscopy. it is found that a higher deposition rate is obtained at higher oxygen flow rates and higher discharge powers. increasing discharge power shows a certain amount of capability to transfer low oxygen content bonds to high oxygen content bonds. organic films can be deposited in a pure nitrogen atmosphere. the film chemical composition can be tuned to a more inorganic structure by admixture of o-2 leading to an increase in sio4 units at high oxygen flow rates."
        },
        {
            "id": "R137383",
            "label": "Study on Plasma Agent Effect of a Direct-Current Atmospheric Pressure Oxygen-Plasma Jet on Inactivation of E. coli Using Bacterial Mutants",
            "doi": "10.1109/TPS.2013.2248395",
            "research_field": {
                "id": "R114008",
                "label": "Applied Physics"
            },
            "abstract": "biosensors of single-gene knockout mutants and physical methods using mesh and quartz glass are employed to discriminate plasma agents and assess their lethal effects generated in a direct-current atmospheric-pressure oxygen plasma jet. radicals generated in plasma are determined by optical emission spectroscopy, along with the o3 density measurement by uv absorption spectroscopy. besides, thermal effect is investigated by an infrared camera. the biosensors include three kinds of escherichia coli (e. coli) k-12 substrains with their mutants, totalling 8 kinds of bacteria. results show that oxidative stress plays a main role in the inactivation process. rather than superoxide o2-, neutral reactive oxygen species such as o3 and o2(a1\u03b4g) are identified as dominant sources for oxidative stress. in addition, dna damage caused by oxidation is found to be an important destruction mechanism."
        },
        {
            "id": "R137386",
            "label": "Cold DC-Operated Air Plasma Jet for the Inactivation of Infectious Microorganisms",
            "doi": "10.1109/TPS.2012.2216292",
            "research_field": {
                "id": "R114008",
                "label": "Applied Physics"
            },
            "abstract": "we evaluated a nonthermal plasma jet for a respective use to prevent infections from bacteria and yeasts. the plasma jet is generated from the flow of ambient air with 8 slm through a microhollow cathode discharge assembly that is operated with a direct current of 30 ma. with these parameters, the temperature in the jet reaches 43 \u00b0c at 10 mm from the discharge. agar plates that were inoculated with staphylococcus aureus, pseudomonas aeruginosa, acinetobacter baumannii, and candida kefyr were treated at this distance, moving the plates through the jet in a meander that covered a 2 cm by 2 cm area. different exposure times were realized by changing the speed of the movement and adjusting the distance between consecutive passes. s. aureus was most responsive to the exposure with a reduction in the number of colony forming units of 5.5 log steps in 40 s. all other microorganisms show a more gradual inactivation with exposure times. for all bacteria, a clearing of the treated area is achieved in about 2.5-3.5 min, corresponding to log-reduction factors of 5.5-6.5. complete inactivation of the yeast requires about 7 min. both s. aureus and c. kefyr show considerable inactivation also outside the immediate treatment area, while p. aeruginosa and a. baumannii do not. we conclude that differences in the morphologies of the membrane structures are responsible for the diverging results, together with a targeted response to different agents provided with the plasma jet. for the gram negative bacteria, we hold short-lived agents, acting across a short range, responsible, while for the other microorganisms, longer lived species seem more important. our measurements show that neither heat, ultraviolet radiation, nor the generation of ozone can be responsible for the observed results. the most prominent long lived reaction product found is nitric oxide, which, by itself or through induced chemical reactions, might affect cell viability."
        },
        {
            "id": "R137389",
            "label": "Arrays of microplasmas for the controlled production of tunable high fluxes of reactive oxygen species at atmospheric pressure",
            "doi": "10.1088/0963-0252/22/3/035012",
            "research_field": {
                "id": "R114008",
                "label": "Applied Physics"
            },
            "abstract": "the atmospheric-pressure generation of singlet delta oxygen (o2(a 1\u03b4g)) by microplasmas was experimentally studied. the remarkable stability of microcathode sustained discharges (mcsds) allowed the operation of dc glow discharges, free from the glow-to-arc transition, in he/o2/no mixtures at atmospheric pressure. from optical diagnostics measurements we deduced the yield of o2(a 1\u03b4g). by operating arrays of several mcsds in series, o2(a 1\u03b4g) densities higher than 1.0 \u00d7 1017 cm\u22123 were efficiently produced and transported over distances longer than 50 cm, corresponding to o2(a 1\u03b4g) partial pressures and production yields greater than 5 mbar and 6%, respectively. at such high o2(a 1\u03b4g) densities, the fluorescence of the so-called o2(a 1\u03b4g) dimol was observed as a red glow at 634 nm up to 1 m downstream. parallel operation of arrays of mcsds was also implemented, generating o2(a 1\u03b4g) fluxes as high as 100 mmol h\u22121. in addition, ozone (o3) densities up to 1016 cm\u22123 were obtained. finally, the density ratio of o2(a 1\u03b4g) to o3 was finely and easily tuned in the range [10\u22123\u201310+5], through the values of the discharge current and no concentration. this opens up opportunities for a large spectrum of new applications, making this plasma source notably very useful for biomedicine."
        },
        {
            "id": "R137392",
            "label": "Steam plasma jet treatment of phenol in aqueous solution at atmospheric pressure",
            "doi": "10.1109/PLASMA.2012.6383486",
            "research_field": {
                "id": "R114008",
                "label": "Applied Physics"
            },
            "abstract": "steam plasma jet (spj) was generated by phenol aqueous solution introduced into water plasma torch as plasma forming gas, which injected into phenol aqueous solution to conduct oxidation degradation of organic pollutants in aqueous solutions. the experimental results indicated that the phenol was not only decomposed in spj, but also degraded in phenol aqueous solution due to high concentration hydroxyl radicals. moreover, the energy efficiencies significantly increased from (1.6\u20131.8) \u00d7 10\u221210 to (4.8\u20138.0) \u00d7 10\u22128 mol \u00b7 j\u22121 with the initial concentration of phenol increased from 0.5 to 50.0 g \u00b7 l\u22121. the main intermediates of phenol decomposition were pyrocatechol, hydroquinone, maleic acid, butanedioic acid, and muconic acid in liquid. the major gaseous effluence products were h2, co, and co2."
        },
        {
            "id": "R137395",
            "label": "Direct current plasma jet at atmospheric pressure operating in nitrogen and air",
            "doi": "10.1063/1.4774328",
            "research_field": {
                "id": "R114008",
                "label": "Applied Physics"
            },
            "abstract": "an atmospheric pressure direct current (dc) plasma jet is investigated in n2 and dry air in terms of plasma properties and generation of active species in the active zone and the afterglow. the influence of working gases and the discharge current on plasma parameters and afterglow properties are studied. the electrical diagnostics show that discharge can be sustained in two different operating modes, depending on the current range: a self-pulsing regime at low current and a glow regime at high current. the gas temperature and the n2 vibrational temperature in the active zone of the jet and in the afterglow are determined by means of emission spectroscopy, based on fitting spectra of n2 second positive system (c3\u03c0-b3\u03c0) and the boltzmann plot method, respectively. the spectra and temperature differences between the n2 and the air plasma jet are presented and analyzed. space-resolved ozone and nitric oxide density measurements are carried out in the afterglow of the jet. the density of ozone, which is formed..."
        },
        {
            "id": "R137398",
            "label": "Transitions Between and Control of Guided and Branching Streamers in DC Nanosecond Pulsed Excited Plasma Jets",
            "doi": "10.1109/TPS.2012.2211621",
            "research_field": {
                "id": "R114008",
                "label": "Applied Physics"
            },
            "abstract": "plasma bullets are ionization fronts created in atmospheric-pressure plasma jets. the propagation behavior of those bullets is, in the literature, explained by the formation of an interface between the inert gas and the ambient air created by the gas flow of the plasma jet, which guides these discharges in the formed gas channel. in this paper, we examine this ionization phenomenon in uniform gases at atmospheric pressure where this interface between two gases is not present. by changing electrical parameters and adding admixtures such as oxygen, nitrogen, and air to the gas flow, the conditions for which plasma bullets are present are investigated. nanosecond time-resolved images have been taken with an iccd camera to observe the propagation behavior of these discharges. it is argued that the inhomogeneous spatial concentration of metastable atoms and ions, due to the laminar gas flow and the operation frequency of the discharge in the range of a few kilohertz, is responsible for the guidance of the ionization fronts. furthermore, conditions have been observed at where the branching of the discharge is stable and reproducible over time in the case of a helium plasma by adding admixtures of oxygen. possible mechanisms for this phenomenon are discussed."
        },
        {
            "id": "R137401",
            "label": "On the spatio-temporal dynamics of a self-pulsed nanosecond transient spark discharge: a spectroscopic and electrical analysis",
            "doi": "10.1088/0963-0252/22/6/065012",
            "research_field": {
                "id": "R114008",
                "label": "Applied Physics"
            },
            "abstract": "a self-pulsing discharge in flowing argon is investigated by means of electrical, optical and spectroscopic methods. the dependence of the discharge self-pulsing frequency on external parameters (applied negative dc voltage, gap dimensions) is determined, and optical and spectroscopic methods are used to investigate the discharge development with high spatial and temporal resolution. high-resolution spectroscopic measurements at several wavelengths reveal the complex dynamics of the transient spark discharge: a pre-phase at the needle tip and capillary edge, propagation of positive and negative streamers, creation of a transient glow discharge structure and a long-lasting afterglow. excited plasma species necessary for the treatment of an exposed sample continue to be present even 80 \u00b5s after the breakdown of the active plasma."
        },
        {
            "id": "R137404",
            "label": "Characteristics of an atmospheric-pressure argon plasma jet excited by a dc voltage",
            "doi": "10.1088/0963-0252/22/4/045007",
            "research_field": {
                "id": "R114008",
                "label": "Applied Physics"
            },
            "abstract": "a dc-excited plasma jet is developed to generate a diffuse plasma plume in flowing argon. the discharge characteristics of the plasma jet are investigated by optical and electrical methods. the results show that the plasma plume is a pulsed discharge even when a dc voltage is applied. the discharge frequency varies with a change in the applied voltage, the gas flow rate and the gas gap width. it is found that the discharges at different positions of the plasma plume are initiated and quenched almost at the same time with a jitter of about 10 ns by the spatially resolved measurement. optical emission spectroscopy is used to investigate the excited electron temperature of the plasma plume. the results show that the excited electron temperature decreases with increasing applied voltage, gas flow rate or gas gap width. these results are analyzed qualitatively."
        },
        {
            "id": "R137407",
            "label": "Discharge Dynamics and Modes of an Atmospheric Pressure Non-Equilibrium Air Plasma Jet",
            "doi": "10.1002/ppap.201200144",
            "research_field": {
                "id": "R114008",
                "label": "Applied Physics"
            },
            "abstract": "a plasma jet operated with atmospheric pressure air is presented. unlike the dynamics of plasma jets working with noble gases, the propagation of the jet that is operated with air is primarily determined by the gas flow. this jet can be generated by applying a continuous, i.e., dc high voltage. however, depending on the applied voltage and gas flow rate a true dc operation can be distinguished from a self-pulsing mode. the gas temperature of the plasma plume when operated in the pulsed mode is lower than for the dc mode. conversely, emission intensities of atomic oxygen, o, and nitrogen species, n 2 and n + 2 , are much higher for the pulsed mode than observed for dc operation."
        },
        {
            "id": "R137410",
            "label": "A brush-shaped air plasma jet operated in glow discharge mode at atmospheric pressure",
            "doi": "10.1063/1.4889923",
            "research_field": {
                "id": "R114008",
                "label": "Applied Physics"
            },
            "abstract": "using ambient air as working gas, a direct-current plasma jet is developed to generate a brush-shaped plasma plume with fairly large volume. although a direct-current power supply is used, the discharge shows a pulsed characteristic. based on the voltage-current curve and fast photography, the brush-shaped plume, like the gliding arc plasma, is in fact a temporal superposition of a moving discharge filament in an arched shape. during it moves away from the nozzle, the discharge evolves from a low-current arc into a normal glow in one discharge cycle. the emission profile is explained qualitatively based on the dynamics of the plasma brush."
        },
        {
            "id": "R137413",
            "label": "Inactivation of Gram-positive biofilms by low-temperature plasma jet at atmospheric pressure",
            "doi": "10.1088/0022-3727/45/34/345202",
            "research_field": {
                "id": "R114008",
                "label": "Applied Physics"
            },
            "abstract": "this work is devoted to the evaluation of the efficiency of a new low-temperature plasma jet driven in ambient air by a dc-corona discharge to inactivate adherent cells and biofilms of gram-positive bacteria. the selected microorganisms were lactic acid bacteria, a weissella confusa strain which has the particularity to excrete a polysaccharide polymer (dextran) when sucrose is present. both adherent cells and biofilms were treated with the low-temperature plasma jet for different exposure times. the antimicrobial efficiency of the plasma was tested against adherent cells and 48 h-old biofilms grown with or without sucrose. bacterial survival was estimated using both colony-forming unit counts and fluorescence-based assays for bacterial cell viability. the experiments show the ability of the low-temperature plasma jet at atmospheric pressure to inactivate the bacteria. an increased resistance of bacteria embedded within biofilms is clearly observed. the resistance is also significantly higher with biofilm in the presence of sucrose, which indicates that dextran could play a protective role."
        },
        {
            "id": "R25000",
            "label": "Predatory Open-Access Journals in India: A Study",
            "doi": "10.5958/0975-6922.2016.00012.7",
            "research_field": {
                "id": "R11",
                "label": "Science"
            },
            "abstract": "this paper analyses the list of predatory journals published by jeffrey beall. the study found that india is publishing the highest number of predatory journals. the state-wise analysis shows that the contribution of madhya pradesh is the highest in india. a trend reveals that majority of these journals are published after the year 2010."
        },
        {
            "id": "R25066",
            "label": "Predicting Personality from Twitter",
            "doi": "10.1109/passat/socialcom.2011.33",
            "research_field": {
                "id": "R11",
                "label": "Science"
            },
            "abstract": "\"social media is a place where users present themselves to the world, revealing personal details and insights into their lives. we are beginning to understand how some of this information can be utilized to improve the users' experiences with interfaces and with one another. in this paper, we are interested in the personality of users. personality has been shown to be relevant to many types of interactions, it has been shown to be useful in predicting job satisfaction, professional and romantic relationship success, and even preference for different interfaces. until now, to accurately gauge users' personalities, they needed to take a personality test. this made it impractical to use personality analysis in many social media domains. in this paper, we present a method by which a user's personality can be accurately predicted through the publicly available information on their twitter profile. we will describe the type of data collected, our methods of analysis, and the machine learning techniques that allow us to successfully predict personality. we then discuss the implications this has for social media design, interface design, and broader domains.\""
        },
        {
            "id": "R25068",
            "label": "Our Twitter Profiles, Our Selves: Predicting Personality with Twitter",
            "doi": "10.1109/passat/socialcom.2011.26",
            "research_field": {
                "id": "R11",
                "label": "Science"
            },
            "abstract": "\"psychological personality has been shown to affect a variety of aspects: preferences for interaction styles in the digital world and for music genres, for example. consequently, the design of personalized user interfaces and music recommender systems might benefit from understanding the relationship between personality and use of social media. since there has not been a study between personality and use of twitter at large, we set out to analyze the relationship between personality and different types of twitter users, including popular users and influentials. for 335 users, we gather personality data, analyze it, and find that both popular users and influentials are extroverts and emotionally stable (low in the trait of neuroticism). interestingly, we also find that popular users are `imaginative' (high in openness), while influentials tend to be `organized' (high in conscientiousness). we then show a way of accurately predicting a user's personality simply based on three counts publicly available on profiles: following, followers, and listed counts. knowing these three quantities about an active user, one can predict the user's five personality traits with a root-mean-squared error below 0.88 on a $[1,5]$ scale. based on these promising results, we argue that being able to predict user personality goes well beyond our initial goal of informing the design of new personalized applications as it, for example, expands current studies on privacy in social media.\""
        },
        {
            "id": "R25070",
            "label": "Leveraging online social networks and external data sources to predict personality",
            "doi": "10.1109/asonam.2011.121",
            "research_field": {
                "id": "R11",
                "label": "Science"
            },
            "abstract": "over the past decade, people have been expressing more and more of their personalities online. online social networks such as facebook.com capture much of individuals\\' personalities through their published interests, attributes and social interactions. knowledge of an individual\\'s personality can be of wide utility, either for social research, targeted marketing or a variety of other fields a key problem to predicting and utilizing personality information is the myriad of ways it is expressed across various people, locations and cultures. similarly, a model predicting personality based on online data which cannot be extrapolated to \"real world\" situations is of limited utility for researchers. this paper presents initial work done on generating a probabilistic model of personality which uses representations of people\\'s connections to other people, places, cultures, and ideas, as expressed through face book. to this end, personality was predicted using a machine learning method known as a bayesian network. the model was trained using face book data combined with external data sources to allow further inference. the results of this paper present one predictive model of personality that this project has produced. this model demonstrates the potential of this methodology in two ways: first, it is able to explain up to 56% of all variation in a personality trait from a sample of 615 individuals. second it is able to clearly present how this variability is explained through findings such as how to determine how agreeable a man is based on his age, number of face book wall posts, and his willingness to disclose his preference for music made by lady gaga."
        },
        {
            "id": "R25075",
            "label": "Predicting Personality with Social Behavior",
            "doi": "10.1109/asonam.2012.58",
            "research_field": {
                "id": "R11",
                "label": "Science"
            },
            "abstract": "\"in this paper, we examine to which degree behavioral measures can be used to predict personality. personality is one factor that dictates people's propensity to trust and their relationships with others. in previous work, we have shown that personality can be predicted relatively accurately by analyzing social media profiles. we demonstrated this using public data from facebook profiles and text from twitter streams. as social situations are crucial in the formation of one's personality, one's social behavior could be a strong indicator of her personality. given most users of social media sites typically have a large number of friends and followers, considering only these aspects may not provide an accurate picture of personality. to overcome this problem, we develop a set of measures based on one's behavior towards her friends and followers. we introduce a number of measures that are based on the intensity and number of social interactions one has with friends along a number of dimensions such as reciprocity and priority. we analyze these features along with a set of features based on the textual analysis of the messages sent by the users. we show that behavioral features are very useful in determining personality and perform as well as textual features.\""
        },
        {
            "id": "R25077",
            "label": "Personality and patterns of Facebook usage",
            "doi": "10.1145/2380718.2380722",
            "research_field": {
                "id": "R11",
                "label": "Science"
            },
            "abstract": "\"we show how users' activity on facebook relates to their personality, as measured by the standard five factor model. our dataset consists of the personality profiles and facebook profile data of 180,000 users. we examine correlations between users' personality and the properties of their facebook profiles such as the size and density of their friendship network, number uploaded photos, number of events attended, number of group memberships, and number of times user has been tagged in photos. our results show significant relationships between personality traits and various features of facebook profiles. we then show how multivariate regression allows prediction of the personality traits of an individual user given their facebook profile. the best accuracy of such predictions is achieved for extraversion and neuroticism, the lowest accuracy is obtained for agreeableness, with openness and conscientiousness lying in the middle.\""
        },
        {
            "id": "R25079",
            "label": "Machine prediction of personality from Facebook profiles",
            "doi": "10.1109/iri.2012.6302998",
            "research_field": {
                "id": "R11",
                "label": "Science"
            },
            "abstract": "\"an increasing number of americans use social networking sites such as facebook, but few fully appreciate the amount of information they share with the world as a result. although studies exist on the sharing of specific types of information (photos, posts, etc.), one area that has been less explored is how facebook profiles can share personality information in a broad, machine-readable fashion. in this study, we apply data-mining and machine learning techniques to predict users' personality traits (specifically, the traits of the big five personality model) using only demographic and text-based attributes extracted from their profiles. we then use these predictions to rank individuals in terms of the five traits, predicting which users will appear in the top or bottom 5% or 10% of these traits. our results show that when using certain models, we can find the top 10% most open individuals with nearly 75% accuracy, and across all traits and directions, we can predict the top 10% with at least 34.5% accuracy (exceeding 21.8%, which is the best accuracy when using just the best-performing profile attribute). these results have privacy implications in terms of allowing advertisers and other groups to focus on a specific subset of individuals based on their personality traits.\""
        },
        {
            "id": "R25081",
            "label": "Private traits and attributes are predictable from digital records of human behavior",
            "doi": "10.1073/pnas.1218772110",
            "research_field": {
                "id": "R11",
                "label": "Science"
            },
            "abstract": "we show that easily accessible digital records of behavior, facebook likes, can be used to automatically and accurately predict a range of highly sensitive personal attributes including: sexual orientation, ethnicity, religious and political views, personality traits, intelligence, happiness, use of addictive substances, parental separation, age, and gender. the analysis presented is based on a dataset of over 58,000 volunteers who provided their facebook likes, detailed demographic profiles, and the results of several psychometric tests. the proposed model uses dimensionality reduction for preprocessing the likes data, which are then entered into logistic/linear regression to predict individual psychodemographic profiles from likes. the model correctly discriminates between homosexual and heterosexual men in 88% of cases, african americans and caucasian americans in 95% of cases, and between democrat and republican in 85% of cases. for the personality trait \u201copenness,\u201d prediction accuracy is close to the test\u2013retest accuracy of a standard personality test. we give examples of associations between attributes and likes and discuss implications for online personalization and privacy."
        },
        {
            "id": "R25083",
            "label": "Evaluating Content-Independent Features for Personality Recognition",
            "doi": "10.1145/2659522.2659527",
            "research_field": {
                "id": "R11",
                "label": "Science"
            },
            "abstract": "this paper describes our submission for the wcpr14 shared task on computational personality recognition. we have investigated whether the features proposed by soler and wanner (2014) for gender prediction might also be useful in personality recognition. we have compared these features with simple approaches using token unigrams, character trigrams and liwc features. although the newly investigated features seem to work quite well on certain personality traits, they do not outperform the simple approaches."
        },
        {
            "id": "R25089",
            "label": "Predicting Personality Traits using Multimodal Information",
            "doi": "10.1145/2659522.2659531",
            "research_field": {
                "id": "R11",
                "label": "Science"
            },
            "abstract": "measuring personality traits has a long story in psychology where analysis has been done by asking sets of questions. these question sets (inventories) have been designed by investigating lexical terms that we use in our daily communications or by analyzing biological phenomena. whether consciously or unconsciously we express our thoughts and behaviors when communicating with others, either verbally, non-verbally or using visual expressions. recently, research in behavioral signal processing has focused on automatically measuring personality traits using different behavioral cues that appear in our daily communication. in this study, we present an approach to automatically recognize personality traits using a video-blog (vlog) corpus, consisting of transcription and extracted audio-visual features. we analyzed linguistic, psycholinguistic and emotional features in addition to the audio-visual features provided with the dataset. we also studied whether we can better predict a trait by identifying other traits. using our best models we obtained very promising results compared to the official baseline."
        },
        {
            "id": "R25097",
            "label": "A retrospective look at PD projects",
            "doi": "10.1145/153571.163264",
            "research_field": {
                "id": "R11",
                "label": "Science"
            },
            "abstract": "w h i l e m o d e r n m e t h o d s f o r i n f o r m a t i o n s y s t e m d e v e l o p m e n t g e n e r a l l y a c c e p t t h a t u s e r s s h o u l d b e i n v o l v e d in s o m e w a y [15], t h e f o r m o f t h e i n v o l v e m e n t d i f f e r s c o n s i d e r a b l y . m o s t l y , u s e r s a r e v i e w e d a s r e l a t i v e l y p a s s i v e s o u r c e s o f i n f o r m a t i o n , a n d t h e i n v o l v e m e n t is r e g a r d e d a s \" f u n c t i o n a l , \" in t h e s e n s e t h a t i t s h o u l d y i e l d b e t t e r s y s t e m r e q u i r e m e n t s a n d i n c r e a s e d a c c e p t a n c e b y u s e r s ."
        },
        {
            "id": "R25103",
            "label": "Sustained Participatory Design: Extending the Iterative Approach",
            "doi": "10.1162/desi_a_00158",
            "research_field": {
                "id": "R11",
                "label": "Science"
            },
            "abstract": "with its 10th biennial anniversary conference in 2008, participatory design (pd) was leaving its teens and must now be considered ready to join the adult world and to think big: pd should engage in large-scale information-systems development and opt for a sustained pd approach applied throughout design and organizational implementation. to pursue this aim we extend the iterative pd approach by (1) emphasizing pd experiments that transcend traditional prototyping and evaluate systems during real work; (2) incorporating improvisational change management including anticipated, emergent, and opportunity-based change; and (3) extending initial design and development into a sustained, stepwise implementation that constitutes an overall technology-driven organizational change. sustained pd is exemplified through a pd experiment in the danish healthcare sector. we reflect on our experiences from this experiment and discuss four challenges pd must address in dealing with large-scale systems development."
        },
        {
            "id": "R25105",
            "label": "User gains and PD aims",
            "doi": "10.1145/1900441.1900461",
            "research_field": {
                "id": "R11",
                "label": "Science"
            },
            "abstract": "\"we present a study of user gains from their participation in a participatory design (pd) project at danish primary schools. we explore user experiences and reported gains from the project in relation to the multiple aims of pd, based on a series of interviews with pupils, teachers, administrators, and consultants, conducted approximately three years after the end of the project. in particular, we reflect on how the pd initiatives were sustained after the project had ended. we propose that not only are ideas and initiatives disseminated directly within the organization, but also through networked relationships among people, stretching across organizations and project groups. moreover, we demonstrate how users' gains related to their acting within these networks. these results suggest a heightened focus on the indirect and distributed channels through which the long-term impact of pd emerges.\""
        },
        {
            "id": "R25107",
            "label": "Participants' view on personal gains and PD process",
            "doi": "10.1145/2662155.2662194",
            "research_field": {
                "id": "R11",
                "label": "Science"
            },
            "abstract": "\"while it is commonly claimed that users of participatory design projects reap benefits from their participation, little research exists that shows if this truly occurs in the real world. in this paper, we introduce the method and results of assessing the participants' perception of their personal benefits and the degree of participation in a large project in the healthcare field. our research shows that a well-executed participatory design project can produce most of the benefits hypothesized in the literature but also highlights the challenges of assessing individual benefits and the pd process.\""
        },
        {
            "id": "R25109",
            "label": "Examining participation",
            "doi": "10.1145/2661435.2661451",
            "research_field": {
                "id": "R11",
                "label": "Science"
            },
            "abstract": "participatory design (pd) seeks to promote and regulate the negotiation of social change. although many methods claim to be participatory, empirical evidence to support them is lacking. few comprehensive criteria exist to describe and evaluate participation as experienced by stakeholders. there is a need for rigorous research tools to study, validate and improve pd practice. this paper presents the development and initial testing of parte (participation evaluation), an interdisciplinary and intercommunity approach to studying and supporting participation in pd. semi-structured interviews based on the framework showed it to be useful in: a) revealing differences in how stakeholders view participation and design, b) developing a personal frame of participation c) exploration of the future of participatory practices; and d) suggesting actions to resolve specific challenges or contradictions in participation at a broader level. the paper discusses the need to move away from considering pd as a practice claimed by designers towards a more open dialogue between all stakeholders to collective redefine \"participation and design\" for social change."
        },
        {
            "id": "R25111",
            "label": "How much participation is enough?",
            "doi": "10.1145/2661435.2661445",
            "research_field": {
                "id": "R11",
                "label": "Science"
            },
            "abstract": "\"this paper considers the relationship between depth of participation (i.e., the effort and resources invested in participation) versus (tangible) outcomes. the discussion is based on experiences from six participatory research projects of different sizes and durations all taking place within a two year period and all aiming to develop new digital technologies to address an identified social need. the paper asks the fundamental question: how much participation is enough? that is, it challenges the notion that more participation is necessarily better, and, by using the experience of these six projects, it asks whether a more light touch or 'lean' participatory process can still achieve good outcomes, but at reduced cost. the paper concludes that participatory design researchers could consider 'agile' principles from the software development field as one way to streamline participatory processes.\""
        },
        {
            "id": "R25118",
            "label": "VisionSense: an advanced lateral collision warning system",
            "doi": "10.1109/ivs.2005.1505118",
            "research_field": {
                "id": "R11",
                "label": "Science"
            },
            "abstract": "visionsense is an advanced driver assistance system which combines a lateral collision warning system with vehicle-to-vehicle communication. this paper shows the results of user needs assessment and traffic safety modelling of visionsense. user needs were determined by means of a web-based survey. the results show, that visionsense is most appreciated when it uses a light signal to warn the driver in a possibly hazardous situation on a highway. the willingness to pay is estimated at 300 euros. another conclusion based on the survey is that frequent car users want less assistance than less frequent drivers. besides the user needs the impact on traffic safety is modelled. the results are indicative and more research has to be done. traffic safety effects of visionsense on a highway were modelled by means of a microscopic car following and lane change algorithm. twelve different traffic scenarios were modelled with and without visionsense. with visionsense no traffic conflicts occur due to lane changing and less lane changes are performed. visionsense is a system that can improve traffic safety in the future."
        },
        {
            "id": "R25122",
            "label": "Assessment of safety levels and an innovative design for the Lane Change Assistant",
            "doi": "10.1109/ivs.2010.5548095",
            "research_field": {
                "id": "R11",
                "label": "Science"
            },
            "abstract": "in this paper we propose a novel design for the lane change assistant (lca). for drivers on the highway, lca advises them on whether it is safe to change lanes under the current traffic conditions. we focus on how the lca can provide a reliable advice in practice by considering the issues of changing circumstances and measurement uncertainties. under some generic assumptions we develop a micro-simulation model for the lane change safety assessment. the model is in line with the car following models and lane change algorithms available in literature. it retains a probabilistic character to accurately represent realistic situations. based on a sensitivity study we are able to develop a robust design for the lca. in this design the system accounts for the practical uncertainties by including appropriate extra safety distances. the driver interface consists of a spectrum of five led lights, each operating on a distinct color (varying from red to green) and guaranteeing a certain safety degree. our results allow car developers to easily acquire reliable designs for the lca."
        },
        {
            "id": "R25124",
            "label": "\"Should I stay or should I go?\"",
            "doi": "10.1145/2639189.2670268",
            "research_field": {
                "id": "R11",
                "label": "Science"
            },
            "abstract": "\"ambient lighting systems have been introduced by several manufacturers to increase the driver's comfort. also, some works proposed warning systems based on light displays. expanding on those works, we are searching for designs of lumicons (i.e. light patterns) that can not only warn drivers in critical situations but also keep them informed in a non-distracting way. we present first ideas for lumicons for a given scenario coming from a participatory design process.\""
        },
        {
            "id": "R25129",
            "label": "A New Driving Assistant for Automobiles",
            "doi": "10.1109/ccece.2007.306",
            "research_field": {
                "id": "R11",
                "label": "Science"
            },
            "abstract": "this paper introduces an inexpensive car security system which addresses the needs for broader area coverage around the vehicle and stronger indication signals to drivers. the new driving assistant features simple ultrasonic-based sensors, implemented at the two front corners and the two blind spots of the vehicle. in order to report the close-by objects to the driver, the system employs a multitude of feedback devices, including tactile vibrators attached to the steering wheel, audible signals, and an led display mounted on the dash board. the sensor system and the feedback devices are controlled in real-time by microcontrollers over a wireless communication network the final prototype system was installed and tested on a ride-on toy car."
        },
        {
            "id": "R25131",
            "label": "Evaluation of Six Night Vision Enhancement Systems: Qualitative and Quantitative Support for Intelligent Image Processing",
            "doi": "10.1518/001872007x200148",
            "research_field": {
                "id": "R11",
                "label": "Science"
            },
            "abstract": "objective: an evaluation study was conducted to answer the question of which system properties of night vision enhancement systems (nvess) provide a benefit for drivers without increasing their workload. background: different infrared sensor, image processing, and display technologies can be integrated into an nves to support nighttime driving. because each of these components has its specific strengths and weaknesses, careful testing is required to determine their best combination. method: six prototypical systems were assessed in two steps. first, a heuristic evaluation with experts from ergonomics, perception, and traffic psychology was conducted. it produced a broad overview of possible effects of system properties on driving. based on these results, an experimental field study with 15 experienced drivers was performed. criteria used to evaluate the development potential of the six prototypes were the usability dimensions of effectiveness, efficiency, and user satisfaction (international organization for standardization, 1998). results: results showed that the intelligibility of information, the easiness with which obstacles could be located in the environment, and the position of the display presenting the output of the system were of crucial importance for the usability of the nves and its acceptance. conclusion: all relevant requirements are met best by nvess that are positioned at an unobtrusive location and are equipped with functions for the automatic identification of objects and for event-based warnings. application: these design recommendations and the presented approach to evaluate the systems can be directly incorporated into the development process of future nvess."
        },
        {
            "id": "R25133",
            "label": "Complementary Audio-Visual Collision Warnings",
            "doi": "10.1177/154193120905302315",
            "research_field": {
                "id": "R11",
                "label": "Science"
            },
            "abstract": "the growing number of driver assistance systems increases the demand for warnings that are intuitively comprehensible. particularly in hazardous situations, such as a threatening collision, a driver must understand the warning immediately. for this reason, collision warnings should convey as much information as needed to interpret the situation properly and to prepare preventive actions. the present study investigated whether informing about the object and the location of an imminent crash by a multimodal warning (visual and auditory) leads to shorter reaction times and fewer collisions compared to warning signals which only inform about the object of the crash (auditory icons) or give no additional information (simple tone). results reveal that multimodal warnings have the potential to produce a significant advantage over unimodal signals as long as their components complement each other in a way that realistically fits the situation at hand."
        },
        {
            "id": "R25135",
            "label": "Integrating Off-Board Cameras and Vehicle On-Board Localization for Pedestrian Safety",
            "doi": "10.1109/tits.2012.2235431",
            "research_field": {
                "id": "R11",
                "label": "Science"
            },
            "abstract": "\"situational awareness for industrial vehicles is crucial to ensure safety of personnel and equipment. while human drivers and onboard sensors are able to detect obstacles and pedestrians within line-of-sight, in complex environments, initially occluded or obscured dynamic objects can unpredictably enter the path of a vehicle. we propose a system that integrates a vision-based offboard pedestrian tracking subsystem with an onboard localization and navigation subsystem. this combination enables warnings to be communicated and effectively extends the vehicle controller's field of view to include areas that would otherwise be blind spots. a simple flashing light interface in the vehicle cabin provides a clear and intuitive interface to alert drivers of potential collisions. alternatively, the system can be also applied to vehicles that have autonomous navigation capabilities, in which case, instead of alert lights, the vehicle is halted or redirected. we implemented and tested the proposed solution on an automated industrial vehicle under autonomous operation and on a human-driven vehicle in a full-scale production facility, over a period of four months.\""
        },
        {
            "id": "R25137",
            "label": "Driver assistance via optical information with spatial reference",
            "doi": "10.1109/itsc.2013.6728524",
            "research_field": {
                "id": "R11",
                "label": "Science"
            },
            "abstract": "\"the occurrence of accidents caused by deficiencies in risk recognition by the driver can be prevented by presenting relevant information in real time to the driver. in this paper it is proposed to draw the driver's attention towards relevant traffic objects, which might be a safety hazard, by a led strip which is affixed 360\u00b0 around the interior of the car's cabin. with this approach a higher number of use cases can be covered than with existing hmis. the effectiveness of this system is evaluated in a driving simulator study with 13 subjects in four critical traffic situations. the gaze attention times are ascertained with eye tracking technology; mental effort and acceptance are determined by questionnaires and the comprehensibility by semi-structured interviews. there are indications of shortened gaze attention times using the led strip compared to the baseline without driver support. the subjects understand the information submitted mostly intuitively. the acceptance ratings overall are in a positive range, but differ between scenarios.\""
        },
        {
            "id": "R25139",
            "label": "LED-A-pillars",
            "doi": "10.1145/2809730.2809750",
            "research_field": {
                "id": "R11",
                "label": "Science"
            },
            "abstract": "as the chassis of cars become more robust, the pillars of a car become broader in order to increase driver safety. as a-pillars grow wider, so too does their negative affect on the panoramic view of the driver and with a smaller field of vision, the risk of overlooking a pedestrian or an object outside the car increases. in order to deal with a-pillar blind spots, this project examined how distances and directions of possible obstacles can be displayed and how different visualization types with led strips on the a-pillars can affect drivers perception. the result of this study shows that such a prototype improves the panoramic view for car drivers resulting in higher security for road users."
        },
        {
            "id": "R25142",
            "label": "A large-scale LED array to support anticipatory driving",
            "doi": "10.1109/icsmc.2011.6083980",
            "research_field": {
                "id": "R11",
                "label": "Science"
            },
            "abstract": "we present a novel assistance system which supports anticipatory driving by means of fostering early deceleration. upcoming technologies like car2x communication provide information about a time interval which is currently uncovered. this information shall be used in the proposed system to inform drivers about future situations which require reduced speed. such situations include traffic jams, construction sites or speed limits. the hmi is an optical output system based on line arrays of rgb-leds. our contribution presents construction details as well as user evaluations. the results show an earlier deceleration of 3.9 \u2013 11.5 s and a shorter deceleration distance of 2 \u2013 166 m."
        },
        {
            "id": "R25144",
            "label": "GPS enabled speed control embedded system speed limiting device with display and engine control interface",
            "doi": "10.1109/lisat.2013.6578244",
            "research_field": {
                "id": "R11",
                "label": "Science"
            },
            "abstract": "\"in the past decade, there have been close to 350,000 fatal crashes in the united states [1]. with various improvements in traffic and vehicle safety, the number of such crashes is decreasing every year. one of the ways to reduce vehicle crashes is to prevent excessive speeding in the roads and highways. the paper aims to outline the design of an embedded system that will automatically control the speed of a motor vehicle based on its location determined by a gps device. the embedded system will make use of an avr atmega128 microcontroller connected to an em-406a gps receiver. the large amount of location input data justifies the use of an atmega128 microcontroller which has 128kb of programmable flash memory as well as 4kb sram, and a 4kb eeprom memory [2]. the output of the atmega128 will be a dogmi63w-a lcd module which will display information of the current and the set-point speed of the vehicle at the current position. a discrete indicator led will flash at a pre-determined frequency when the speed of the vehicle has exceeded the recommended speed limit. finally, the system will have outputs that will communicate with the engine control unit (ecu) of the vehicle. for the limited scope of this project, the ecu is simulated as an external device with two inputs that will acknowledge pulse-trains of particular frequencies to limit the speed of a vehicle. the speed control system will be programmed using mixed language c and assembly with the latter in use for some pre-written subroutines to drive the lcd module. the gps module will transmit national marine electronics association (nmea) data strings to the microcontroller (mcu) using serial peripheral interface (spi). the mcu will use the location coordinates (latitude and longitude) and the speed from the nmea rmc output string. the current speed is then compared against the recommended speed for the vehicle's location. the memory locations in the atmega128 can be used to store set-point speed values against a particular set of location co-ordinates. apart from its implementation in human operated vehicles, the project can be used to control speed of autonomous cars and to implement the idea of a variable speed limit on roads introduced by the department of transportation [3].\""
        },
        {
            "id": "R43000",
            "label": "Pattern Based Model Reuse Using Colored Petri Nets",
            "doi": "10.1109/iccsa.2019.000-7",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "colored petri net (cpn) is a graphical modeling language for simulation and modeling and for verification of discrete event systems. cpn allows developers to define a model in the form of reusable components. a model component is an independent element, which is specified using a formalized description, can conform to a certain component standard, has a well-defined interface, and encapsulates certain behavior. modern components can help the developer reuse existing models according to their requirement as it reduces the cost and time of development. composability is the capability to select and integrate various components to fulfill user requirements. composability provides the means to achieve reusability where \"reuse\" is the ability of a simulation component to be reclaimed for various applications. we propose a verification framework for developers to select and assemble cpn-based components and verify their composability. the goal of this paper is to provide a pattern which helps developer in making models of concurrent systems. we present a case study of a restaurant model as proof of concept. a verified composition affirms reuse of model components in a meaningful manner by satisfying given requirement specifications."
        },
        {
            "id": "R49480",
            "label": "Software Architecture Optimization Methods: A Systematic Literature Review",
            "doi": "10.1109/tse.2012.64",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "due to significant industrial demands toward software systems with increasing complexity and challenging quality requirements, software architecture design has become an important development activity and the research domain is rapidly evolving. in the last decades, software architecture optimization methods, which aim to automate the search for an optimal architecture design with respect to a (set of) quality attribute(s), have proliferated. however, the reported results are fragmented over different research communities, multiple system domains, and multiple quality attributes. to integrate the existing research results, we have performed a systematic literature review and analyzed the results of 188 research papers from the different research communities. based on this survey, a taxonomy has been created which is used to classify the existing research. furthermore, the systematic analysis of the research literature provided in this review aims to help the research community in consolidating the existing research efforts and deriving a research agenda for future developments."
        },
        {
            "id": "R53034",
            "label": "MODELING SAFEST AND OPTIMAL EMERGENCY EVACUATION PLAN FOR LARGE-SCALE PEDESTRIANS ENVIRONMENTS",
            "doi": "10.1109/wsc.2018.8632418",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "large-scale events are always vulnerable to natural disasters and man-made chaos which poses great threat to crowd safety. such events need an appropriate evacuation plan to alleviate the risk of causalities. we propose a modeling framework for large-scale evacuation of pedestrians during emergency situation. proposed framework presents optimal and safest path evacuation for a hypothetical large-scale crowd scenario. the main aim is to provide the safest and nearest evacuation path because during disastrous situations there is possibility of exit gate blockade and directions of evacuees may have to be changed at run time. for this purpose run time diversions are given to evacuees to ensure their quick and safest exit. in this work, different evacuation algorithms are implemented and compared to determine the optimal solution in terms of evacuation time and crowd safety. the recommended framework incorporates anylogic simulation environment to design complex spatial environment for large-scale pedestrians as agents."
        },
        {
            "id": "R108199",
            "label": "A Little Bird Told Me: Mining Tweets for Requirements and Software Evolution",
            "doi": "10.1109/re.2017.88",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"twitter is one of the most popular social networks. previous research found that users employ twitter to communicate about software applications via short messages, commonly referred to as tweets, and that these tweets can be useful for requirements engineering and software evolution. however, due to their large number---in the range of thousands per day for popular applications---a manual analysis is unfeasible.in this work we present alertme, an approach to automatically classify, group and rank tweets about software applications. we apply machine learning techniques for automatically classifying tweets requesting improvements, topic modeling for grouping semantically related tweets and a weighted function for ranking tweets according to specific attributes, such as content category, sentiment and number of retweets. we ran our approach on 68,108 collected tweets from three software applications and compared its results against software practitioners' judgement. our results show that alertme is an effective approach for filtering, summarizing and ranking tweets about software applications. alertme enables the exploitation of twitter as a feedback channel for information relevant to software evolution, including end-user requirements.\""
        },
        {
            "id": "R108208",
            "label": "Facilitating developer-user interactions with mobile app review digests",
            "doi": "10.1145/2468356.2468681",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"as users are interacting with a large of mobile apps under various usage contexts, user involvements in an app design process has become a critical issue. despite this fact, existing apps or app store platforms only provide a limited form of user involvements such as posting app reviews and sending email reports. while building a unified platform for facilitating user involvements with various apps is our ultimate goal, we present our preliminary work on handling developers' information overload attributed to a large number of app comments. to address this issue, we first perform a simple content analysis on app reviews from the developer's standpoint. we then propose an algorithm that automatically identifies informative reviews reflecting user involvements. the preliminary evaluation results document the efficiency of our algorithm.\""
        },
        {
            "id": "R108341",
            "label": "Release planning of mobile apps based on user reviews",
            "doi": "10.1145/2884781.2884818",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "developers have to to constantly improve their apps by fixing critical bugs and implementing the most desired features in order to gain shares in the continuously increasing and competitive market of mobile apps. a precious source of information to plan such activities is represented by reviews left by users on the app store. however, in order to exploit such information developers need to manually analyze such reviews. this is something not doable if, as frequently happens, the app receives hundreds of reviews per day. in this paper we introduce clap (crowd listener for release planning), a thorough solution to (i) categorize user reviews based on the information they carry out (e.g., bug reporting), (ii) cluster together related reviews (e.g., all reviews reporting the same bug), and (iii) automatically prioritize the clusters of reviews to be implemented when planning the subsequent app release. we evaluated all the steps behind clap, showing its high accuracy in categorizing and clustering reviews and the meaningfulness of the recommended prioritizations. also, given the availability of clap as a working tool, we assessed its practical applicability in industrial environments."
        },
        {
            "id": "R108344",
            "label": "How We Refactor, and How We Know It",
            "doi": "10.1109/tse.2011.41",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "much of what we know about how programmers refactor in the wild is based on studies that examine just a few software projects. researchers have rarely taken the time to replicate these studies in other contexts or to examine the assumptions on which they are based. to help put refactoring research on a sound scientific basis, we draw conclusions using four data sets spanning more than 13 000 developers, 240 000 tool-assisted refactorings, 2500 developer hours, and 3400 version control commits. using these data, we cast doubt on several previously stated assumptions about how programmers refactor, while validating others. for example, we find that programmers frequently do not indicate refactoring activity in commit logs, which contradicts assumptions made by several previous researchers. in contrast, we were able to confirm the assumption that programmers do frequently intersperse refactoring with other program changes. by confirming assumptions and replicating studies made by other researchers, we can have greater confidence that those researchers' conclusions are generalizable."
        },
        {
            "id": "R108347",
            "label": "The Usability (or Not) of Refactoring Tools",
            "doi": "10.1109/saner50967.2021.00030",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "although software developers typically have access to numerous refactoring tools, most developers avoid using these tools despite their benefits. researchers have identified many reasons for the disuse of refactoring tools, including a lack of awareness by the developers, a lack of predictability of the tools, and a lack of need for the tools. in this paper, we build on this earlier work and employ the iso 9241-11 definition of usability to develop a theory of usability for refactoring tools. we investigate existing refactoring tools using this theory by analyzing how 17 developers experience refactoring tools in three software change tasks we asked them to perform. we analyze qualitatively the resulting interview transcripts based on our theory and report on a number of observations that can inform tool designers interested in improving the usability of refactoring tools. for instance, we found a desire for developers to guide how a refactoring tool changes the code and a need for refactoring tools to describe changes made to developers. refactoring tools are currently expected to preserve program behavior. these observations indicate that it may be necessary to give developers more control over this property, including the ability to relax it, for the tools to be usable; that is, for the tools to be effective, efficient and satisfying for the developer to employ."
        },
        {
            "id": "R108350",
            "label": "Improving Usability of Software Refactoring Tools",
            "doi": "10.1109/aswec.2007.24",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"post-deployment maintenance and evolution can account for up to 75% of the cost of developing a software system. software refactoring can reduce the costs associated with evolution by improving system quality. although refactoring can yield benefits, the process includes potentially complex, error-prone, tedious and time-consuming tasks. it is these tasks that automated refactoring tools seek to address. however, although the refactoring process is well-defined, current refactoring tools do not support the full process. to develop better automated refactoring support, we have completed a usability study of software refactoring tools. in the study, we analysed the task of software refactoring using the iso 9241-11 usability standard and fitts' list of task allocation. expanding on this analysis, we reviewed 11 collections of usability guidelines and combined these into a single list of 38 guidelines. from this list, we developed 81 usability requirements for refactoring tools. using these requirements, the software refactoring tools eclipse 3.2, condenser 1.05, refactorit 2.5.1, and eclipse 3.2 with the simian ui 2.2.12 plugin were studied. based on the analysis, we have selected a subset of the requirements that can be incorporated into a prototype refactoring tool intended to address the full refactoring process.\""
        },
        {
            "id": "R111441",
            "label": "Crowd Out the Competition",
            "doi": "10.1109/crowdre.2015.7367583",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "myerp is a fictional developer of an enterprise resource planning (erp) system. driven by the competition, they face the challenge of losing market share if they fail to de-ploy a software as a service (saas) erp system to the european market quickly, but with high quality product. this also means that the requirements engineering (re) activities will have to be performed efficiently and provide solid results. an additional problem they face is that their (potential) stakeholders are phys-ically distributed, it makes sense to consider them a \"crowd\". this competition paper suggests a crowd-based re approach that first identifies the crowd, then collects and analyzes their feedback to derive wishes and needs, and validate the results through prototyping. for this, techniques are introduced that have so far been rarely employed within re, but more \"traditional\" re techniques, will also be integrated and/or adapted to attain the best possible result in the case of myerp."
        },
        {
            "id": "R111923",
            "label": "Same App, Different App Stores: A Comparative Study",
            "doi": "10.1109/mobilesoft.2017.3",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "to attract more users, implementing the same mobile app for different platforms has become a common industry practice. app stores provide a unique channel for users to share feedback on the acquired apps through ratings and textual reviews. however, each mobile platform has its own online store for distributing apps to users. to understand the characteristics of and discrepancies in how users perceive the same app implemented for and distributed through different platforms, we present a large-scale comparative study of cross-platform apps. we mine the characteristics of 80,000 app-pairs (160k apps in total) from a corpus of 2.4 million apps collected from the apple and google play app stores. we quantitatively compare their app-store attributes, such as stars, versions, and prices. we measure the aggregated user-perceived ratings and find many discrepancies across the platforms. further, we employ machine learning to classify 1.7 million textual user reviews obtained from 2,000 of the mined app-pairs. we analyze discrepancies and root causes of user complaints to understand cross-platform development challenges that impact cross-platform user-perceived ratings. we also follow up with the developers to understand the reasons behind identified discrepancies."
        },
        {
            "id": "R111969",
            "label": "User Feedback from Tweets vs App Store Reviews: An Exploratory Study of Frequency, Timing and Content",
            "doi": "10.1109/aire.2018.00008",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "context: user feedback on apps is essential for gauging market needs and maintaining a competitive edge in the mobile apps development industry. app store reviews have been a primary resource for this feedback, however, recent studies have observed that twitter is another potentially valuable source for this information. objective: the objective of this study is to assess user feedback from twitter in terms of timing as well as content and compare with the app store reviews. method: this study employs various text analysis and natural language processing methods such as semantic analysis and latent dirichlet allocation (lda) to analyze tweets and app store reviews. additionally, supervised learning classifiers are used to classify them as semantically similar tweet and app store reviews. results: in spite of a difference in the magnitude between tweets and app store review counts, frequency analysis shows that bug report and feature request are discussed mostly on twitter first as the number of tweets during the reporting time reached the peak a few days earlier. likewise, timing analysis on a set of 426 tweets and 2,383 reviews (which are bug reports and feature requests) show that approximately 15% appear on twitter first. of these 15% tweets, 72% are related to functional or behavioural aspects of the mobile app. content analysis shows that user feedback in tweets mostly focuses on critical issues related to the feature failure and improper functionality. conclusion: the results of this investigation show that the twitter is not only a strong contender for useful information but also a faster source of information for mobile app improvement."
        },
        {
            "id": "R111979",
            "label": "\"What Parts of Your Apps are Loved by Users?\" (T)",
            "doi": "10.1109/ase.2015.57",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "recently, begel et al. found that one of the most important questions software developers ask is \"what parts of software are used/loved by users.\" user reviews provide an effective channel to address this question. however, most existing review summarization tools treat reviews as bags-of-words (i.e., mixed review categories) and are limited to extract software aspects and user preferences. we present a novel review summarization framework, sur-miner. instead of a bags-of-words assumption, it classifies reviews into five categories and extracts aspects for sentences which include aspect evaluation using a pattern-based parser. then, sur-miner visualizes the summaries using two interactive diagrams. our evaluation on seventeen popular apps shows that sur-miner summarizes more accurate and clearer aspects than state-of-the-art techniques, with an f1-score of 0.81, significantly greater than that of reviewspotlight (0.56) and guzmans\\' method (0.55). feedback from developers shows that 88% developers agreed with the usefulness of the summaries from sur-miner."
        },
        {
            "id": "R111988",
            "label": "A Needle in a Haystack: What Do Twitter Users Say about Software?",
            "doi": "10.1109/re.2016.67",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "users of the twitter microblogging platform share a vast amount of information about various topics through short messages on a daily basis. some of these so called tweets include information that is relevant for software companies and could, for example, help requirements engineers to identify user needs. therefore, tweets have the potential to aid in the continuous evolution of software applications. despite the existence of such relevant tweets, little is known about their number and content. in this paper we report on the results of an exploratory study in which we analyzed the usage characteristics, content and automatic classification potential of tweets about software applications by using descriptive statistics, content analysis and machine learning techniques. although the manual search of relevant information within the vast stream of tweets can be compared to looking for a needle in a haystack, our analysis shows that tweets provide a valuable input for software companies. furthermore, our results demonstrate that machine learning techniques have the capacity to identify and harvest relevant information automatically."
        },
        {
            "id": "R112000",
            "label": "Ensemble Methods for App Review Classification: An Approach for Software Evolution (N)",
            "doi": "10.1109/ase.2015.88",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "app marketplaces are distribution platforms for mobile applications that serve as a communication channel between users and developers. these platforms allow users to write reviews about downloaded apps. recent studies found that such reviews include information that is useful for software evolution. however, the manual analysis of a large amount of user reviews is a tedious and time consuming task. in this work we propose a taxonomy for classifying app reviews into categories relevant for software evolution. additionally, we describe an experiment that investigates the performance of individual machine learning algorithms and its ensembles for automatically classifying the app reviews. we evaluated the performance of the machine learning techniques on 4550 reviews that were systematically labeled using content analysis methods. overall, the ensembles had a better performance than the individual classifiers, with an average precision of 0.74 and 0.59 recall."
        },
        {
            "id": "R112015",
            "label": "A Little Bird Told Me: Mining Tweets for Requirements and Software Evolution",
            "doi": "10.1109/re.2017.88",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"twitter is one of the most popular social networks. previous research found that users employ twitter to communicate about software applications via short messages, commonly referred to as tweets, and that these tweets can be useful for requirements engineering and software evolution. however, due to their large number---in the range of thousands per day for popular applications---a manual analysis is unfeasible.in this work we present alertme, an approach to automatically classify, group and rank tweets about software applications. we apply machine learning techniques for automatically classifying tweets requesting improvements, topic modeling for grouping semantically related tweets and a weighted function for ranking tweets according to specific attributes, such as content category, sentiment and number of retweets. we ran our approach on 68,108 collected tweets from three software applications and compared its results against software practitioners' judgement. our results show that alertme is an effective approach for filtering, summarizing and ranking tweets about software applications. alertme enables the exploitation of twitter as a feedback channel for information relevant to software evolution, including end-user requirements.\""
        },
        {
            "id": "R112033",
            "label": "Listening to the Crowd for the Release Planning of Mobile Apps",
            "doi": "10.1109/tse.2017.2759112",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "the market for mobile apps is getting bigger and bigger, and it is expected to be worth over 100 billion dollars in 2020. to have a chance to succeed in such a competitive environment, developers need to build and maintain high-quality apps, continuously astonishing their users with the coolest new features. mobile app marketplaces allow users to release reviews. despite reviews are aimed at recommending apps among users, they also contain precious information for developers, reporting bugs and suggesting new features. to exploit such a source of information, developers are supposed to manually read user reviews, something not doable when hundreds of them are collected per day. to help developers dealing with such a task, we developed clap (crowd listener for release planning), a web application able to (i) categorize user reviews based on the information they carry out, (ii) cluster together related reviews, and (iii) prioritize the clusters of reviews to be implemented when planning the subsequent app release. we evaluated all the steps behind clap, showing its high accuracy in categorizing and clustering reviews and the meaningfulness of the recommended prioritizations. also, given the availability of clap as a working tool, we assessed its applicability in industrial environments."
        },
        {
            "id": "R112044",
            "label": "Can app changelogs improve requirements classification from app reviews?: an exploratory study",
            "doi": "10.1145/3239235.3267428",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "[background] recent research on mining app reviews for software evolution indicated that the elicitation and analysis of user requirements can benefit from supplementing user reviews by data from other sources. however, only a few studies reported results of leveraging app changelogs together with app reviews. [aims] motivated by those findings, this exploratory experimental study looks into the role of app changelogs in the classification of requirements derived from app reviews. we aim at understanding if the use of app changelogs can lead to more accurate identification and classification of functional and non-functional requirements from app reviews. we also want to know which classification technique works better in this context. [method] we did a case study on the effect of app changelogs on automatic classification of app reviews. specifically, manual labeling, text preprocessing, and four supervised machine learning algorithms were applied to a series of experiments, varying in the number of app changelogs in the experimental data. [results] we compared the accuracy of requirements classification from app reviews, by training the four classifiers with varying combinations of app reviews and changelogs. among the four algorithms, na\u00efve bayes was found to be more accurate for categorizing app reviews. [conclusions] the results show that official app changelogs did not contribute to more accurate identification and classification of requirements from app reviews. in addition, na\u00efve bayes seems to be more suitable for our further research on this topic."
        },
        {
            "id": "R112407",
            "label": "Which Feature is Unusable? Detecting Usability and User Experience Issues from User Reviews",
            "doi": "10.1109/rew.2017.76",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"usability and user experience (uux) strongly affect software quality and success. user reviews allow software users to report uux issues. however, this information can be difficult to access due to the varying quality of the reviews, its large numbers and unstructured nature. in this work we propose an approach to automatically detect the uux strengths and issues of software features according to user reviews. we use a collocation algorithm for extracting the features, lexical sentiment analysis for uncovering users' satisfaction about a particular feature and machine learning for detecting the specific uux issues affecting the software application. additionally, we present two visualizations of the results. an initial evaluation of the approach against human judgement obtained mixed results.\""
        },
        {
            "id": "R112416",
            "label": "Using the crowds to satisfy unbounded requirements",
            "doi": "10.1109/crowdre.2015.7367584",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "the internet is a social space that is shaped by humans through the development of websites, the release of web services, the collaborative creation of encyclopedias and forums, the exchange of information through social networks, the provision of work through crowdsourcing platforms, etc. this landscape offers novel possibilities for software systems to satisfy their requirements, e.g., by retrieving and aggregating the information from internet websites as well as by crowdsourcing the execution of certain functions. in this paper, we present a special type of functional requirements (called unbounded) that is not fully satisfiable and whose satisfaction is increased by gathering evidence from multiple sources. in addition to charac- terizing unbounded requirements, we explain how to maximize their satisfaction by asking and by combining opinions of mul- tiple sources: people, services, information, and algorithms. we provide evidence of the existence of these requirements through examples by studying a modern web application (spotify) and from a traditional system (microsoft word)."
        },
        {
            "id": "R112425",
            "label": "Refinement and Resolution of Just-in-Time Requirements in Open Source Software: A Case Study",
            "doi": "10.1109/rew.2017.42",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "just-in-time (jit) requirements are characterized as not following the traditional requirement engineering approach, instead focusing on elaboration when the implementation begins. in this experience report, we analyze both functional and nonfunctional jit requirements from three successful open source software (oss) projects, including firefox, lucene, and mylyn, to explore the common activities that shaped those requirements. we identify a novel refinement and resolution process that all studied requirements followed from requirement inception to their complete realization and subsequent release. this research provides new insights into how oss project teams create quality features from simple initial descriptions of jit requirements. our study also initiates three captivating questions regarding jit requirements and opens new avenues for further research in this emerging field."
        },
        {
            "id": "R112434",
            "label": "Users \u2014 The Hidden Software Product Quality Experts?: A Study on How App Users Report Quality Aspects in Online Reviews",
            "doi": "10.1109/re.2017.73",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "[context and motivation] research on eliciting requirements from a large number of online reviews using automated means has focused on functional aspects. assuring the quality of an app is vital for its success. this is why user feedback concerning quality issues should be considered as well [question/problem] but to what extent do online reviews of apps address quality characteristics? and how much potential is there to extract such knowledge through automation? [principal ideas/results] by tagging online reviews, we found that users mainly write about \"usability\" and \"reliability\", but the majority of statements are on a subcharacteristic level, most notably regarding \"operability\", \"adaptability\", \"fault tolerance\", and \"interoperability\". a set of 16 language patterns regarding \"usability\" correctly identified 1,528 statements from a large dataset far more efficiently than our manual analysis of a small subset. [contribution] we found that statements can especially be derived from online reviews about qualities by which users are directly affected, although with some ambiguity. language patterns can identify statements about qualities with high precision, though the recall is modest at this time. nevertheless, our results have shown that online reviews are an unused big data source for quality requirements."
        },
        {
            "id": "R112472",
            "label": "CRAFT: A Crowd-Annotated Feedback Technique",
            "doi": "10.1109/rew.2017.27",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "the ever increasing accessibility of the web for the crowd offered by various electronic devices such as smartphones has facilitated the communication of the needs, ideas, and wishes of millions of stakeholders. to cater for the scale of this input and reduce the overhead of manual elicitation methods, data mining and text mining techniques have been utilised to automatically capture and categorise this stream of feedback, which is also used, amongst other things, by stakeholders to communicate their requirements to software developers. such techniques, however, fall short of identifying some of the peculiarities and idiosyncrasies of the natural language that people use colloquially. this paper proposes craft, a technique that utilises the power of the crowd to support richer, more powerful text mining by enabling the crowd to categorise and annotate feedback through a context menu. this, in turn, helps requirements engineers to better identify user requirements within such feedback. this paper presents the theoretical foundations as well as the initial evaluation of this crowd-based feedback annotation technique for requirements identification."
        },
        {
            "id": "R113008",
            "label": "Canary: Extracting Requirements-Related Information from Online Discussions",
            "doi": "10.1109/re.2017.83",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "online discussions about software applications generate a large amount of requirements-related information. this information can potentially be usefully applied in requirements engineering; however currently, there are few systematic approaches for extracting such information. to address this gap, we propose canary, an approach for extracting and querying requirements-related information in online discussions. the highlight of our approach is a high-level query language that combines aspects of both requirements and discussion in online forums. we give the semantics of the query language in terms of relational databases and sql. we demonstrate the usefulness of the language using examples on real data extracted from online discussions. our approach relies on human annotations of online discussions. we highlight the subtleties involved in interpreting the content in online discussions and the assumptions and choices we made to effectively address them. we demonstrate the feasibility of generating high-quality annotations by obtaining them from lay amazon mechanical turk users."
        },
        {
            "id": "R113030",
            "label": "Conceptualising, extracting and analysing requirements arguments in users' forums: The CrowdRE\u2010Arg framework",
            "doi": "10.1002/smr.2309",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"due to the pervasive use of online forums and social media, users' feedback are more accessible today and can be used within a requirements engineering context. however, such information is often fragmented, with multiple perspectives from multiple parties involved during on\u2010going interactions. in this paper, the authors propose a crowd\u2010based requirements engineering approach by argumentation (crowdre\u2010arg). the framework is based on the analysis of the textual conversations found in user forums, identification of features, issues and the arguments that are in favour or opposing a given requirements statement. the analysis is to generate an argumentation model of the involved user statements, retrieve the conflicting\u2010viewpoints, reason about the winning\u2010arguments and present that to systems analysts to make informed\u2010requirements decisions. for this purpose, the authors adopted a bipolar argumentation framework and a coalition\u2010based meta\u2010argumentation framework as well as user voting techniques. the crowdre\u2010arg approach and its algorithms are illustrated through two sample conversations threads taken from the reddit forum. additionally, the authors devised algorithms that can identify conflict\u2010free features or issues based on their supporting and attacking arguments. the authors tested these machine learning algorithms on a set of 3,051 user comments, preprocessed using the content analysis technique. the results show that the proposed algorithms correctly and efficiently identify conflict\u2010free features and issues along with their winning arguments.\""
        },
        {
            "id": "R113054",
            "label": "A gradual approach to crowd-based requirements engineering: The case of conference online social networks",
            "doi": "10.1109/crowdre.2015.7367585",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "this paper proposes a gradual approach to crowd-based requirements engineering (re) for supporting the establishment of a more engaged crowd, hence, mitigating the low involvement risk in crowd-based re. our approach advocates involving micro-crowds (mcs), where in each micro-crowd, the population is relatively cohesive and familiar with each other. using this approach, the evolving product is developed iteratively. at each iteration, a new mc can join the already established crowd to enhance the requirements for the next version, while adding terminology to an evolving folksonomy. we are currently using this approach in an on-going research project to develop an online social network (osn) for academic researchers that will facilitate discussions and knowledge sharing around conferences."
        },
        {
            "id": "R113067",
            "label": "Mining User Rationale from Software Reviews",
            "doi": "10.1109/re.2017.86",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "rationale refers to the reasoning and justification behind human decisions, opinions, and beliefs. in software engineering, rationale management focuses on capturing design and requirements decisions and on organizing and reusing project knowledge. this paper takes a different view on rationale written by users in online reviews. we studied 32,414 reviews for 52 software applications in the amazon store. through a grounded theory approach and peer content analysis, we investigated how users argue and justify their decisions, e.g. about upgrading, installing, or switching software applications. we also studied the occurrence frequency of rationale concepts such as issues encountered or alternatives considered in the reviews and found that assessment criteria like performance, compatibility, and usability represent the most pervasive concept. we then used the truth set of manually labeled review sentences to explore how accurately we can mine rationale concepts from the reviews. support vector classifier, naive bayes, and logistic regression, trained on the review metadata, syntax tree of the review text, and influential terms, achieved a precision around 80% for predicting sentences with alternatives and decisions, with top recall values of 98%. on the review level, precision was up to 13% higher with recall values reaching 99%. we discuss the findings and the rationale importance for supporting deliberation in user communities and synthesizing the reviews for developers."
        },
        {
            "id": "R113085",
            "label": "Discovering Requirements through Goal-Driven Process Mining",
            "doi": "10.1109/rew.2017.61",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"software systems are designed to support their users in performing tasks that are parts of more general processes. unfortunately, software designers often make invalid assumptions about the users' processes and therefore about the requirements to support such processes. eliciting and validating such assumptions through manual means (e.g., through observations, interviews, and workshops) is expensive, time-consuming, and may fail to identify the users' real processes. using process mining may reduce these problems by automating the monitoring and discovery of the actual processes followed by a crowd of users. the crowd provides an opportunity to involve diverse groups of users to interact with a system and conduct their intended processes. this implicit feedback in the form of discovered processes can then be used to modify the existing system's functionalities and ensure whether or not a software product is used as initially designed. in addition, the analysis of user-system interactions may reveal lacking functionalities and quality issues. these ideas are illustrated on the greensoft personal energy management system.\""
        },
        {
            "id": "R113122",
            "label": "UCFrame: A Use Case Framework for Crowd-Centric Requirement Acquisition",
            "doi": "10.1145/2894784.2894795",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "to build needed mobile applications in specific domains, requirements should be collected and analyzed in holistic approach. however, resource is limited for small vendor groups to perform holistic requirement acquisition and elicitation. the rise of crowdsourcing and crowdfunding gives small vendor groups new opportunities to build needed mobile applications for the crowd. by finding prior stakeholders and gathering requirements effectively from the crowd, mobile application projects can establish sound foundation in early phase of software process. therefore, integration of crowd-based requirement engineering into software process is important for small vendor groups. conventional requirement acquisition and elicitation methods are analyst-centric. very little discussion is in adapting requirement acquisition tools for crowdcentric context. in this study, several tool features of use case documentation are revised in crowd-centric context. these features constitute a use case-based framework, called ucframe, for crowd-centric requirement acquisition. an instantiation of ucframe is also presented to demonstrate the effectiveness of ucframe in collecting crowd requirements for building two mobile applications."
        },
        {
            "id": "R113137",
            "label": "Mining Context-Aware User Requirements from Crowd Contributed Mobile Data",
            "doi": "10.1145/2875913.2875933",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "internetware is required to respond quickly to emergent user requirements or requirements changes by providing application upgrade or making context-aware recommendations. as user requirements in internet computing environment are often changing fast and new requirements emerge more and more in a creative way, traditional requirements engineering approaches based on requirements elicitation and analysis cannot ensure the quick response of internetware. in this paper, we propose an approach for mining context-aware user requirements from crowd contributed mobile data. the approach captures behavior records contributed by a crowd of mobile users and automatically mines context-aware user behavior patterns (i.e., when, where and under what conditions users require a specific service) from them using apriori-m algorithm. based on the mined user behaviors, emergent requirements or requirements changes can be inferred from the mined user behavior patterns and solutions that satisfy the requirements can be recommended to users. to evaluate the proposed approach, we conduct an experimental study and show the effectiveness of the requirements mining approach."
        },
        {
            "id": "R113151",
            "label": "Linguistic Analysis of Crowd Requirements: An Experimental Study",
            "doi": "10.1109/empire.2018.00010",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"users of today's online software services are often diversified and distributed, whose needs are hard to elicit using conventional re approaches. as a consequence, crowd-based, data intensive requirements engineering approaches are considered important. in this paper, we have conducted an experimental study on a dataset of 2,966 requirements statements to evaluate the performance of three text clustering algorithms. the purpose of the study is to aggregate similar requirement statements suggested by the crowd users, and also to identify domain objects and operations, as well as required features from the given requirements statements dataset. the experimental results are then cross-checked with original tags provided by data providers for validation.\""
        },
        {
            "id": "R113160",
            "label": "Customer Rating Reactions Can Be Predicted Purely using App Features",
            "doi": "10.1109/re.2018.00018",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "in this paper we provide empirical evidence that the rating that an app attracts can be accurately predicted from the features it offers. our results, based on an analysis of 11,537 apps from the samsung android and blackberry world app stores, indicate that the rating of 89% of these apps can be predicted with 100% accuracy. our prediction model is built by using feature and rating information from the existing apps offered in the app store and it yields highly accurate rating predictions, using only a few (11-12) existing apps for case-based prediction. these findings may have important implications for requirements engineering in app stores: they indicate that app developers may be able to obtain (very accurate) assessments of the customer reaction to their proposed feature sets (requirements), thereby providing new opportunities to support the requirements elicitation process for app developers."
        },
        {
            "id": "R113173",
            "label": "Software Feature Request Detection in Issue Tracking Systems",
            "doi": "10.1109/re.2016.8",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "communication about requirements is often handled in issue tracking systems, especially in a distributed setting. as issue tracking systems also contain bug reports or programming tasks, the software feature requests of the users are often difficult to identify. this paper investigates natural language processing and machine learning features to detect software feature requests in natural language data of issue tracking systems. it compares traditional linguistic machine learning features, such as \"bag of words\", with more advanced features, such as subject-action-object, and evaluates combinations of machine learning features derived from the natural language and features taken from the issue tracking system meta-data. our investigation shows that some combinations of machine learning features derived from natural language and the issue tracking system meta-data outperform traditional approaches. we show that issues or data fields (e.g. descriptions or comments), which contain software feature requests, can be identified reasonably well, but hardly the exact sentence. finally, we show that the choice of machine learning algorithms should depend on the goal, e.g. maximization of the detection rate or balance between detection rate and precision. in addition, the paper contributes a double coded gold standard and an open-source implementation to further pursue this topic."
        },
        {
            "id": "R113196",
            "label": "A Needle in a Haystack: What Do Twitter Users Say about Software?",
            "doi": "10.1109/re.2016.67",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "users of the twitter microblogging platform share a vast amount of information about various topics through short messages on a daily basis. some of these so called tweets include information that is relevant for software companies and could, for example, help requirements engineers to identify user needs. therefore, tweets have the potential to aid in the continuous evolution of software applications. despite the existence of such relevant tweets, little is known about their number and content. in this paper we report on the results of an exploratory study in which we analyzed the usage characteristics, content and automatic classification potential of tweets about software applications by using descriptive statistics, content analysis and machine learning techniques. although the manual search of relevant information within the vast stream of tweets can be compared to looking for a needle in a haystack, our analysis shows that tweets provide a valuable input for software companies. furthermore, our results demonstrate that machine learning techniques have the capacity to identify and harvest relevant information automatically."
        },
        {
            "id": "R113204",
            "label": "Mining Android App Descriptions for Permission Requirements Recommendation",
            "doi": "10.1109/re.2018.00024",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"during the development or maintenance of an android app, the app developer needs to determine the app's security and privacy requirements such as permission requirements. permission requirements include two folds. first, what permissions (i.e., access to sensitive resources, e.g., location or contact list) the app needs to request. second, how to explain the reason of permission usages to users. in this paper, we focus on the multiple challenges that developers face when creating permission-usage explanations. we propose a novel framework, clap, that mines potential explanations from the descriptions of similar apps. clap leverages information retrieval and text summarization techniques to find frequent permission usages. we evaluate clap on a large dataset containing 1.4 million android apps. the evaluation results outperform existing state-of-the-art approaches, showing great promise of clap as a tool for assisting developers and permission requirements discovery.\""
        },
        {
            "id": "R135602",
            "label": "Model-Driven Architecture Based Software Development for Epidemiological Surveillance Systems",
            "doi": "10.3233/SHTI190279",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "epidemiological surveillance systems enable collection, analysis and dissemination of information on the monitored disease to different stakeholders. it may be done manually or using a software. given the poor performances of manual systems, the software approach is generally adopted. epidemiological surveillance systems are based on existing softwares, softwares developed from scratch given the specifications or softwares provided by a vendor. these solutions are not always suitable because epidemiological surveillance systems evolve quickly (new drugs, new treatment protocols, etc.), leading to software updates, which can take time (while waiting for a new version) and be expensive. in this article, we present the use of the model-driven architecture (mda) approach to model and generate epidemiological surveillance systems. the result is a complete mda based methodology and tool to develop epidemiological surveillance systems. the tool was used to model and generate softwares that are now used for epidemiological surveillance of tuberculosis in cameroon."
        },
        {
            "id": "R138070",
            "label": "Grand challenges in model-driven engineering: an analysis of the state of the research",
            "doi": "10.1007/s10270-019-00773-6",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "abstract in 2017 and 2018, two events were held\u2014in marburg, germany, and san vigilio di marebbe, italy, respectively\u2014focusing on an analysis of the state of research, state of practice, and state of the art in model-driven engineering (mde). the events brought together experts from industry, academia, and the open-source community to assess what has changed in research in mde over the last 10\\xa0years, what challenges remain, and what new challenges have arisen. this article reports on the results of those meetings, and presents a set of grand challenges that emerged from discussions and synthesis. these challenges could lead to research initiatives for the community going forward.\\n"
        },
        {
            "id": "R175392",
            "label": "Automatically improve software architecture models for performance, reliability, and cost using evolutionary algorithms",
            "doi": "10.1145/1712605.1712624",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "quantitative prediction of quality properties (i.e. extra-functional properties such as performance, reliability, and cost) of software architectures during design supports a systematic software engineering approach. designing architectures that exhibit a good trade-off between multiple quality criteria is hard, because even after a functional design has been created, many remaining degrees of freedom in the software architecture span a large, discontinuous design space. in current practice, software architects try to find solutions manually, which is time-consuming, can be error-prone and can lead to suboptimal designs. we propose an automated approach to search the design space for good solutions. starting with a given initial architectural model, the approach iteratively modifies and evaluates architectural models. our approach applies a multi-criteria genetic algorithm to software architectures modelled with the palladio component model. it supports quantitative performance, reliability, and cost prediction and can be extended to other quantitative quality criteria of software architectures. we validate the applicability of our approach by applying it to an architecture model of a component-based business information system and analyse its quality criteria trade-offs by automatically investigating more than 1200 alternative design candidates."
        },
        {
            "id": "R186081",
            "label": "Metrics Based Verification and Validation Maturity Model (MB-V2M2)",
            "doi": "10.1109/step.2002.1267622",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "verification and validation (v&v) is only marginally addressed in software process improvement models like cmm and cmmi. a roadmap for the establishment of a sound verification and validation process in software development organizations is badly needed. this paper presents a basis for a roadmap; it describes a framework for improvement of the v&v process, based on the testing maturity model (tmm), but with considerable enhancements. the model, tentatively named mb-v/sup 2/m/sup 2/ (metrics based verification and validation maturity model), has been initiated by a consortium of industrial companies, consultancy & service agencies and an academic institute, operating and residing in the netherlands. mb-v/sup 2/m/sup 2/ is designed to be universally applicable, to unite the strengths of known (verification and validation) improvement models and to reflect proven work practices. it recommends a metrics base to select process improvements and to track and control implementation of improvement actions. this paper outlines the model and addresses the current status."
        },
        {
            "id": "R186093",
            "label": "Assessing Business-IT Allignment Maturity",
            "doi": "10.4018/978-1-59140-140-7.ch004",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "strategic alignment focuses on the activities that management performs to achieve cohesive goals across the it (information technology) and other functional organizations (e.g., finance, marketing, h/r, r&amp;d, manufacturing). therefore, alignment addresses both how it is in harmony with the business, and how the business should, or could, be in harmony with it. alignment evolves into a relationship where the function of it and other business functions adapt their strategies together. achieving alignment is evolutionary and dynamic. it requires strong support from senior management, good working relationships, strong leadership, appropriate prioritization, trust, and effective communication, as well as a thorough understanding of the business and technical environments. the strategic alignment maturity assessment provides organizations with a vehicle to evaluate these activities. knowing the maturity of its strategic choices and alignment practices make it possible for a firm to see where it stands and how it can improve. this chapter discusses an approach for assessing the maturity of the business-it alignment. once maturity is understood, an organization can identify opportunities for enhancing the harmonious relationship of business and it."
        },
        {
            "id": "R192345",
            "label": "On the impact of using different templates on creating and understanding user stories",
            "doi": "10.1109/re51729.2021.00026",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "context: user stories are often used for elicitation and prioritisation of requirements. however, the lack of a widely adopted user story template, covering benefit and the usage (or not) of a persona, can affect user stories\u2019 quality, leading to ambiguity, lack of completeness, or accidental complexity. objectives: our goal was to analyse the differences between 4 alternative user story templates when creating and understanding user stories. methods: we conducted a quasi-experiment. we asked 41 participants to perform creation and understanding tasks with the user story templates. we measured their accuracy, using metrics of task success; their speed, with task duration; visual effort, collected with an eye-tracker; and participants\u2019 perceived effort, evaluated with nasa-tlx. results: regarding the impact of the different templates in creating user stories, we observed statistically significant differences in some of the metrics for accuracy, speed and visual effort. for understanding user stories, we observed small differences in terms of visual effort. conclusions: although some templates outperformed others in a few metrics, no template obtained the best overall result. as such, we found no compelling evidence that one template is \"better\" than the others."
        },
        {
            "id": "R192386",
            "label": "A Survey of Instructional Approaches in the Requirements Engineering Education Literature",
            "doi": "10.1109/re51729.2021.00030",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "requirements engineering (re) has established itself as a core software engineering discipline. it is well acknowledged that good re leads to higher quality software and considerably reduces the risk of failure or exceeding budgets of software development projects. therefore, it is of vital importance to train future software engineers in re and educate future requirements engineers to adequately manage requirements in various projects. however, to date there exists no central concept of what the most useful educational approaches are in re education in order to best interweave theory with practice. to lay the foundation for this important mission, we conducted a systematic literature review. in this paper, we report on the results and provide a synthesis of instructional approaches in re education. findings show that experiential learning through projects, collaboration, and realistic stakeholder involvement are among the most promising trends to teach both re theory and develop student soft skills."
        },
        {
            "id": "R192398",
            "label": "Agile Teams\u2019 Perception in Privacy Requirements Elicitation: LGPD\u2019s compliance in Brazil",
            "doi": "10.1109/re51729.2021.00013",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "context: the implementation of the brazilian general data protection law (lgpd) may impact activities carried out by the software development teams. it is necessary for developers to know the existing techniques and tools to carry out privacy requirements elicitation. objectives: in this research, we investigated the perception of agile software development team members from different organizations, regarding the impact that lgpd will have on the activities of the software development process. methods: we conducted an online survey and a systematic literature review to identify the techniques, methodologies and tools used in the literature to perform privacy requirements elicitation in the context of agile software development (asd). in addition, we also investigated the perception of an agile team from a federal public administration organization regarding the impacts of the obligation to develop software in accordance with the lgpd. results: our findings reveal that agile teams know the concepts related to data privacy legislation, but they do not use the techniques proposed in the literature to perform privacy requirements elicitation. in addition, agile teams face problems with outdated software requirements specifications and stakeholders\u2019 lack of knowledge regarding data privacy. conclusions: agile teams need to improve their knowledge on privacy requirements."
        },
        {
            "id": "R192428",
            "label": "Ambiguity and Generality in Natural Language Privacy Policies",
            "doi": "10.1109/re51729.2021.00014",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "privacy policies are legal documents containing application data practices. these documents are well-established sources of requirements in software engineering. however, privacy policies are written in natural language, thus subject to ambiguity and abstraction. eliciting requirements from privacy policies is a challenging task as these ambiguities can result in more than one interpretation of a given information type (e.g., ambiguous information type \"device information\" in the statement \"we collect your device information\"). to address this challenge, we propose an automated approach to infer semantic relations among information types and construct an ontology to guide requirements authors in the selection of the most appropriate information type terms. our solution utilizes word embeddings and convolutional neural networks (cnn) to classify information type pairs as either hypernymy, synonymy, or unknown. we evaluate our model on a manually-built ontology, yielding predictions that identify hypernymy relations in information type pairs with 0.904 f-1 score, suggesting a large reduction in effort required for ontology construction."
        },
        {
            "id": "R192445",
            "label": "Automated Traceability for Domain Modelling Decisions Empowered by Artificial Intelligence",
            "doi": "10.1109/re51729.2021.00023",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "domain modelling abstracts real-world entities and their relationships in the form of class diagrams for a given domain problem space. modellers often perform domain modelling to reduce the gap between understanding the problem description which expresses requirements in natural language and the concise interpretation of these requirements. however, the manual practice of domain modelling is both time-consuming and error-prone. these issues are further aggravated when problem descriptions are long, which makes it hard to trace modelling decisions from domain models to problem descriptions or vice-versa leading to completeness and conciseness issues. automated support for tracing domain modelling decisions in both directions is thus advantageous. in this paper, we propose an automated approach that uses artificial intelligence techniques to extract domain models along with their trace links. we present a traceability information model to enable traceability of modelling decisions in both directions and provide its proof-of-concept in the form of a tool. the evaluation on a set of unseen problem descriptions shows that our approach is promising with an overall median f2 score of 82.04%. we conduct an exploratory user study to assess the benefits and limitations of our approach and present the lessons learned from this study."
        },
        {
            "id": "R193022",
            "label": "Design Decisions in the Construction of Traceability Information Models for Safe Automotive Systems",
            "doi": "",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "traceability management relies on a supporting model, the traceability information model (tim), that defines which types of relationships exist between which artifacts and contains additional constraints such as multiplicities. constructing a tim that is fit for purpose is crucial to ensure that a traceability strategy yields the desired benefits. however, which design decisions are critical in the construction of tims and which impact they have on the usefulness and applicability of traceability is still an open question. in this paper, we use two cases of tims constructed for safety-critical, automotive systems with industrial safety experts, to identify key design decisions. we also propose a comparison scheme for tims based on a systematic literature review and evaluate the two cases as well as tims from the literature according to the scheme. based on our analyses, we thus derive key insights into tim construction and the design decisions that ensure that a tim is fit for purpose."
        },
        {
            "id": "R193035",
            "label": "Exploring explainability: a definition, a model, and a knowledge catalogue",
            "doi": "",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "the growing complexity of software systems and the influence of software-supported decisions in our society awoke the need for software that is transparent, accountable, and trust-worthy. explainability has been identified as a means to achieve these qualities. it is recognized as an emerging non-functional requirement (nfr) that has a significant impact on system quality. however, in order to incorporate this nfr into systems, we need to understand what explainability means from a software engineering perspective and how it impacts other quality aspects in a system. this allows for an early analysis of the benefits and possible design issues that arise from interrelationships between different quality aspects. nevertheless, explainability is currently under-researched in the domain of requirements engineering and there is a lack of conceptual models and knowledge catalogues that support the requirements engineering process and system design. in this work, we bridge this gap by proposing a definition, a model, and a catalogue for explainability. they illustrate how explainability interacts with other quality aspects and how it may impact various quality dimensions of a system. to this end, we conducted an interdisciplinary systematic literature review and validated our findings with experts in workshops."
        },
        {
            "id": "R193044",
            "label": "From Ideas to Expressed Needs: an Empirical Study on the Evolution of Requirements during Elicitation",
            "doi": "",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "requirements are elicited from the customer and other stakeholders through an iterative process of interviews, prototyping, and other interactive sessions. many communication phenomena may emerge in these early iterations, that lead initial ideas to be transformed, renegotiated, or reframed. understanding how this process takes place can help in solving possible communication issues as well as their consequences. in this work, we perform an exploratory study of descriptive nature to understand in which way requirements get transformed from initial ideas into documented needs. to this end, we select 30 subjects that act as requirements analysts, and we perform a set of elicitation sessions with a fictional customer. the customer is required to study a sample requirements document for a system beforehand and to answer the questions of the analysts about the system. after the elicitation sessions, the analysts produce user stories for the system. these are compared with the original ones by two researchers to assess to which extent and in which way the initial requirements evolved throughout the interactive sessions. our results show that between 30% and 38% of the produced user stories include content that can be fully traced to the initial ones, while the rest of the content is dedicated to new requirements. we also show what types of requirements are introduced through the elicitation process, and how they vary depending on the analyst. our work contributes to theory in requirements engineering, with empirically grounded, quantitative data, concerning the impact of elicitation activities with respect to initial ideas."
        },
        {
            "id": "R193070",
            "label": "Non-functional requirements for machine learning: understanding current use and challenges in industry",
            "doi": "",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "machine learning (ml) is an application of artificial intelligence (ai) that uses big data to produce complex predictions and decision-making systems, which would be challenging to obtain otherwise. to ensure the success of ml-enabled systems, it is essential to be aware of certain qualities of ml solutions (performance, transparency, fairness), known from a requirement engineering (re) perspective as non-functional requirements (nfrs). however, when systems involve ml, nfrs for traditional software may not apply in the same ways; some nfrs may become more prominent or less important; nfrs may be defined over the ml model, data, or the entire system; and nfrs for ml may be measured differently. in this work, we aim to understand the state-of-the-art and challenges of dealing with nfrs for ml in industry. we interviewed ten engineering practitioners working with nfrs and ml. we find examples of (1) the identification and measurement of nfrs for ml, (2) identification of more and less important nfrs for ml, and (3) the challenges associated with nfrs and ml in the industry. this knowledge paints a picture of how ml-related nfrs are treated in practice and helps to guide future re for ml efforts."
        },
        {
            "id": "R193089",
            "label": "On the Role of User Feedback in Software Evolution: a Practitioners\u2019 Perspective",
            "doi": "",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "user feedback is indispensable in software evolution. previous work has proposed ways for automatically extracting requirements, bug reports and other valuable information from feedback. however, little is actually known about how user feedback\u2014 especially the one available through newer channels, such as social media\u2014is incorporated in development processes. to date, only a few case studies discuss the matter and the results are not always consistent. we carried out a mixed methods study to understand the current state of practice of harnessing user feedback in software development. qualitatively, we performed interviews with 18 software practitioners to get a deeper understanding of the role of user feedback in software evolution. quantitatively, we surveyed 101 software practitioners to cross-validate the interview findings and improve the generalizability of the results. we found that feedback is captured to (1) identify bugs, features and usability issues, (2) get a better understanding of the user, and (3) prioritize requirements. our results indicate that analyzing feedback is time-consuming and has a number of challenges. among them, feedback is typically analyzed manually and is spread over a wide range of channels and company departments. our findings stress the current importance for cross-department cooperation and call for the exploration of tools that can centralize user feedback."
        },
        {
            "id": "R193108",
            "label": "Perspectives on Regulatory Compliance in Software Engineering",
            "doi": "",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"compliance reviews within a software organization are internal attempts to verify regulatory and security requirements during product development before its release. however, these reviews are not enough to adequately assess and address regulatory and security requirements throughout a software's development lifecycle. we believe requirements engineers can benefit from an improved understanding of how software practitioners treat and perceive compliance requirements. this paper describes an interview study seeking to understand how regulatory and security standard requirements are addressed, how burdensome they may be for businesses, and how our participants perceived them in the software development lifecycle. we interviewed 15 software practitioners from 13 organizations with different roles in the software development process and working in various industry domains, including big tech, healthcare, data analysis, finance, and small businesses. our findings suggest that, for our participants, the software release process is the ultimate focus for regulatory and security compliance reviews. also, most participants suggested that having a defined process for addressing compliance requirements was freeing rather than burdensome. finally, participants generally saw compliance requirements as an investment for both employees and customers. these findings may be unintuitive, and we discuss seven lessons this work may hold for requirements engineering.\""
        },
        {
            "id": "R193295",
            "label": "The practical role of context modeling in the elicitation of context-aware functionalities: a survey",
            "doi": "",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "context-aware functionalities are functionalities that consider the context to produce a certain system behavior, typically an adaptation or recommendation. as contextual elements such as time, location, weather, user activity, device characteristics, network status, and countless others are becoming increasingly more accessible, the potential for adding context awareness to applications is enormous. identifying novel, unexpected, and even delightful context-aware functionalities in practice can be challenging, though: what context information is relevant for a given user task? how can contextual elements be combined? what if there is a large number of contextual elements? context modeling has been described in the literature as an essential aspect in the elicitation of context-aware functionalities; however, reports on the state of the practice are rare. in this study, we conducted a survey with industrial practitioners, mostly experienced professionals from large enterprises, to investigate how context models and context-modeling activities have been used to support the elicitation of context-aware functionalities. the results indicate a gap between research and industry: context models are rarely used in practice, and context-modeling activities such as analysis of relevance and especially analysis of combinations of contextual elements have been overlooked due to their high complexity, despite practitioners recognizing their importance."
        },
        {
            "id": "R193308",
            "label": "The Role of Linguistic Relativity on the Identification of Sustainability Requirements: An Empirical Study",
            "doi": "",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "linguistic-relativity-theory states that language and its structure influence people\u2019s world view and cognition. we investigate how this theory impacts the identification of requirements in practice. to this end, we conducted two controlled experiments with 101 participants. we randomly showed participants a set of requirements dimensions (i.e. a language structure) either with a focus on software quality or on sustainability and asked them to identify the requirements for a grocery shopping app according to these dimensions. participants of the control group were not given any dimensions. the results show that the use of requirements dimensions significantly increases the number of identified requirements in comparison to the control group. furthermore, participants who were given the sustainability dimensions identified more sustainability requirements. in follow up interviews with 16 practitioners, the interviewees reported benefits of the dimensions such as a holistic guidance but were also concerned about the customers acceptance. furthermore, they stated challenges of implementing sustainability dimensions in the daily business but also suggested solutions like establishing sustainability as a common standard. our study indicates that carefully structuring requirements engineering along sustainability dimensions can guide development teams towards considering and ensuring software sustainability."
        },
        {
            "id": "R193330",
            "label": "Towards Achieving Trust Through Transparency and Ethics",
            "doi": "",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "the ubiquitous presence of software in the products we use, together with artificial intelligence in these products, has led to an increasing need for consumer trust. consumers often lose faith in products, and the lack of trust propagates to the companies behind them. this is even more so in mission-critical systems such as autonomous vehicles and clinical support systems. this paper follows grounded theory principles to elicit knowledge related to trust, ethics, and transparency. we approach these qualities as non-functional requirements (nfrs), aiming to build catalogs to subsidize the construction of socially responsible software. the corpus we have used was built on a selected collection of literature on corporate social responsibility, with an emphasis on business ethics. our challenge is how to encode the social perspective knowledge, mainly through the view of corporate social responsibility, on how organizations or institutions achieve trustworthiness. since our ground perspective is that of nfrs, results are presented by a catalogue of trust as a non-functional requirement, represented as a softgoal interdependency graph (sig). the sig language helps software engineers in understanding alternatives they have to improve trust in software products."
        },
        {
            "id": "R193336",
            "label": "Unsupervised Topic Discovery in User Comments",
            "doi": "",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "on social media platforms like twitter, users regularly share their opinions and comments with software vendors and service providers. popular software products might get thousands of user comments per day. research has shown that such comments contain valuable information for stakeholders, such as feature ideas, problem reports, or support inquiries. however, it is hard to manually manage and grasp a large amount of user comments, which can be redundant and of a different quality. consequently, researchers suggested automated approaches to extract valuable comments, e.g., through problem report classifiers. however, these approaches do not aggregate semantically similar comments into specific aspects to provide insights like how often users reported a certain problem.we introduce an approach for automatically discovering topics composed of semantically similar user comments based on deep bidirectional natural language processing algorithms. stakeholders can use our approach without the need to configure critical parameters like the number of clusters. we present our approach and report on a rigorous multiple-step empirical evaluation to assess how cohesive and meaningful the resulting clusters are. each evaluation step was peer-coded and resulted in inter-coder agreements of up to 98%, giving us high confidence in the approach. we also report a thematic analysis on the topics discovered from tweets in the telecommunication domain."
        },
        {
            "id": "R193341",
            "label": "What can Open Domain Model Tell Us about the Missing Software Requirements: A Preliminary Study",
            "doi": "",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "completeness is one of the most important attributes of software requirement specification. unfortunately, incompleteness is one of the most difficult violations to detect. some approaches have been proposed to detect missing requirements based on the requirement-oriented domain model. however, these kinds of models are actually lack for lots of domains. fortunately, the domain models constructed for different purposes can usually be found online. this raises a question: whether or not these domain models are useful for finding the missing functional information in requirement specification? to explore this question, we design and conduct a preliminary study by computing the overlapping rate between the entities in domain models and the concepts of natural language software requirements, and then digging into four regularities of the occurrence of these entities(concepts) based on two example domains. the usefulness of these regularities, especially the one based our proposed metric ahme (with 54% and 70% of f2 on the two domains), has been initially evaluated with an additional experiment."
        },
        {
            "id": "R193357",
            "label": "What\u2019s up with Requirements Engineering for Artificial Intelligence Systems?",
            "doi": "",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "in traditional approaches to building software systems (that do not include an artificial intelligent (ai) or machine learning (ml) component), requirements engineering (re) activities are well-established and researched. however, building software systems with one or more ai components may depend heavily on data with limited or no insight into the system\u2019s workings. therefore, engineering such systems poses significant new challenges to re. our search showed that literature has focused on using ai to manage re activities, with limited research on re for ai (re4ai). our study\u2019s main objective was to investigate current approaches in writing requirements for ai/ml systems, identify available tools and techniques used to model requirements, and find existing challenges and limitations. we performed a systematic literature review (slr) of current re4ai methods and identified 27 primary studies. using these studies, we analysed the key tools and techniques used to specify and model requirements and found several challenges and limitations of existing re4ai practices. we further provide recommendations for future research, based on our analysis of the primary studies and mapping to industry guidelines in google pair). the slr findings highlighted that present re applications were not adaptive to manage most ai/ml systems and emphasised the need to provide new techniques and tools to support re4ai."
        },
        {
            "id": "R193371",
            "label": "Automated recommendation of templates for legal requirements",
            "doi": "10.1109/RE48521.2020.00027",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "[context] in legal requirements elicitation, requirements analysts need to extract obligations from legal texts. however, legal texts often express obligations only indirectly, for example, by attributing a right to the counterpart. this phenomenon has already been described in the requirements engineering (re) literature [1]. [objectives] we investigate the use of requirements templates for the systematic elicitation of legal requirements. our work is motivated by two observations: (1) the existing literature does not provide a harmonized view on the requirements templates that are useful for legal re; (2) despite the promising recent advancements in natural language processing (nlp), automated support for legal re through the suggestion of requirements templates has not been achieved yet. our objective is to take steps toward addressing these limitations. [methods] we review and reconcile the legal requirement templates proposed in re. subsequently, we conduct a qualitative study to define nlp rules for template recommendation. [results and conclusions] our contributions consist of (a) a harmonized list of requirements templates pertinent to legal re, and (b) rules for the automatic recommendation of such templates. we evaluate our rules through a case study on 400 statements from two legal domains. the results indicate a recall and precision of 82,3% and 79,8%, respectively. we show that introducing some limited interaction with the analyst considerably improves accuracy. specifically, our human-feedback strategy increases recall by 12% and precision by 10,8%, thus yielding an overall recall of 94,3% and overall precision of 90,6%."
        },
        {
            "id": "R193385",
            "label": "The way it makes you feel predicting users\u2019 engagement during interviews with biofeedback and supervised learning",
            "doi": "",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "capturing users\u2019 engagement is crucial for gathering feedback about the features of a software product. in a market-driven context, current approaches to collect and analyze users\u2019 feedback are based on techniques leveraging information extracted from product reviews and social media. these approaches are hardly applicable in bespoke software development, or in contexts in which one needs to gather information from specific users. in such cases, companies need to resort to face-to-face interviews to get feedback on their products. in this paper, we propose to utilize biofeedback to complement interviews with information about the engagement of the user on the discussed features and topics. we evaluate our approach by interviewing users while gathering their biometric data using an empatica e4 wristband. our results show that we can predict users\u2019 engagement by training supervised machine learning algorithms on the biometric data. the results of our work can be used to facilitate the prioritization of product features and to guide the interview based on users\u2019 engagement."
        },
        {
            "id": "R193406",
            "label": "Theory as a source of software requirements",
            "doi": "",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "today, when undertaking requirements elicitation, engineers attend to the needs and wants of the user groups considered relevant for the software system. however, answers to some relevant question (e.g., how to improve adoption of the intended system) cannot always be addressed through direct need and want elicitation. using an example of energy demand-response systems, this paper demonstrates that use of grounded theory analysis can help address such questions. the theory emerging from such analysis produces a set of additional requirements which cannot be directly elicited from individuals/groups, and would otherwise be missed out. thus, we demonstrate that the theory generated through grounded theory analysis can serve as an additional valuable source of software system requirements."
        },
        {
            "id": "R193418",
            "label": "Voice of the users: A demographic study of software feedback behaviour",
            "doi": "",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "user feedback on mobile app stores, product forums, and on social media can contain product development insights. there has been a lot of recent research studying this feedback and developing methods to automatically extract requirement-related information. this feedback is generally considered to be the \u201cvoice of the users\u201d; however, only a subset of software users provide online feedback. if the demographics of the online feedback givers are not representative of the user base, this introduces the possibility of developing software that does not meet the needs of all users. it is, therefore, important to understand who provides online feedback to ensure the needs of underrepresented groups are not being missed.in this work, we directly survey 1040 software users about their feedback habits, software use, and demographic information. their responses indicate that there are statistically significant differences in who gives feedback on each online channel, with respect to traditional demographics (gender, age, etc). we also identify key differences in what motivates software users to engage with each of the three channels. our findings provide valuable context for requirements elicited from online feedback and show that considering information from all channels will provide a more comprehensive view of user needs."
        },
        {
            "id": "R193467",
            "label": "Cutting through the jungle: Disambiguating model-based traceability terminology",
            "doi": "",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "traceability, a classic requirements engineering topic, is increasingly used in the context of model-based engineering. however, researchers and practitioners lack a concise terminology to discuss aspects of requirements traceability in situations in which engineers heavily rely on models and model-based engineering. while others have previously surveyed the domain, no one has so far provided a clear, unambiguous set of terms that can be used to discuss traceability in such a context. we therefore set out to cut a path through the jungle of terminology for model-based traceability, ground it in established terminology from requirements engineering, and derive an unambiguous set of relevant terms. we also map the terminology used in existing primary and secondary studies to our taxonomy to show differences and commonalities. the contribution of this paper is thus a terminology for model-based traceability that allows requirements engineers and engineers working with models to unambiguously discuss their joint traceability efforts."
        },
        {
            "id": "R193473",
            "label": "Continual human value analysis in software development: A goal model based approach",
            "doi": "",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "software failures that demonstrate violations of human values can result in financial losses, reputation damages and social implications. therefore, integrating human values into software is vital to satisfy stakeholder needs. however, developing methodological approaches that allow systematic integration of human values throughout the software development life cycle is an open challenge. this paper proposes the continual value(s) assessment (cva) framework that uses extended goal and feature modeling techniques to support systematic integration, tracing and evaluation of human values in software systems. the cva framework prescribes (i) brainstorming of value implications of system features based on conventional system artefacts and (ii) the expansion of the existing set of system features to better serve stakeholder values expectations. in a pilot study, we use an emergency alarm system for the elderly to demonstrate the feasibility of the framework. we further discuss the challenges we faced while applying the framework and present the lessons learned from the pilot study."
        },
        {
            "id": "R193481",
            "label": "Extracting and classifying requirements from software engineering contracts",
            "doi": "",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "in this paper, we present our work on extracting and classifying requirements from large software engineering contracts. typically, the process of requirements elicitation begins after a contractual agreement is signed by all participants. our interactions with the legal compliance team in a large vendor organization reveal that business contracts can help in the identification of high-level requirements relevant to the success of software engineering projects. we posit that requirements engineering as a discipline has an even wider scope than software engineering of which it is traditionally considered to be a sub-discipline. this is because software engineering-specific requirements are but a part of the success story of any large project. the requirements that emerge from contracts are obligatory in nature, whether or not they pertain to core software development. therefore, it is important that these are extracted and classified for the benefit of software engineers and other stakeholders responsible for a project. we discuss the results of an exploratory study and a range of experiments from the use of regular expressions to bidirectional encoder representations from transformers for automating the extraction and classification of requirements from software engineering contracts. with bidirectional encoder representations from transformers, we obtained a high f-score of greater than eighty four percent for classification of requirements."
        },
        {
            "id": "R193490",
            "label": "Which app features are being used? Learning app feature usages from interaction data",
            "doi": "",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "in the dynamic and fast-growing app market, monitoring and understanding how past releases are actually being used is indispensable for successful app maintenance and evolution. current app usage analytics tools either log execution events, e.g., in stack traces, or general usage information such as the app activation time, location, and device. in this paper, we focus on analyzing the usages of the single app features as described in release notes and app pages. we suggest monitoring nine app-independent, privacy-friendly interaction events for training a machine learning model to learn app feature usages. we conducted a crowdsourcing study with 55 participants who labeled 5,815 feature usages of 170 unique apps for 18 days. our within-apps evaluation shows that we could achieve encouraging precision and recall values already with ten labeled feature usages. for certain popular features such as browse newsfeed or send an email, we achieved f1 values above 88%. betweenapps feature learning seems feasible with f1 values of up to 86%."
        },
        {
            "id": "R193798",
            "label": "Classifying user requirements from online feedback in small dataset environments using deep learning",
            "doi": "",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "an overwhelming number of users access app repositories like app store/google play and social media platforms like twitter, where they provide feedback on digital experiences. this vast textual corpus comprising user feedback has the potential to unearth detailed insights regarding the users\u2019 opinions on products and services. various tools have been proposed that employ natural language processing (nlp) and traditional machine learning (ml) based models as an inexpensive mechanism to identify requirements in user feedback. however, they fall short on their classification accuracy over unseen data due to factors like the cost of generating voluminous de-biased labeled datasets and general inefficiency. recently, van vliet et al. [1] achieved state-of-the-art results extracting and classifying requirements from user reviews through traditional crowdsourcing. based on their reference classification tasks and outcomes, we successfully developed and validated a deep-learning-backed artificial intelligence pipeline to achieve a state-of-the-art averaged classification accuracy of \u223c87% on standard tasks for user feedback analysis. this approach, which comprises a bert-based sequence classifier, proved effective even in extremely low-volume dataset environments. additionally, our approach drastically reduces the time and costs of evaluation, and improves on the accuracy measures achieved using traditional ml-/nlp-based techniques."
        },
        {
            "id": "R193828",
            "label": "Mining reddit as a new source for software requirements",
            "doi": "",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "mining app stores and social media has proven to be a good source for collecting user feedback to foster requirements engineering and software evolution. recent literature on mining software-related data from social platforms, such as twitter and facebook, shows that it complements app store mining. however, there are many other platforms where users discuss and provide feedback on software applications that are not thoroughly researched and analysed. one of such platforms is reddit. in this paper, we introduce reddit as a new potential data source and explore if and how requirements engineering and software evolution can benefit from obtaining user feedback from reddit. we also present an exploratory study in which we analysed the usage characteristics (i.e., frequency of posts, number of comments, and number of users for each subreddit) of reddit posts about software applications. furthermore, we examined the content of the posts and the results reveal that almost 54% of posts contain useful information. finally, we investigated the potential of automatic classification and applied machine learning algorithms to unstructured and noisy reddit data to perform automated classification into the categories of bug reports, feature related, and irrelevant. we found that the support vector machine algorithm with the f1-score of 84% can be effective in categorizing reddit posts. our results show that reddit posts provide useful feedback on software applications that can foster requirements engineering and software evolution."
        },
        {
            "id": "R193849",
            "label": "TEM: a transparency engineering methodology enabling users\u2019 trust judgement",
            "doi": "",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "transparency is key to enhancing users\u2019 trust by enabling their judgment on the outcomes and consequences of a system\u2019s operations. this paper presents the transparency engineering methodology (tem) to generate transparency requirements that enable users\u2019 trust judgement. the idea is to identify where transparency is lacking and to address this through patterns augmenting the specification of data, use case, and process requirements. due to the complexity of software, it is impossible (and undesirable) to achieve full transparency throughout the system. however, transparency can be improved for selected system aspects. this is demonstrated using the results from an industrial case study with a medical technology company where, with the help of tem, existing functional requirements were refined, and transparency requirements generated systematically."
        },
        {
            "id": "R193866",
            "label": "The Rise and Fall of COVID-19 Contact-Tracing Apps: when NFRs Collide with Pandemic",
            "doi": "10.1109/RE51729.2021.00017",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "to complement the manual contact-tracing methods, a flood of coronavirus-related apps was launched in the first half of 2020. despite the incredible promises made by the governments, contact-tracing apps did not live up to expectations. we provide a contextual perspective of the government commissioned contact-tracing apps from four countries to understand the non-functional requirements (nfrs) and socio-technical factors that hindered the success of these apps. we collected the user reviews from the app stores for ios and android versions and identified top news articles related to each app. our analysis revealed that the dominant factors behind the negligible success of these apps are complex and entangled with the cultural and political dimensions rather than being just technical. the multilayer diversity of the target users also impacted the design and development of contact-tracing apps in an extremely challenging situation. this perspective paper brings into light important elements, such as politics and socio-cultural aspects that should be studied in the design of contact-tracing apps, and public apps in general."
        },
        {
            "id": "R193920",
            "label": "Same same but different: Finding similar user feedback across multiple platforms and languages",
            "doi": "10.1109/RE48521.2020.00017",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "users submit feedback about the software they use through application distributions platforms, i.e., app stores, and social media. previous research has found that this type of feedback contains valuable information for software evolution, such as bug reports, or feature requests. however, popular applications receive thousands of feedback entities per day, making their manual analysis unrealistic. in this work, we present an approach to automatically identify similar user feedback across different languages and platforms. at the core of the approach is a word aligner that aligns words based on their semantic similarity and the similarity of their local semantic contexts. additionally, we make use of machine translation, sentiment analysis, and text classification, to extract the sentiment polarity and content nature of user feedback written in different languages. we use the results of these components to compute a similarity score between user feedback pairs. we evaluated our approach on user feedback entities written in four different languages, and retrieved from five different mobile applications obtained from four different app stores and social networking sites. the obtained results are encouraging. compared to human assessment, the overall performance for monolingual user feedback pairs yielded a strong correlation of 0.79. for the crosslingual feedback pairs the correlation was also strong, with a value of 0.78."
        },
        {
            "id": "R193929",
            "label": "NoRBERT: Transfer learning for requirements classification",
            "doi": "",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "classifying requirements is crucial for automatically handling natural language requirements. the performance of existing automatic classification approaches diminishes when applied to unseen projects because requirements usually vary in wording and style. the main problem is poor generalization. we propose norbert that fine-tunes bert, a language model that has proven useful for transfer learning. we apply our approach to different tasks in the domain of requirements classification. we achieve similar or better results f1-scores of up to 94%) on both seen and unseen projects for classifying functional and non-functional requirements on the promise nfr dataset. norbert outperforms recent approaches at classifying non-functional requirements subclasses. the most frequent classes are classified with an average f1-score of 87%. in an unseen project setup on a relabeled promise nfr dataset, our approach achieves an improvement of 15 percentage points in average f1 score compared to recent approaches. additionally, we propose to classify functional requirements according to the included concerns, i.e., function, data, and behavior. we labeled the functional requirements in the promise nfr dataset and applied our approach. norbert achieves an f1-score of up to 92%. overall, norbert improves requirements classification and can be applied to unseen projects with convincing results."
        },
        {
            "id": "R152014",
            "label": "From scenario modeling to scenario programming for reactive systems with dynamic topology",
            "doi": "10.1145/3106237.3122827",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "software-intensive systems often consist of cooperating reactive components. in mobile and reconfigurable systems, their topology changes at run-time, which influences how the components must cooperate. the scenario modeling language (sml) offers a formal approach for specifying the reactive behavior such systems that aligns with how humans conceive and communicate behavioral requirements. simulation and formal checks can find specification flaws early. we present a framework for the scenario-based programming (sbp) that reflects the concepts of sml in java and makes the scenario modeling approach available for programming. sbp code can also be generated from sml and extended with platform-specific code, thus streamlining the transition from design to implementation. as an example serves a car-to-x communication system. demo video and artifact: http://scenariotools.org/esecfse-2017-tool-demo/"
        },
        {
            "id": "R152020",
            "label": "A Scenario-based MDE Process for Developing Reactive Systems: A Cleaning Robot Example",
            "doi": "",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "this paper presents the scenariotools solution for developing a cleaning robot system, an instance of the rover problem of the mde tools challenge 2017. we present an mde process that consists of (1) the modeling of the system behavior as a scenario-based assume-guarantee specification with sml (scenario modeling language), (2) the formal realizabilitychecking and verification of the specification, (3) the generation of sbp (scenario-based programming) java code from the sml specification, and, finally, (4) adding platform-specific code to connect specification-level events with platform-level sensorand actuator-events. the resulting code can be executed on a raspberrypi-based robot. the approach is suited for developing reactive systems with multiple cooperating components. its strength is that the scenario-based modeling corresponds closely to how humans conceive and communicate behavioral requirements. sml in particular supports the modeling of environment assumptions and dynamic component structures. the formal checks ensure that the system satisfies its specification."
        },
        {
            "id": "R159789",
            "label": "The Potential of Using Vision Videos for CrowdRE: Video Comments as a Source of Feedback",
            "doi": "10.1109/REW53955.2021.00053",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "vision videos are established for soliciting feedback and stimulating discussions in requirements engineering (re) practices such as focus groups. different researchers motivated the transfer of these benefits into crowd-based re (crowdre) by using vision videos on social media platforms. so far, however, little research explored the potential of using vision videos for crowdre in detail. in this paper, we analyze and assess this potential, in particular, focusing on video comments as a source of feedback. in a case study, we analyzed 4505 comments on a vision video from youtube. we found that the video solicited 2770 comments from 2660 viewers in four days. this is more than 50% of all comments the video received in four years. even though only a certain fraction of these comments are relevant to re, the relevant comments address typical intentions and topics of user feedback, such as feature request or problem report. besides the typical user feedback categories, we found more than 300 comments that address the topic safety which has not appeared in previous analyses of user feedback. in an automated analysis, we compared the performance of three machine learning algorithms on classifying the video comments. despite certain differences, the algorithms classified the video comments well. based on these findings, we conclude that the use of vision videos for crowdre has a large potential. despite the preliminary nature of the case study, we are optimistic that vision videos can motivate stakeholders to actively participate in a crowd and solicit numerous of video comments as a valuable source of feedback."
        },
        {
            "id": "R159793",
            "label": "Viewing Vision Videos Online: Opportunities for Distributed Stakeholders",
            "doi": "10.1109/rew53955.2021.00054",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "creating shared understanding between stakeholders is essential for the success of software projects. conflicting requirements originating from misaligned mental models can hinder the development process. the use of videos to present abstract system visions is one approach to counteract this problem. these videos are usually shown in in-person meetings. however, face-to-face meetings are not suited to every situation and every stakeholder, for example due to scheduling constraints. methods for the use of vision videos in online settings are necessary. furthermore, methods enabling an asynchronous use of vision videos are needed for cases when conjoined meetings are impossible even in an online setting.in this paper, we compare synchronous and asynchronous viewings of vision videos in online settings. the two methods are piloted in a preliminary experiment. the results show a difference in the amount of arguments regarding the presented visions. on average, participants who took part in asynchronous meetings stated more arguments. our results point to multiple advantages and disadvantages as well as use cases for each type. for example, a synchronous meeting could be chosen when all involved stakeholders can attend the appointment to discuss the vision and to quickly resolve ambiguities. an asynchronous meeting could be held if a joint meeting is not feasible due to time constraints. we also discuss how our findings can be applied to the elicitation of requirements from a crowd of stakeholders."
        },
        {
            "id": "R74509",
            "label": "Mapping human values and scrum roles: a study on students' preferences",
            "doi": "10.1145/3387940.3391467",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"despite the long tradition on the study of human values, the impact of this field in the software engineering domain is rarely studied. to these regards, this study focuses on applying human values to agile software development process, more specifically to scrum roles. thus, the goal of the study is to explore possible associations between human values and scrum roles preferences among students. questionnaires are designed by employing the short schwartz's value survey and are distributed among 57 students. the results of the quantitative analysis process consisting of descriptive statistics, linear regression models and pearson correlation coefficients, revealed that values such as power and self-direction influence the preference for the product owner role, the value of hedonism influences the preference for scrum masters and self-direction is associated with team members' preference.\""
        },
        {
            "id": "R74516",
            "label": "Measuring human values in software engineering",
            "doi": "10.1145/3239235.3267427",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"background: human values, such as prestige, social justice, and financial success, influence software production decision-making processes. while their subjectivity makes some values difficult to measure, their impact on software motivates our research. aim: to contribute to the scientific understanding and the empirical investigation of human values in software engineering (se). approach: drawing from social psychology, we consider values as mental representations to be investigated on three levels: at a system (l1), personal (l2), and instantiation level (l3). method: we design and develop a selection of tools for the investigation of values at each level, and focus on the design, development, and use of the values q-sort. results: from our study with 12 software practitioners, it is possible to extract three values `prototypes' indicative of an emergent typology of values considerations in se. conclusions: the values q-sort generates quantitative values prototypes indicating values relations (l1) as well as rich personal narratives (l2) that reflect specific software practices (l3). it thus offers a systematic, empirical approach to capturing values in se.\""
        },
        {
            "id": "R74547",
            "label": "Supporting Requirements Elicitation by Tool-Supported Video Analysis",
            "doi": "10.1109/re.2016.10",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"workshops are an established technique for requirements elicitation. a lot of information is revealed during a workshop, which is generally captured via textual minutes. the scribe suffers from a cognitive overload due to the difficulty of gathering all information, listening and writing at the same time. video recording is used as additional option to capture more information, including non-verbal gestures. since a workshop can take several hours, the recorded video will be long and may be disconnected from the scribe's notes. therefore, the weak and unclear structure of the video complicates the access to the recorded information, for example in subsequent requirements engineering activities. we propose the combination of textual minutes and video with a software tool. our objective is connecting textual notes with the corresponding part of the video. by highlighting relevant sections of a video and attaching notes that summarize those sections, a more useful structure can be achieved. this structure allows an easy and fast access to the relevant information and their corresponding video context. thus, a scribe's overload can be mitigated and further use of a video can be simplified. tool-supported analysis of such an enriched video can facilitate the access to all communicated information of a workshop. this allows an easier elicitation of high-quality requirements. we performed a preliminary evaluation of our approach in an experimental set-up with 12 participants. they were able to elicit higher-quality requirements with our software tool.\""
        },
        {
            "id": "R74688",
            "label": "Video as a By-Product of Digital Prototyping: Capturing the Dynamic Aspect of Interaction",
            "doi": "10.1109/rew.2017.16",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "requirements engineering provides several practices to analyze how a user wants to interact with a future software. mockups, prototypes, and scenarios are suitable to understand usability issues and user requirements early. nevertheless, users are often dissatisfied with the usability of a resulting software. apparently, previously explored information was lost or no longer accessible during the development phase.scenarios are one effective practice to describe behavior. however, they are commonly notated in natural language which is often improper to capture and communicate interaction knowledge comprehensible to developers and users. the dynamic aspect of interaction is lost if only static descriptions are used. digital prototyping enables the creation of interactive prototypes by adding responsive controls to hand-or digitally drawn mockups. we propose to capture the events of these controls to obtain a representation of the interaction. from this data, we generate videos, which demonstrate interaction sequences, as additional support for textual scenarios.variants of scenarios can be created by modifying the captured event sequences and mockups. any change is unproblematic since videos only need to be regenerated. thus, we achieve video as a by-product of digital prototyping. this reduces the effort compared to video recording such as screencasts. a first evaluation showed that such a generated video supports a faster understanding of a textual scenario compared to static mockups."
        },
        {
            "id": "R75426",
            "label": "Security and Cryptographic Challenges for Authentication Based on Biometrics Data",
            "doi": "10.3390/cryptography2040039",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "authentication systems based on biometrics characteristics and data represents one of the most important trend in the evolution of the society, e.g., smart city, internet-of-things (iot), cloud computing, big data. in the near future, biometrics systems will be everywhere in the society, such as government, education, smart cities, banks etc. due to its uniqueness, characteristic, biometrics systems will become more and more vulnerable, privacy being one of the most important challenges. the classic cryptographic primitives are not sufficient to assure a strong level of secureness for privacy. the current paper has several objectives. the main objective consists in creating a framework based on cryptographic modules which can be applied in systems with biometric authentication methods. the technologies used in creating the framework are: c#, java, c++, python, and haskell. the wide range of technologies for developing the algorithms give the readers the possibility and not only, to choose the proper modules for their own research or business direction. the cryptographic modules contain algorithms based on machine learning and modern cryptographic algorithms: aes (advanced encryption system), sha-256, rc4, rc5, rc6, mars, blowfish, twofish, threefish, rsa (rivest-shamir-adleman), elliptic curve, and diffie hellman. as methods for implementing with success the cryptographic modules, we will propose a methodology which can be used as a how-to guide. the article will focus only on the first category, machine learning, and data clustering, algorithms with applicability in the cloud computing environment. for tests we have used a virtual machine (virtual box) with apache hadoop and a biometric analysis tool. the weakness of the algorithms and methods implemented within the framework will be evaluated and presented in order for the reader to acknowledge the latest status of the security analysis and the vulnerabilities founded in the mentioned algorithms. another important result of the authors consists in creating a scheme for biometric enrollment (in results). the purpose of the scheme is to give a big overview on how to use it, step by step, in real life, and how to use the algorithms. in the end, as a conclusion, the current work paper gives a comprehensive background on the most important and challenging aspects on how to design and implement an authentication system based on biometrics characteristics."
        },
        {
            "id": "R75435",
            "label": "MapReduce: simplified data processing on large clusters",
            "doi": "10.1145/1327452.1327492",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\" \\n mapreduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. users specify the computation in terms of a\\n map \\n and a\\n reduce \\n function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. programmers find the system easy to use: more than ten thousand distinct mapreduce programs have been implemented internally at google over the past four years, and an average of one hundred thousand mapreduce jobs are executed on google's clusters every day, processing a total of more than twenty petabytes of data per day.\\n \""
        },
        {
            "id": "R76118",
            "label": "CrowdREquire: A Requirements Engineering Crowdsourcing Platform",
            "doi": "",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "this paper describes crowdrequire, a platform that supports requirements engineering using the crowdsourcing concept. the power of the crowd is in the diversity of talents and expertise available within the crowd and crowdrequire specifies how requirements engineering can harness skills available in the crowd. in developing crowdrequire, this paper designs a crowdsourcing business model and market strategy for crowdsourcing requirements engineering irrespective of the professions and areas of expertise of the crowd involved. this is also a specific application of crowdsourcing which establishes the general applicability and efficacy of crowdsourcing. the results obtained could be used as a reference for other crowdsourcing systems as well."
        },
        {
            "id": "R76123",
            "label": "Crowdsourcing to elicit requirements for MyERP application",
            "doi": "10.1109/crowdre.2015.7367586",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "crowdsourcing is an emerging method to collect requirements for software systems. applications seeking global acceptance need to meet the expectations of a wide range of users. collecting requirements and arriving at consensus with a wide range of users is difficult using traditional method of requirements elicitation. this paper presents crowdsourcing based approach for german medium-size software company myerp that might help the company to get access to requirements from non-german customers. we present the tasks involved in the proposed solution that would help the company meet the goal of eliciting requirements at a fast pace with non-german customers."
        },
        {
            "id": "R76126",
            "label": "Crowd-centric Requirements Engineering",
            "doi": "10.1109/ucc.2014.96",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "requirements engineering is a preliminary and crucial phase for the correctness and quality of software systems. despite the agreement on the positive correlation between user involvement in requirements engineering and software success, current development methods employ a too narrow concept of that user and rely on a recruited set of users considered to be representative. such approaches might not cater for the diversity and dynamism of the actual users and the context of software usage. this is especially true in new paradigms such as cloud and mobile computing. to overcome these limitations, we propose crowd-centric requirements engineering (ccre) as a revised method for requirements engineering where users become primary contributors, resulting in higher-quality requirements and increased user satisfaction. ccre relies on crowd sourcing to support a broader user involvement, and on gamification to motivate that voluntary involvement."
        },
        {
            "id": "R76341",
            "label": "The Crowd in Requirements Engineering: The Landscape and Challenges",
            "doi": "10.1109/ms.2017.33",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "crowd-based requirements engineering (crowdre) could significantly change re. performing re activities such as elicitation with the crowd of stakeholders turns re into a participatory effort, leads to more accurate requirements, and ultimately boosts software quality. although any stakeholder in the crowd can contribute, crowdre emphasizes one stakeholder group whose role is often trivialized: users. crowdre empowers the management of requirements, such as their prioritization and segmentation, in a dynamic, evolved style through collecting and harnessing a continuous flow of user feedback and monitoring data on the usage context. to analyze the large amount of data obtained from the crowd, automated approaches are key. this article presents current research topics in crowdre; discusses the benefits, challenges, and lessons learned from projects and experiments; and assesses how to apply the methods and tools in industrial contexts. this article is part of a special issue on crowdsourcing for software engineering."
        },
        {
            "id": "R76353",
            "label": "Providing a User Forum is not enough: First Experiences of a Software Company with CrowdRE",
            "doi": "10.1109/rew.2017.21",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"crowd-based requirements engineering (crowdre) is promising to derive requirements by gathering and analyzing information from the crowd. setting up crowdre in practice seems challenging, although first solutions to support crowdre exist. in this paper, we report on a german software company's experience on crowd involvement by using feedback communication channels and a monitoring solution for user-event data. in our case study, we identified several problem areas that a software company is confronted with to setup an environment for gathering requirements from the crowd. we conclude that a crowdre process cannot be implemented ad-hoc and that future work is needed to create and analyze a continuous feedback and monitoring data stream.\""
        },
        {
            "id": "R76792",
            "label": "Mining Twitter Feeds for Software User Requirements",
            "doi": "10.1109/re.2017.14",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"twitter enables large populations of end-users of software to publicly share their experiences and concerns about software systems in the form of micro-blogs. such data can be collected and classified to help software developers infer users' needs, detect bugs in their code, and plan for future releases of their systems. however, automatically capturing, classifying, and presenting useful tweets is not a trivial task. challenges stem from the scale of the data available, its unique format, diverse nature, and high percentage of irrelevant information and spam. motivated by these challenges, this paper reports on a three-fold study that is aimed at leveraging twitter as a main source of software user requirements. the main objective is to enable a responsive, interactive, and adaptive data-driven requirements engineering process. our analysis is conducted using 4,000 tweets collected from the twitter feeds of 10 software systems sampled from a broad range of application domains. the results reveal that around 50% of collected tweets contain useful technical information. the results also show that text classifiers such as support vector machines and naive bayes can be very effective in capturing and categorizing technically informative tweets. additionally, the paper describes and evaluates multiple summarization strategies for generating meaningful summaries of informative software-relevant tweets.\""
        },
        {
            "id": "R76818",
            "label": "App Review Analysis Via Active Learning: Reducing Supervision Effort without Compromising Classification Accuracy",
            "doi": "10.1109/re.2018.00026",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "automated app review analysis is an important avenue for extracting a variety of requirements-related information. typically, a first step toward performing such analysis is preparing a training dataset, where developers (experts) identify a set of reviews and, manually, annotate them according to a given task. having sufficiently large training data is important for both achieving a high prediction accuracy and avoiding overfitting. given millions of reviews, preparing a training set is laborious. we propose to incorporate active learning, a machine learning paradigm, in order to reduce the human effort involved in app review analysis. our app review classification framework exploits three active learning strategies based on uncertainty sampling. we apply these strategies to an existing dataset of 4,400 app reviews for classifying app reviews as features, bugs, rating, and user experience. we find that active learning, compared to a training dataset chosen randomly, yields a significantly higher prediction accuracy under multiple scenarios."
        },
        {
            "id": "R78371",
            "label": "Automatic Classification of Non-Functional Requirements from Augmented App User Reviews",
            "doi": "10.1145/3084226.3084241",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"context: the leading app distribution platforms, apple app store, google play, and windows phone store, have over 4 million apps. research shows that user reviews contain abundant useful information which may help developers to improve their apps. extracting and considering non-functional requirements (nfrs), which describe a set of quality attributes wanted for an app and are hidden in user reviews, can help developers to deliver a product which meets users' expectations. objective: developers need to be aware of the nfrs from massive user reviews during software maintenance and evolution. automatic user reviews classification based on an nfr standard provides a feasible way to achieve this goal. method: in this paper, user reviews were automatically classified into four types of nfrs (reliability, usability, portability, and performance), functional requirements (frs), and others. we combined four classification techniques bow, tf-idf, chi2, and aur-bow (proposed in this work) with three machine learning algorithms naive bayes, j48, and bagging to classify user reviews. we conducted experiments to compare the f-measures of the classification results through all the combinations of the techniques and algorithms. results: we found that the combination of aur-bow with bagging achieves the best result (a precision of 71.4%, a recall of 72.3%, and an f-measure of 71.8%) among all the combinations. conclusion: our finding shows that augmented user reviews can lead to better classification results, and the machine learning algorithm bagging is more suitable for nfrs classification from user reviews than na\u00efve bayes and j48.\""
        },
        {
            "id": "R78392",
            "label": "Bug report, feature request, or simply praise? On automatically classifying app reviews",
            "doi": "10.1109/re.2015.7320414",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "app stores like google play and apple appstore have over 3 million apps covering nearly every kind of software and service. billions of users regularly download, use, and review these apps. recent studies have shown that reviews written by the users represent a rich source of information for the app vendors and the developers, as they include information about bugs, ideas for new features, or documentation of released features. this paper introduces several probabilistic techniques to classify app reviews into four types: bug reports, feature requests, user experiences, and ratings. for this we use review metadata such as the star rating and the tense, as well as, text classification, natural language processing, and sentiment analysis techniques. we conducted a series of experiments to compare the accuracy of the techniques and compared them with simple string matching. we found that metadata alone results in a poor classification accuracy. when combined with natural language processing, the classification precision got between 70-95% while the recall between 80-90%. multiple binary classifiers outperformed single multiclass classifiers. our results impact the design of review analytics tools which help app vendors, developers, and users to deal with the large amount of reviews, filter critical reviews, and assign them to the appropriate stakeholders."
        },
        {
            "id": "R78432",
            "label": "Software Feature Request Detection in Issue Tracking Systems",
            "doi": "10.1109/re.2016.8",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "communication about requirements is often handled in issue tracking systems, especially in a distributed setting. as issue tracking systems also contain bug reports or programming tasks, the software feature requests of the users are often difficult to identify. this paper investigates natural language processing and machine learning features to detect software feature requests in natural language data of issue tracking systems. it compares traditional linguistic machine learning features, such as \"bag of words\", with more advanced features, such as subject-action-object, and evaluates combinations of machine learning features derived from the natural language and features taken from the issue tracking system meta-data. our investigation shows that some combinations of machine learning features derived from natural language and the issue tracking system meta-data outperform traditional approaches. we show that issues or data fields (e.g. descriptions or comments), which contain software feature requests, can be identified reasonably well, but hardly the exact sentence. finally, we show that the choice of machine learning algorithms should depend on the goal, e.g. maximization of the detection rate or balance between detection rate and precision. in addition, the paper contributes a double coded gold standard and an open-source implementation to further pursue this topic."
        },
        {
            "id": "R78466",
            "label": "How can I improve my app? Classifying user reviews for software maintenance and evolution",
            "doi": "10.1109/icsm.2015.7332474",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "app stores, such as google play or the apple store, allow users to provide feedback on apps by posting review comments and giving star ratings. these platforms constitute a useful electronic mean in which application developers and users can productively exchange information about apps. previous research showed that users feedback contains usage scenarios, bug reports and feature requests, that can help app developers to accomplish software maintenance and evolution tasks. however, in the case of the most popular apps, the large amount of received feedback, its unstructured nature and varying quality can make the identification of useful user feedback a very challenging task. in this paper we present a taxonomy to classify app reviews into categories relevant to software maintenance and evolution, as well as an approach that merges three techniques: (1) natural language processing, (2) text analysis and (3) sentiment analysis to automatically classify app reviews into the proposed categories. we show that the combined use of these techniques allows to achieve better results (a precision of 75% and a recall of 74%) than results obtained using each technique individually (precision of 70% and a recall of 67%)."
        },
        {
            "id": "R193898",
            "label": "A deep context-wise method for coreference detection in natural language requirements",
            "doi": "",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "requirements are usually written by different stakeholders with diverse backgrounds and skills and evolve continuously. therefore inconsistency caused by specialized jargons and different domains, is inevitable. in particular, entity coreference in requirement engineering (re) is that different linguistic expressions refer to the same real-world entity. it leads to misconception about technical terminologies, and impacts the readability and understandability of requirements negatively. manual detection entity coreference is labor-intensive and time-consuming. in this paper, we propose a deep context-wise semantic method named deepcoref to entity coreference detection. it consists of one fine-tuning bert model for context representation and a word2vec-based network for entity representation. we use a multi-layer perception in the end to fuse and make a trade-off between two representations for obtaining a better representation of entities. the input of the network is requirement contextual text and related entities, and the output is the predictive label to infer whether two entities are coreferent. the evaluation on industry data shows that our approach significantly outperforms three baselines with average precision and recall of 96.10% and 96.06% respectively. we also compare deepcoref with three variants to demonstrate the performance enhancement from different components."
        },
        {
            "id": "R193925",
            "label": "Trace link recovery using semantic relation graphs and spreading activation",
            "doi": "10.1109/RE48521.2020.00015",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "trace link recovery tries to identify and link related existing requirements with each other to support further engineering tasks. existing approaches are mainly based on algebraic information retrieval or machine-learning. machinelearning approaches usually demand reasonably large and labeled datasets to train. algebraic information retrieval approaches like distance between tf-idf scores also work on smaller datasets without training but are limited in providing explanations for trace links. in this work, we present a trace link recovery approach that is based on an explicit representation of the content of requirements as a semantic relation graph and uses spreading activation to answer trace queries over this graph. our approach is fully automated including an nlp pipeline to transform unrestricted natural language requirements into a graph. we evaluate our approach on five common datasets. depending on the selected configuration, the predictive power strongly varies. with the best tested configuration, the approach achieves a mean average precision of 40% and a lag of 50%. even though the predictive power of our approach does not outperform state-of-the-art approaches, we think that an explicit knowledge representation is an interesting artifact to explore in trace link recovery approaches to generate explanations and refine results."
        },
        {
            "id": "R194054",
            "label": "The Lack of Shared Understanding of Non-Functional Requirements in Continuous Software Engineering: Accidental or Essential?",
            "doi": "10.1109/re48521.2020.00021",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "building shared understanding of requirements is key to ensuring downstream software activities are efficient and effective. however, in continuous software engineering (cse) some lack of shared understanding is an expected, and essential, part of a rapid feedback learning cycle. at the same time, there is a key trade-off with avoidable costs, such as rework, that come from accidental gaps in shared understanding. this tradeoff is even more challenging for non-functional requirements (nfrs), which have significant implications for product success. comprehending and managing nfrs is especially difficult in small, agile organizations. how such organizations manage shared understanding of nfrs in cse is understudied. we conducted a case study of three small organizations scaling up cse to further understand and identify factors that contribute to lack of shared understanding of nfrs, and its relationship to rework. our in-depth analysis identified 41 nfr-related software tasks as rework due to a lack of shared understanding of nfrs. of these 41 tasks 78% were due to avoidable (accidental) lack of shared understanding of nfrs. using a mixed-methods approach we identify factors that contribute to lack of shared understanding of nfrs, such as the lack of domain knowledge, rapid pace of change, and cross-organizational communication problems. we also identify recommended strategies to mitigate lack of shared understanding through more effective management of requirements knowledge in such organizations. we conclude by discussing the complex relationship between shared understanding of requirements, rework and, cse."
        },
        {
            "id": "R194084",
            "label": "Requirements Dependency Extraction by Integrating Active Learning with Ontology-Based Retrieval",
            "doi": "10.1109/re48521.2020.00020",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "context: incomplete or incorrect detection of requirement dependencies has proven to result in reduced release quality and substantial rework. additionally, the extraction of dependencies is challenging since requirements are mostly documented in natural language, which makes it a cognitively difficult task. moreover, with ever-changing and new requirements, a manual analysis process must be repeated, which imposes extra hardship even for domain experts.objective: the three main objectives of this research are: 1) proposing a new dependency extraction method using a variant of active learning (al). 2) evaluating this al and ontology-based retrieval (obr) as baseline methods for dependency extraction on the two industrial data sets. 3) analyzing the value gained from integrating these diverse approaches to form two hybrid methods.method: building on the general al, ensemble and semi-supervised machine learning, a variant of al was developed, which was further integrated with obr to form two hybrid methods (hybrid1, hybrid2) for extracting three types of dependencies (requires, refines, other): hybrid1 used obr as a substitute for human expert; hybrid2 used dependencies extracted through the obr as an additional input for training set in al.results: for two industrial case studies, al extracted more dependencies than obr. hybrid1 showed improvement for both data sets. for one of them, f1 score increased to 82.6% compared to the al baseline score of 49.9%. hybrid2 increased the accuracy by 25% to the level of 75.8% compared to the al baseline accuracy. obr also complemented the al approach by reducing 50% of the human effort."
        },
        {
            "id": "R194093",
            "label": "OASIS: Weakening User Obligations for Security-critical Systems",
            "doi": "10.1109/re48521.2020.00023",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "security-critical systems typically place some requirements on the behaviour of their users, obliging them to follow certain instructions when using those systems. security vulnerabilities can arise when users do not fully satisfy their obligations. in this paper, we propose an approach that improves system security by ensuring that attack scenarios are mitigated even when the users deviate from their expected behaviour. the approach uses structured transition systems to present and reason about user obligations. the aim is to identify potential vulnerabilities by weakening the assumptions on how the user will behave. we present an algorithm that combines iterative abstraction and controller synthesis to produce a new software specification that maintains the satisfaction of security requirements while weakening user obligations. we demonstrate the feasibility of our approach through two examples from the e-voting and e-commerce domains."
        },
        {
            "id": "R194114",
            "label": "How developers believe Invisibility impacts NFRs related to User Interaction",
            "doi": "10.1109/re48521.2020.00022",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "the advance of ubiquitous computing (ubicomp) and internet of things (iot) brought a new set of non-functional requirements (nfrs), especially related to human-computer interaction (hci). invisibility is one of these nfrs, and it refers to either the merging of technology in the user environment or the decrease of the interaction workload. this new nfr may impact traditional nfrs (e.g., usability), revealing positive correlations, when one nfr helps another, and negative correlations, when a procedure favors an nfr but creates difficulty for another one. software engineers need to know about these correlations, so they can select appropriate strategies to satisfy invisibility and traditional nfrs. correlations between nfrs are usually stored in catalogs, which is a well-defined body of knowledge gathered from previous experience. although invisibility has been recently cataloged with development strategies, the literature still lacks catalogs with correlations for this nfr. therefore, this work aims at capturing and cataloging invisibility correlations for ubicomp and iot systems. to do that, we also propose to systematize the definition of correlations using the following well-defined research methods: interview, content analysis and questionnaire. as a result, we defined a catalog with 110 positive and negative correlations with 9 nfrs. this well-defined body of knowledge is useful for supporting software engineers to select strategies to satisfy invisibility and other nfrs related to user interaction."
        },
        {
            "id": "R194125",
            "label": "Data-driven Risk Management for Requirements Engineering: An Automated Approach based on Bayesian Networks",
            "doi": "10.1109/re48521.2020.00024",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "requirements engineering (re) is a means to reduce the risk of delivering a product that does not fulfill the stakeholders\u2019 needs. therefore, a major challenge in re is to decide how much re is needed and what re methods to apply. the quality of such decisions is strongly based on the re expert\u2019s experience and expertise in carefully analyzing the context and current state of a project. recent work, however, shows that lack of experience and qualification are common causes for problems in re. we trained a series of bayesian networks on data from the napire survey to model relationships between re problems, their causes, and effects in projects with different contextual characteristics. these models were used to conduct (1) a post-mortem (diagnostic) analysis, deriving probable causes of suboptimal re performance, and (2) to conduct a preventive analysis, predicting probable issues a young project might encounter. the method was subject to a rigorous cross-validation procedure for both use cases before assessing its applicability to real-world scenarios with a case study."
        },
        {
            "id": "R194139",
            "label": "An AI-assisted Approach for Checking the Completeness of Privacy Policies Against GDPR",
            "doi": "10.1109/re48521.2020.00025",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "privacy policies are critical for helping individuals make informed decisions about their personal data. in europe, privacy policies are subject to compliance with the general data protection regulation (gdpr). if done entirely manually, checking whether a given privacy policy complies with gdpr is both time-consuming and error-prone. automated support for this task is thus advantageous. at the moment, there is an evident lack of such support on the market. in this paper, we tackle an important dimension of gdpr compliance checking for privacy policies. specifically, we provide automated support for checking whether the content of a given privacy policy is complete according to the provisions stipulated by gdpr. to do so, we present: (1) a conceptual model to characterize the information content envisaged by gdpr for privacy policies, (2) an ai-assisted approach for classifying the information content in gdpr privacy policies and subsequently checking how well the classified content meets the completeness criteria of interest; and (3) an evaluation of our approach through a case study over 24 unseen privacy policies. for classification, we leverage a combination of natural language processing and supervised machine learning. our experimental material is comprised of 234 real privacy policies from the fund industry. our empirical results indicate that our approach detected 45 of the total of 47 incompleteness issues in the 24 privacy policies it was applied to. over these policies, the approach had eight false positives. the approach thus has a precision of 85% and recall of 96% over our case study."
        },
        {
            "id": "R194237",
            "label": "Do we Really Know What we are Building? Raising Awareness of Potential Sustainability Effects of Software Systems in Requirements Engineering",
            "doi": "10.1109/re.2019.00013",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "integrating novel software systems in our society, economy, and environment can have far-reaching effects. as a result, software systems should be designed in such a way as to maintain or improve the sustainability of the socio-technical system of their destination. however, a paradigm shift is required to raise awareness of software professionals on the potential sustainability effects of software systems. while requirements engineering is considered the key to driving this change, requirements engineers lack the knowledge, experience and methodological support for doing so. this paper presents a question-based framework for raising awareness of the potential effects of software systems on sustainability, as the first step towards enabling the required paradigm shift. a feasibility study of the framework was carried out with two groups of computer science students. the results of the study indicate that the framework helps enable discussions about potential effects that software systems could have on sustainability."
        },
        {
            "id": "R194253",
            "label": "Can a Conversation Paint a Picture? Mining Requirements In Software Forums",
            "doi": "10.1109/re.2019.00014",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "the modern software landscape is highly competitive. software companies need to quickly fix reported bugs and release requested new features, or they risk negative reviews and reduced market share. the amount of online user feedback prevents manual analysis. past research has investigated automated requirement mining techniques on online platforms like app stores and twitter, but online product forums have not been studied. in this paper, we show that online product forums are a rich source of user feedback that may be used to elicit product requirements. the information contained in forum questions is different from what has been described in the related work on app stores or twitter. users often provide detailed context to specific problems they encounter with a software product and other users respond with workarounds or to confirm the problem. through the analysis of two large forums, we identify 18 different types of information (classifications) contained in forums that can be relevant to maintenance and evolution tasks. we show that a state-of-the-art app store tool is unable to accurately classify forum data, which may be due to the differences in content. thus, specific techniques are likely needed to mine requirements from product forums. in an exploratory study, we developed classifiers with forum specific features. promising results are achieved for all classifiers with f-measure scores ranging from 70.3% to 89.8%."
        },
        {
            "id": "R194281",
            "label": "Learning Requirements Elicitation Interviews with Role-Playing, Self-Assessment and Peer-Review",
            "doi": "10.1109/re.2019.00015",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "interviews are largely used in the practice of requirements elicitation. nevertheless, performing an effective interview often depends on soft-skills, and on knowledge acquired through experience. when it comes to requirements engineering education and training (reet), limited resources and few well-founded pedagogical approaches are available to allow students to acquire and improve their skills as interviewers. this paper presents a novel pedagogical approach that combines role-playing, peer-review and self-assessment to enable students to reflect on their mistakes, and improve their interview skills. we evaluate the approach through a controlled quasi-experiment. the study shows that the approach significantly reduces the amount of mistakes made by the students. feedback from the participants confirms the usefulness and easiness of the proposed training. this work contributes to the body of knowledge of reet with an empirically evaluated method for teaching inter-views. furthermore, we share the pedagogical material used, to enable other educators to apply and possibly tailor the approach."
        },
        {
            "id": "R194317",
            "label": "Optimizing for Recall in Automatic Requirements Classification: An Empirical Study",
            "doi": "10.1109/re.2019.00016",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "using machine learning to solve requirements engineering problems can be a tricky task. even though certain algorithms have exceptional performance, their recall is usually below 100%. one key aspect in the implementation of machine learning tools is the balance between recall and precision. tools that do not find all correct answers may be considered useless. however, some tasks are very complicated and even requirements engineers struggle to solve them perfectly. if a tool achieves performance comparable to a trained engineer while reducing her workload considerably, it is considered to be useful. one such task is the classification of specification content elements into requirements and non-requirements. in this paper, we analyze this specific requirements classification problem and assess the importance of recall by performing an empirical study. we compared two groups of students who performed this task with and without tool support, respectively. we use the results to compute an estimate of f for the ff score, allowing us to choose the optimal balance between precision and recall. furthermore, we use the results to assess the practical time savings realized by the approach. by using the tool, users may not be able to find all defects in a document, however, they will be able to find close to all of them in a fraction of the time necessary. this demonstrates the practical usefulness of our approach and machine learning tools in general."
        },
        {
            "id": "R194345",
            "label": "A Machine Learning-Based Approach for Demarcating Requirements in Textual Specifications",
            "doi": "10.1109/re.2019.00017",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "a simple but important task during the analysis of a textual requirements specification is to determine which statements in the specification represent requirements. in principle, by following suitable writing and markup conventions, one can provide an immediate and unequivocal demarcation of requirements at the time a specification is being developed. however, neither the presence nor a fully accurate enforcement of such conventions is guaranteed. the result is that, in many practical situations, analysts end up resorting to after-the-fact reviews for sifting requirements from other material in a requirements specification. this is both tedious and time-consuming. we propose an automated approach for demarcating requirements in free-form requirements specifications. the approach, which is based on machine learning, can be applied to a wide variety of specifications in different domains and with different writing styles. we train and evaluate our approach over an independently labeled dataset comprised of 30 industrial requirements specifications. over this dataset, our approach yields an average precision of 81.2% and an average recall of 95.7%. compared to simple baselines that demarcate requirements based on the presence of modal verbs and identifiers, our approach leads to an average gain of 16.4% in precision and 25.5% in recall."
        },
        {
            "id": "R194360",
            "label": "Analysis of Requirements-Related Arguments in User Forums",
            "doi": "10.1109/re.2019.00018",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "in the past, users were asked to express their needs and intentions by writing a structured requirements document in natural language. due to the pervasive use of online forums and social media, user feedback is more accessible today. however, the information obtained is often fragmented, involving multipleperspectives from multiple parties on an on-going basis. in this paper, we propose a crowd-based requirements engineering approach by argumentation (crowdre-arg), which analyses the conversations from user forum, identifies the arguments in favor or opposing of a given requirements related discussion topic. by generating the argumentation model of the involved user statements, we are able to recover the conflicting viewpoints, to reason about the winning arguments for informed requirements decisions. the proposed approach is illustrated with a data set of sample conversations about the design of a new google-map feature from reddit. also, we apply natural language processing techniques and machine learning algorithms to support the automated execution of the crowdre-arg approach."
        },
        {
            "id": "R194373",
            "label": "The Role of Environment Assertions in Requirements-Based Testing",
            "doi": "10.1109/re.2019.00019",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "software developers dedicate a major portion of their development effort towards testing and quality assurance (qa) activities, especially during and around the implementation phase. nevertheless, we continue to see an alarmingly increasing trend in the cost and consequences of software failure. in an attempt to mitigate such loss and address software issues at a much earlier stage, researchers have recently emphasized on the successful coordination of requirements engineering and testing. in addition, the notion of requirements-based testing (rbt) has also emerged with a focus on checking the correctness, completeness, unambiguity, and logical consistency of requirements. one seminal work points out that requirements reside in the environment which is comprised of certain problem domain phenomena. environmental assertions, which connect some of these phenomena in the indicative mood, play a key role in deciding whether a software solution is acceptable. despite that requirements are located in the environment, little is known about if and how the environment assertions would impact testing and qa activities. in order to address this gap, we present a detailed empirical study, with 114 developers, on the prominence of environment assertions in rbt. although the results suggest that paying attention to correct, complete, and useful environment assertions has a positive impact on rbt, developers often face difficulty in formulating good assertions from scratch. our work, to that end, illuminates the potential usefulness of automated support in generating environment assertions."
        },
        {
            "id": "R194401",
            "label": "An Approach for Reviewing Security-Related Aspects in Agile Requirements Specifications of Web Applications",
            "doi": "10.1109/re.2019.00020",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "defects in requirements specifications can have severe consequences during the software development lifecycle. some of them result in overall project failure due to incorrect or missing quality characteristics such as security. there are several concerns that make security difficult to deal with; for instance, (1) when stakeholders discuss general requirements in meetings, they are often unaware that they should also discuss security-related topics, and (2) they typically do not have enough expertise in security. this often leads to unspecified or ill-defined security-related aspects. these concerns become even more challenging in agile contexts, where lightweight documentation is typically involved. the goal of this paper is to design and evaluate an approach for reviewing security-related aspects in agile requirements specifications of web applications. the approach considers user stories and security specifications as input and relates those user stories to security properties via natural language processing. based on the related security properties, our approach then identifies high-level security requirements from the open web application security project to be verified and generates a reading technique to support reviewers in detecting defects. we evaluate our approach via two controlled experiment trials. we compare the effectiveness and efficiency of novice inspectors verifying security aspects in agile requirements using our approach against using the complete list of high-level security requirements. the (statistically significant) results indicate that using our approach has a positive impact (with large effect size) on the performance of inspectors in terms of effectiveness and efficiency."
        },
        {
            "id": "R194414",
            "label": "Detecting Bad Smells in Use Case Descriptions",
            "doi": "10.1109/re.2019.00021",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "use case modeling is very popular to represent the functionality of the system to be developed, and it consists of two parts: use case diagram and use case description. use case descriptions are written in structured natural language (nl), and the usage of nl can lead to poor descriptions such as ambiguous, inconsistent and/or incomplete descriptions, etc. poor descriptions lead to missing requirements and eliciting incorrect requirements as well as less comprehensiveness of produced use case models. this paper proposes a technique to automate detecting bad smells of use case descriptions, symptoms of poor descriptions. at first, to clarify bad smells, we analyzed existing use case models to discover poor use case descriptions concretely and developed the list of bad smells, i.e., a catalogue of bad smells. some of the bad smells can be refined into measures using the goal-question-metric paradigm to automate their detection. the main contribution of this paper is the automated detection of bad smells. we have implemented an automated smell detector for 22 bad smells at first and assessed its usefulness by an experiment. as a result, the first version of our tool got a precision ratio of 0.591 and recall ratio of 0.981."
        },
        {
            "id": "R194425",
            "label": "Visualization Requirements for Business Intelligence Analytics: A Goal-Based, Iterative Framework",
            "doi": "10.1109/re.2019.00022",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "information visualization plays a key role in business intelligence analytics. with ever larger amounts of data that need to be interpreted, using the right visualizations is crucial in order to understand the underlying patterns and results obtained by analysis algorithms. despite its importance, defining the right visualization is still a challenging task. business users are rarely experts in information visualization, and they may not exactly know the most adequate visualization tools or patterns for their goals. consequently, misinterpreted graphs and wrong results can be obtained, leading to missed opportunities and significant losses for companies. the main problem underneath is a lack of tools and methodologies that allow non-expert users to define their visualization and data analysis goals in business terms. in order to tackle this problem, we present an iterative goal-oriented approach based on the i* language for the automatic derivation of data visualizations. our approach links non-expert user requirements to the data to be analyzed, choosing the most suited visualization techniques in a semi-automatic way. the great advantage of our proposal is that we provide non-expert users with the best suited visualizations according to their information needs and their data with little effort and without requiring expertise in information visualization."
        },
        {
            "id": "R194428",
            "label": "Predicting How to Test Requirements: An Automated Approach",
            "doi": "10.1109/re.2019.00023",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "an important task in requirements engineering is to identify and determine how to verify a requirement (e.g., by manual review, testing, or simulation; also called potential verification method). this information is required to effectively create test cases and verification plans for requirements. [objective] in this paper, we propose an automatic approach to classify natural language requirements with respect to their potential verification methods (pvm). [method] our approach uses a convolutional neural network architecture to implement a multiclass and multilabel classifier that assigns probabilities to a predefined set of six possible verification methods, which we derived from an industrial guideline. additionally, we implemented a backtracing approach to analyze and visualize the reasons for the network\u2019s decisions. [results] in a 10-fold cross validation on a set of about 27,000 industrial requirements, our approach achieved a macro averaged f1 score of 0.79 across all labels. for the classification into test or non-test, the approach achieves an even higher f1 score of 0.94. [conclusions] the results show that our approach might help to increase the quality of requirements specifications with respect to the pvm attribute and guide engineers in effectively deriving test cases and verification plans."
        },
        {
            "id": "R194431",
            "label": "Do End-Users Want Explanations? Analyzing the Role of Explainability as an Emerging Aspect of Non-Functional Requirements",
            "doi": "10.1109/re.2019.00032",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"software systems are getting more and more complex. their ubiquitous presence makes users more dependent on them and their correctness in many aspects of daily life. thus, there is a rising need to make software systems and their decisions more comprehensible. this seems to call for more transparency in software-supported decisions. therefore, transparency is gaining importance as a non-functional requirement. however, the abstract quality aspect of transparency needs to be better understood and related to mechanisms that can foster it. integrating explanations in software to leverage systems' opacity has been discussed often. yet, an important first step is to understand user requirements with respect to explainable software behavior: are users really interested in transparency, and are explanations considered an adequate mechanism to achieve it? we conducted a survey with 107 end-users to assess their opinion on the current status of transparency in software systems, and what they consider main advantages and disadvantages of explanations embedded in software. the overall attitude towards embedded explanations was positive. however, we also identified potential disadvantages. we assess the relation between explanations and transparency and analyze its possible impact on software quality.\""
        },
        {
            "id": "R194445",
            "label": "Automated Recommendation of Software Refactorings Based on Feature Requests",
            "doi": "10.1109/re.2019.00029",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "during software evolution, developers often receive new requirements expressed as feature requests. to implement the requested features, developers have to perform necessary modifications (refactorings) to prepare for new adaptation that accommodates the new requirements. software refactoring is a well-known technique that has been extensively used to improve software quality such as maintainability and extensibility. however, it is often challenging to determine which kind of refactorings should be applied. consequently, several approaches based on various heuristics have been proposed to recommend refactorings. however, there is still lack of automated support to recommend refactorings given a feature request. to this end, in this paper, we propose a novel approach that recommends refactorings based on the history of the previously requested features and applied refactorings. first, we exploit the stateof-the-art refactoring detection tools to identify the previous refactorings applied to implement the past feature requests. second, we train a machine classifier with the history data of the feature requests and refactorings applied on the commits that implemented the corresponding feature requests. the machine classifier is then used to predict refactorings for new feature requests. we evaluate the proposed approach on the dataset of 43 open source java projects and the results suggest that the proposed approach can accurately recommend refactorings (average precision 73%)."
        },
        {
            "id": "R194458",
            "label": "Analysing Gender Differences in Building Social Goal Models: A Quasi-Experiment",
            "doi": "10.1109/re.2019.00027",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"context: recent research has shown gender differences in problem-solving, and gender biases in how software supports it. gendermag has 5 problem-solving facets related to gender-inclusiveness: motivation for using the software, information processing style, computer self-efficacy, attitude towards risk, and ways of learning new technology. some facet values are more frequent in women, others in men. the role these facets may play when building social goal models is largely unexplored. objectives: we evaluated the impact of different levels of gendermag facets on creating and modifying istar 2.0 models. methods: we performed a quasi-experiment. we characterised 100 participants according to each gendermag facet. participants performed creation and modification tasks on istar 2.0. we measured their accuracy, speed, and ease, using metrics of task success, time, and effort, collected with eye-tracking, eeg and eda sensors, and participants' feedback. results: although participants with facet levels frequently seen in women had lower perceived performance and speed, their accuracy was higher. we also observed some statistically significant differences in visual effort, mental effort, and stress. conclusions: participants with a comprehensive information processing style and a more conservative attitude towards risk (characteristics more frequently seen in women) solved the tasks with a lower speed but higher accuracy.\""
        },
        {
            "id": "R194474",
            "label": "How do Practitioners Capture and Utilize User Feedback During Continuous Software Engineering?",
            "doi": "10.1109/re.2019.00026",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"continuous software engineering (cse) evolved as a process for rapid software evolution. continuous delivery enables developers to frequently retrieve user feedback on the latest software increment. developers use these insights for requirements validation and verification. despite the importance of users, reports about user feedback in cse practice are sparse. we conducted 20 interviews with practitioners from 17 companies that apply cse. we asked practitioners how they capture and utilize user feedback. in this paper, we detail the practitioners' answers by posing three research questions. to improve continuous user feedback capture and utilization with respect to requirements engineering, we derived five recommendations: first, internal sources should be approached, as they provide a rich source of user feedback; second, existing tool support should be adapted and extended to automate user feedback processing; third, a concept of reference points should be established to relate user feedback to requirements; fourth, the utilization of user feedback for requirements validation should be increased; and last, the interaction with user feedback should be enabled and supported by increasing developer-user communication. we conclude that a continuous user understanding activity can improve requirements engineering by contributing to both the completeness and correctness of requirements.\""
        },
        {
            "id": "R194607",
            "label": "Extracting and Analyzing Context Information in User-Support Conversations on Twitter",
            "doi": "10.1109/re.2019.00024",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "while many apps include built-in options to report bugs or request features, users still provide an increasing amount of feedback via social media, like twitter. compared to traditional issue trackers, the reporting process in social media is unstructured and the feedback often lacks basic context information, such as the app version or the device concerned when experiencing the issue. to make this feedback actionable to developers, support teams engage in recurring, effortful conversations with app users to clarify missing context items. this paper introduces a simple approach that accurately extracts basic context information from unstructured, informal user feedback on mobile apps, including the platform, device, app version, and system version. evaluated against a truthset of 3014 tweets from official twitter support accounts of the 3 popular apps netflix, snapchat, and spotify, our approach achieved precisions from 81% to 99% and recalls from 86% to 98% for the different context item types. combined with a chatbot that automatically requests missing context items from reporting users, our approach aims at auto-populating issue trackers with structured bug reports."
        },
        {
            "id": "R194610",
            "label": "Requirements Classification with Interpretable Machine Learning and Dependency Parsing",
            "doi": "10.1109/re.2019.00025",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"requirements classification is a traditional application of machine learning (ml) to re that helps handle large requirements datasets. a prime example of an re classification problem is the distinction between functional and non-functional (quality) requirements. state-of-the-art classifiers build their effectiveness on a large set of word features like text n-grams or pos n-grams, which do not fully capture the essence of a requirement. as a result, it is arduous for human analysts to interpret the classification results by exploring the classifier's inner workings. we propose the use of more general linguistic features, such as dependency types, for the construction of interpretable ml classifiers for re. through a feature engineering effort, in which we are assisted by modern introspection tools that reveal the hidden inner workings of ml classifiers, we derive a set of 17 linguistic features. while classifiers that use our proposed features fit the training set slightly worse than those that use high-dimensional feature sets, our approach performs generally better on validation datasets and it is more interpretable.\""
        },
        {
            "id": "R194641",
            "label": "Digital Discrimination in Sharing Economy A Requirements Engineering Perspective",
            "doi": "10.1109/re48521.2020.00031",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "recent evidence has revealed that sharing economy platforms such as uber, airbnb, and taskrabbit, have become active hubs for digital discrimination. this new form of discrimination refers to a phenomenon where a business transaction is influenced by race, gender, age, or any other non-business related characteristic of providers or consumers. existing research often tackles this problem from a socio-economic and regulatory points of view. however, the research on the design aspects of sharing economy software, which enable such complex sociotechnical problems to emerge online, is still underdeveloped. to bridge this gap, in this paper, we propose a new perspective on digital discrimination, tackling the problem from a requirements engineering point of view. specifically, we analyze a large dataset of online user feedback as well as synthesize existing literature to identify and classify pervasive discrimination concerns in the sharing economy market. based on this analysis, we devise a crowd-driven domain model to represent these concerns along with their relations to the functional features and user goals of sharing economy platforms. this model is intended to provide requirements engineers, working on sharing economy software, with systematic insights into the complex types of socio-technical problems that can emerge in the operational environments of their systems."
        },
        {
            "id": "R194686",
            "label": "The Manager Perspective on Requirements Impact on Automotive Systems Development Speed",
            "doi": "10.1109/re.2018.00-55",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "context: historically, automotive manufacturers have adopted rigid requirements engineering processes, which allowed them to meet safety-critical requirements while integrating thousands of physical and software components into a highly complex and differentiated product. nowadays, needs of improving development speed are pushing companies in this domain towards new ways of developing software. objectives: we aim at obtaining a manager perspective on how the goal to increase development speed impacts how software intense automotive systems are developed and their requirements managed. methods: we used a qualitative multiple-case study, based on 20 semi-structured interviews, at two automotive manufacturers. our sampling strategy focuses on manager roles, complemented with technical specialists. results: we found that both a requirements style dominated by safety concerns, and decomposition of requirements over many levels of abstraction impact development speed negatively. furthermore, the use of requirements as part of legal contracts with suppliers hiders fast collaboration. suggestions for potential improvements include domain-specific tooling, model-based requirements, test automation, and a combination of lightweight pre-development requirements engineering with precise specifications post-development. conclusions: we offer an empirical account of expectations and needs for new requirements engineering approaches in the automotive domain, necessary to coordinate hundreds of collaborating organizations developing software-intensive and potentially safety-critical systems."
        },
        {
            "id": "R194698",
            "label": "A Qualitative Study on using GuideGen to Keep Requirements and Acceptance Tests Aligned",
            "doi": "10.1109/re.2018.00-54",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "software requirements constantly change, thus impacting all other artifacts of an evolving system. in order to keep the system in a consistent state, changes in requirements should be documented and applied accordingly to all affected artifacts, including acceptance tests. in practice, however, changes in requirements are not always documented nor applied to the affected acceptance tests. this is mostly due to poor communication, lack of time or work overload, and eventually leads to project delays, unintended costs and unsatisfied customers. guidegen is a tool-supported approach for keeping requirements and acceptance tests aligned. when a requirement is changed, guidegen automatically generates guidance in natural language on how to modify impacted acceptance tests and communicates this information to the concerned parties. in this paper, we evaluate guidegen in terms of its perceived usefulness for practitioners and its applicability to real software projects. the evaluation was conducted via interviews with 23 industrial practitioners from ten companies based in europe. the results indicate that guidegen is a useful approach that facilitates requirements change management and the communication of changes between requirements and test engineers. the participants also identified potential for improvement, in particular for using guidegen in large projects."
        },
        {
            "id": "R194870",
            "label": "Requirements Reference Models Revisited: Accommodating Hierarchy in System Design",
            "doi": "10.1109/re.2019.00028",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"reference models such as parnas' four-variable model, jackson's and zaves' world machine model, and gunther et al.'s wrspm model abstractly define and relate key artifacts in requirements engineering. such reference models are intended to serve as a frame of reference for engineers to understand and reason about the artifacts involved in requirements engineering. however, when discussing the requirements of modern systems that are developed in a hierarchical and middle-out manner, these reference models do not provide a framework in which the relationship between requirements and architecture is explicitly discussed. conceptual clarity about this relationship is crucial since the architecture and requirements for such systems become intrinsically intertwined as the architectural choices made during development influence the requirements and vice-versa. hence, to precisely determine the scope of specifying requirements, distinguish requirements from architecture details, reason about the requirements, and determine how the requirements are realized in the system, we argue that a requirements reference model intended as a reference for such systems must explicitly discuss the architecture - requirements relationship. to that end, we define a hierarchical reference model that formally, yet abstractly, captures the intertwined relationship between the architecture and requirements in a way that will serve the same purpose as other models, but be more suitable for modern systems where architecture and requirements co-evolve. to illustrate the concepts in this model, we use a generic patient-controlled analgesic infusion pump system as a case example.\""
        },
        {
            "id": "R194875",
            "label": "A Metamodeling Approach to Support the Engineering of Modeling Method Requirements",
            "doi": "10.1109/re.2019.00030",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "the notion of \"modeling method requirements\" refers to a category typically neglected by re taxonomies and frameworks - i.e., those requirements that motivate the realization of (conceptual) modeling methods and tools. they can be considered domain-specific, in the sense that all modeling methods provide a knowledge schema for some selected application domain (narrow or broad). besides this inherent domain-specific nature, we are investigating how the characteristics of modeling methods inform the re perspective, and how in turn re can support the engineering of such artifacts. thus, the work at hand aims to raise awareness about modeling method requirements in the re community. the core contribution is the cochaco (concept-characteristic-connector) method for the representation and management of such requirements, as well as for streamlining with subsequent engineering phases. cochaco is itself a modeling method - i.e., it achieves its goals through diagrammatic modeling means for which a supporting tool was prototyped and evolved. the proposal originates in required support for the initial phase of the agile modeling method engineering (amme) methodology, which was successfully applied in developing a variety of project-specific modeling tools. from this accumulated experience, awareness of \"modeling method requirements\" emerged and informed the design decisions of cochaco."
        },
        {
            "id": "R194884",
            "label": "Extraction of System States from Natural Language Requirements",
            "doi": "10.1109/re.2019.00031",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "in recent years, simulations have proven to be an important means to verify the behavior of complex software systems. the different states of a system are monitored in the simulations and are compared against the requirements specification. so far, system states in natural language requirements cannot be automatically linked to signals from the simulation. however, the manual mapping between requirements and simulation is a time-consuming task. named-entity recognition is a sub-task from the field of automated information retrieval and is used to classify parts of natural language texts into categories. in this paper, we use a self-trained named-entity recognition model with bidirectional lstms and cnns to extract states from requirements specifications. we present an almost entirely automated approach and an iterative semi-automated approach to train our model. the automated and iterative approach are compared and discussed with respect to the usual manual extraction. we show that the manual extraction of states in 2,000 requirements takes nine hours. our automated approach achieves an f1-score of 0.51 with 15 minutes of manual work and the iterative approach achieves an f1-score of 0.62 with 100 minutes of work."
        },
        {
            "id": "R194895",
            "label": "Scalable Analysis of Real-Time Requirements",
            "doi": "10.1109/re.2019.00033",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "detecting issues in real-time requirements is usually a trade-off between flexibility and cost: the effort expended depends on how expensive it is to fix a defect introduced by faulty, ambiguous or incomplete requirements. the most rigorous techniques for real-time requirement analysis depend on the formalisation of these requirements. completely formalised real-time requirements allow the detection of issues that are hard to find through other means, like real-time inconsistency (i.e., \"do the requirements lead to deadlocks and starvation of the system?\") or vacuity (i.e., \"are some requirements trivially satisfied\"). current analysis techniques for real-time requirements suffer from scalability issues \u2013 larger sets of such requirements are usually intractable. we present a new technique to analyse formalised real-time requirements for various properties. our technique leverages recent advances in software model checking and automatic theorem proving by converting the analysis problem for real-time requirements to a program analysis task. we also report preliminary results from an ongoing, large scale application of our technique in the automotive domain at bosch."
        },
        {
            "id": "R194905",
            "label": "Arithmetic Semantics of Feature and Goal Models for Adaptive Cyber-Physical Systems",
            "doi": "10.1109/re.2019.00034",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "many cyber-physical systems (cpss) today are self-adaptive, in order to handle frequent changes in environmental conditions and requirements. in cpss, goal-based reasoning is often used to include stakeholder and social concerns in decision making during design and runtime adaptation activities. to better support some of these activities, arithmetic semantics for goal models were proposed to enable the generation of mathematical functions usable by systems. however, goal models often allow invalid combinations of alternatives, which can be prevented by companion feature models. in this paper, to enable the generation of valid and optimal configurations for adaptive cpss, we propose new arithmetic semantics for feature models that enable their transformations to mathematical functions (in several programming languages) further restricting the ones generated from goal models. the composition of feature and goal functions results in a smaller design space, leading to fewer but valid solutions that can be generated (e.g., through optimization) and used in simulations and running adaptive cpss with social concerns. finally, a simulation model in sysml is proposed in this paper to demonstrate the feasibility and usefulness of this composition."
        },
        {
            "id": "R194922",
            "label": "The Next Release Problem Revisited: A New Avenue for Goal Models",
            "doi": "10.1109/re.2018.00-56",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "context. goal models have long been critiqued for the time it takes to construct them as well as for their limited cognitive and visual scalability. is such criticism general or does it depend on the supported task? objectives. we advocate for the latter and the aim of this paper is to demonstrate that the next release problem is a suitable application domain for goal models. this hypothesis stems from the fact that product release management is a long-term investment, and software products are commonly managed in \"themes\" which are smaller focus areas of the product. methods. we employ a version of goal models that is tailored for the next release problem by capturing requirements, synergies among them, constraints, and release objectives. such goal model allows discovering optimal solutions considering multiple criteria for the next release. results. a retrospective case study confirms that goal models are easier to read and comprehend when organized in themes, and that the reasoning results help product managers decide for the next release. our scalability experiments show that, through reasoning based on optimization modulo theories, the discovery of the optimal solution is fast and scales sufficiently well with respect to the model size, connectivity, and number of alternative solutions."
        },
        {
            "id": "R194932",
            "label": "Enhancing Automated Requirements Traceability by Resolving Polysemy",
            "doi": "10.1109/re.2018.00-53",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"requirements traceability provides critical support throughout all phases of software engineering. automated tracing based on information retrieval (ir) reduces the effort required to perform a manual trace. unfortunately, ir-based trace recovery suffers from low precision due to polysemy, which refers to the coexistence of multiple meanings for a term appearing in different requirements. latent semantic indexing (lsi) has been introduced as a method to tackle polysemy, as well as synonymy. however, little is known about the scope and significance of polysemous terms in requirements tracing. while quantifying the effect, we present a novel method based on artificial neural networks (ann) to enhance the capability of automatically resolving polysemous terms. the core idea is to build an ann model which leverages a term's highest-scoring coreferences in different requirements to learn whether this term has the same meaning in those requirements. experimental results based on 2 benchmark datasets and 6 long-lived open-source software projects show that our approach outperforms lsi on identifying polysemous terms and hence increasing the precision of automated tracing.\""
        },
        {
            "id": "R194954",
            "label": "Vetting Automatically Generated Trace Links: What Information is Useful to Human Analysts?",
            "doi": "10.1109/re.2018.00-52",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"automated traceability has been investigated for over a decade with promising results. however, a human analyst is needed to vet the generated trace links to ensure their quality. the process of vetting trace links is not trivial and while previous studies have analyzed the performance of the human analyst, they have not focused on the analyst's information needs. the aim of this study is to investigate what context information the human analyst needs. we used design science research, in which we conducted interviews with ten practitioners in the traceability area to understand the information needed by human analysts. we then compared the information collected from the interviews with existing literature. we created a prototype tool that presents this information to the human analyst. to further understand the role of context information, we conducted a controlled experiment with 33 participants. our interviews reveal that human analysts need information from three different sources: 1) from the artifacts connected by the link, 2) from the traceability information model, and 3) from the tracing algorithm. the experiment results show that the content of the connected artifacts is more useful to the analyst than the contextual information of the artifacts.\""
        },
        {
            "id": "R194974",
            "label": "Modeling User Concerns in the App Store: A Case Study on the Rise and Fall of Yik Yak",
            "doi": "10.1109/re.2018.00-51",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"mobile application (app) stores have lowered the barriers to app market entry, leading to an accelerated and unprecedented pace of mobile software production. to survive in such a highly competitive and vibrant market, release engineering decisions should be driven by a systematic analysis of the complex interplay between the user, system, and market components of the mobile app ecosystem. to demonstrate the feasibility and value of such analysis, in this paper, we present a case study on the rise and fall of yik yak, one of the most popular social networking apps at its peak. in particular, we identify and analyze the design decisions that led to the downfall of yik yak and track rival apps' attempts to take advantage of this failure. we further perform a systematic in-depth analysis to identify the main user concerns in the domain of anonymous social networking apps and model their relations to the core features of the domain. such a model can be utilized by app developers to devise sustainable release engineering strategies that can address urgent user concerns and maintain market viability.\""
        },
        {
            "id": "R195005",
            "label": "Catalog of Invisibility Requirements for UbiComp and IoT Applications",
            "doi": "10.1109/re.2018.00019",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "a new set of non-functional requirements (nfrs) have appeared with the advent of ubiquitous computing (ubicomp) and more recently internet of things (iot). invisibility is one of these nfrs that means the ability to hide technology from users. although invisibility is long seen as an essential characteristic for achieving the goals of ubicomp, it has not been cataloged regarding its subcharacteristics and solutions, making its design and requirements specification in such applications a challenging task. considering the softgoal interdependency graph (sig), which is a well-known format to catalog nfrs, this work aims at capturing subcharacteristics and solutions for invisibility and cataloging them in a sig. since there is no systematic approach on how to build sigs, we also propose to systematize the definition of invisibility sig using the following well-defined research methods: snowballing, database search, grounded theory and questionnaires. as a result, we got an invisibility sig composed of two main subcharacteristics, twelve sub-subcharacteristics, ten general solutions and fifty-six specific solutions. this organized body of knowledge is useful for supporting software engineers to specify requirements and practical solutions for ubicomp and iot applications. furthermore, the proposed methodology used to capture and catalog requirements in a sig can be reused for other nfrs."
        },
        {
            "id": "R195023",
            "label": "RE and Society - A Perspective on RE in Times of Smart Cities and Smart Rural Areas",
            "doi": "10.1109/re.2018.00020",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "our requirements engineering (re) community has known for decades that the success or failure of re methods heavily depends on the context in which they are applied. thus, many experiences have been gained and shared in the community that reflect which re methods are suitable for a specific context, such as embedded systems development (e.g., automotive or military domain) or information systems development (e.g., banking or flight control domain). nowadays, in times of smart cities and their counterpart smart rural areas, where newly introduced it systems have a strong effect on our society, a new and challenging context arises for re, which opens up new research questions. as a contribution to this situation and to foster discussions in our community about whether our re methods are appropriate in this new \"social context\", this perspective paper reflects on the state of the art and on our own experiences in applying re in the context of smart rural areas. these results might also pertain in the context of smart cities that pose similar challenges to re. in addition, we present a framework comprising both an initial classification of social contexts, particularly their end users, and a classification for re methods. example usage scenarios illustrate how this framework helps to reflect on the suitability of our re methods, and, if necessary, provides the basis for adapting them or creating new ones. finally, we outline a roadmap with research questions and related activities with which we want to encourage our community to perform the proposed research activities in order to enrich our body of experiences and adapt our methods to this highly relevant context."
        },
        {
            "id": "R195107",
            "label": "On Systems of Systems Engineering: A Requirements Engineering Perspective and Research Agenda",
            "doi": "10.1109/re.2018.00021",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"the emergence of systems of systems (soss) and systems of systems engineering (sose) is largely driven by global societal needs including energy-water-food nexus, population demographics, global climate, integrated transport, security and social activity. however, due to their scale, structural and functional complexity and emergent properties, these global spanning cyber-physical systems of systems are becoming increasingly complex and more difficult for current requirements engineering (re) practices to handle. in this paper, we firstly introduce sose as an emerging discipline and key characteristics of soss. we then highlight the challenges that the re discipline must respond to. we discuss some weaknesses of current re techniques and approaches to cope with the complexity of soss. we then argue that there is a need for the global re community to evolve current re approaches and to develop new ways of thinking, new re capabilities and possibly a new re science as a key mechanism for addressing requirements engineering complexities posed by systems of systems. we then outline a requirements engineering perspective and research agenda that identifies 'top-10' research themes informed by a cluster of four systems of systems engineering projects funded by the european commission's horizon 2020 research programme.\""
        },
        {
            "id": "R195113",
            "label": "Automated Extraction of Semantic Legal Metadata using Natural Language Processing",
            "doi": "10.1109/re.2018.00022",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "[context] semantic legal metadata provides information that helps with understanding and interpreting the meaning of legal provisions. such metadata is important for the systematic analysis of legal requirements. [objectives] our work is motivated by two observations: (1) the existing requirements engineering (re) literature does not provide a harmonized view on the semantic metadata types that are useful for legal requirements analysis. (2) automated support for the extraction of semantic legal metadata is scarce, and further does not exploit the full potential of natural language processing (nlp). our objective is to take steps toward addressing these limitations. [methods] we review and reconcile the semantic legal metadata types proposed in re. subsequently, we conduct a qualitative study aimed at investigating how the identified metadata types can be extracted automatically. [results and conclusions] we propose (1) a harmonized conceptual model for the semantic metadata types pertinent to legal requirements analysis, and (2) automated extraction rules for these metadata types based on nlp. we evaluate the extraction rules through a case study. our results indicate that the rules generate metadata annotations with high accuracy."
        },
        {
            "id": "R195124",
            "label": "The Grace Period Has Ended: An Approach to Operationalize GDPR Requirements",
            "doi": "10.1109/re.2018.00023",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"the general data protection regulation (gdpr) aims to protect personal data of eu residents and can impose severe sanctions for non-compliance. organizations are currently implementing various measures to ensure their software systems fulfill gdpr obligations such as identifying a legal basis for data processing or enforcing data anonymization. however, as regulations are formulated vaguely, it is difficult for practitioners to extract and operationalize legal requirements from the gdpr. this paper aims to help organizations understand the data protection obligations imposed by the gdpr and identify measures to ensure compliance. to achieve this goal, we propose guideme, a 6-step systematic approach that supports elicitation of solution requirements that link gdpr data protection obligations with the privacy controls that fulfill these obligations and that should be implemented in an organization's software system. we illustrate and evaluate our approach using an example of a university information system. our results demonstrate that the solution requirements elicited using our approach are aligned with the recommendations of privacy experts and are expressed correctly.\""
        },
        {
            "id": "R195129",
            "label": "Semantic Incompleteness in Privacy Policy Goals",
            "doi": "10.1109/re.2018.00025",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"companies that collect personal information online often maintain privacy policies that are required to accurately reflect their data practices and privacy goals. to be comprehensive and flexible for future practices, policies contain ambiguity that summarize practices over multiple types of products and business contexts. ambiguity in data practice descriptions undermines policies as an effective way to communicate system design choices to users, and as a reliable regulatory mechanism. in this paper, we report an investigation to identify incompleteness by representing data practice descriptions as semantic frames. the approach is a grounded analysis to discover which data actions and semantic roles correspond are needed to construct complete data practice descriptions. our results include 281 data action instances obtained from 202 manually annotated statements across five privacy policies. therein, we identified 878 instances of 17 types of semantic roles. incomplete data practice descriptions undermine user comprehension, and can affect the user's perceived privacy risk, which we measure using factorial vignette surveys. we observed that user perception of risk decreases when two roles are present in a statement: the condition under which a data action is performed, and the purpose for which the user's information is used.\""
        },
        {
            "id": "R195144",
            "label": "Learning from Mistakes: An Empirical Study of Elicitation Interviews Performed by Novices",
            "doi": "10.1109/re.2018.00027",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "[context] interviews are the most widely used elicitation technique in requirements engineering. however, conducting effective requirements elicitation interviews is challenging, due to the combination of technical and soft skills that requirements analysts often acquire after a long period of professional practice. empirical evidence about training the novices on conducting effective requirements elicitation interviews is scarce. [objectives] we present a list of most common mistakes that novices make in requirements elicitation interviews. the objective is to assist the educators in teaching interviewing skills to student analysts. [re-search method] we conducted an empirical study involving role-playing and authentic assessment with 110 students, teamed up in 28 groups, to conduct interviews with a customer. one re-searcher made observation notes during the interview while two researchers reviewed the recordings. we qualitatively analyzed the data to identify the themes and classify the mistakes. [results and conclusion] we identified 34 unique mistakes classified into 7 high level themes. we also give examples of the mistakes made by the novices in each theme, to assist the educationists and trainers. our research design is a novel combination of well-known pedagogical approaches described in sufficient details to make it re-peatable for future requirements engineering education and training research."
        },
        {
            "id": "R195153",
            "label": "Efficiency and Effectiveness of Requirements Elicitation Techniques for Children",
            "doi": "10.1109/re.2018.00028",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "[context] the market for software targeting children, both for education and entertainment, is growing. existing work, mainly from hci, has considered the effectiveness of elicitation techniques for eliciting requirements from children as part of a design process. [objective] however, we are lacking work which compares requirements elicitation techniques when used with children. [methods] this study compares five elicitation techniques, taking into consideration the effectiveness and efficiency of each technique. techniques were used with a total of 54 children aged 8-13, eliciting requirements for a museum flight simulator. we compare techniques by looking at the number and type of requirements discovered, perceived participant satisfaction, resources required, perceived usefulness, and requirements coverage of domain specific categories. [conclusions] we observed notable differences between the techniques, including the effectiveness of observations and relative ineffectiveness of questionnaires. we present a set of guidelines to aid industry in eliciting requirements for child-friendly software."
        },
        {
            "id": "R195173",
            "label": "Towards Ubiquitous RE: A Perspective on Requirements Engineering in the Era of Digital Transformation",
            "doi": "10.1109/re.2018.00029",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "we are now living in the era of digital transformation: innovative and digital business models are transforming the global business world and society. however, the authors of this paper have perceived barriers that prevent requirements engineers from contributing properly to the development of the software systems that underpin the digital transformation. we also realized that breaking down each of these barriers would contribute to requirements engineering (re) becoming ubiquitous in certain dimensions: re everywhere, with everyone, for everything, automated, accepting openness, and cross-domain. in this paper, we analyze each dimension of ubiquity in the scope of the interaction between requirements engineers and end users. in particular, we point out the transformation that is required to break down each barrier, present the perspective of the scientific community and our own practical perspective, and discuss our vision on how to achieve this dimension of ubiquity. our goal is to raise the interest of the research community in providing approaches to address the barriers and move towards ubiquitous re."
        },
        {
            "id": "R195179",
            "label": "FAME: Supporting Continuous Requirements Elicitation by Combining User Feedback and Monitoring",
            "doi": "10.1109/re.2018.00030",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "context: software evolution ensures that software systems in use stay up to date and provide value for end-users. however, it is challenging for requirements engineers to continuously elicit needs for systems used by heterogeneous end-users who are out of organisational reach. objective: we aim at supporting continuous requirements elicitation by combining user feedback and usage monitoring. online feedback mechanisms enable end-users to remotely communicate problems, experiences, and opinions, while monitoring provides valuable information about runtime events. it is argued that bringing both information sources together can help requirements engineers to understand end-user needs better. method/tool: we present fame, a framework for the combined and simultaneous collection of feedback and monitoring data in web and mobile contexts to support continuous requirements elicitation. in addition to a detailed discussion of our technical solution, we present the first evidence that fame can be successfully introduced in real-world contexts. therefore, we deployed fame in a web application of a german small and medium-sized enterprise (sme) to collect user feedback and usage data. results/conclusion: our results suggest that fame not only can be successfully used in industrial environments but that bringing feedback and monitoring data together helps the sme to improve their understanding of end-user needs, ultimately supporting continuous requirements elicitation."
        },
        {
            "id": "R195182",
            "label": "On the Impact of Semantic Transparency on Understanding and Reviewing Social Goal Models",
            "doi": "10.1109/re.2018.00031",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"context: i* is one of the most influential languages in the requirements engineering research community. perhaps due to its complexity and low adoption in industry, it became a natural candidate for studies aiming at improving its concrete syntax and the stakeholders' ability to correctly interpret i* models. objectives: we evaluate the impact of semantic transparency on understanding and reviewing i* models, in the presence of a language key. methods: we performed a quasi-experiment comparing the standard i* concrete syntax with an alternative that has an increased semantic transparency. we asked 57 novice participants to perform understanding and reviewing tasks on i* models, and measured their accuracy, speed and ease, using metrics of task success, time and effort, collected with eye-tracking and participants' feedback. results: we found no evidence of improved accuracy or speed attributable to the alternative concrete syntax. although participants' perceived ease was similar, they devoted significantly less visual effort to the model and the provided language key, when using the alternative concrete syntax. conclusions: the context provided by the model and language key may mitigate the i* symbol recognition deficit reported in previous works. however, the alternative concrete syntax required a significantly lower visual effort.\""
        },
        {
            "id": "R195201",
            "label": "An Experimental Comparison of Two Navigation Techniques for Requirements Modeling Tools",
            "doi": "10.1109/re.2018.00032",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"in requirements engineering, many modeling tasks require viewing different parts of a model concurrently. however, traditional zoom+scroll navigation uses a single focus zoom, i.e., at a given point in time, a user can zoom in on a single spot in the model only. therefore, new focus+context navigation techniques have been proposed that allow multiple foci at the same time. in this paper, we report on an experiment with students where we compare the participants' performance when using a requirements modeling tool with traditional zoom+scroll navigation vs. one with so-called flexiview navigation which is a focus+context technique with multiple foci. the participants had to perform typical modeling tasks such as searching, editing, and traversing a model. all tasks were performed on medium-sized tablets with a tool for manipulating so-called imitgraphs. imitgraphs are enriched node-and-edge diagrams that can mimic various diagram types such as class, activity, or goal decomposition diagrams. we found that navigation with flexiview outperformed zoom+scroll navigation with respect to task completion time, number of mistakes, cognitive load, and user satisfaction.\""
        },
        {
            "id": "R195216",
            "label": "Morse: Reducing the Feature Interaction Explosion Problem using Subject Matter Knowledge as Abstract Requirements",
            "doi": "10.1109/re.2018.00033",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "the feature interaction problem appears in many different kinds of complex systems, especially systems whose elements are created or maintained by separate entities - for example, a modern automobile that incorporates electronic systems produced by different suppliers. cross-cutting concerns, such as safety and security, require a comprehensive analysis of the possible interactions. however, there is a combinatorial explosion in the number of feature combinations to be considered. our work approaches the feature interaction problem from a novel point of view: we seek to use the abstract subject matter knowledge of domain experts to deduce why some features will not interact, rather than trying to discover or resolve the interactions. in this paper, we present a method that can automatically reduce the required number of combinations and situations that have to be evaluated or resolved for feature interactions. our tool, called morse, rules out feature combinations that cannot have interactions based on traceable deductions from relatively simple abstract requirements that capture relevant subject matter knowledge. our method is useful as a means of focusing attention on particular situations where more detailed functional requirements may be needed to avoid unacceptable risk arising from unintended interactions between features. relatively simple abstract requirements that capture relevant subject matter knowledge. our method is useful as a means of focusing attention on particular situations where more detailed functional requirements may be needed to avoid unacceptable risk arising from unintended interactions between features."
        },
        {
            "id": "R195218",
            "label": "Discovering, Analyzing, and Managing Safety Stories in Agile Projects",
            "doi": "10.1109/re.2018.00034",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "traditionally, safety-critical projects have been developed using the waterfall process. however, this makes it costly and challenging to incrementally introduce new features and to certify the modified product for use. as a result, there has been increasing interest in adopting agile development paradigms within the safety-critical domain. this in turn introduces numerous challenges. in this paper we address the specific problems of discovering, analyzing, specifying, and managing safety requirements within the agile scrum process. we propose safetyscrum, a methodology that augments the scrum lifecycle with incrementally applied safety-related activities and introduces the notion of \"safety debt\" for incrementally tracking the current safety status of a project. we demonstrate the viability of safetyscrum for managing safety stories in an agile development environment by applying it to a project in which our existing unmanned aerial vehicle system is enhanced to support a river-rescue scenario."
        },
        {
            "id": "R195224",
            "label": "Understanding Challenging Situations in Agile Quality Requirements Engineering and Their Solution strategies: Insights from a Case Study",
            "doi": "10.1109/re.2018.00035",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "in the last few years, agile development methods are getting increasingly popular in large-scale distributed contexts. despite this popularity, empirical studies have reported several challenges that large-scale distributed agile projects face regarding the implementation of quality requirements. however, there is little known about the mechanisms behind those challenges and the practices currently used by agile practitioners to adequately assure the implementation of quality requirements in distributed context. to look deeper into this, we performed a qualitative multi-case study in six different organizations in the netherlands. our multi-case study included seventeen semi-structured open-ended in-depth interviews with agile practitioners of different background and expertise. the analysis of the collected data re-sulted in identifying eleven mechanisms that could be associated with the previously published list of challenges. moreover, the analysis uncovered nine practices used by agile practitioners as solutions to the challenges, in order to ensure the implementation of quality requirements. last, we have mapped the identified mechanisms and practices to the previously identified challenges to get insight into the possible cause and mitigation of those challenges."
        },
        {
            "id": "R195495",
            "label": "SAFE: A Simple Approach for Feature Extraction from App Descriptions and App Reviews",
            "doi": "10.1109/re.2017.71",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "a main advantage of app stores is that they aggregate important information created by both developers and users. in the app store product pages, developers usually describe and maintain the features of their apps. in the app reviews, users comment these features. recent studies focused on mining app features either as described by developers or as reviewed by users. however, extracting and matching the features from the app descriptions and the reviews is essential to bear the app store advantages, e.g. allowing analysts to identify which app features are actually being reviewed and which are not. in this paper, we propose safe, a novel uniform approach to extract app features from the single app pages, the single reviews and to match them. we manually build 18 part-of-speech patterns and 5 sentence patterns that are frequently used in text referring to app features. we then apply these patterns with several text pre-and post-processing steps. a major advantage of our approach is that it does not require large training and configuration data. to evaluate its accuracy, we manually extracted the features mentioned in the pages and reviews of 10 apps. the extraction precision and recall outperformed two state-of-the-art approaches. for well-maintained app pages such as for google drive our approach has a precision of 87% and on average 56% for 10 evaluated apps. safe also matches 87% of the features extracted from user reviews to those extracted from the app descriptions."
        },
        {
            "id": "R195513",
            "label": "A Framework for Improving the Verifiability of Visual Notation Design Grounded in the Physics of Notations",
            "doi": "10.1109/re.2017.37",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "this paper proposes a systematic framework for applying the physics of notations (pon), a theory for the design of cognitively effective visual notations. the pon consists of nine principles, but not all principles lend themselves equally to a clear and unambiguous operationalization. as a result, many visual notations designed according to the pon apply it in different ways. the proposed framework guides what information is required of a reported pon application to ensure that the application of each principle is verifiable. the framework utilizes an evidence-driven design rationale model to structure information needed to assess principles requiring user involvement or cognitive theories. this approach aims to reduce ambiguity in some of the principles by making design choices explicit, and highlighting the level of evidence presented to support it. we demonstrate the proposed framework in a showcase of a recently published visual notation which has been designed with the pon in mind."
        },
        {
            "id": "R195519",
            "label": "Using Argumentation to Explain Ambiguity in Requirements Elicitation Interviews",
            "doi": "10.1109/re.2017.27",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"the requirements elicitation process often starts with an interview between a customer and a requirements analyst. during these interviews, ambiguities in the dialogic discourse may reveal the presence of tacit knowledge that needs to be made explicit. it is therefore important to understand the nature of ambiguities in interviews and to provide analysts with cognitive tools to identify and alleviate ambiguities. ambiguities perceived by analysts are sometimes triggered by specific categories of terms used by the customer such as pronouns, quantifiers, and vague or under-specified terms. however, many of the ambiguities that arise in practice cannot be rooted in single terms. rather, entire fragments of speech and their relation to the mental state of the analyst need to be considered.in this paper, we show that particular types of ambiguities can be characterised by means of argumentation theory. argumentation is the study of how conclusions can be reached through logical reasoning. in an argumentation theory, statements are represented as arguments, and conflict relations among statements are represented as attacks. based on a set of ambiguous fragments extracted from interviews, we define a model of the mental state of the analyst during an interview and translate it into an argumentation theory. then, we show that many of the ambiguities can be characterized in terms of 'attacks' on arguments. the main novelty of this work is in addressing the problem of explaining fragment-level ambiguities in requirements elicitation interviews through the formal modeling of the analyst's mental model using argumentation theory. our contribution provides a data-grounded, theoretical basis to have a more complete understanding of the ambiguity phenomenon, and lays the foundations to design intelligent computer-based agents that are able to automatically identify ambiguities.\""
        },
        {
            "id": "R195551",
            "label": "Feedback Gathering from an Industrial Point of View",
            "doi": "10.1109/re.2017.9",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "feedback communication channels allow end-users to express their needs, which can be considered in software development and evolution. although feedback gathering and analysis have been identified as an important topic and several researchers have started their investigation, information is scarce on how software companies currently elicit end-user feedback. in this study, we explore the experiences of software companies with respect to feedback gathering. the results of a case study and online survey indicate two sides of the same coin: on the one hand, most software companies are aware of the relevance of end-user feedback for software evolution and provide feedback channels, which allow end-users to communicate their needs and problems. on the other hand, the quantity and quality of the feedback received varies. we conclude that software companies still do not fully exploit the potential of end-user feedback for software development and evolution."
        },
        {
            "id": "R195578",
            "label": "What Requirements Knowledge Do Developers Need to Manage Change in Safety-Critical Systems?",
            "doi": "10.1109/re.2017.65",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"developers maintaining safety-critical systems need to assess the impact a proposed change would have upon existing safety controls. by leveraging the network of traceability links that are present in most safety-critical systems, we can push timely information about related hazards, environmental assumptions, and safety requirements to developers. in this work we take a design science approach to discover the informational needs of developers as they engage in software maintenance activities and then propose and evaluate techniques for presenting and visualizing this information. through a human-centered study involving five safety-critical system practitioners and 14 experienced developers, we analyze the way in which developers use requirements knowledge while maintaining safety-critical code, identify their informational needs, and propose and evaluate a supporting visualization technique. the insights proposed as a result of this study can be used to design requirements-based knowledge tools for supporting developers' maintenance tasks.\""
        },
        {
            "id": "R195587",
            "label": "What Questions do Requirements Engineers Ask?",
            "doi": "10.1109/re.2017.76",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "requirements engineering (re) is comprised of various tasks related to discovering, documenting, and maintaining different kinds of requirements. to accomplish these tasks, a requirements engineer or business analyst needs to retrieve and combine information from multiple sources such as use case models, interview scripts, and business rules. however, collecting and analyzing all the required data can be tedious and the resulting data is often incomplete with inadequate trace links. analyzing real-world queries can shed light on the questions requirements professionals would like to ask and the artifacts needed to support such questions. we therefore conducted an online survey with requirements professionals in the it industry. our analysis included 29 survey responses and a total of 159 natural language queries. using open coding and grounded theory, we analyzed and grouped these queries into 9 different query purposes and 54 sub-purposes, and also identified frequently used artifacts. the results from the survey could help project-level planners identify important questions, proactively instrument their environments with supporting tools, and strategically collect data that is needed to answer the queries of interest to their project."
        },
        {
            "id": "R195597",
            "label": "Datasets from Fifteen Years of Automated Requirements Traceability Research: Current State, Characteristics, and Quality",
            "doi": "10.1109/re.2017.80",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "software datasets play a crucial role in advancing automated software traceability research. they can be used by researchers in different ways to develop or validate new automated approaches. the diversity and quality of the datasets within a research community have a significant impact on the accuracy, generalizability, and reproducibility of the results and consequently on the usefulness and practicality of the techniques under study. collecting and assessing the quality of such datasets are not trivial tasks and have been reported as an obstacle by many researchers in the domain of software engineering. this paper presents a first-of-its-kind study to review and assess the datasets that have been used in software traceability research over the last fifteen years. it presents and articulates the current status of these datasets, their characteristics, and their threats to validity. furthermore, this paper introduces a traceability-dataset quality assessment (t-dqa) framework to categorize software traceability datasets and assist researchers to select appropriate datasets for their research based on different characteristics of the datasets and the context in which those datasets will be used."
        },
        {
            "id": "R195664",
            "label": "The Trouble with Security Requirements",
            "doi": "10.1109/re.2017.13",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"manifold approaches to security requirements engineering have been proposed, yet there is no consensus how to elicit, analyze, or express security needs. this perspective paper systematizes the problem space of security requirements engineering. security needs result from the interplay of three dimensions: threats, security goals, and system design. elementary statements can be made in each dimension, but such one-dimensional requirements remain partial and insufficient. to understand security needs, one has to analyze their interaction. distinct analysis tasks arise for each pair of dimensions and are supported by different techniques: risk analysis, as in coras, between threats and security goals; security design, as exemplified by the framework of haley et al., between goals and design; and security design analysis, such as microsoft's threat modeling technique with data flow diagrams and stride, between design and threats. all three perspectives are necessary to develop secure systems. security requirements engineering must iterate through them, because threats determine the relevance of security goals, security design seeks ways to fulfill them, and design choices themselves influence threats and security goals.\""
        },
        {
            "id": "R195669",
            "label": "Safety-Focused Security Requirements Elicitation for Medical Device Software",
            "doi": "10.1109/re.2017.21",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "security attacks on medical devices have been shown to have potential safety concerns. because of this, stakeholders (device makers, regulators, users, etc.) have increasing interest in enhancing security in medical devices. an effective means to approach this objective is to integrate systematic security requirements elicitation and analysis into the design and evaluation of medical device software. this paper extends the sequence-based enumeration approach, a systematic approach for defining the behavior of embedded software, to analyze the requirement documents of a medical device for the purpose of eliciting security requirements. as a proof of concept, we apply our approach on a concrete case study, which shows that the extended approach is useful for identifying sequences of medical device events that might be harmful to the patient, for example because the events are initiated by an active adversary trying to use the device in a malicious way. we then show how security requirements may be formulated based on the identified threats. by exploring these sequences systematically, the developers can reliably assess what, where, and how the security threats may manifest in their system, what the safety implications are, and finally they can evaluate the resulting requirements and mitigations."
        },
        {
            "id": "R195679",
            "label": "Reinforcing Security Requirements with Multifactor Quality Measurement",
            "doi": "10.1109/re.2017.77",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "choosing how to write natural language scenarios is challenging, because stakeholders may over-generalize their descriptions or overlook or be unaware of alternate scenarios. in security, for example, this can result in weak security constraints that are too general, or missing constraints. another challenge is that analysts are unclear on where to stop generating new scenarios. in this paper, we introduce the multifactor quality method (mqm) to help requirements analysts to empirically collect system constraints in scenarios based on elicited expert preferences. the method combines quantitative statistical analysis to measure system quality with qualitative coding to extract new requirements. the method is bootstrapped with minimal analyst expertise in the domain affected by the quality area, and then guides an analyst toward selecting expert-recommended requirements to monotonically increase system quality. we report the results of applying the method to security. this include 550 requirements elicited from 69 security experts during a bootstrapping stage, and subsequent evaluation of these results in a verification stage with 45 security experts to measure the overall improvement of the new requirements. security experts in our studies have an average of 10 years of experience. our results show that using our method, we detect an increase in the security quality ratings collected in the verification stage. finally, we discuss how our proposed method helps to improve security requirements elicitation, analysis, and measurement."
        },
        {
            "id": "R195692",
            "label": "\u201cSHORT\u201der Reasoning About Larger Requirements Models",
            "doi": "10.1109/re.2017.31",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "when requirements engineering(re) models are unreasonably complex, they cannot support efficient decision making. short is a tool to simplify that reasoning by exploiting the \"key\" decisions within re models. these \"keys\" have the property that once values are assigned to them, it is very fast to reason over the remaining decisions. using these \"keys\", reasoning about re models can be greatly shortened by focusing stakeholder discussion on just these key decisions.this paper evaluates the short tool on eight complex re models. we find that the number of keys are typically only 12% of all decisions. since they are so few in number, keys can be used to reason faster about models. for example, using keys, we can optimize over those models (to achieve the most goals at least cost) two to three orders of magnitude faster than standard methods. better yet, finding those keys is not difficult: short runs in low order polynomial time and terminates in a few minutes for the largest models."
        },
        {
            "id": "R195713",
            "label": "Modeling and Reasoning with Changing Intentions: An Experiment",
            "doi": "10.1109/re.2017.19",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"existing modeling approaches in requirements engineering assume that stakeholder goals are static: once set, they remain the same throughout the lifecycle of the project. of course, such goals, like anything else, may change over time. in earlier work, we introduced evolving intentions: an approach that allows stakeholders to specify how evaluations of goal model elements change over time. simulation over evolving intentions enables stakeholders to ask a variety of 'what if' questions, and evaluate possible evolutions of a goal model. growingleaf is a web-based tool that implements both the modeling and analysis components of this approach. in this paper, we investigate the effectiveness and usability of evolving intentions, simulation over evolving intentions, and growingleaf. we report on a between-subjects experiment we conducted with fifteen graduate students familiar with requirements engineering. using qualitative, quantitative, and timing data, we show that evolving intentions were intuitive, that simulation over evolving intentions increased the subjects' understanding and produced meaningful results, and that growingleaf was found to be effective and usable.\""
        },
        {
            "id": "R195728",
            "label": "Does Goal-Oriented Requirements Engineering Achieve Its Goal?",
            "doi": "10.1109/re.2017.40",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "the number of papers and articles on goals would suggest that goal-oriented requirements engineering is a well understood and mature area within the requirements engineering discipline. in particular, there is a wealth of published material on formal goal modelling approaches. however, the uptake of the goal approaches advocated by academics and researchers within real world settings appears to be quite low. where goals are used in industrial practice their use is mainly informal and the methods used are inconsistent. there appears to be a significant gap between research and practice in the use of goals within requirements engineering. a two-part study was undertaken to check whether there is evidence to support this view of a disconnection between research and industry. firstly, a literature survey of requirements engineering papers about goals reveals a large body of published material, but the majority has little industrial involvement. secondly, a questionnaire completed by experienced requirements engineering practitioners suggests that use of goals in practice is inconsistent, informal, and rarely utilises formal modelling approaches. this paper proposes future work that would close the gap between research and practice in the use of goals within requirements engineering."
        },
        {
            "id": "R195743",
            "label": "New Frontiers for Requirements Engineering",
            "doi": "10.1109/re.2017.23",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "requirements engineering (re) has grown from its humble beginnings to embrace a wide variety of techniques, drawn from many disciplines, and the diversity of tasks currently performed under the label of re has grown beyond that encom-passed by software development. we briefly review how re has evolved and observe that re is now a collection of best practices for pragmatic, outcome-focused critical thinking \u2013 applicable to any domain. we discuss an alternative perspective on, and de-scription of, the discipline of re and advocate for the evolution of re toward a discipline that supports the application of re prac-tice to any domain. we call upon re practitioners to proactively engage in alternative domains and call upon researchers that adopt practices from other domains to actively engage with their inspiring domains. for both, we ask that they report upon their experience so that we can continue to expand re frontiers."
        },
        {
            "id": "R195749",
            "label": "How Much Undocumented Knowledge is there in Agile Software Development?: Case Study on Industrial Project Using Issue Tracking System and Version Control System",
            "doi": "10.1109/re.2017.33",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "in agile software development projects, software engineers prioritize implementation over documentation to eliminate needless documentation. is the cost of missing documentation greater than the cost of producing unnecessary or unused documentation? even without these documents, software engineers maintain other software artifacts, such as tickets in an issue tracking system (its) or source code committed to a version control system (vcs). do these artifacts contain the necessary knowledge? in this paper, we examine undocumented knowledge in an agile software development project at ntt. for our study, we collected 159 commit logs in a vcs and 102 tickets in the its from the three-month period of the project. we propose a ticket-commit network chart (tcc) that visually represents time-series commit activities along with filed issue tickets. we also implement a tool to generate the tcc using both commit log and ticket data. our study revealed that in 16% of all commits, software engineers committed source code to the vcs without a corresponding issue ticket in the its. had these commits been based on individual issue tickets, these \"unissued\" tickets would have accounted for 20% of all tickets. software users and requirements engineers also evaluated the contents of these commits and found that 42% of the \"unissued\" tickets were required for software operation and 23% of those were required for requirements modification."
        },
        {
            "id": "R195767",
            "label": "Software Requirements Analyst Profile: A Descriptive Study of Brazil and Mexico",
            "doi": "10.1109/re.2017.22",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "software requirements analyst work is considered crucial in the software development lifecycle. this paper presents a descriptive study on the software requirements analyst profile, considering brazilian and mexican markets, two countries that lead it investment ranking in latin america. to identify the competences expected by the brazilian and mexican markets for software requirements professionals was the study objective. the competency model considers a set of knowledge, skills and attitudes. content analysis and nvivo software were used to categorize 311 job ads for software requirements analysts between 2016 and 2017. in terms of knowledge, the importance of higher education for both countries was identified. \"using techniques and tools,\" \"methodological competence,\" \"good written communication,\" and \"good verbal communication\" were identified as relevant skills. the most important attitudes identified were: \"analytical thinking,\" \"organization,\" \"interpersonal relationship\" and \"information sharing.\" most of results obtained confirm requirements analyst skills pointed out by other authors, but there are some differences due to the geographic context. at the same time, findings can help improve the understanding of different approaches in the requirements engineering field in the two countries and professionals working particularly in the latin american market."
        },
        {
            "id": "R195789",
            "label": "Improving the Identification of Hedonic Quality in User Requirements \u2014 A Controlled Experiment",
            "doi": "10.1109/re.2017.49",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "context and motivation systematically engineering a good user experience (ux) into a computer-based system under development demands that the user requirements of the system reflect all needs, including emotional, of all stakeholders. user requirements address two different types of qualities: pragmatic qualities (pqs), that address system functionality and usability, and hedonic qualities (hqs) that address the stakeholder\\'s psychological well-being. studies show that users tend to describe such satisfying uxes mainly with pqs, and that some users seem to believe that they are describing a hq when they are actually describing a pq. question/problem the problem is to see if classification of any user requirement as pq-related or hq-related is difficult, and if so, why. principal ideas/results we conducted a controlled experiment in which twelve requirements-engineering and ux professionals, hereinafter called \"classifiers\" classified each of 105 user requirements as pq-related or hq-related. the experiment shows that neither (1) a classifier\\'s involvement in the project from which the requirements came nor (2) the classifier\\'s use of a detailed model of the qualities in addition to the standard definitions of \"pq\" and \"hq\" has a positive effect on the consistency of the classifier\\'s classification with that of others. contribution the experiment revealed that classification of user requirements is a lot harder than initially assumed."
        },
        {
            "id": "R195806",
            "label": "Usability Insights for Requirements Engineering Tools: A User Study with Practitioners in Aeronautics",
            "doi": "10.1109/re.2017.20",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "requirements engineering plays a crucial role in coordinating the different stakeholders needed for safe aeronautics systems engineering. we conducted a qualitative study, using interviews and mockups, with fifteen industrial practitioners from four aeronautics companies, in order to investigate what tasks are actually performed by requirements engineers and how current tools support these tasks. we found that re-specific tools constrain engineers to a rigid workflow, which is conflicting with the adaptive exploration of the problem. engineers often start by using general-purpose tools to foster exploration and collaborative work with suppliers, at the expense of traceability. when engineers shift to requirements refinement and verification, they must use re-specific tools to grant traceability. then, the lack of tool usability yields significant time loss and dissatisfaction. based on scenarios of observed re practices and walkthrough, we formulate usability insights for re-specific tools in order to conciliate flexibility and traceability throughout the re process."
        },
        {
            "id": "R195849",
            "label": "Detecting Vague Words &amp; Phrases in Requirements Documents in a Multilingual Environment",
            "doi": "10.1109/re.2017.24",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "vagueness in software requirements documents can lead to several maintenance problems, especially when the customer and development team do not share the same language. currently, companies rely on human translators to maintain communication and limit vagueness by translating the requirement documents by hand. in this paper, we describe two approaches that automatically identify vagueness in requirements documents in a multilingual environment. we perform two studies for calibration purposes under strict industrial limitations, and describe the tool that we ultimately deploy. in the first study, six participants, two native portuguese speakers and four native spanish speakers, evaluated both approaches. then, we conducted a field study to test the performance of the best approach in real-world environments at two companies. we describe several lessons learned for research and industrial deployment."
        },
        {
            "id": "R195857",
            "label": "A Case Study on Evaluating the Relevance of Some Rules for Writing Requirements Through an Online Survey",
            "doi": "10.1109/re.2017.11",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"as part of a research project that aims at proposing a new methodology for defining a series of rules for writing good requirements \u2013 often referred to as a controlled natural language (cnl) \u2013 for the french space agency (cnes, centre national d'\u00e9tudes spatiales), we asked both experienced engineers and non-experts to fill in an online questionnaire in order to gather their perception about requirements written according to recommendations commonly found in cnls, and to compare them with seemingly more natural and less restrictive formulations. the examples we used for this case study were adapted from genuine requirements in french, extracted from several specifications of a recent space project. our main goal is to evaluate whether (and to what extent) the writing rules we considered may be relevant for the engineers at cnes. in particular, we try to identify cases where the experts' opinions differ from the recommended use and where these rules could thus probably be adapted.\""
        },
        {
            "id": "R195866",
            "label": "A Case Study on a Specification Approach Using Activity Diagrams in Requirements Documents",
            "doi": "10.1109/re.2017.28",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"rising complexity of systems has long been a major challenge in requirements engineering. this manifests in more extensive and harder to understand requirements documents. at the daimler ag, an approach is applied that combines the use of activity diagrams with natural language specifications to specify system functions. the approach starts with an activity diagram that is created to get an early overview. the contained information is then transferred to a textual requirements document, where details are added and the behavior is refined. while the approach aims to reduce efforts needed to understand a system's behavior, the application of the approach itself causes new challenges on its own. by examining existing specifications at daimler, we identified nine categories of inconsistencies and deviations between activity diagrams and their textual representations. in a case study, we examined one system in detail to assess how often these occur. in a follow-up survey, we presented instances of the categories to different stakeholders of the system and let them asses the categories regarding their severity. our analysis indicates that a coexistence of textual and graphical representations of models without proper tool support results in inconsistencies and deviations that may cause severe maintenance costs or even provoke faults in subsequent development steps.\""
        },
        {
            "id": "R195982",
            "label": "A Formalization Method to Process Structured Natural Language to Logic Expressions to Detect Redundant Specification and Test Statements",
            "doi": "10.1109/re.2017.38",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "automotive systems are constantly increasing in complexity and size. beside the increase of requirements specifications and related test specification due to new systems and higher system interaction, we observe an increase of redundant specifications. as the predominant specification language (both for requirements and test cases) is still natural text, it is not easy to detect these redundancies. in principle, to detect these redundancies, each statement has to be compared to all others. this proves to be difficult because of number and informal expression of statements. in this paper we propose a solution to the problem of detecting redundant specification and test statements described in structured natural language. we propose a formalization process for requirements specification and test statements, allowing us to detect redundant statements and thus reduce the efforts for specification and validation. specification pattern systems and linear temporal logic provide the base for our process. we did evaluate the method in the context of mercedes-benz passenger car development. the results show that for the investigated sample set of test statements, we could detect about 30% of test steps as redundant. this indicates the savings potential of our approach."
        },
        {
            "id": "R195986",
            "label": "Do Words Make a Difference? An Empirical Study on the Impact of Taxonomies on the Classification of Requirements",
            "doi": "10.1109/re.2017.57",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "\"requirements taxonomies help to classify and channel the requirements in a project. a very simple taxonomy is the distinction between functional and non-functional requirements. furthermore, a taxonomy helps to decide if a statement is a requirement at all or just something else (e.g., 'information'). the quality of a taxonomy is important as we do not want to put a statement in the wrong category.in this paper, we argue that we need to take cognitive psychology into account in this task of requirements classification. cognitive psychology focuses on the abilities and limitations of the human mind. we present a controlled experiment and a replication in which we compare three requirements taxonomies.the participants had to evaluate a set of requirements based on the given taxonomies. the results of these experiments show that there are differences between the taxonomies: interestingly, the question whether a statement is identified as a requirement or not depends on the taxonomy. these experiments present initial results, we assume that these results are related to phenomena of cognitive psychology.we conclude that the wording should be carefully taken into account in the definition of the categories of a high quality requirements taxonomy.\""
        },
        {
            "id": "R195995",
            "label": "Requirements Capture and Analysis in ASSERT(TM)",
            "doi": "10.1109/re.2017.54",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "capturing high-level requirements in a human readable but formal representation suitable for analysis is an important goal for ge. to that end we have augmented an existing controlled-english modeling language with a new controlled-english requirements capture language to create the requirements capture frontend of the assert(tm) tool suite. requirements captured in assert can be analyzed for a number of possible shortcomings, both individually and collectively. once a set of requirements has reached a satisfactory level of completeness, consistency, etc., it can then be further used to generate test cases and test procedures. this paper will focus on the requirements capture and analysis functions of assert and will illustrate its capabilities with a sample problem previously used as a challenge problem for requirements specification."
        },
        {
            "id": "R195998",
            "label": "Mining Associations Between Quality Concerns and Functional Requirements",
            "doi": "10.1109/re.2017.68",
            "research_field": {
                "id": "R140",
                "label": "Software Engineering"
            },
            "abstract": "the cost and effort of developing software systems in a new technical area can be extensive. an organization must perform a domain analysis to discover competing products, analyze their architectures and features, and ultimately discover and specify product requirements. however, delivering high quality products, depends not only on gaining an understanding of functional requirements, but also of qualities such as performance, reliability, security, and usability. discovering such concerns early in the requirements process drives architectural design decisions. this paper extends our prior work on mining functional requirements from large collections of domain documents, by proposing and evaluating a new technique for discovering and specifying quality concerns related to specific functional components. we evaluate our approach against three domains of positive train control, electronic health records, and medical infusion pumps, and show that it significantly outperforms a basic information retrieval approach. finally we classified the forms of retrieved information, discussed the utility of different types, and conducted a small study with an experienced engineer to investigate the quality of requirements produced using our approach."
        },
        {
            "id": "R159779",
            "label": "Return of the Vision Video: Can Corporate Vision Videos Serve as Setting for Participation?",
            "doi": "",
            "research_field": {
                "id": "R265",
                "label": "Computer-Aided Engineering and Design"
            },
            "abstract": "this paper examines the role of corporate vision videos as a possible setting for participation when exploring the future potentials (and pitfalls) of new technological concepts. we propose that through the recent decade\u2019s rise web 2.0 platforms, and the viral effects of user sharing, the corporate vision video of today might take on a significantly different role than before, and act as a participatory design approach. this address the changing landscaping for participatory and user-involved design processes, in the wake of new digital forms of participation, communication and collaboration, which have radically changed the possible power dynamics of the production life cycle of new product developments. through a case study, we pose the question of whether the online engagements around corporate vision videos can be viewed as a form of participation in a design process, and thus revitalize the relevance of vision videos as a design resource?"
        },
        {
            "id": "R11022",
            "label": "Evaluating Architectural Choices for Deep Learning Approaches for Question Answering Over Knowledge Bases",
            "doi": "10.1109/icosc.2019.8665496",
            "research_field": {
                "id": "R133",
                "label": "Artificial Intelligence"
            },
            "abstract": "the task of answering natural language questions over knowledge bases has received wide attention in recent years. various deep learning architectures have been proposed for this task. however, architectural design choices are typically not systematically compared nor evaluated under the same conditions. in this paper, we contribute to a better understanding of the impact of architectural design choices by evaluating four different architectures under the same conditions. we address the task of answering simple questions, consisting in predicting the subject and predicate of a triple given a question. in order to provide a fair comparison of different architectures, we evaluate them under the same strategy for inferring the subject, and compare different architectures for inferring the predicate. the architecture for inferring the subject is based on a standard lstm model trained to recognize the span of the subject in the question and on a linking component that links the subject span to an entity in the knowledge base. the architectures for predicate inference are based on i) a standard softmax classifier ranging over all predicates as output, ii) a model that predicts a low-dimensional encoding of the property and subject entity, iii) a model that learns to score a pair of subject and predicate given the question as well as iv) a model based on the well-known fasttext model. the comparison of architectures shows that fasttext provides better results than other architectures."
        },
        {
            "id": "R3076",
            "label": "An expert system based on texture features and decision tree classifier for diagnosis of tumor in brain MR images",
            "doi": "10.1109/ic3i.2014.7019690",
            "research_field": {
                "id": "R133",
                "label": "Artificial Intelligence"
            },
            "abstract": "in this paper a new tumor classification system has been designed and developed for mri systems. the mr imaging is a mostly used scheme for high excellence in medical imaging, it gives clear imageing capability especially in brain imaging where the soft-tissues contrast and non invasiveness is a clear advantage. the proposed method consists of three stages namely pre-processing, feature extraction and classification. in the first stage, gausian filter is applied for extracting the noise for experimental image. in the second stage, statistical texture features are extracted for the purpose of classification. finally, the decision tree classifier is used to classify the type of tumor image. in our proposed system classification has two divisions: i) training stage and ii) testing stage. in the training stage, various features are extracted from the tumor and non tumor images. in testing stage, based on the knowledge base, the classifier classify the image into tumor and non- tumor. thus, the proposed system has been evaluated on a dataset of 40 patients. the proposed system was found efficient in classification with a success of more than 95% of accuracy."
        },
        {
            "id": "R38180",
            "label": "End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures",
            "doi": "10.18653/v1/p16-1105",
            "research_field": {
                "id": "R133",
                "label": "Artificial Intelligence"
            },
            "abstract": "we present a novel end-to-end neural model to extract entities and relations between them. our recurrent neural network based model captures both word sequence and dependency tree substructure information by stacking bidirectional tree-structured lstm-rnns on bidirectional sequential lstm-rnns. this allows our model to jointly represent both entities and relations with shared parameters in a single model. we further encourage detection of entities during training and use of entity information in relation extraction via entity pretraining and scheduled sampling. our model improves over the state-of-the-art feature-based model on end-to-end relation extraction, achieving 12.1% and 5.7% relative error reductions in f1-score on ace2005 and ace2004, respectively. we also show that our lstm-rnn based model compares favorably to the state-of-the-art cnn based model (in f1-score) on nominal relation classification (semeval-2010 task 8). finally, we present an extensive ablation analysis of several model components."
        },
        {
            "id": "R38225",
            "label": "Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme",
            "doi": "10.18653/v1/p17-1113",
            "research_field": {
                "id": "R133",
                "label": "Artificial Intelligence"
            },
            "abstract": "\"joint extraction of entities and relations is an important task in information extraction. to tackle this problem, we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem. then, based on our tagging scheme, we study different end-to-end models to extract entities and their relations directly, without identifying entities and relations separately. we conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods. what's more, the end-to-end model proposed in this paper, achieves the best results on the public dataset.\""
        },
        {
            "id": "R39210",
            "label": "A Survey of Recommender Systems Based on Deep Learning",
            "doi": "10.1109/access.2018.2880197",
            "research_field": {
                "id": "R133",
                "label": "Artificial Intelligence"
            },
            "abstract": "in recent years, deep learning\u2019s revolutionary advances in speech recognition, image analysis, and natural language processing have gained significant attention. deep learning technology has become a hotspot research field in the artificial intelligence and has been applied into recommender system. in contrast to traditional recommendation models, deep learning is able to effectively capture the non-linear and non-trivial user-item relationships and enables the codification of more complex abstractions as data representations in the higher layers. in this paper, we provide a comprehensive review of the related research contents of deep learning-based recommender systems. first, we introduce the basic terminologies and the background concepts of recommender systems and deep learning technology. second, we describe the main current research on deep learning-based recommender systems. third, we provide the possible research directions of deep learning-based recommender systems in the future. finally, concludes this paper."
        },
        {
            "id": "R41079",
            "label": "Speech Recognition Using Deep Neural Networks: A Systematic Review",
            "doi": "10.1109/access.2019.2896880",
            "research_field": {
                "id": "R133",
                "label": "Artificial Intelligence"
            },
            "abstract": "over the past decades, a tremendous amount of research has been done on the use of machine learning for speech processing applications, especially speech recognition. however, in the past few years, research has focused on utilizing deep learning for speech-related applications. this new area of machine learning has yielded far better results when compared to others in a variety of applications including speech, and thus became a very attractive area of research. this paper provides a thorough examination of the different studies that have been conducted since 2006, when deep learning first arose as a new area of machine learning, for speech applications. a thorough statistical analysis is provided in this review which was conducted by extracting specific information from 174 papers published between the years 2006 and 2018. the results provided in this paper shed light on the trends of research in this area as well as bring focus to new research topics."
        },
        {
            "id": "R4857",
            "label": "How are topics born? Understanding the research dynamics preceding the emergence of new areas",
            "doi": "10.7717/peerj-cs.119",
            "research_field": {
                "id": "R133",
                "label": "Artificial Intelligence"
            },
            "abstract": "the ability to promptly recognise new research trends is strategic for many stakeholders, including universities, institutional funding bodies, academic publishers and companies. while the literature describes several approaches which aim to identify the emergence of new research topics early in their lifecycle, these rely on the assumption that the topic in question is already associated with a number of publications and consistently referred to by a community of researchers. hence, detecting the emergence of a new research area at an embryonic stage , i.e., before the topic has been consistently labelled by a community of researchers and associated with a number of publications, is still an open challenge. in this paper, we begin to address this challenge by performing a study of the dynamics preceding the creation of new topics. this study indicates that the emergence of a new topic is anticipated by a significant increase in the pace of collaboration between relevant research areas, which can be seen as the \u2018parents\u2019 of the new topic. these initial findings (i) confirm our hypothesis that it is possible in principle to detect the emergence of a new topic at the embryonic stage, (ii) provide new empirical evidence supporting relevant theories in philosophy of science, and also (iii) suggest that new topics tend to emerge in an environment in which weakly interconnected research areas begin to cross-fertilise."
        },
        {
            "id": "R49468",
            "label": "Retinal Blood Vessel Segmentation Using Hybrid Features and Multi-Layer Perceptron Neural Networks",
            "doi": "10.3390/sym12060894",
            "research_field": {
                "id": "R133",
                "label": "Artificial Intelligence"
            },
            "abstract": "segmentation of retinal blood vessels is the first step for several computer aided-diagnosis systems (cad), not only for ocular disease diagnosis such as diabetic retinopathy (dr) but also of non-ocular disease, such as hypertension, stroke and cardiovascular diseases. in this paper, a supervised learning-based method, using a multi-layer perceptron neural network and carefully selected vector of features, is proposed. in particular, for each pixel of a retinal fundus image, we construct a 24-d feature vector, encoding information on the local intensity, morphology transformation, principal moments of phase congruency, hessian, and difference of gaussian values. a post-processing technique depending on mathematical morphological operators is used to optimise the segmentation. moreover, the selected feature vector succeeded in outfitting the symmetric features that provided the final blood vessel probability as a binary map image. the proposed method is tested on three known datasets: digital retinal image for extraction (drive), structure analysis of the retina (stare), and chased_db1 datasets. the experimental results, both visual and quantitative, testify to the robustness of the proposed method. this proposed method achieved 0.9607, 0.7542, and 0.9843 in drive, 0.9632, 0.7806, and 0.9825 on stare, 0.9577, 0.7585 and 0.9846 in chase_db1, with respectable accuracy, sensitivity, and specificity performance metrics. furthermore, they testify that the method is superior to seven similar state-of-the-art methods."
        },
        {
            "id": "R5289",
            "label": "Controlling an autonomous agent using internal value based action selection",
            "doi": "10.1504/ijista.2007.012491",
            "research_field": {
                "id": "R133",
                "label": "Artificial Intelligence"
            },
            "abstract": "in this paper we describe an approach of controlling an autonomous robot by means of a hierarchical control structure, with a learning action selection. since damasio\\'s \"descartes\\' error\" in 1994 the number of approaches to action selection that use internal values, derived from psychological models of emotions or drives has increased significantly. the approach realises a learning action selection mechanism in a hierarchy of sensory and actuatory layers. the sensory values yield the internal states, as a basis for action selection. in addition they are used to calculate the reinforcement signal that trains the action selection."
        },
        {
            "id": "R6286",
            "label": "Template-based question answering",
            "doi": "10.1145/2187836.2187923",
            "research_field": {
                "id": "R133",
                "label": "Artificial Intelligence"
            },
            "abstract": "as an increasing amount of rdf data is published as linked data, intuitive ways of accessing this data become more and more important. question answering approaches have been proposed as a good compromise between intuitiveness and expressivity. most question answering systems translate questions into triples which are matched against the rdf data to retrieve an answer, typically relying on some similarity metric. however, in many cases, triples do not represent a faithful representation of the semantic structure of the natural language question, with the result that more expressive queries can not be answered. to circumvent this problem, we present a novel approach that relies on a parse of the question to produce a sparql template that directly mirrors the internal structure of the question. this template is then instantiated using statistical entity identification and predicate detection. we show that this approach is competitive and discuss cases of questions that can be answered with our approach but not with competing approaches."
        },
        {
            "id": "R6294",
            "label": "Natural language question answering over RDF",
            "doi": "10.1145/2588555.2610525",
            "research_field": {
                "id": "R133",
                "label": "Artificial Intelligence"
            },
            "abstract": "rdf question/answering (q/a) allows users to ask questions in natural languages over a knowledge base represented by rdf. to answer a national language question, the existing work takes a two-stage approach: question understanding and query evaluation. their focus is on question understanding to deal with the disambiguation of the natural language phrases. the most common technique is the joint disambiguation, which has the exponential search space. in this paper, we propose a systematic framework to answer natural language questions over rdf repository (rdf q/a) from a graph data-driven perspective. we propose a semantic query graph to model the query intention in the natural language question in a structural way, based on which, rdf q/a is reduced to subgraph matching problem. more importantly, we resolve the ambiguity of natural language questions at the time when matches of query are found. the cost of disambiguation is saved if there are no matching found. we compare our method with some state-of-the-art rdf q/a systems in the benchmark dataset. extensive experiments confirm that our method not only improves the precision but also speeds up query performance greatly."
        },
        {
            "id": "R6300",
            "label": "Question answering over biomedical linked data with Grammatical Framework",
            "doi": "10.3233/SW-160223",
            "research_field": {
                "id": "R133",
                "label": "Artificial Intelligence"
            },
            "abstract": "the blending of linked data with ontologies leverages the access to data. gfmed introduces grammars for a controlled natural language targeted towards biomedical linked data and the corresponding controlled sparql language. the grammars are described in grammatical framework and introduce linguistic and sparql phrases mostly about drugs, diseases and relationships between them. the semantic and linguistic chunks correspond to description logic constructors. problems and solutions for querying biomedical linked data with romanian, besides english, are also considered in the context of gf."
        },
        {
            "id": "R6313",
            "label": "Natural language queries over heterogeneous linked data graphs",
            "doi": "10.1145/2557500.2557534",
            "research_field": {
                "id": "R133",
                "label": "Artificial Intelligence"
            },
            "abstract": "the demand to access large amounts of heterogeneous structured data is emerging as a trend for many users and applications. however, the effort involved in querying heterogeneous and distributed third-party databases can create major barriers for data consumers. at the core of this problem is the semantic gap between the way users express their information needs and the representation of the data. this work aims to provide a natural language interface and an associated semantic index to support an increased level of vocabulary independency for queries over linked data/semantic web datasets, using a distributional-compositional semantics approach. distributional semantics focuses on the automatic construction of a semantic model based on the statistical distribution of co-occurring words in large-scale texts. the proposed query model targets the following features: (i) a principled semantic approximation approach with low adaptation effort (independent from manually created resources such as ontologies, thesauri or dictionaries), (ii) comprehensive semantic matching supported by the inclusion of large volumes of distributional (unstructured) commonsense knowledge into the semantic approximation process and (iii) expressive natural language queries. the approach is evaluated using natural language queries on an open domain dataset and achieved avg. recall=0.81, mean avg. precision=0.62 and mean reciprocal rank=0.49."
        },
        {
            "id": "R110733",
            "label": "Extractive Summarization of Meeting Recordings",
            "doi": "",
            "research_field": {
                "id": "R322",
                "label": "Computational Linguistics"
            },
            "abstract": "several approaches to automatic speech summarization are discussed below, using the icsi meetings corpus. we contrast feature-based approaches using prosodic and lexical features with maximal marginal relevance and latent semantic analysis approaches to summarization. while the latter two techniques are borrowed directly from the field of text summarization, feature-based approaches using prosodic information are able to utilize characteristics unique to speech data. we also investigate how the summarization results might deteriorate when carried out on asr output as opposed to manual transcripts. all of the summaries are of an extractive variety, and are compared using the software rouge."
        },
        {
            "id": "R12208",
            "label": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "doi": "10.18653/v1/n19-1423",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "abstract": "we introduce a new language representation model called bert, which stands for bidirectional encoder representations from transformers. unlike recent language representation models (peters et al., 2018a; radford et al., 2018), bert is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. as a result, the pre-trained bert model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. bert is conceptually simple and empirically powerful. it obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the glue score to 80.5 (7.7 point absolute improvement), multinli accuracy to 86.7% (4.6% absolute improvement), squad v1.1 question answering test f1 to 93.2 (1.5 point absolute improvement) and squad v2.0 test f1 to 83.1 (5.1 point absolute improvement)."
        },
        {
            "id": "R172408",
            "label": "A Survey on Recent Advances in Named Entity Recognition from Deep Learning models",
            "doi": "",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "abstract": "named entity recognition (ner) is a key component in nlp systems for question answering, information retrieval, relation extraction, etc. ner systems have been studied and developed widely for decades, but accurate systems using deep neural networks (nn) have only been introduced in the last few years. we present a comprehensive survey of deep neural network architectures for ner, and contrast them with previous approaches to ner based on feature engineering and other supervised or semi-supervised learning algorithms. our results highlight the improvements achieved by neural networks, and show how incorporating some of the lessons learned from past work on feature-based ner systems can yield further improvements."
        },
        {
            "id": "R172432",
            "label": "A unified architecture for natural language processing: Deep neural networks with multitask learning",
            "doi": "",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "abstract": "we describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. the entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. all the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. we show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance."
        },
        {
            "id": "R172500",
            "label": "Natural language processing (almost) from scratch",
            "doi": "",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "abstract": "we propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. this versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. this work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements."
        },
        {
            "id": "R172520",
            "label": "Farewell Freebase: Migrating the SimpleQuestions Dataset to DBpedia",
            "doi": "",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "abstract": "question answering over knowledge graphs is an important problem of interest both commercially and academically. there is substantial interest in the class of natural language questions that can be answered via the lookup of a single fact, driven by the availability of the popular simplequestions dataset. the problem with this dataset, however, is that answer triples are provided from freebase, which has been defunct for several years. as a result, it is difficult to build \u201creal-world\u201d question answering systems that are operationally deployable. furthermore, a defunct knowledge graph means that much of the infrastructure for querying, browsing, and manipulating triples no longer exists. to address this problem, we present simpledbpediaqa, a new benchmark dataset for simple question answering over knowledge graphs that was created by mapping simplequestions entities and predicates from freebase to dbpedia. although this mapping is conceptually straightforward, there are a number of nuances that make the task non-trivial, owing to the different conceptual organizations of the two knowledge graphs. to lay the foundation for future research using this dataset, we leverage recent work to provide simple yet strong baselines with and without neural networks."
        },
        {
            "id": "R172664",
            "label": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF",
            "doi": "10.18653/v1/p16-1101",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "abstract": "state-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. in this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional lstm, cnn and crf. our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. we evaluate our system on two data sets for two sequence labeling tasks --- penn treebank wsj corpus for part-of-speech (pos) tagging and conll 2003 corpus for named entity recognition (ner). we obtain state-of-the-art performance on both the two data --- 97.55\\\\% accuracy for pos tagging and 91.21\\\\% f1 for ner."
        },
        {
            "id": "R172672",
            "label": "Named Entity Recognition with Bidirectional LSTM-CNNs",
            "doi": "",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "abstract": "named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. in this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional lstm and cnn architecture, eliminating the need for most feature engineering. we also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the conll-2003 dataset and surpasses the previously reported state of the art performance on the ontonotes 5.0 dataset by 2.13 f1 points. by using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an f1 score of 91.62 on conll-2003 and 86.28 on ontonotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information."
        },
        {
            "id": "R175469",
            "label": "Extracting a Knowledge Base of Mechanisms from COVID-19 Papers",
            "doi": "10.18653/v1/2021.naacl-main.355",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "abstract": "the covid-19 pandemic has spawned a diverse body of scientific literature that is challenging to navigate, stimulating interest in automated tools to help find useful knowledge. we pursue the construction of a knowledge base (kb) of mechanisms\u2014a fundamental concept across the sciences, which encompasses activities, functions and causal relations, ranging from cellular processes to economic impacts. we extract this information from the natural language of scientific papers by developing a broad, unified schema that strikes a balance between relevance and breadth. we annotate a dataset of mechanisms with our schema and train a model to extract mechanism relations from papers. our experiments demonstrate the utility of our kb in supporting interdisciplinary scientific search over covid-19 literature, outperforming the prominent pubmed search in a study with clinical experts. our search engine, dataset and code are publicly available."
        },
        {
            "id": "R182418",
            "label": "SPECTER: Document-level Representation Learning using Citation-informed Transformers",
            "doi": "10.18653/v1/2020.acl-main.207",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "abstract": "representation learning is a critical ingredient for natural language processing systems. recent transformer language models like bert learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. for applications on scientific documents, such as classification and recommendation, accurate embeddings of documents are a necessity. we propose specter, a new method to generate document-level embedding of scientific papers based on pretraining a transformer language model on a powerful signal of document-level relatedness: the citation graph. unlike existing pretrained language models, specter can be easily applied to downstream applications without task-specific fine-tuning. additionally, to encourage further research on document-level models, we introduce scidocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. we show that specter outperforms a variety of competitive baselines on the benchmark."
        },
        {
            "id": "R184238",
            "label": "A Survey on Recent Advances in Named Entity Recognition from Deep Learning models",
            "doi": "",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "abstract": "named entity recognition (ner) is a key component in nlp systems for question answering, information retrieval, relation extraction, etc. ner systems have been studied and developed widely for decades, but accurate systems using deep neural networks (nn) have only been introduced in the last few years. we present a comprehensive survey of deep neural network architectures for ner, and contrast them with previous approaches to ner based on feature engineering and other supervised or semi-supervised learning algorithms. our results highlight the improvements achieved by neural networks, and show how incorporating some of the lessons learned from past work on feature-based ner systems can yield further improvements."
        },
        {
            "id": "R187500",
            "label": "Incorporating non-local information into information extraction systems by Gibbs sampling",
            "doi": "10.3115/1219840.1219885",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "abstract": "most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. we show how to solve this dilemma with gibbs sampling, a simple monte carlo method used to perform approximate inference in factored probabilistic models. by using simulated annealing in place of viterbi decoding in sequence models such as hmms, cmms, and crfs, it is possible to incorporate non-local structure while preserving tractable inference. we use this technique to augment an existing crf-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. this technique results in an error reduction of up to 9% over state-of-the-art systems on two established information extraction tasks."
        },
        {
            "id": "R189373",
            "label": "Falcon 2.0: An Entity and Relation Linking Tool over Wikidata",
            "doi": "10.1145/3340531.3412777",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "abstract": "the natural language processing (nlp) community has significantly contributed to the solutions for entity and relation recognition from a natural language text, and possibly linking them to proper matches in knowledge graphs (kgs). considering wikidata as the background kg, there are still limited tools to link knowledge within the text to wikidata. in this paper, we present falcon 2.0, the first joint entity and relation linking tool over wikidata. it receives a short natural language text in the english language and outputs a ranked list of entities and relations annotated with the proper candidates in wikidata. the candidates are represented by their internationalized resource identifier (iri) in wikidata. falcon 2.0 resorts to the english language model for the recognition task (e.g., n-gram tiling and n-gram splitting), and then an optimization approach for the linking task. we have empirically studied the performance of falcon 2.0 on wikidata and concluded that it outperforms all the existing baselines. falcon 2.0 is open source and can be reused by the community; all the required instructions of falcon 2.0 are well-documented at our github repository (https://github.com/sdm-tib/falcon2.0). we also demonstrate an online api, which can be run without any technical expertise. falcon 2.0 and its background knowledge bases are available as resources at https://labs.tib.eu/falcon/falcon2/."
        },
        {
            "id": "R189391",
            "label": "Deep Reinforcement Learning for Mention-Ranking Coreference Models",
            "doi": "10.18653/v1/d16-1245",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "abstract": "coreference resolution systems are typically trained with heuristic loss functions that require careful tuning. in this paper we instead apply reinforcement learning to directly optimize a neural mention-ranking model for coreference evaluation metrics. we experiment with two approaches: the reinforce policy gradient algorithm and a reward-rescaled max-margin objective. we find the latter to be more effective, resulting in significant improvements over the current state-of-the-art on the english and chinese portions of the conll 2012 shared task."
        },
        {
            "id": "R189450",
            "label": "FNG-IE: an improved graph-based method for keyword extraction from scholarly big-data",
            "doi": "10.7717/peerj-cs.389",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "abstract": "keyword extraction is essential in determining influenced keywords from huge documents as the research repositories are becoming massive in volume day by day. the research community is drowning in data and starving for information. the keywords are the words that describe the theme of the whole document in a precise way by consisting of just a few words. furthermore, many state-of-the-art approaches are available for keyword extraction from a huge collection of documents and are classified into three types, the statistical approaches, machine learning, and graph-based methods. the machine learning approaches require a large training dataset that needs to be developed manually by domain experts, which sometimes is difficult to produce while determining influenced keywords. however, this research focused on enhancing state-of-the-art graph-based methods to extract keywords when the training dataset is unavailable. this research first converted the handcrafted dataset, collected from impact factor journals into n -grams combinations, ranging from unigram to pentagram and also enhanced traditional graph-based approaches. the experiment was conducted on a handcrafted dataset, and all methods were applied on it. domain experts performed the user study to evaluate the results. the results were observed from every method and were evaluated with the user study using precision, recall and f-measure as evaluation matrices. the results showed that the proposed method (fng-ie) performed well and scored near the machine learning approaches score."
        },
        {
            "id": "R189458",
            "label": "AxCell: Automatic Extraction of Results from Machine Learning Papers",
            "doi": "10.18653/v1/2020.emnlp-main.692",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "abstract": "tracking progress in machine learning has become increasingly difficult with the recent explosion in the number of papers. in this paper, we present axcell, an automatic machine learning pipeline for extracting results from papers. axcell uses several novel components, including a table segmentation subtask, to learn relevant structural knowledge that aids extraction. when compared with existing methods, our approach significantly improves the state of the art for results extraction. we also release a structured, annotated dataset for training models for results extraction, and a dataset for evaluating the performance of models on this task. lastly, we show the viability of our approach enables it to be used for semi-automated results extraction in production, suggesting our improvements make this task practically viable for the first time. code is available on github."
        },
        {
            "id": "R191261",
            "label": "LinkBERT: Pretraining Language Models with Document Links",
            "doi": "10.18653/v1/2022.acl-long.551",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "abstract": "language model (lm) pretraining captures various knowledge from text corpora, helping downstream tasks. however, existing methods such as bert model a single document, and do not capture dependencies or knowledge that span across documents. in this work, we propose linkbert, an lm pretraining method that leverages links between documents, e.g., hyperlinks. given a text corpus, we view it as a graph of documents and create lm inputs by placing linked documents in the same context. we then pretrain the lm with two joint self-supervised objectives: masked language modeling and our new proposal, document relation prediction. we show that linkbert outperforms bert on various downstream tasks across two domains: the general domain (pretrained on wikipedia with hyperlinks) and biomedical domain (pretrained on pubmed with citation links). linkbert is especially effective for multi-hop reasoning and few-shot qa (+5% absolute improvement on hotpotqa and triviaqa), and our biomedical linkbert sets new states of the art on various bionlp tasks (+7% on bioasq and usmle). we release our pretrained models, linkbert and biolinkbert, as well as code and data."
        },
        {
            "id": "R193649",
            "label": "A Study of Neural Machine Translation from Chinese to Urdu",
            "doi": "10.32629/jai.v2i4.82",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "abstract": "&lt;div&gt;machine translation (mt) is used for giving a translation from a source language to a target language. machine translation simply translates text or speech from one language to another language, but this process is not sufficient to give the perfect translation of a text due to the requirement of identification of whole expressions and their direct counterparts. neural machine translation (nmt) is one of the most standard machine translation methods, which has made great progress in the recent years especially in non-universal languages. however, local language translation software for other foreign languages is limited and needs improving. in this paper, the chinese language is translated to the urdu language with the help of open neural machine translation (opennmt) in deep learning. firstly, a chinese&lt;/div&gt;&lt;div&gt;to urdu language sentences datasets were established and supported with seven million sentences. after that, these datasets were trained by using the open neural machine translation (opennmt) method. at the final stage, the translation was compared to the desired translation with the help of the bleu score method.&lt;/div&gt;"
        },
        {
            "id": "R193655",
            "label": "Research on Chinese-Urdu Machine Translation Based On Deep Learning",
            "doi": "10.32629/jai.v3i2.279",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "abstract": "\" urdu is pakistan 's national language. however, chinese expertise is very negligible in pakistan and the asian nations. yet fewer research has been undertaken in the area of computer translation on chinese to urdu. in order to solve the above problems, we designed of an electronic dictionary for chinese-urdu, and studied the sentence-level machine translation technology which is based on deep learning. the design of an electronic dictionary chinese-urdu machine translation system we collected and constructed an electronic dictionary containing 24000 entries from chinese to urdu. for sentence we used english as an intermediate language, and based on the existing parallel corpus of chinese to english and english to urdu, we constructed a bilingual parallel corpus containing 66000 sentences from chinese to urdu. the corpus has trained by using two nmt models (lstm,transformer model) and the above two translation model were compared to the desired translation, with the help of bilingual valuation understudy (bleu) score.\\xa0 on nmt, the lstm model is gain of 0.067 to 0.41 in bleu score while on transformer model, there is gain of 0.077 to 0.52 in bleu which is better than from lstm model score. furthermore, we compared the proposed model with google and microsoft translation. \""
        },
        {
            "id": "R193658",
            "label": "A Seq to Seq Machine Translation from Urdu to Chinese",
            "doi": "10.32629/jai.v4i1.359",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "abstract": "machine translation (mt) is a subtype of computational linguistics that uses to implement the translation between different natural languages (nl). simply word to word exchanging on machine translation is not enough to give desire result. neural machine translation is one of the standard methods of machine learning which make a huge improvement in recent time especially in local and some national languages. however these languages translation are not enough and need to focus on it. in this research we translate urdu to chinese language with the help of neural machine translation (nmt) in deep learning methods. first we build a monolingual corpus of urdu and chinese languages, after that we train our model using neural machine translation (nmt) and then compare the data-test result to accurate translation with the help of bleu score method."
        },
        {
            "id": "R203383",
            "label": "TDMSci: A Specialized Corpus for Scientific Literature Entity Tagging of Tasks Datasets and Metrics",
            "doi": "10.18653/v1/2021.eacl-main.59",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "abstract": "tasks, datasets and evaluation metrics are important concepts for understanding experimental scientific papers. however, previous work on information extraction for scientific literature mainly focuses on the abstracts only, and does not treat datasets as a separate type of entity (zadeh and schumann, 2016; luan et al., 2018). in this paper, we present a new corpus that contains domain expert annotations for task (t), dataset (d), metric (m) entities 2,000 sentences extracted from nlp papers. we report experiment results on tdm extraction using a simple data augmentation strategy and apply our tagger to around 30,000 nlp papers from the acl anthology. the corpus is made publicly available to the community for fostering research on scientific publication summarization (erera et al., 2019) and knowledge discovery."
        },
        {
            "id": "R141003",
            "label": "SemEval-2021 Task 5: Toxic Spans Detection",
            "doi": "10.18653/v1/2021.semeval-1.6",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "abstract": "the toxic spans detection task of semeval-2021 required participants to predict the spans of toxic posts that were responsible for the toxic label of the posts. the task could be addressed as supervised sequence labeling, using training data with gold toxic spans provided by the organisers. it could also be treated as rationale extraction, using classifiers trained on potentially larger external datasets of posts manually annotated as toxic or not, without toxic span annotations. for the supervised sequence labeling approach and evaluation purposes, posts previously labeled as toxic were crowd-annotated for toxic spans. participants submitted their predicted spans for a held-out test set and were scored using character-based f1. this overview summarises the work of the 36 teams that provided system descriptions."
        },
        {
            "id": "R141010",
            "label": "UnImplicit Shared Task Report: Detecting Clarification Requirements in Instructional Text",
            "doi": "10.18653/v1/2021.unimplicit-1.4",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "abstract": "this paper describes the data, task setup, and results of the shared task at the first workshop on understanding implicit and underspecified language (unimplicit). the task requires computational models to predict whether a sentence contains aspects of meaning that are contextually unspecified and thus require clarification. two teams participated and the best scoring system achieved an accuracy of 68%."
        },
        {
            "id": "R141018",
            "label": "RDoC Task at BioNLP-OST 2019",
            "doi": "10.18653/v1/d19-5729",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "abstract": "bionlp open shared tasks (bionlp-ost) is an international competition organized to facilitate development and sharing of computational tasks of biomedical text mining and solutions to them. for bionlp-ost 2019, we introduced a new mental health informatics task called \u201crdoc task\u201d, which is composed of two subtasks: information retrieval and sentence extraction through national institutes of mental health\u2019s research domain criteria framework. five and four teams around the world participated in the two tasks, respectively. according to the performance on the two tasks, we observe that there is room for improvement for text mining on brain research and mental illness."
        },
        {
            "id": "R141066",
            "label": "CLPsych 2018 Shared Task: Predicting Current and Future\n            Psychological Health from Childhood Essays",
            "doi": "10.18653/v1/w18-0604",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "abstract": "we describe the shared task for the clpsych 2018 workshop, which focused on predicting current and future psychological health from an essay authored in childhood. language-based predictions of a person\u2019s current health have the potential to supplement traditional psychological assessment such as questionnaires, improving intake risk measurement and monitoring. predictions of future psychological health can aid with both early detection and the development of preventative care. research into the mental health trajectory of people, beginning from their childhood, has thus far been an area of little work within the nlp community. this shared task represents one of the first attempts to evaluate the use of early language to predict future health; this has the potential to support a wide variety of clinical health care tasks, from early assessment of lifetime risk for mental health problems, to optimal timing for targeted interventions aimed at both prevention and treatment."
        },
        {
            "id": "R141070",
            "label": "Named Entity Recognition on Code-Switched Data: Overview of the CALCS 2018 Shared Task",
            "doi": "10.18653/v1/w18-3219",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "abstract": "in the third shared task of the computational approaches to linguistic code-switching (calcs) workshop, we focus on named entity recognition (ner) on code-switched social-media data. we divide the shared task into two competitions based on the english-spanish (eng-spa) and modern standard arabic-egyptian (msa-egy) language pairs. we use twitter data and 9 entity types to establish a new dataset for code-switched ner benchmarks. in addition to the cs phenomenon, the diversity of the entities and the social media challenges make the task considerably hard to process. as a result, the best scores of the competitions are 63.76% and 71.61% for eng-spa and msa-egy, respectively. we present the scores of 9 participants and discuss the most common challenges among submissions."
        },
        {
            "id": "R141092",
            "label": "DSTC7 Task 1: Noetic End-to-End Response Selection",
            "doi": "10.18653/v1/w19-4107",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "abstract": "goal-oriented dialogue in complex domains is an extremely challenging problem and there are relatively few datasets. this task provided two new resources that presented different challenges: one was focused but small, while the other was large but diverse. we also considered several new variations on the next utterance selection problem: (1) increasing the number of candidates, (2) including paraphrases, and (3) not including a correct option in the candidate set. twenty teams participated, developing a range of neural network models, including some that successfully incorporated external data to boost performance. both datasets have been publicly released, enabling future work to build on these results, working towards robust goal-oriented dialogue systems."
        },
        {
            "id": "R142108",
            "label": "SemEval-2013 Task 7: The Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge",
            "doi": "",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "abstract": "we present the results of the joint student response analysis and 8th recognizing textual entailment challenge, aiming to bring together researchers in educational nlp technology and textual entailment. the task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment. thus, we offered to the community a 5-way student response labeling task, as well as 3-way and 2way rte-style tasks on educational data. in addition, a partial entailment task was piloted. we present and compare results from 9 participating teams, and discuss future directions."
        },
        {
            "id": "R145757",
            "label": "SemEval-2018 Task 7: Semantic Relation Extraction and Classification in Scientific Papers",
            "doi": "10.18653/v1/s18-1111",
            "research_field": {
                "id": "R145261",
                "label": "Natural Language Processing"
            },
            "abstract": "this paper describes the first task on semantic relation extraction and classification in scientific paper abstracts at semeval 2018. the challenge focuses on domain-specific semantic relations and includes three different subtasks. the subtasks were designed so as to compare and quantify the effect of different pre-processing steps on the relation classification results. we expect the task to be relevant for a broad range of researchers working on extracting specialized knowledge from domain corpora, for example but not limited to scientific or bio-medical information extraction. the task attracted a total of 32 participants, with 158 submissions across different scenarios."
        },
        {
            "id": "R108821",
            "label": "A broadband plasma radiation detector with spatial resolution based on the optical scanning of the fluorescence of a phosphor",
            "doi": "10.1063/1.1142318",
            "research_field": {
                "id": "R175",
                "label": "Atomic, Molecular and Optical Physics"
            },
            "abstract": "a detector which converts the line\u2010integrated plasma radiation profile to visible light within a selected spectral range, by means of a film of sodium salicylate, is presented. the phosphor fluorescent emission is spatially scanned by a rapidly vibrating mirror and detected by a filtered photomultiplier, allowing one to measure the time evolution of plasma radiation profiles in real time. a detailed description of the detector, calibration method, and its performances in the tj\u2010i tokamak are included."
        },
        {
            "id": "R108823",
            "label": "Absolute intensities of the vacuum ultraviolet spectra in a metal-etch plasma processing discharge",
            "doi": "10.1116/1.582044",
            "research_field": {
                "id": "R175",
                "label": "Atomic, Molecular and Optical Physics"
            },
            "abstract": "in this paper we report absolute intensities of vacuum ultraviolet and near ultraviolet emission lines (4.8 ev to 18 ev ) for aluminum etching discharges in an inductively coupled plasma reactor. we report line intensities as a function of wafer type, pressure, gas mixture and rf excitation level. iri a standard aluminum etching mixture containing c12 and bc13 almost all the light emitted at energies exceeding 8.8 ev was due to neutral atomic chlorine. optical trapping of the wv radiation in the discharge complicates calculations of vuv fluxes to the wafer. however, we see total photon fluxes to the wailer at energies above 8.8 ev on the order of 4 x 1014 photons/cm2sec with anon- reactive wafer and 0.7 x 10 `4 photons/cm2sec with a reactive wtier. the maj ority of the radiation observed was between 8.9 and 9.3 ev. at these energies, the photons have enough energy to create electron-hole pairs in si02, but may penetrate up to a micron into the si02 before being absorbed. relevance of these measurements to vacuum-w photon-induced darnage of si02 during etching is discussed."
        },
        {
            "id": "R108934",
            "label": "Detector system with high time resolution for the continuous measurement of spectra in the vacuum ultraviolet wavelength range",
            "doi": "10.1063/1.1763261",
            "research_field": {
                "id": "R175",
                "label": "Atomic, Molecular and Optical Physics"
            },
            "abstract": "a new detector system with high time resolution (1 ms) has been developed and applied for the continuous measurement of spectra in the vacuum ultraviolet (vuv) and extreme ultraviolet (euv) wavelength region at the fusion plasma experiment torus experiment for technology-oriented research (textor). the system consists of an open multichannel-plate (mcp) detector with subsequent first generation (gen i) light amplifier and a camera head which is based on a linear photodiode array with 1024 elements (pixels). the camera head provides the output signals of the individual pixels sequentially as an analog voltage with a full spectra rate of 1000 per second, which are measured using a pc-based data acquisition system. three vacuum spectrometers operating in the vuv/euv region (10\u2013130 nm) have been equipped with the new system and a successful campaign of measurements from about 4000 discharges at textor has been performed. spectra are recorded with a usable linear dynamic range of 10 bit and a wavelength resolution corresponding to a width of 3\u20134 pixels.a new detector system with high time resolution (1 ms) has been developed and applied for the continuous measurement of spectra in the vacuum ultraviolet (vuv) and extreme ultraviolet (euv) wavelength region at the fusion plasma experiment torus experiment for technology-oriented research (textor). the system consists of an open multichannel-plate (mcp) detector with subsequent first generation (gen i) light amplifier and a camera head which is based on a linear photodiode array with 1024 elements (pixels). the camera head provides the output signals of the individual pixels sequentially as an analog voltage with a full spectra rate of 1000 per second, which are measured using a pc-based data acquisition system. three vacuum spectrometers operating in the vuv/euv region (10\u2013130 nm) have been equipped with the new system and a successful campaign of measurements from about 4000 discharges at textor has been performed. spectra are recorded with a usable linear dynamic range of 10 bit and a wavelength resolu..."
        },
        {
            "id": "R108936",
            "label": "Absolute vacuum ultraviolet flux in inductively coupled plasmas and chemical modifications of 193 nm photoresist",
            "doi": "10.1063/1.3125260",
            "research_field": {
                "id": "R175",
                "label": "Atomic, Molecular and Optical Physics"
            },
            "abstract": "vacuum ultraviolet (vuv) photons in plasma processing systems are known to alter surface chemistry and may damage gate dielectrics and photoresist. we characterize absolute vuv fluxes to surfaces exposed in an inductively coupled argon plasma, 1\u201350 mtorr, 25\u2013400 w, using a calibrated vuv spectrometer. we also demonstrate an alternative method to estimate vuv fluence in an inductively coupled plasma (icp) reactor using a chemical dosimeter-type monitor. we illustrate the technique with argon icp and xenon lamp exposure experiments, comparing direct vuv measurements with measured chemical changes in 193 nm photoresist-covered si wafers following vuv exposure."
        },
        {
            "id": "R108938",
            "label": "Prediction of UV spectra and UV-radiation damage in actual plasma etching processes using on-wafer monitoring technique",
            "doi": "10.1063/1.3313924",
            "research_field": {
                "id": "R175",
                "label": "Atomic, Molecular and Optical Physics"
            },
            "abstract": "uv radiation during plasma processing affects the surface of materials. nevertheless, the interaction of uv photons with surface is not clearly understood because of the difficulty in monitoring photons during plasma processing. for this purpose, we have previously proposed an on-wafer monitoring technique for uv photons. for this study, using the combination of this on-wafer monitoring technique and a neural network, we established a relationship between the data obtained from the on-wafer monitoring technique and uv spectra. also, we obtained absolute intensities of uv radiation by calibrating arbitrary units of uv intensity with a 126 nm excimer lamp. as a result, uv spectra and their absolute intensities could be predicted with the on-wafer monitoring. furthermore, we developed a prediction system with the on-wafer monitoring technique to simulate uv-radiation damage in dielectric films during plasma etching. uv-induced damage in sioc films was predicted in this study. our prediction results of damage..."
        },
        {
            "id": "R108940",
            "label": "Vacuum ultraviolet emission from microwave Ar-H2 plasmas",
            "doi": "10.1063/1.4796134",
            "research_field": {
                "id": "R175",
                "label": "Atomic, Molecular and Optical Physics"
            },
            "abstract": "vacuum ultraviolet emission from ar-h2 wave driven microwave (2.45\\u2009ghz) plasmas operating at low pressures (0.1\u20131\\u2009mbar) has been investigated. the emitted spectra show the presence of the ar resonance lines at 104.8 and 106.7\\u2009nm and of the lyman-\u03b1,\u03b2 atomic lines at 121.6\\u2009nm and 102.6\\u2009nm, respectively. the increase of the hydrogen amount in the mixture results in an abrupt increase of the werner and lyman molecular bands intensity. the lyman-\u03b2 intensity shows little changes in the range of 5%\u201330% of hydrogen in the mixture while the lyman-\u03b1 intensity tends to decrease as the percentage of hydrogen increases."
        },
        {
            "id": "R108942",
            "label": "Comparison of surface vacuum ultraviolet emissions with resonance level number densities. I. Argon plasmas",
            "doi": "10.1116/1.4859376",
            "research_field": {
                "id": "R175",
                "label": "Atomic, Molecular and Optical Physics"
            },
            "abstract": "vacuum ultraviolet (vuv) photons emitted from excited atomic states are ubiquitous in material processing plasmas. the highly energetic photons can induce surface damage by driving surface reactions, disordering surface regions, and affecting bonds in the bulk material. in argon plasmas, the vuv emissions are due to the decay of the 1s4 and 1s2 principal resonance levels with emission wavelengths of 104.8 and 106.7\\u2009nm, respectively. the authors have measured the number densities of atoms in the two resonance levels using both white light optical absorption spectroscopy and radiation-trapping induced changes in the 3p54p\u21923p54s branching fractions measured via visible/near-infrared optical emission spectroscopy in an argon inductively coupled plasma as a function of both pressure and power. an emission model that takes into account radiation trapping was used to calculate the vuv emission rate. the model results were compared to experimental measurements made with a national institute of standards and techn..."
        },
        {
            "id": "R108944",
            "label": "Comparison of surface vacuum ultraviolet emissions with resonance level number densities. II. Rare-gas plasmas and Ar-molecular gas mixtures",
            "doi": "10.1116/1.4904036",
            "research_field": {
                "id": "R175",
                "label": "Atomic, Molecular and Optical Physics"
            },
            "abstract": "vacuum ultraviolet (vuv) emissions from excited plasma species can play a variety of roles in processing plasmas, including damaging the surface properties of materials used in semiconductor processing. depending on their wavelength, vuv photons can easily transmit thin upper dielectric layers and affect the electrical characteristics of the devices. despite their importance, measuring vuv fluxes is complicated by the fact that few materials transmit at vuv wavelengths, and both detectors and windows are easily damaged by plasma exposure. the authors have previously reported on measuring vuv fluxes in pure argon plasmas by monitoring the concentrations of ar(3p54s) resonance atoms that produce the vuv emissions using noninvasive optical emission spectroscopy in the visible/near-infrared wavelength range [boffard et al., j. vac. sci. technol., a 32, 021304 (2014)]. here, the authors extend this technique to other rare-gases (ne, kr, and xe) and argon-molecular gas plasmas (ar/h2, ar/o2, and ar/n2). results..."
        },
        {
            "id": "R108946",
            "label": "Quantification of the VUV radiation in low pressure hydrogen and nitrogen plasmas",
            "doi": "10.1088/0963-0252/25/4/045006",
            "research_field": {
                "id": "R175",
                "label": "Atomic, Molecular and Optical Physics"
            },
            "abstract": "hydrogen and nitrogen containing discharges emit intense radiation in a broad wavelength region in the vuv. the measured radiant power of individual molecular transitions and atomic lines between 117\\u2009nm and 280\\u2009nm are compared to those obtained in the visible spectral range and moreover to the rf power supplied to the icp discharge. in hydrogen plasmas driven at 540\\u2009w of rf power up to 110\\u2009w are radiated in the vuv, whereas less than 2\\u2009w is emitted in the vis. in nitrogen plasmas the power level of about 25\\u2009w is emitted both in the vuv and in the vis. in hydrogen\u2013nitrogen mixtures, the nh radiation increases the vuv amount. the analysis of molecular and atomic hydrogen emission supported by a collisional radiative model allowed determining plasma parameters and particle densities and thus particle fluxes. a comparison of the fluxes showed that the photon fluxes determined from the measured emission are similar to the ion fluxes, whereas the atomic hydrogen fluxes are by far dominant. photon fluxes up to 5\\u2009\\u2009\u00d7\\u2009\\u20091020 m\u22122 s\u22121 are obtained, demonstrating that the vuv radiation should not be neglected in surface modifications processes, whereas the radiant power converted to vuv photons is to be considered in power balances. varying the admixture of nitrogen to hydrogen offers a possibility to tune photon fluxes in the respective wavelength intervals."
        },
        {
            "id": "R108948",
            "label": "A microwave plasma source for VUV atmospheric photochemistry",
            "doi": "10.1088/0022-3727/49/39/395202",
            "research_field": {
                "id": "R175",
                "label": "Atomic, Molecular and Optical Physics"
            },
            "abstract": "microwave plasma discharges working at low pressure are nowadays a well-developed technique mainly used to provide radiations at different wavelengths. the aim of this work is to show that those discharges are an efficient windowless vuv photon source for planetary atmospheric photochemistry experiments. to do this, we use a surfatron-type discharge with a neon gas flow in the mbar pressure range coupled to a photochemical reactor. working in the vuv range allows to focus on nitrogen-dominated atmospheres ({\\\\lambda}<100nm). the experimental setup makes sure that no other energy sources (electrons, metastable atoms) than the vuv photons interact with the reactive medium. neon owns two resonance lines at 73.6 and 74.3 nm which behave differently regarding the pressure or power conditions. in parallel, the vuv photon flux emitted at 73.6 nm has been experimentally estimated in different conditions of pressure and power and varies in a large range between 2x1013 this http url-2 and 4x1014 this http url-2 which is comparable to a vuv synchrotron photon flux. our first case study is the atmosphere of titan and its n2-ch4 atmosphere. with this vuv source, the production of hcn and c2n2, two major titan compounds, is detected, ensuring the suitability of the source for atmospheric photochemistry experiments."
        },
        {
            "id": "R108950",
            "label": "In situ measurement of VUV/UV radiation from low-pressure microwave-produced plasma in Ar/O2 gas mixtures",
            "doi": "10.1088/1361-6501/aa7816",
            "research_field": {
                "id": "R175",
                "label": "Atomic, Molecular and Optical Physics"
            },
            "abstract": "ultraviolet (uv) and vacuum ultraviolet (vuv) spectral irradiance is determined in low-pressure microwave-produced plasma, which is regularly used for polymer surface treatment. the re-emitted fluorescence in the uv/vis spectral range from a sodium salicylate layer is measured. this fluorescence is related to vuv/uv radiation in different spectral bands based on cut-off filters. the background produced by direct emitted radiation in the fluorescence spectral region is quantified using a specific background filter, thus enabling the use of the whole fluorescence spectral range. a novel procedure is applied to determine the absolute value of the vuv/uv irradiance on a substrate. for that, an independent measurement of the absolute spectral emissivity of the plasma in the uv is performed. the measured irradiances on a substrate from a 25 pa ar/o2-produced plasma are in the range of 1015\u20131016 (photon s\u22121cm\u22122). these values include the contribution from impurities present in the discharge."
        },
        {
            "id": "R108952",
            "label": "Vacuum ultraviolet radiation emitted by microwave driven argon plasmas",
            "doi": "10.1063/1.4981535",
            "research_field": {
                "id": "R175",
                "label": "Atomic, Molecular and Optical Physics"
            },
            "abstract": "\"vacuum ultraviolet (vuv) radiation emitted by microwave driven argon plasmas has been investigated at low-pressure conditions (0.36 mbar). a classical surface-wave sustained discharge at 2.45\\u2009ghz has been used as plasma source. vuv radiation has been detected by emission spectroscopy in the 30\u2013125\\u2009nm spectral range. the spectrum exhibits atomic and ionic argon emissions with the most intense spectral lines corresponding to the atomic resonance lines, at 104.8\\u2009nm and 106.7\\u2009nm, and to the ion lines, at 92.0\\u2009nm and 93.2\\u2009nm. emissions at lower wavelengths were also detected, including lines with no information concerning level transitions in the well-known nist database (e.g., the atomic line at 89.4\\u2009nm). the dependence of the lines' intensity on the microwave power delivered to the launcher was investigated. the electron density was estimated to be around 1012\\u2009cm\u22123 using the stark broadening of the hydrogen h\u03b2 line at 486.1\\u2009nm. the main population and loss mechanisms considered in the model for the excited a...\""
        },
        {
            "id": "R108954",
            "label": "Ultraviolet/vacuum-ultraviolet emission from a high power magnetron sputtering plasma with an aluminum target",
            "doi": "10.1088/1361-6463/ab52f8",
            "research_field": {
                "id": "R175",
                "label": "Atomic, Molecular and Optical Physics"
            },
            "abstract": "we report the in situ measurement of the ultraviolet/vacuum-ultraviolet (uv/vuv) emission from a plasma produced by high power impulse magnetron sputtering with aluminum target, using argon as background gas. the uv/vuv detection system is based upon the quantification of the re-emitted fluorescence from a sodium salicylate layer that is placed in a housing inside the vacuum chamber, at 11\\u2009cm from the center of the cathode. the detector is equipped with filters that allow for differentiating various spectral regions, and with a front collimating tube that provides a spatial resolution\\u2009\\u2009\u2248\\u2009\\u20090.5\\u2009cm. using various views of the plasma, the measured absolutely calibrated photon rates enable to calculate emissivities and irradiances based on a model of the ionization region. we present results that demonstrate that al+ ions are responsible for most of the vuv irradiance. we also discuss the photoelectric emission due to irradiances on the target produced by high energy photons from resonance lines of ar+."
        },
        {
            "id": "R108956",
            "label": "VUV radiation flux from argon DC magnetron plasma",
            "doi": "10.1088/1361-6463/ab813f",
            "research_field": {
                "id": "R175",
                "label": "Atomic, Molecular and Optical Physics"
            },
            "abstract": "vacuum ultraviolet (vuv) flux of argon plasma radiation in a dc magnetron discharge with a plane circular titanium cathode is measured. it is found that the intensity of vuv radiation, mainly indicated by the resonance lines of argon atoms at 104.8 and 106.7 nm and ions at 92 and 93.2 nm, is proportional to the discharge current and decreases with pressure. following the results of the measurements, a numerical model of resonance radiation transport is developed to determine the vuv flux to the substrate placed near the sputtering cathode where direct measurements are impossible due to the fast contamination of the detector by sputtered atoms. in the case of a substrate located 10 cm opposite the cathode surface, the upper limit of estimated vuv flux is of the order of 1015 photons\\u2009cm\u22122\\u2009s\u22121 at a coating deposition rate of 1.5 nm\\u2009s\u22121 for 2 and 12 mtorr gas pressures. based on the measurements, the damage to a porous low-k dielectric by vuv radiation during the deposition of barrier layers in the dc magnetron discharge is first estimated."
        },
        {
            "id": "R189876",
            "label": "Precision Wavelength Determination of 2^1P_1 - 1^1S_0 and 2^3P_1 - 1^1S_0 Transitions in Helium-Like Sulfur Ions",
            "doi": "10.1088/0031-8949/25/6b/004",
            "research_field": {
                "id": "R114010",
                "label": "Atomic Physics"
            },
            "abstract": "transitions from the 21p1 - and 23p1 -state to the ground state 11s0 in helium-like sulphur ions have been measured with an accuracy of 4 \u00d7 10-5. energy calibration is described in detail and two reference wavelengths have been reevaluated. substantial line-blending was observed, due to long-lived spectator electrons. the two transition energies were corrected for doppler shift and compared with most refined theoretical calculations, including terms of order \u03b14z6 in the breit operator and terms of order \u03b15z6 in the quantum-electrodynamical corrections. the experimental contributions to the ground-state qed shifts agree within its error (\u223c 15%) with the theoretical values."
        },
        {
            "id": "R189910",
            "label": "Spectroscopy of hydrogenlike and heliumlike argon",
            "doi": "10.1103/physreva.28.1413",
            "research_field": {
                "id": "R114010",
                "label": "Atomic Physics"
            },
            "abstract": "the x-ray transitions ($n=2\\\\ensuremath{\\\\rightarrow}n=1$) emitted by fast hydrogenlike and heliumlike argon ions have been studied. the absolute energy of the lyman $\\\\ensuremath{\\\\alpha}$ lines of hydrogenlike ions has been measured and the value of the ($1s$) lamb shift of argon evaluated for the first time. the $^{3}p_{1,2}$ and $^{1}p_{1}$ transitions of heliumlike argon have also been studied at very high precision and their energies compared to multiconfiguration dirac-fock calculations. the energies of lyman $\\\\ensuremath{\\\\alpha}$ lines are of 3323.2\\\\ifmmode\\\\pm\\\\else\\\\textpm\\\\fi{}0.5 ev ($\\\\mathrm{ly}{\\\\ensuremath{\\\\alpha}}_{1}$) and 3318.1\\\\ifmmode\\\\pm\\\\else\\\\textpm\\\\fi{}0.5 ev ($\\\\mathrm{ly}{\\\\ensuremath{\\\\alpha}}_{2}$), and those of $n=2\\\\ensuremath{\\\\rightarrow}n=1$ transitions for heliumlike argon are of 3123.6\\\\ifmmode\\\\pm\\\\else\\\\textpm\\\\fi{}0.25 ev ($^{3}p_{1}$), 3126.4\\\\ifmmode\\\\pm\\\\else\\\\textpm\\\\fi{}0.4 ev ($^{3}p_{2}$), and 3139.6\\\\ifmmode\\\\pm\\\\else\\\\textpm\\\\fi{}0.25 ev ($^{1}p_{1}$)."
        },
        {
            "id": "R190068",
            "label": "Precision X-ray wavelength measurements in helium-like argon recoil ions",
            "doi": "10.1088/0022-3700/17/21/001",
            "research_field": {
                "id": "R114010",
                "label": "Atomic Physics"
            },
            "abstract": "\"the authors report precise wavelength measurements of the 1s2 1s0-1s2p3p1,2,1p1 transitions in ar16+ produced by collisions of 5.9 mev amu-1 u66+ ions with an argon gas target. by use of this 'recoil source', the precision is not limited by doppler shifts while the influence of spectator electrons is minimised by observation of their relative importance as a function of gas pressure. the accuracy obtained is at the 12 p.p.m. level dominated by the x-ray calibration standard. the measurement is thus sensitive to quantum-electrodynamic (qed) and electron correlation effects.\""
        },
        {
            "id": "R190165",
            "label": "Precision measurement of the K_\u03b1 transitions in heliumlike Ge^30+",
            "doi": "10.1103/physreva.45.329",
            "research_field": {
                "id": "R114010",
                "label": "Atomic Physics"
            },
            "abstract": "a measurement of the 1{ital s}2{ital p} {sup 1}{ital p}{sub 1}{r arrow}1{ital s}{sup 2} {ital s}{sub 0} resonance transition in heliumlike germanium (ge{sup 30+}) has been made on the lawrence livermore national laboratory electron-beam ion trap to a precision of 21 ppm. the result is compared with theoretical values and confirms a trend previously seen in the differences between experiment and theory for this transition as a function of {ital z}. results for the 1{ital s}2{ital p} {sup 3}{ital p}{sub 1}{r arrow}1{ital s}{sup 2} {ital s}{sub 0} and 1{ital s}2{ital p} {sup 3}{ital p}{sub 2}{r arrow}1{ital s}{sup 2} {ital s}{sub 0} intercombination lines and for the 1{ital s}2{ital s} {sup 3}{ital s}{sub 1}{r arrow}1{ital s}{sup 2} {sup 1}{ital s}{sub 0} forbidden line are also presented and show similar differences with theoretical predictions."
        },
        {
            "id": "R190520",
            "label": "High Precision Spectroscopic Studies of Few Electron Ions",
            "doi": "10.1063/1.2948737",
            "research_field": {
                "id": "R114010",
                "label": "Atomic Physics"
            },
            "abstract": "in this report some experiments are described in which the lyman ..cap alpha.. lines of hydrogen-like and helium-like argon and iron ions have been measured. the principle of all these experiments was to study with a crystal spectrometer the x rays emitted in flight by the ions, at 90/sup 0/ with respect to the direction of the beam. 5 references, 14 figures, 9 tables."
        },
        {
            "id": "R190553",
            "label": "High-precision spectroscopic study of heliumlike iron",
            "doi": "10.1103/physreva.29.3143",
            "research_field": {
                "id": "R114010",
                "label": "Atomic Physics"
            },
            "abstract": "the x-ray spectrum emitted by high-velocity heliumlike iron ions has been studied with a crystal spectrometer. the absolute energies of the n = 2..-->..n = 1 lines have been measured with a precision of 40 ppm. a very good agreement has been found between our experimental values and a very accurate multiconfiguration dirac-fock calculation. the precision of the measurement and the calculation of the energies are such that for the first time the magnetic correlation energy (spin-spin) as well as the screening of quantum-electrodynamic effects can now be appreciated. the contamination of the considered lines by the so-called dielectronic recombination satellites has been studied in great detail by varying the nature and the thickness of the targets."
        },
        {
            "id": "R138527",
            "label": "Sensitivity analysis applied to a variational data assimilation of a simulated pollution transport problem: SENSITIVITY ANALYSIS IN VARIATIONAL DATA ASSIMILATION PROBLEM",
            "doi": "10.1002/fld.4274",
            "research_field": {
                "id": "R109",
                "label": "Control Theory"
            },
            "abstract": "understanding the impact of the changes in pollutant emission from a foreign region onto a target region is a key factor for taking appropriate mitigating actions. this requires a sensitivity analysis of a response function (defined on the target region) with respect to the source(s) of pollutant(s). the basic and straightforward approach to sensitivity analysis consists of multiple simulations of the pollution transport model with variations of the parameters that define the source of the pollutant. a more systematic approach uses the adjoint of the pollution transport model derived from applying the principle of variations. both approaches assume that the transport velocity and the initial distribution of the pollutant are known. however, when observations of both the velocity and concentration fields are available, the transport velocity and the initial distribution of the pollutant are given by the solution of a data assimilation problem. as a consequence, the sensitivity analysis should be carried out on the optimality system of the data assimilation problem, and not on the direct model alone. this leads to a sensitivity analysis that involves the second\u2010order adjoint model, which is presented in the present work. it is especially shown theoretically and with numerical experiments that the sensitivity on the optimality system includes important terms that are ignored by the sensitivity on the direct model. the latter shows only the direct effects of the variation of the source on the response function while the first shows the indirect effects in addition to the direct effects. copyright \u00a9 2016 john wiley & sons, ltd."
        },
        {
            "id": "R187658",
            "label": "An online convex optimization algorithm for controlling linear systems with state and input constraints",
            "doi": "10.23919/acc50511.2021.9482877",
            "research_field": {
                "id": "R109",
                "label": "Control Theory"
            },
            "abstract": "\"this paper studies the problem of controlling linear dynamical systems subject to point-wise-in-time constraints. we present an algorithm similar to online gradient descent, that can handle time-varying and a priori unknown convex cost functions while restraining the system states and inputs to polytopic constraint sets. analysis of the algorithm's performance, measured by dynamic regret, reveals that sub-linear regret is achieved if the variation of the cost functions is sublinear in time. finally, we present an example to illustrate implementation details as well as the algorithm's performance and show that the proposed algorithm ensures constraint satisfaction.\""
        },
        {
            "id": "R187671",
            "label": "Data-driven online convex optimization for control of dynamical systems",
            "doi": "10.1109/cdc45484.2021.9683550",
            "research_field": {
                "id": "R109",
                "label": "Control Theory"
            },
            "abstract": "we propose a data-driven online convex optimization algorithm for controlling dynamical systems. in particular, the control scheme makes use of an initially measured input-output trajectory and behavioral systems theory which enable it to handle unknown discrete-time linear time-invariant systems as well as a priori unknown time-varying cost functions. further, only output feedback instead of full state measurements is required for the proposed approach. analysis of the closed loop\u2019s performance reveals that the algorithm achieves sublinear regret if the variation of the cost functions is sublinear. the effectiveness of the proposed algorithm, even in the case of noisy measurements, is illustrated by a simulation example."
        },
        {
            "id": "R187843",
            "label": "Online Optimization as a Feedback Controller: Stability and Tracking",
            "doi": "10.1109/tcns.2019.2906916",
            "research_field": {
                "id": "R109",
                "label": "Control Theory"
            },
            "abstract": "this paper develops and analyzes feedback-based online optimization methods to regulate the output of a linear time invariant (lti) dynamical system to the optimal solution of a time-varying convex optimization problem. the design of the algorithm is based on continuous-time primal-dual dynamics, properly modified to incorporate feedback from the lti dynamical system, applied to a proximal augmented lagrangian function. the resultant closed-loop algorithm tracks the solution of the time-varying optimization problem without requiring knowledge of (time varying) disturbances in the dynamical system. the analysis leverages integral quadratic constraints to provide linear matrix inequality (lmi) conditions that guarantee global exponential stability and bounded tracking error. analytical results show that under a sufficient time-scale separation between the dynamics of the lti dynamical system and the algorithm, the lmi conditions can be always satisfied. this paper further proposes a modified algorithm that can track an approximate solution trajectory of the constrained optimization problem under less restrictive assumptions. as an illustrative example, the proposed algorithms are showcased for power transmission systems, to compress the time scales between secondary and tertiary control, and allow to simultaneously power rebalancing and tracking of the dc optimal power flow points."
        },
        {
            "id": "R38662",
            "label": "Physical chemistry of the groups IVA (Ti, Zr), VA (V, Nb, Ta) and rare earth elements in steel",
            "doi": "10.2355/isijinternational1966.15.145",
            "research_field": {
                "id": "R254",
                "label": "Materials Science and Engineering"
            },
            "abstract": "\"vanadium, niobium and tantalum that belong to the grouj) va in the j)eriodic table have strong affinity .lor nitrogen and carbon in steel. besides, these elements differ from aluminium, and titanium and zirconium in the group iva in deoxidizing power, and are not oxidized remarkably in liquid steel . these, therefore, are vely interesting as allqying-elemen/s in steelmaking process. on the other hand, the rare earths are very' important elements as a good scavenger 0.1 oxygen and sulphur in liqu id steelfrom their chemical activities. in this paper, (1) chemical reactions between each 0.1 the iva elements, the va elements, the rare earths and aluminium and each 0.1 oxygen, nitrogen, carbon and sulphur, respectively, both in liquid and solid steels, and (2) roles and influences 0.1 these elements on steelmaking process and on the quality and property ~r steel, are discussed from a viewpoint 0.1 physical chemistly\u00b7\""
        },
        {
            "id": "R38668",
            "label": "Effect of Distribution of TiN Precipitate Particles on the Austenite Grain Size of Low Carbon Low Alloy Steels",
            "doi": "10.2355/isijinternational1966.18.198",
            "research_field": {
                "id": "R254",
                "label": "Materials Science and Engineering"
            },
            "abstract": "i t is wcl l known that the toughness of steel is improved by the a ustenite grain r efinement. there a re many ways to obtain a fin e grain size . a typical example is to use fine aln or nb(cn) precipitate.1 ,2) gladman3 ) h as developed a genera l theory on the mechanism controlling the austenite grain size in which he correlated the critical stage of grain coarsening with the size a nd the vo lume frac tion of precipitates . there are many expel\"imenta l evidences concerning the austenite grain refin ement using carbides and nitrid es. george and ira ni ) determined grain coarsening temperature of th e ti bearing steels as about i 200\u00b0c, which is higher by loo\u00b0c tha n th at of the cases of other precipitate particles. sim il a r experimental resu lts on ti containing steels were obtained by bashford and george. 5 ) i t has been suggested from these works that the size and the volume fraction of tin a re the major factors which control the austenite grain coarsening at high tempel\"atu res , but no observation has been offered. the present work is to study the dissolution , coalescence a nd precipita tion of tin in the austenite temperature range with special reference to the austenite grain size. a simple model is proposed on the m echa nism of controlling the austenite g rain size by precipitate pa rti cles."
        },
        {
            "id": "R189410",
            "label": "Graphene Oxide Papers Modified by Divalent Ions\u2014Enhancing Mechanical Properties <i>via</i> Chemical Cross-Linking",
            "doi": "10.1021/nn700349a",
            "research_field": {
                "id": "R254",
                "label": "Materials Science and Engineering"
            },
            "abstract": "significant enhancement in mechanical stiffness (10-200%) and fracture strength (approximately 50%) of graphene oxide paper, a novel paperlike material made from individual graphene oxide sheets, can be achieved upon modification with a small amount (less than 1 wt %) of mg(2+) and ca(2+). these results can be readily rationalized in terms of the chemical interactions between the functional groups of the graphene oxide sheets and the divalent metals ions. while oxygen functional groups on the basal planes of the sheets and the carboxylate groups on the edges can both bond to mg(2+) and ca(2+), the main contribution to mechanical enhancement of the paper comes from the latter."
        },
        {
            "id": "R189415",
            "label": "Byssal threads inspired ionic cross-linked narce-like graphene oxide paper with superior mechanical strength",
            "doi": "10.1039/c4ra08319a",
            "research_field": {
                "id": "R254",
                "label": "Materials Science and Engineering"
            },
            "abstract": "\"artificial nacre-like graphene oxide paper has sparked great excitement in the scientific community for its unique properties. the preparation of a bioinspired high-strength nanocomposite paper via a simple vacuum-assisted assembly technique from graphene oxide (go), tannic acid (ta) and fe3+ ions is reported in this article. the fabricated papers were characterized by x-ray diffraction (xrd), scanning electron microscopy (sem), thermogravimetric analysis (tga), fourier transformed infrared (ftir) spectroscopy, x-ray photoelectron spectroscopy (xps) and dynamic mechanical analysis (dma). we show that fe3+ ions only induce limited improvement in the mechanical properties of the graphene oxide paper, while the efficient cross-linking of neighboring sheets by fe3+\u2013ta complex network can significantly improve the fracture strength and young's modulus of graphene oxide paper by 150% and 521%, respectively, with an optimal content of 5.7 wt% fe3+. with general surface binding affinity, ta molecules can be adsorbed to go sheets and provide binding sites for fe3+. the fe3+\u2013ta coordinated compound serves as the \u201cmortar\u201d to stick the go \u201cbricks\u201d together. the mechanical properties of our paper can be simply varied by controlling the cross-linking condition. the obtained nacre-like ultrastrong go papers could find potential in energy and sustainability applications.\""
        },
        {
            "id": "R189421",
            "label": "Bio-Inspired Borate Cross-Linking in Ultra-Stiff Graphene Oxide Thin Films",
            "doi": "10.1002/adma.201101544",
            "research_field": {
                "id": "R254",
                "label": "Materials Science and Engineering"
            },
            "abstract": "adjacent graphene oxide nanosheets in a thin-film structure have been covalently cross-linked in a fashion similar to the cell walls of higher-order plants. the resulting ultra-stiff structure exhibits a maximum storage modulus of 127 gpa that can be tuned by varying borate concentration."
        },
        {
            "id": "R189423",
            "label": "Graphene Oxide Sheets Chemically Cross-Linked by Polyallylamine",
            "doi": "10.1021/jp907613s",
            "research_field": {
                "id": "R254",
                "label": "Materials Science and Engineering"
            },
            "abstract": "we report that a homogeneous aqueous colloidal suspension of chemically cross-linked graphene oxide sheets was generated by addition of polyallylamine to an aqueous suspension of graphene oxide sheets followed by sonication of the mixture. this is the first example for producing a homogeneous colloidal suspension of cross-linked graphene sheets. as a demonstration of the utility of such a colloidal suspension, \u201cpaper\u201d material samples made by simple filtration from such a suspension of cross-linked graphene oxide sheets showed excellent mechanical stiffness and strength."
        },
        {
            "id": "R189426",
            "label": "High-Nanofiller-Content Graphene Oxide-Polymer Nanocomposites via Vacuum-Assisted Self-Assembly",
            "doi": "10.1002/adfm.201000723",
            "research_field": {
                "id": "R254",
                "label": "Materials Science and Engineering"
            },
            "abstract": "highly ordered, homogeneous polymer nanocomposites of layered graphene oxide are prepared using a vacuum\u2010assisted self\u2010assembly (vasa) technique. in vasa, all components (nanofiller and polymer) are pre\u2010mixed prior to assembly under a flow, making it compatible with either hydrophilic poly(vinyl alcohol) (pva) or hydrophobic poly(methyl methacrylate) (pmma) for the preparation of composites with over 50 wt% filler. this process is complimentary to layer\u2010by\u2010layer assembly, where the assembling components are required to interact strongly (e.g., via coulombic attraction). the nanosheets within the vasa\u2010assembled composites exhibit a high degree of order with tunable intersheet spacing, depending on the polymer content. graphene oxide\u2013pva nanocomposites, prepared from water, exhibit greatly improved modulus values in comparison to films of either pure pva or pure graphene oxide. modulus values for graphene oxide\u2013pmma nanocomposites, prepared from dimethylformamide, are intermediate to those of the pure components. the differences in structure, modulus, and strength can be attributed to the gallery composition, specifically the hydrogen bonding ability of the intercalating species"
        },
        {
            "id": "R189429",
            "label": "The Effect of Interlayer Adhesion on the Mechanical Behaviors of Macroscopic Graphene Oxide Papers",
            "doi": "10.1021/nn103331x",
            "research_field": {
                "id": "R254",
                "label": "Materials Science and Engineering"
            },
            "abstract": "\"high mechanical performances of macroscopic graphene oxide (go) papers are attracting great interest owing to their merits of lightweight and multiple functionalities. however, the loading role of individual nanosheets and its effect on the mechanical properties of the macroscopic go papers are not yet well understood. herein, we effectively tailored the interlayer adhesions of the go papers by introducing small molecules, that is, glutaraldehyde (ga) and water molecules, into the gallery regions. with the help of in situ raman spectroscopy, we compared the varied load-reinforcing roles of nanosheets, and further predicted the young's moduli of the go papers. systematic mechanical tests have proven that the enhancement of the tensile modulus and strength of the ga-treated go paper arose from the improved load-bearing capability of the nanosheets. on the basis of raman and macroscopic mechanical tests, the influences of interlayer adhesions on the fracture mechanisms of the strained go papers were inferred.\""
        },
        {
            "id": "R189554",
            "label": "A Constrained Assembly Strategy for High-Strength Natural Nanoclay Film",
            "doi": "10.1021/acsnano.2c00023",
            "research_field": {
                "id": "R254",
                "label": "Materials Science and Engineering"
            },
            "abstract": "developing high-performance materials from existing natural materials is highly desired because of their environmental friendliness and low cost; two-dimensional nanoclay exfoliated from layered silicate minerals is a good building block to construct multilayered macroscopic assemblies for achieving high mechanical and functional properties. nevertheless, the efforts have been frustrated by insufficient inter-nanosheet stress transfer and nanosheet misalignment caused by capillary force during solution-based spontaneous assembly, degrading the mechanical strength of clay-based materials. herein, a constrained assembly strategy that is implemented by in-plane stretching a robust water-containing nanoclay network with hydrogen and ionic bonding is developed to adjust the 2d topography of nanosheets within multilayered nanoclay film. in-plane stretching overcomes capillary force during water removal and thus restrains nanosheet conformation transition from nearly flat to wrinkled, leading to a highly aligned multilayered nanostructure with synergistic hydrogen and ionic bonding. it is proved that inter-nanosheet hydrogen and ionic bonding and nanosheet conformation extension generate profound mechanical reinforcement. the tensile strength and modulus of natural nanoclay film reach up to 429.0 mpa and 43.8 gpa and surpass the counterparts fabricated by normal spontaneous assembly. additionally, improved heat insulation function and good nonflammability are shown for the natural nanoclay film and extend its potential for realistic uses."
        },
        {
            "id": "R189565",
            "label": "Facile Access to Large-Scale, Self-Assembled, Nacre-Inspired, High-Performance Materials with Tunable Nanoscale Periodicities",
            "doi": "10.1021/am400350q",
            "research_field": {
                "id": "R254",
                "label": "Materials Science and Engineering"
            },
            "abstract": "although advances have been reported to mimic the mechanically excellent structure of natural nacre, larger-scale applications are still limited due to time and energy-intensive preparation pathways. herein, we demonstrate that simple high-shear homogenization of dispersions containing biobased high molecular weight sodium carboxymethyl cellulose (700 kg/mol, cmc) and natural sodium montmorillonite (mtm), serving as the soft energy-dissipating phase and reinforcing platelets, respectively, can be used to prepare large-area and thick films with well-aligned hard/soft nacre-mimetic mesostructure. during this process, core-shell nanoplatelets with intrinsic hard/soft structure form, which then self-assemble into a layered nanocomposite during water removal. the nanoscale periodicities of the alternating hard/soft layers can be precisely tuned by changing the ratio of cmc to mtm, which allows studying the evolution of mechanical properties as a function of the lamellar nanoscale periodicity and fractions of hard to soft material. remarkable mechanical stiffness (25 gpa) and strength (320 mpa) can be obtained placing these materials among the top end of nacre-inspired materials reported so far. mechanical homogenization also allows direct preparation of concentrated, yet homogeneous, gel-like dispersions of high nanoclay content, suited to doctor-blade large-area and thick films with essentially the same properties as films cast from dilute dispersions. in terms of functional properties, we report high-transparency, shape-persistent fire-blocking and the ability to surface-pattern via inkjet printing. considering the simple, fully scalable, waterborne preparation pathway, and the use of nature-based components, we foresee applications as ecofriendly, bioinspired materials to promote sustainable engineering materials and novel types of functional barrier coatings and substrates."
        },
        {
            "id": "R189567",
            "label": "Synergistic Toughening of Bioinspired Poly(vinyl alcohol)\u2013Clay\u2013Nanofibrillar Cellulose Artificial Nacre",
            "doi": "10.1021/nn406428n",
            "research_field": {
                "id": "R254",
                "label": "Materials Science and Engineering"
            },
            "abstract": "inspired by the layered aragonite platelet/nanofibrillar chitin/protein ternary structure and integration of extraordinary strength and toughness of natural nacre, artificial nacre based on clay platelet/nanofibrillar cellulose/poly(vinyl alcohol) is constructed through an evaporation-induced self-assembly technique. the synergistic toughening effect from clay platelets and nanofibrillar cellulose is successfully demonstrated. the artificial nacre achieves an excellent balance of strength and toughness and a fatigue-resistant property, superior to natural nacre and other conventional layered clay/polymer binary composites."
        },
        {
            "id": "R189569",
            "label": "Hierarchical Nacre Mimetics with Synergistic Mechanical Properties by Control of Molecular Interactions in Self-Healing Polymers",
            "doi": "10.1002/anie.201502323",
            "research_field": {
                "id": "R254",
                "label": "Materials Science and Engineering"
            },
            "abstract": "designing the reversible interactions of biopolymers remains a grand challenge for an integral mimicry of mechanically superior biological composites. yet, they are the key to synergistic combinations of stiffness and toughness by providing sacrificial bonds with hidden length scales. to address this challenge, dynamic polymers were designed with low glass-transition temperature t(g) and bonded by quadruple hydrogen-bonding motifs, and subsequently assembled with high-aspect-ratio synthetic nanoclays to generate nacre-mimetic films. the high dynamics and self-healing of the polymers render transparent films with a near-perfectly aligned structure. varying the polymer composition allows molecular control over the mechanical properties up to very stiff and very strong films (e\u224845\\u2005gpa, \u03c3(uts)\u2248270\\u2005mpa). stable crack propagation and multiple toughening mechanisms occur in situations of balanced dynamics, enabling synergistic combinations of stiffness and toughness. excellent gas barrier properties complement the multifunctional property profile."
        },
        {
            "id": "R178358",
            "label": "Large Scale Self-Assembly of Smectic Nanocomposite Films by Doctor Blading versus Spray Coating: Impact of Crystal Quality on Barrier Properties",
            "doi": "10.1021/acs.macromol.7b00701",
            "research_field": {
                "id": "R137665",
                "label": "Coating and Surface Technology"
            },
            "abstract": "flexible transparent barrier films are required in various fields of application ranging from flexible, transparent food packaging to display encapsulation. environmentally friendly, waterborne polymer\u2013clay nanocomposites would be preferred but fail to meet in particular requirements for ultra high water vapor barriers. here we show that self-assembly of nanocomposite films into one-dimensional crystalline (smectic) polymer\u2013clay domains is a so-far overlooked key-factor capable of suppressing water vapor diffusivity despite appreciable swelling at elevated temperatures and relative humidity (r.h.). moreover, barrier performance was shown to improve with quality of the crystalline order. in this respect, spray coating is superior to doctor blading because it yields significantly better ordered structures. for spray-coated waterborne nanocomposite films (21.4 \u03bcm) ultra high barrier specifications are met at 23 \u00b0c and 50% r.h. with oxygen transmission rates (otr) < 0.0005 cm3 m\u20132 day\u20131 bar\u20131 and water vapor ..."
        },
        {
            "id": "R178362",
            "label": "Clay-Based Nanocomposite Coating for Flexible Optoelectronics Applying Commercial Polymers",
            "doi": "10.1021/nn400713e",
            "research_field": {
                "id": "R137665",
                "label": "Coating and Surface Technology"
            },
            "abstract": "transparency, flexibility, and especially ultralow oxygen (otr) and water vapor (wvtr) transmission rates are the key issues to be addressed for packaging of flexible organic photovoltaics and organic light-emitting diodes. concomitant optimization of all essential features is still a big challenge. here we present a thin (1.5 \u03bcm), highly transparent, and at the same time flexible nanocomposite coating with an exceptionally low otr and wvtr (1.0 \u00d7 10(-2) cm(3) m(-2) day(-1) bar(-1) and <0.05 g m(-2) day(-1) at 50% rh, respectively). a commercially available polyurethane (desmodur n 3600 and desmophen 670 ba, bayer materialscience ag) was filled with a delaminated synthetic layered silicate exhibiting huge aspect ratios of about 25,000. functional films were prepared by simple doctor-blading a suspension of the matrix and the organophilized clay. this preparation procedure is technically benign, is easy to scale up, and may readily be applied for encapsulation of sensitive flexible electronics."
        },
        {
            "id": "R12223",
            "label": "Modelling the epidemic trend of the 2019 novel coronavirus outbreak in China",
            "doi": "10.1101/2020.01.23.916726",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "abstract": "we present a timely evaluation of the chinese 2019-ncov epidemic in its initial phase, where 2019-ncov demonstrates comparable transmissibility but lower fatality rates than sars and mers. a quick diagnosis that leads to case isolation and integrated interventions will have a major impact on its future trend. nevertheless, as china is facing its spring festival travel rush and the epidemic has spread beyond its borders, further investigation on its potential spatiotemporal transmission pattern and novel intervention strategies are warranted."
        },
        {
            "id": "R12226",
            "label": "Time-varying transmission dynamics of Novel Coronavirus Pneumonia in China",
            "doi": "10.1101/2020.01.25.919787",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "abstract": "abstract rationale several studies have estimated basic production number of novel coronavirus pneumonia (ncp). however, the time-varying transmission dynamics of ncp during the outbreak remain unclear. objectives we aimed to estimate the basic and time-varying transmission dynamics of ncp across china, and compared them with sars. methods data on ncp cases by february 7, 2020 were collected from epidemiological investigations or official websites. data on severe acute respiratory syndrome (sars) cases in guangdong province, beijing and hong kong during 2002-2003 were also obtained. we estimated the doubling time, basic reproduction number ( r 0 ) and time-varying reproduction number ( r t ) of ncp and sars. measurements and main results as of february 7, 2020, 34,598 ncp cases were identified in china, and daily confirmed cases decreased after february 4. the doubling time of ncp nationwide was 2.4 days which was shorter than that of sars in guangdong (14.3 days), hong kong (5.7 days) and beijing (12.4 days). the r 0 of ncp cases nationwide and in wuhan were 4.5 and 4.4 respectively, which were higher than r 0 of sars in guangdong ( r 0 =2.3), hongkong ( r 0 =2.3), and beijing ( r 0 =2.6). the r t for ncp continuously decreased especially after january 16 nationwide and in wuhan. the r 0 for secondary ncp cases in guangdong was 0.6, and the r t values were less than 1 during the epidemic. conclusions ncp may have a higher transmissibility than sars, and the efforts of containing the outbreak are effective. however, the efforts are needed to persist in for reducing time-varying reproduction number below one. at a glance commentary scientific knowledge on the subject since december 29, 2019, pneumonia infection with 2019-ncov, now named as novel coronavirus pneumonia (ncp), occurred in wuhan, hubei province, china. the disease has rapidly spread from wuhan to other areas. as a novel virus, the time-varying transmission dynamics of ncp remain unclear, and it is also important to compare it with sars. what this study adds to the field we compared the transmission dynamics of ncp with sars, and found that ncp has a higher transmissibility than sars. time-varying production number indicates that rigorous control measures taken by governments are effective across china, and persistent efforts are needed to be taken for reducing instantaneous reproduction number below one."
        },
        {
            "id": "R12231",
            "label": "Novel coronavirus 2019-nCoV: early estimation of epidemiological parameters and epidemic predictions",
            "doi": "10.1101/2020.01.23.20018549",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "abstract": "abstract since first identified, the epidemic scale of the recently emerged novel coronavirus (2019-ncov) in wuhan, china, has increased rapidly, with cases arising across china and other countries and regions. using a transmission model, we estimate a basic reproductive number of 3.11 (95%ci, 2.39\u20134.13); 58\u201376% of transmissions must be prevented to stop increasing; wuhan case ascertainment of 5.0% (3.6\u20137.4); 21022 (11090\u201333490) total infections in wuhan 1 to 22 january. changes to previous version case data updated to include 22 jan 2020; we did not use cases reported after this period as cases were reported at the province level hereafter, and large-scale control interventions were initiated on 23 jan 2020; improved likelihood function, better accounting for first 41 confirmed cases, and now using all infections (rather than just cases detected) in wuhan for prediction of infection in international travellers; improved characterization of uncertainty in parameters, and calculation of epidemic trajectory confidence intervals using a more statistically rigorous method; extended range of latent period in sensitivity analysis to reflect reports of up to 6 day incubation period in household clusters; removed travel restriction analysis, as different modelling approaches (e.g. stochastic transmission, rather than deterministic transmission) are more appropriate to such analyses."
        },
        {
            "id": "R12233",
            "label": "Early transmissibility assessment of a novel coronavirus in Wuhan",
            "doi": "10.2139/ssrn.3524675",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "abstract": "between december 1, 2019 and january 26, 2020, nearly 3000 cases of respiratory illness caused by a novel coronavirus originating in wuhan, china have been reported. in this short analysis, we combine publicly available cumulative case data from the ongoing outbreak with phenomenological modeling methods to conduct an early transmissibility assessment. our model suggests that the basic reproduction number associated with the outbreak (at time of writing) may range from 2.0 to 3.1. though these estimates are preliminary and subject to change, they are consistent with previous findings regarding the transmissibility of the related sars-coronavirus and indicate the possibility of epidemic potential."
        },
        {
            "id": "R12235",
            "label": "Estimating the effective reproduction number of the 2019-nCoV in China",
            "doi": "10.1101/2020.01.27.20018952",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "abstract": "abstract we estimate the effective reproduction number for 2019-ncov based on the daily reported cases from china cdc. the results indicate that 2019-ncov has a higher effective reproduction number than sars with a comparable fatality rate. article summary line this modeling study indicates that 2019-ncov has a higher effective reproduction number than sars with a comparable fatality rate."
        },
        {
            "id": "R12237",
            "label": "Preliminary estimation of the basic reproduction number of novel coronavirus (2019-nCoV) in China, from 2019 to 2020: A data-driven analysis in the early phase of the outbreak",
            "doi": "10.1101/2020.01.23.916395",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "abstract": "abstract backgrounds an ongoing outbreak of a novel coronavirus (2019-ncov) pneumonia hit a major city of china, wuhan, december 2019 and subsequently reached other provinces/regions of china and countries. we present estimates of the basic reproduction number, r 0 , of 2019-ncov in the early phase of the outbreak. methods accounting for the impact of the variations in disease reporting rate, we modelled the epidemic curve of 2019-ncov cases time series, in mainland china from january 10 to january 24, 2020, through the exponential growth. with the estimated intrinsic growth rate ( \u03b3 ), we estimated r 0 by using the serial intervals (si) of two other well-known coronavirus diseases, mers and sars, as approximations for the true unknown si. findings the early outbreak data largely follows the exponential growth. we estimated that the mean r 0 ranges from 2.24 (95%ci: 1.96-2.55) to 3.58 (95%ci: 2.89-4.39) associated with 8-fold to 2-fold increase in the reporting rate. we demonstrated that changes in reporting rate substantially affect estimates of r 0 . conclusion the mean estimate of r 0 for the 2019-ncov ranges from 2.24 to 3.58, and significantly larger than 1. our findings indicate the potential of 2019-ncov to cause outbreaks."
        },
        {
            "id": "R12243",
            "label": "Pattern of early human-to-human transmission of Wuhan 2019-nCoV",
            "doi": "10.1101/2020.01.23.917351",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "abstract": "abstract on december 31, 2019, the world health organization was notified about a cluster of pneumonia of unknown aetiology in the city of wuhan, china. chinese authorities later identified a new coronavirus (2019-ncov) as the causative agent of the outbreak. as of january 23, 2020, 655 cases have been confirmed in china and several other countries. understanding the transmission characteristics and the potential for sustained human-to-human transmission of 2019-ncov is critically important for coordinating current screening and containment strategies, and determining whether the outbreak constitutes a public health emergency of international concern (pheic). we performed stochastic simulations of early outbreak trajectories that are consistent with the epidemiological findings to date. we found the basic reproduction number, r 0 , to be around 2.2 (90% high density interval 1.4\u20143.8), indicating the potential for sustained human-to-human transmission. transmission characteristics appear to be of a similar magnitude to severe acute respiratory syndrome-related coronavirus (sars-cov) and the 1918 pandemic influenza. these findings underline the importance of heightened screening, surveillance and control efforts, particularly at airports and other travel hubs, in order to prevent further international spread of 2019-ncov."
        },
        {
            "id": "R12245",
            "label": "Estimation of the Transmission Risk of 2019-nCov and Its Implication for Public Health Interventions",
            "doi": "10.2139/ssrn.3525558 ",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "abstract": "english abstract: background: since the emergence of the first pneumonia cases in wuhan, china, the novel coronavirus (2019-ncov) infection has been quickly spreading out to other provinces and neighbouring countries. estimation of the basic reproduction number by means of mathematical modelling can be helpful for determining the potential and severity of an outbreak, and providing critical information for identifying the type of disease interventions and intensity.\\r\\n\\r\\nmethods: a deterministic compartmental model was devised based on the clinical progression of the disease, epidemiological status of the individuals, and the intervention measures.\\r\\n\\r\\nfindings: the estimation results based on likelihood and model analysis reveal that the control reproduction number may be as high as 6.47 (95% ci 5.71-7.23). sensitivity analyses reveal that interventions, such as intensive contact tracing followed by quarantine and isolation, can effectively reduce the control reproduction number and transmission risk, with the effect of travel restriction of wuhan on 2019-ncov infection in beijing being almost equivalent to increasing quarantine by 100-thousand baseline value.\\r\\n\\r\\ninterpretation: it is essential to assess how the expensive, resource-intensive measures implemented by the chinese authorities can contribute to the prevention and control of the 2019-ncov infection, and how long should be maintained. under the most restrictive measures, the outbreak is expected to peak within two weeks (since january 23rd 2020) with significant low peak value. with travel restriction (no imported exposed individuals to beijing), the number of infected individuals in 7 days will decrease by 91.14% in beijing, compared with the scenario of no travel restriction.\\r\\n\\r\\nmandarin abstract: \u80cc\u666f\uff1a\u81ea\u4ece\u4e2d\u56fd\u6b66\u6c49\u51fa\u73b0\u7b2c\u4e00\u4f8b\u80ba\u708e\u75c5\u4f8b\u4ee5\u6765\uff0c\u65b0\u578b\u51a0\u72b6\u75c5\u6bd2\uff082019-ncov\uff09\u611f\u67d3\u5df2\u8fc5\u901f\u4f20\u64ad\u5230\u5176\u4ed6\u7701\u4efd\u548c\u5468\u8fb9\u56fd\u5bb6\u3002\u901a\u8fc7\u6570\u5b66\u6a21\u578b\u4f30\u8ba1\u57fa\u672c\u518d\u751f\u6570\uff0c\u6709\u52a9\u4e8e\u786e\u5b9a\u75ab\u60c5\u7206\u53d1\u7684\u53ef\u80fd\u6027\u548c\u4e25\u91cd\u6027\uff0c\u5e76\u4e3a\u786e\u5b9a\u75be\u75c5\u5e72\u9884\u7c7b\u578b\u548c\u5f3a\u5ea6\u63d0\u4f9b\u5173\u952e\u4fe1\u606f\u3002\\r\\n\\r\\n\u65b9\u6cd5\uff1a\u6839\u636e\u75be\u75c5\u7684\u4e34\u5e8a\u8fdb\u5c55\uff0c\u4e2a\u4f53\u7684\u6d41\u884c\u75c5\u5b66\u72b6\u51b5\u548c\u5e72\u9884\u63aa\u65bd\uff0c\u8bbe\u8ba1\u786e\u5b9a\u6027\u7684\u4ed3\u5ba4\u6a21\u578b\u3002\\r\\n\\r\\n\u7ed3\u679c\uff1a\u57fa\u4e8e\u4f3c\u7136\u51fd\u6570\u548c\u6a21\u578b\u5206\u6790\u7684\u4f30\u8ba1\u7ed3\u679c\u8868\u660e\uff0c\u63a7\u5236\u518d\u751f\u6570\u53ef\u80fd\u9ad8\u8fbe6.47\uff0895\uff05ci 5.71-7.23\uff09\u3002\u654f\u611f\u6027\u5206\u6790\u663e\u793a\uff0c\u5bc6\u96c6\u63a5\u89e6\u8ffd\u8e2a\u548c\u9694\u79bb\u7b49\u5e72\u9884\u63aa\u65bd\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u63a7\u5236\u518d\u751f\u6570\u548c\u4f20\u64ad\u98ce\u9669\uff0c\u6b66\u6c49\u5c01\u57ce\u63aa\u65bd\u5bf9\u5317\u4eac2019-ncov\u611f\u67d3\u7684\u5f71\u54cd\u51e0\u4e4e\u7b49\u540c\u4e8e\u589e\u52a0\u9694\u79bb\u63aa\u65bd10\u4e07\u7684\u57fa\u7ebf\u503c\u3002\\r\\n\\r\\n\u89e3\u91ca\uff1a\u5fc5\u987b\u8bc4\u4f30\u4e2d\u56fd\u5f53\u5c40\u5b9e\u65bd\u7684\u6602\u8d35\uff0c\u8d44\u6e90\u5bc6\u96c6\u578b\u63aa\u65bd\u5982\u4f55\u6709\u52a9\u4e8e\u9884\u9632\u548c\u63a7\u52362019-ncov\u611f\u67d3\uff0c\u4ee5\u53ca\u5e94\u7ef4\u6301\u591a\u957f\u65f6\u95f4\u3002\u5728\u6700\u4e25\u683c\u7684\u63aa\u65bd\u4e0b\uff0c\u9884\u8ba1\u75ab\u60c5\u5c06\u5728\u4e24\u5468\u5185\uff08\u81ea2020\u5e741\u670823\u65e5\u8d77\uff09\u8fbe\u5230\u5cf0\u503c\uff0c\u5cf0\u503c\u8f83\u4f4e\u3002\u4e0e\u6ca1\u6709\u51fa\u884c\u9650\u5236\u7684\u60c5\u51b5\u76f8\u6bd4\uff0c\u6709\u4e86\u51fa\u884c\u9650\u5236\uff08\u5373\u6ca1\u6709\u8f93\u5165\u7684\u6f5c\u4f0f\u7c7b\u4e2a\u4f53\u8fdb\u5165\u5317\u4eac\uff09\uff0c\u5317\u4eac\u76847\u5929\u611f\u67d3\u8005\u6570\u91cf\u5c06\u51cf\u5c1191.14\uff05\u3002"
        },
        {
            "id": "R12247",
            "label": "Early transmission dynamics in wuhan, china, of novel coronavirus-infected pneumonia",
            "doi": "10.1056/NEJMoa2001316",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "abstract": "abstract background the initial cases of novel coronavirus (2019-ncov)\u2013infected pneumonia (ncip) occurred in wuhan, hubei province, china, in december 2019 and january 2020. we analyzed data on the first 425 confirmed cases in wuhan to determine the epidemiologic characteristics of ncip. methods we collected information on demographic characteristics, exposure history, and illness timelines of laboratory-confirmed cases of ncip that had been reported by january 22, 2020. we described characteristics of the cases and estimated the key epidemiologic time-delay distributions. in the early period of exponential growth, we estimated the epidemic doubling time and the basic reproductive number. results among the first 425 patients with confirmed ncip, the median age was 59 years and 56% were male. the majority of cases (55%) with onset before january 1, 2020, were linked to the huanan seafood wholesale market, as compared with 8.6% of the subsequent cases. the mean incubation period was 5.2 days (95% confidence interval [ci], 4.1 to 7.0), with the 95th percentile of the distribution at 12.5 days. in its early stages, the epidemic doubled in size every 7.4 days. with a mean serial interval of 7.5 days (95% ci, 5.3 to 19), the basic reproductive number was estimated to be 2.2 (95% ci, 1.4 to 3.9). conclusions on the basis of this information, there is evidence that human-to-human transmission has occurred among close contacts since the middle of december 2019. considerable efforts to reduce transmission will be required to control outbreaks if similar dynamics apply elsewhere. measures to prevent or reduce transmission should be implemented in populations at risk. (funded by the ministry of science and technology of china and others.)"
        },
        {
            "id": "R36106",
            "label": "Characterizing the transmission and identifying the control strategy for COVID-19 through epidemiological modeling",
            "doi": "10.1101/2020.02.24.20026773",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "abstract": "abstract the outbreak of the novel coronavirus disease, covid-19, originating from wuhan, china in early december, has infected more than 70,000 people in china and other countries and has caused more than 2,000 deaths. as the disease continues to spread, the biomedical society urgently began identifying effective approaches to prevent further outbreaks. through rigorous epidemiological analysis, we characterized the fast transmission of covid-19 with a basic reproductive number 5.6 and proved a sole zoonotic source to originate in wuhan. no changes in transmission have been noted across generations. by evaluating different control strategies through predictive modeling and monte carlo simulations, a comprehensive quarantine in hospitals and quarantine stations has been found to be the most effective approach. government action to immediately enforce this quarantine is highly recommended."
        },
        {
            "id": "R36109",
            "label": "Transmission interval estimates suggest pre-symptomatic spread of COVID-19",
            "doi": "10.1101/2020.03.03.20029983",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "abstract": "abstract background as the covid-19 epidemic is spreading, incoming data allows us to quantify values of key variables that determine the transmission and the effort required to control the epidemic. we determine the incubation period and serial interval distribution for transmission clusters in singapore and in tianjin. we infer the basic reproduction number and identify the extent of pre-symptomatic transmission. methods we collected outbreak information from singapore and tianjin, china, reported from jan.19-feb.26 and jan.21-feb.27, respectively. we estimated incubation periods and serial intervals in both populations. results the mean incubation period was 7.1 (6.13, 8.25) days for singapore and 9 (7.92, 10.2) days for tianjin. both datasets had shorter incubation periods for earlier-occurring cases. the mean serial interval was 4.56 (2.69, 6.42) days for singapore and 4.22 (3.43, 5.01) for tianjin. we inferred that early in the outbreaks, infection was transmitted on average 2.55 and 2.89 days before symptom onset (singapore, tianjin). the estimated basic reproduction number for singapore was 1.97 (1.45, 2.48) secondary cases per infective; for tianjin it was 1.87 (1.65, 2.09) secondary cases per infective. conclusions estimated serial intervals are shorter than incubation periods in both singapore and tianjin, suggesting that pre-symptomatic transmission is occurring. shorter serial intervals lead to lower estimates of r0, which suggest that half of all secondary infections should be prevented to control spread."
        },
        {
            "id": "R36114",
            "label": "Estimation of the epidemic properties of the 2019 novel coronavirus: A mathematical modeling study",
            "doi": "10.1101/2020.02.18.20024315",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "abstract": "abstract background the 2019 novel coronavirus (covid-19) emerged in wuhan, china in december 2019 and has been spreading rapidly in china. decisions about its pandemic threat and the appropriate level of public health response depend heavily on estimates of its basic reproduction number and assessments of interventions conducted in the early stages of the epidemic. methods we conducted a mathematical modeling study using five independent methods to assess the basic reproduction number (r0) of covid-19, using data on confirmed cases obtained from the china national health commission for the period 10 th january \u2013 8 th february. we analyzed the data for the period before the closure of wuhan city (10 th january \u2013 23 rd january) and the post-closure period (23 rd january \u2013 8 th february) and for the whole period, to assess both the epidemic risk of the virus and the effectiveness of the closure of wuhan city on spread of covid-19. findings before the closure of wuhan city the basic reproduction number of covid-19 was 4.38 (95% ci: 3.63 \u2013 5.13), dropping to 3.41 (95% ci: 3.16 \u2013 3.65) after the closure of wuhan city. over the entire epidemic period covid-19 had a basic reproduction number of 3.39 (95% ci: 3.09 \u2013 3.70), indicating it has a very high transmissibility. interpretation covid-19 is a highly transmissible virus with a very high risk of epidemic outbreak once it emerges in metropolitan areas. the closure of wuhan city was effective in reducing the severity of the epidemic, but even after closure of the city and the subsequent expansion of that closure to other parts of hubei the virus remained extremely infectious. emergency planners in other cities should consider this high infectiousness when considering responses to this virus. funding national natural science foundation of china, china medical board, national science and technology major project of china"
        },
        {
            "id": "R36118",
            "label": "The Novel Coronavirus, 2019-nCoV, is Highly Contagious and More Infectious Than Initially Estimated",
            "doi": "10.1101/2020.02.07.20021154",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "abstract": "abstract the novel coronavirus (2019-ncov) is a recently emerged human pathogen that has spread widely since january 2020. initially, the basic reproductive number, r 0 , was estimated to be 2.2 to 2.7. here we provide a new estimate of this quantity. we collected extensive individual case reports and estimated key epidemiology parameters, including the incubation period. integrating these estimates and high-resolution real-time human travel and infection data with mathematical models, we estimated that the number of infected individuals during early epidemic double every 2.4 days, and the r 0 value is likely to be between 4.7 and 6.6. we further show that quarantine and contact tracing of symptomatic individuals alone may not be effective and early, strong control measures are needed to stop transmission of the virus. one-sentence summary by collecting and analyzing spatiotemporal data, we estimated the transmission potential for 2019-ncov."
        },
        {
            "id": "R36128",
            "label": "Risk estimation and prediction by modeling the transmission of the novel coronavirus (COVID-19) in mainland China excluding Hubei province",
            "doi": "10.1101/2020.03.01.20029629",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "abstract": "abstract background in december 2019, an outbreak of coronavirus disease (covid-19) was identified in wuhan, china and, later on, detected in other parts of china. our aim is to evaluate the effectiveness of the evolution of interventions and self-protection measures, estimate the risk of partial lifting control measures and predict the epidemic trend of the virus in mainland china excluding hubei province based on the published data and a novel mathematical model. methods a novel covid-19 transmission dynamic model incorporating the intervention measures implemented in china is proposed. covid-19 daily data of mainland china excluding hubei province, including the cumulative confirmed cases, the cumulative deaths, newly confirmed cases and the cumulative recovered cases for the period january 20th-march 3rd, 2020, were archived from the national health commission of china (nhcc). we parameterize the model by using the markov chain monte carlo (mcmc) method and estimate the control reproduction number r c , as well as the effective daily reproduction ratio r e ( t ), of the disease transmission in mainland china excluding hubei province. results the estimation outcomes indicate that r c is 3.36 (95% ci 3.20-3.64) and r e ( t ) has dropped below 1 since january 31st, 2020, which implies that the containment strategies implemented by the chinese government in mainland china excluding hubei province are indeed effective and magnificently suppressed covid-19 transmission. moreover, our results show that relieving personal protection too early may lead to the spread of disease for a longer time and more people would be infected, and may even cause epidemic or outbreak again. by calculating the effective reproduction ratio, we prove that the contact rate should be kept at least less than 30% of the normal level by april, 2020. conclusions to ensure the epidemic ending rapidly, it is necessary to maintain the current integrated restrict interventions and self-protection measures, including travel restriction, quarantine of entry, contact tracing followed by quarantine and isolation and reduction of contact, like wearing masks, etc. people should be fully aware of the real-time epidemic situation and keep sufficient personal protection until april. if all the above conditions are met, the outbreak is expected to be ended by april in mainland china apart from hubei province."
        },
        {
            "id": "R36130",
            "label": "Assessing the plausibility of subcritical transmission of 2019-nCoV in the United States",
            "doi": "10.1101/2020.02.08.20021311",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "abstract": "abstract rapid assessment of the transmission potential of an emerging or reemerging pathogen is a cornerstone of public health response. a simple approach is shown for using the number of disease introductions and secondary cases to determine whether the upper bound of the reproduction number exceeds the critical value of one."
        },
        {
            "id": "R36132",
            "label": "Lessons drawn from China and South Korea for managing COVID-19 epidemic: insights from a comparative modeling study",
            "doi": "10.1101/2020.03.09.20033464",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "abstract": "abstract we conducted a comparative study of covid-19 epidemic in three different settings: mainland china, the guangdong province of china and south korea, by formulating two disease transmission dynamics models incorporating epidemic characteristics and setting-specific interventions, and fitting the models to multi-source data to identify initial and effective reproduction numbers and evaluate effectiveness of interventions. we estimated the initial basic reproduction number for south korea, the guangdong province and mainland china as 2.6 (95% confidence interval (ci): (2.5, 2.7)), 3.0 (95%ci: (2.6, 3.3)) and 3.8 (95%ci: (3.5,4.2)), respectively, given a serial interval with mean of 5 days with standard deviation of 3 days. we found that the effective reproduction number for the guangdong province and mainland china has fallen below the threshold 1 since february 8 th and 18 th respectively, while the effective reproduction number for south korea remains high, suggesting that the interventions implemented need to be enhanced in order to halt further infections. we also project the epidemic trend in south korea under different scenarios where a portion or the entirety of the integrated package of interventions in china is used. we show that a coherent and integrated approach with stringent public health interventions is the key to the success of containing the epidemic in china and specially its provinces outside its epicenter, and we show that this approach can also be effective to mitigate the burden of the covid-19 epidemic in south korea. the experience of outbreak control in mainland china should be a guiding reference for the rest of the world including south korea."
        },
        {
            "id": "R36138",
            "label": "Estimating the generation interval for COVID-19 based on symptom onset data",
            "doi": "10.1101/2020.03.05.20031815",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "abstract": "abstract background estimating key infectious disease parameters from the covid-19 outbreak is quintessential for modelling studies and guiding intervention strategies. whereas different estimates for the incubation period distribution and the serial interval distribution have been reported, estimates of the generation interval for covid-19 have not been provided. methods we used outbreak data from clusters in singapore and tianjin, china to estimate the generation interval from symptom onset data while acknowledging uncertainty about the incubation period distribution and the underlying transmission network. from those estimates we obtained the proportions pre-symptomatic transmission and reproduction numbers. results the mean generation interval was 5.20 (95%ci 3.78-6.78) days for singapore and 3.95 (95%ci 3.01-4.91) days for tianjin, china when relying on a previously reported incubation period with mean 5.2 and sd 2.8 days. the proportion of pre-symptomatic transmission was 48% (95%ci 32-67%) for singapore and 62% (95%ci 50-76%) for tianjin, china. estimates of the reproduction number based on the generation interval distribution were slightly higher than those based on the serial interval distribution. conclusions estimating generation and serial interval distributions from outbreak data requires careful investigation of the underlying transmission network. detailed contact tracing information is essential for correctly estimating these quantities."
        },
        {
            "id": "R36143",
            "label": "A Cybernetics-based Dynamic Infection Model for Analyzing SARS-COV-2 Infection Stability and Predicting Uncontrollable Risks",
            "doi": "10.1101/2020.03.13.20034082",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "abstract": "abstract since december 2019, covid-19 has raged in wuhan and subsequently all over china and the world. we propose a cybernetics-based dynamic infection model (cdim) to the dynamic infection process with a probability distributed incubation delay and feedback principle. reproductive trends and the stability of the sars-cov-2 infection in a city can then be analyzed, and the uncontrollable risks can be forecasted before they really happen. the infection mechanism of a city is depicted using the philosophy of cybernetics and approaches of the control engineering. distinguished with other epidemiological models, such as sir, seir, etc., that compute the theoretical number of infected people in a closed population, cdim considers the immigration and emigration population as system inputs, and administrative and medical resources as dynamic control variables. the epidemic regulation can be simulated in the model to support the decision-making for containing the outbreak. city case studies are demonstrated for verification and validation."
        },
        {
            "id": "R36146",
            "label": "COVID-19 outbreak in Algeria: A mathematical model to predict the incidence",
            "doi": "10.1101/2020.03.20.20039891",
            "research_field": {
                "id": "R57",
                "label": "Virology"
            },
            "abstract": "abstract introduction since december 29, 2019 a pandemic of new novel coronavirus-infected pneumonia named covid-19 has started from wuhan, china, has led to 254 996 confirmed cases until midday march 20, 2020. sporadic cases have been imported worldwide, in algeria, the first case reported on february 25, 2020 was imported from italy, and then the epidemic has spread to other parts of the country very quickly with 139 confirmed cases until march 21, 2020. methods it is crucial to estimate the cases number growth in the early stages of the outbreak, to this end, we have implemented the alg-covid-19 model which allows to predict the incidence and the reproduction number r0 in the coming months in order to help decision makers. the alg-covis-19 model initial equation 1, estimates the cumulative cases at t prediction time using two parameters: the reproduction number r0 and the serial interval si. results we found r0=2.55 based on actual incidence at the first 25 days, using the serial interval si= 4,4 and the prediction time t=26. the herd immunity hi estimated is hi=61%. also, the covid-19 incidence predicted with the alg-covid-19 model fits closely the actual incidence during the first 26 days of the epidemic in algeria fig. 1.a. which allows us to use it. according to alg-covid-19 model, the number of cases will exceed 5000 on the 42 th day (april 7 th ) and it will double to 10000 on 46th day of the epidemic (april 11 th ), thus, exponential phase will begin (table 1; fig.1.b) and increases continuously until reaching \u00e0 herd immunity of 61% unless serious preventive measures are considered. discussion this model is valid only when the majority of the population is vulnerable to covid-19 infection, however, it can be updated to fit the new parameters values."
        },
        {
            "id": "R41120",
            "label": "Improving purity and process volume during direct electrolytic reduction of solid SiO 2 in molten CaCl 2 for the production of solar-grade silicon",
            "doi": "10.1002/ente.201300001",
            "research_field": {
                "id": "R122",
                "label": "Chemistry"
            },
            "abstract": "the direct electrolytic reduction of solid sio2 is investigated in molten cacl2 at 1123\\u2005k to produce solar-grade silicon. the target concentrations of impurities for the primary si are calculated from the acceptable concentrations of impurities in solar-grade silicon (sog-si) and the segregation coefficients for the impurity elements. the concentrations of most metal impurities are significantly decreased below their target concentrations by using a quartz vessel and new types of sio2-contacting electrodes. the electrolytic reduction rate is increased by improving an electron pathway from the lead material to the sio2, which demonstrates that the characteristics of the electric contact are important factors affecting the reduction rate. pellet- and basket-type electrodes are tested to improve the process volume for powdery and granular sio2. based on the purity of the si product after melting, refining, and solidifying, the potential of the technology is discussed."
        },
        {
            "id": "R41122",
            "label": "Direct Electrolytic Reduction of Solid Silicon Dioxide in molten LiCl-KCl-CaCl at 773 K",
            "doi": "10.1149/1.2042910",
            "research_field": {
                "id": "R122",
                "label": "Chemistry"
            },
            "abstract": "we investigated electrolytic reduction of solid sio 2 by a contacting electrode method in molten licl-kcl-cacl 2 at 773 k. the results of cyclic voltammetry indicated that reduction of sio 2 occurs at potential more negative than 0.85 v (vs ca 2 + , li + /ca-li). samples were prepared by potentiostatic electrolysis for 2 h at 0.25, 0.50, 0.70, and 1.00 v. energy dispersive x-ray analysis and raman spectra clarified that the reduction products at 0.50 and 0.70 v are composed of amorphous si and microcrystalline si. scanning electron microscope (sem) observations revealed that the morphology of the produced si is spongelike with a particle size smaller than 50 nm. the mechanism of si formation was discussed by comparing the sem observations and the raman spectra of the si samples prepared at 773 and 1123 k. the reduction mechanism of the direct electrolytic reduction of sio 2 at lower temperature was also discussed."
        },
        {
            "id": "R41128",
            "label": "Verification and implications of the dissolution-electrodeposition process during the electro-reduction of solid silica in molten CaCl 2",
            "doi": "10.1039/C2RA20698F",
            "research_field": {
                "id": "R122",
                "label": "Chemistry"
            },
            "abstract": "with the verification of the existence of the dissolution-electrodeposition mechanism during the electro-reduction of solid silica in molten cacl2, the present study not only provides direct scientific support for the controllable electrolytic extraction of nanostructured silicon in molten salts but it also opens an avenue to a continuous silicon extraction process via the electro-deposition of dissolved silicates in molten cacl2. in addition, the present study increases the general understanding of the versatile material extraction route via the electro-deoxidization process of solid oxides in molten salts, which also provokes reconsiderations on the electrochemistry of insulating compounds."
        },
        {
            "id": "R41132",
            "label": "The use of silicon wafer barriers in the electrochemical reduction of solid silica to form silicon in molten salts",
            "doi": "10.1149/07711.2011ecst",
            "research_field": {
                "id": "R122",
                "label": "Chemistry"
            },
            "abstract": "nowadays, silicon is the most critical element in solar cells and/or solar chips. silicon having 98 to 99% si as being metallurgical grade, requires further refinement/purification processes such as zone refining [1,2] and/or siemens process [3] to upgrade it for solar applications. a promising method, based on straightforward electrochemical reduction of oxides by ffc cambridge process [4], was adopted to form silicon from porous sio2 pellets in molten cacl2 and cacl2-nacl salt mixture [5]. it was reported that silicon powder contaminated by iron and nickel emanated from stainless steel cathode, consequently disqualified the product from solar applications. sio2 pellets sintered at 1300oc for 4 hours, were placed in between pure silicon wafer plates to defeat the contamination problem. encouraging results indicated a reliable alternative method of direct solar grade silicon production for expanding solar energy field."
        },
        {
            "id": "R41134",
            "label": "Oscillatory behavior in electrochemical deposition reaction of polycrystalline silicon thin films through reduction of silicon tetrachloride in a molten salt electrolyte",
            "doi": "10.1246/cl.1996.569",
            "research_field": {
                "id": "R122",
                "label": "Chemistry"
            },
            "abstract": "a new electrochemical oscillation is found for reduction reaction of silicon tetrachloride on a partially immersed single crystal n-si electrode in a lithium chloride-potassium chloride eutectic melt electrolyte. the reduction of sicl4, which is almost insoluble in the electrolyte, occurs mainly near the upper edge of an electrolyte meniscus on the electrode, and it is discussed that the oscillation is caused by a change in the height of the meniscus due to a change in the chemical structure (and hence the interfacial tension) of the electrode surface with progress of the silicon deposition reaction."
        },
        {
            "id": "R41136",
            "label": "Toward cost-effective manufacturing of silicon solar cells: electrodeposition of high-quality Si films in a CaCl 2 -based molten salt",
            "doi": "10.1002/anie.201707635",
            "research_field": {
                "id": "R122",
                "label": "Chemistry"
            },
            "abstract": "electrodeposition of si films from a si-containing electrolyte is a cost-effective approach for the manufacturing of solar cells. proposals relying on fluoride-based molten salts have suffered from low product quality due to difficulties in impurity control. here we demonstrate the successful electrodeposition of high-quality si films from a cacl2 -based molten salt. soluble siiv -o anions generated from solid sio2 are electrodeposited onto a graphite substrate to form a dense film of crystalline si. impurities in the deposited si film are controlled at low concentrations (both b and p are less than 1\\u2005ppm). in the photoelectrochemical measurements, the film shows p-type semiconductor character and large photocurrent. a p-n junction fabricated from the deposited si film exhibits clear photovoltaic effects. this study represents the first step to the ultimate goal of developing a cost-effective manufacturing process for si solar cells based on electrodeposition."
        }
    ]
}