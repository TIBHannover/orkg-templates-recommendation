{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "templates_recommendation_training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kep9kuKjsfI-"
      },
      "source": [
        "# Introduction\n",
        "**This notebook is considered to fine-tune the pre-trained SciBERT model to solve the NLI problem equivalent to ORKG's templates recommendation service.**\n",
        "\n",
        "The notebook requires the data to be uploaded manually to google drive (please search for \"TODO\" below). Please check [this repository](https://gitlab.com/TIBHannover/orkg/nlp/experiments/orkg-templates-recommendation/) for further information about generating the data files.\n",
        "\n",
        "For any other questions, please contact omar.araboghli@outlook.com or jennifer.dsouza@tib.eu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4YeZzow51U0"
      },
      "source": [
        "# Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Unr7gRwHIoIq"
      },
      "source": [
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n",
        "\n",
        "!pip uninstall -y torch\n",
        "!pip install torch==1.8.2+cpu torchvision==0.9.2+cpu -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n",
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pRwJWQoKQsG"
      },
      "source": [
        "import torch\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "\n",
        "device = xm.xla_device()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNBcWSohCopF"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# TODO: adapt the file paths.\n",
        "!cp '/content/drive/MyDrive/master_thesis/templates_recommendation/04_4/training_set.json' './training_set.json'\n",
        "!cp '/content/drive/MyDrive/master_thesis/templates_recommendation/04_4/test_set.json' './test_set.json'\n",
        "!cp '/content/drive/MyDrive/master_thesis/templates_recommendation/04_4/validation_set.json' './validation_set.json'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_ju9k_IdYjJ"
      },
      "source": [
        "# Constants / Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTThVrZvMG-G"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "MODEL_FILE_NAME = 'orkgnlp-templates-recommendation-scibert.pt'\n",
        "N_EPOCHS = 10\n",
        "WARMUP_PERCENT = 0.2\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 2e-5\n",
        "EPS = 1e-6\n",
        "MAX_SEQUENCE_LENGTH = 512\n",
        "TRAINING_SET_PATH = './training_set.json'\n",
        "TEST_SET_PATH = './test_set.json'\n",
        "VALIDATION_SET_PATH = './validation_set.json'\n",
        "CLASSES = {\n",
        "    'entailment': 0,\n",
        "    'contradiction': 1,\n",
        "    'neutral': 2\n",
        "}\n",
        "\n",
        "# TODO: replace v1 with your directory name for the output of tensorboard events\n",
        "RUNS_LOG_DIR = 'runs/v1/'\n",
        "# TODO: please use your own google bucket name where the model should be uploaded\n",
        "BUCKET = ''"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-Tzd1KNzqbi"
      },
      "source": [
        "# Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkPVaiXL_HWG"
      },
      "source": [
        "import json\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, path, get_attention_mask, get_token_type, tokenize, tokens_to_ids, target_transform):\n",
        "\n",
        "        self.data = BERTDataset.read_json(path)\n",
        "        self.get_attention_mask = get_attention_mask\n",
        "        self.get_token_type = get_token_type\n",
        "        self.tokenize = tokenize\n",
        "        self.tokens_to_ids = tokens_to_ids\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data['entailments'] + self.data['contradictions'] + self.data['neutrals'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        instances = self.data['entailments'] + self.data['contradictions'] + self.data['neutrals']\n",
        "\n",
        "        # append the [PAD] character to the sequence, so that all sequences in \n",
        "        # a batch have the same length (MAX_SEQUENCE_LENGTH)\n",
        "        item = self.tokenize(instances[idx]['sequence'])\n",
        "        item += ['[PAD]'] * (MAX_SEQUENCE_LENGTH - len(item))\n",
        "        \n",
        "        attention_mask = self.get_attention_mask(item)\n",
        "        token_type = self.get_token_type(item)\n",
        "        label = self.target_transform(instances[idx]['target'])\n",
        "\n",
        "        return self.tokens_to_ids(item)[:MAX_SEQUENCE_LENGTH], attention_mask[:MAX_SEQUENCE_LENGTH], token_type[:MAX_SEQUENCE_LENGTH], label\n",
        "\n",
        "    @staticmethod\n",
        "    def read_json(input_path):\n",
        "        with open(input_path, encoding='utf-8') as f:\n",
        "            json_data = json.load(f)\n",
        "\n",
        "        return json_data\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MVYUuSo-2iC"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BERTNLIModel(nn.Module):\n",
        "    def __init__(self, bert_model, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.bert = bert_model\n",
        "        embedding_dim = bert_model.config.to_dict()['hidden_size']\n",
        "        self.out = nn.Linear(embedding_dim, output_dim)\n",
        "\n",
        "    def forward(self, sequence, attn_mask, token_type):\n",
        "        embedded = self.bert(input_ids=sequence, attention_mask=attn_mask, token_type_ids=token_type)[1]\n",
        "        output = self.out(embedded)\n",
        "        return output"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTpwECjbG5zF"
      },
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "class SciBERT:\n",
        "    DEFAULT_PATH = 'allenai/scibert_scivocab_uncased'\n",
        "\n",
        "    @staticmethod\n",
        "    def model(path=DEFAULT_PATH):\n",
        "        return BertModel.from_pretrained(path)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer(path=DEFAULT_PATH):\n",
        "        return BertTokenizer.from_pretrained(path)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PddVd8ry6-0"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J74O_MHktmCo"
      },
      "source": [
        "def get_attention_mask(tokens):\n",
        "    return [1] * len(tokens)\n",
        "\n",
        "\n",
        "def get_token_type(tokens):\n",
        "    sep_index = tokens.index('[SEP]') + 1\n",
        "    return [0] * sep_index + [1] * (len(tokens) - sep_index)\n",
        "\n",
        "\n",
        "def target_transform(label_text):\n",
        "  return CLASSES[label_text]\n",
        "\n",
        "\n",
        "def collate_fn(instances):\n",
        "  \"\"\"\n",
        "    produces DataLoader's batches with different shapes\n",
        "  \"\"\"\n",
        "  batch = []\n",
        "\n",
        "  for i in range(len(instances[0])):\n",
        "      batch.append([instance[i] for instance in instances])\n",
        "\n",
        "  return batch\n",
        "\n",
        "\n",
        "def categorical_accuracy(predictions, y):\n",
        "    max_predictions = predictions.argmax(dim=1, keepdim=True)\n",
        "    correct = (max_predictions.squeeze(1) == y).float()\n",
        "\n",
        "    return correct.sum() / len(y)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PUnGr8XzQa8"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyMBr_hsIk-A"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Tokenize\n",
        "bert_tokenizer = SciBERT.tokenizer()\n",
        "\n",
        "# Data\n",
        "training_data = BERTDataset(TRAINING_SET_PATH, get_attention_mask, get_token_type, bert_tokenizer.tokenize, bert_tokenizer.convert_tokens_to_ids, target_transform)\n",
        "validation_data = BERTDataset(VALIDATION_SET_PATH, get_attention_mask, get_token_type, bert_tokenizer.tokenize, bert_tokenizer.convert_tokens_to_ids, target_transform)\n",
        "\n",
        "# Data loaders\n",
        "training_data_loader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "validation_data_loader = DataLoader(validation_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYPRP-y8zbEi"
      },
      "source": [
        "# Learning Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gsqIlxIB0pr"
      },
      "source": [
        "import torch\n",
        "\n",
        "def train(model, data_loader, optimizer, criterion, scheduler):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  model.train()\n",
        "\n",
        "  for i, batch in enumerate(data_loader):\n",
        "    optimizer.zero_grad() # clear gradients first\n",
        "\n",
        "    sequence = torch.tensor(batch[0]).to(device)\n",
        "    attn_mask = torch.tensor(batch[1]).to(device)\n",
        "    token_type = torch.tensor(batch[2]).to(device)\n",
        "    label = torch.tensor(batch[3]).to(device)\n",
        "\n",
        "    predictions = model(sequence, attn_mask, token_type)\n",
        "    loss = criterion(predictions, label)\n",
        "    acc = categorical_accuracy(predictions, label)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    xm.optimizer_step(optimizer)\n",
        "    xm.mark_step()\n",
        "    scheduler.step()\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += acc.item()\n",
        "\n",
        "  return epoch_loss / len(data_loader), epoch_acc / len(data_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWis3OVDGTRj"
      },
      "source": [
        "def evaluate(model, data_loader, criterion, device):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for i, batch in enumerate(data_loader):\n",
        "      sequence = torch.tensor(batch[0]).to(device)\n",
        "      attn_mask = torch.tensor(batch[1]).to(device)\n",
        "      token_type = torch.tensor(batch[2]).to(device)\n",
        "      label = torch.tensor(batch[3]).to(device)\n",
        "\n",
        "      predictions = model(sequence, attn_mask, token_type)\n",
        "      loss = criterion(predictions, label)\n",
        "      acc = categorical_accuracy(predictions, label)\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "\n",
        "  return epoch_loss / len(data_loader), epoch_acc / len(data_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETOI8A0QQDM6"
      },
      "source": [
        "import math\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from transformers import AdamW, get_constant_schedule_with_warmup\n",
        "\n",
        "writer = SummaryWriter(log_dir=RUNS_LOG_DIR)\n",
        "total_steps = math.ceil(N_EPOCHS * len(training_data) * 1. / BATCH_SIZE)\n",
        "warmup_steps = int(total_steps * WARMUP_PERCENT)\n",
        "\n",
        "bert_nli_model = BERTNLIModel(SciBERT.model(), output_dim=len(CLASSES)).to(device)\n",
        "optimizer = AdamW(bert_nli_model.parameters(), lr=LEARNING_RATE, eps=EPS, correct_bias=False)\n",
        "scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "best_valid_loss = float('inf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-3qcV1BHB-0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "937f1a55-9680-4705-bfb0-440cad318f84"
      },
      "source": [
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "  train_loss, train_acc = train(bert_nli_model, training_data_loader, optimizer, criterion, scheduler)\n",
        "  valid_loss, valid_acc = evaluate(bert_nli_model, validation_data_loader, criterion)\n",
        "\n",
        "  writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
        "  writer.add_scalar(\"Accuracy/train\", train_acc, epoch)\n",
        "  writer.add_scalar(\"Loss/validation\", valid_loss, epoch)\n",
        "  writer.add_scalar(\"Accuracy/validation\", valid_acc, epoch)\n",
        "\n",
        "  print(f'Epoch: {epoch + 1:02}')\n",
        "  print(f'--> Train Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
        "  print(f'--> Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc * 100:.2f}%')\n",
        "\n",
        "  if valid_loss < best_valid_loss:\n",
        "    best_valid_loss = valid_loss\n",
        "    xm.save(bert_nli_model.state_dict(), MODEL_FILE_NAME)\n",
        "    writer.flush()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01\n",
            "--> Train Loss: 0.600 | Train Acc: 75.88%\n",
            "--> Val. Loss: 0.166 | Val. Acc: 95.41%\n",
            "Epoch: 02\n",
            "--> Train Loss: 0.160 | Train Acc: 94.27%\n",
            "--> Val. Loss: 0.152 | Val. Acc: 96.43%\n",
            "Epoch: 03\n",
            "--> Train Loss: 0.100 | Train Acc: 96.71%\n",
            "--> Val. Loss: 0.149 | Val. Acc: 95.41%\n",
            "Epoch: 04\n",
            "--> Train Loss: 0.072 | Train Acc: 97.47%\n",
            "--> Val. Loss: 0.099 | Val. Acc: 96.88%\n",
            "Epoch: 05\n",
            "--> Train Loss: 0.035 | Train Acc: 98.97%\n",
            "--> Val. Loss: 0.099 | Val. Acc: 97.77%\n",
            "Epoch: 06\n",
            "--> Train Loss: 0.027 | Train Acc: 99.17%\n",
            "--> Val. Loss: 0.088 | Val. Acc: 97.32%\n",
            "Epoch: 07\n",
            "--> Train Loss: 0.020 | Train Acc: 99.28%\n",
            "--> Val. Loss: 0.092 | Val. Acc: 96.75%\n",
            "Epoch: 08\n",
            "--> Train Loss: 0.008 | Train Acc: 99.85%\n",
            "--> Val. Loss: 0.119 | Val. Acc: 97.32%\n",
            "Epoch: 09\n",
            "--> Train Loss: 0.002 | Train Acc: 100.00%\n",
            "--> Val. Loss: 0.099 | Val. Acc: 97.77%\n",
            "Epoch: 10\n",
            "--> Train Loss: 0.006 | Train Acc: 99.79%\n",
            "--> Val. Loss: 0.091 | Val. Acc: 98.21%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k75hir0Iz4Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8090e11-a2be-4640-d878-06770a38034a"
      },
      "source": [
        "bert_nli_model.load_state_dict(torch.load(MODEL_FILE_NAME))\n",
        "\n",
        "test_data = BERTDataset(TEST_SET_PATH, get_attention_mask, get_token_type, bert_tokenizer.tokenize, bert_tokenizer.convert_tokens_to_ids, target_transform)\n",
        "test_data_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "test_loss, test_acc = evaluate(bert_nli_model, test_data_loader, criterion, device)\n",
        "\n",
        "writer.add_scalar(\"Loss/test\", test_loss, 0)\n",
        "writer.add_scalar(\"Accuracy/test\", test_acc, 0)\n",
        "writer.flush()\n",
        "writer.close()\n",
        "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc * 100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.059 |  Test Acc: 97.28%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvWaXulzqirb"
      },
      "source": [
        "def predict_similar_template(model, tokenizer, template_string, paper_string, device):\n",
        "    model.eval()\n",
        "\n",
        "    # premise = templates_string, hypothesis = paper_string\n",
        "    sequence = '[CLS] {} [SEP] {} [SEP]'.format(template_string, paper_string)\n",
        "    sequence_tokens = tokenizer.tokenize(sequence)\n",
        "\n",
        "    attention_mask = get_attention_mask(sequence_tokens)\n",
        "    token_type = get_token_type(sequence_tokens)\n",
        "    sequence_tokens = tokenizer.convert_tokens_to_ids(sequence_tokens)\n",
        "\n",
        "    attention_mask = torch.tensor(attention_mask).unsqueeze(0).to(device)\n",
        "    token_type = torch.tensor(token_type).unsqueeze(0).to(device)\n",
        "    sequence_tokens = torch.tensor(sequence_tokens).unsqueeze(0).to(device)\n",
        "\n",
        "    prediction = model(sequence_tokens, attention_mask, token_type)\n",
        "    print(prediction)\n",
        "    return prediction\n",
        "\n",
        "premise = 'ontology learning from data sources has dataset application domain input format output format accuracy f1 score precision knowledge source has data source learning purpose terms learning axiom learning properties learning properties hierarchy learning rule learning class hierarchy learning learning method learning tool evaluation metrics related work validation tool training corpus testing corpus class learning implemented technologies relationships learning instance learning taxonomy learning validation comment assessment of acquired knowledge recall f measure'\n",
        "hypothesis = 'multimedia ontology learning for automatic annotation and video browsing in this work, we offer an approach to combine standard multimedia analysis techniques with knowledge drawn from conceptual metadata provided by domain experts of a specialized scholarly domain, to learn a domain specific multimedia ontology from a set of annotated examples a standard bayesian network learning algorithm that learns structure and parameters of a bayesian network is extended to include media observables in the learning an expert group provides domain knowledge to construct a basic ontology of the domain as well as to annotate a set of training videos these annotations help derive the associations between high level semantic concepts of the domain and low level mpeg 7 based features representing audio visual content of the videos we construct a more robust and refined version of this ontology by learning from this set of conceptually annotated videos to encode this knowledge, we use mowl, a multimedia extension of web ontology language (owl) which is capable of describing domain concepts in terms of their media properties and of capturing the inherent uncertainties involved we use the ontology specified knowledge for recognizing concepts relevant to a video to annotate fresh addition to the video database with relevant concepts in the ontology these conceptual annotations are used to create hyperlinks in the video collection, to provide an effective video browsing interface to the user'\n",
        "\n",
        "predict_similar_template(bert_nli_model, bert_tokenizer, premise, hypothesis, device).argmax(dim=-1).item()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iahXE3naqVHs"
      },
      "source": [
        "# Store files\n",
        "## Tensorboard events"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ksAWQQ4Fggt"
      },
      "source": [
        "!zip -r runs.zip ./runs\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"./runs.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTob6NsyqfcX"
      },
      "source": [
        "## Upload to Google Cloud Storage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL_vgf5PPa9O"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Upload model to bucket\n",
        "!gsutil cp {MODEL_FILE_NAME} gs://{BUCKET}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting to PretrainedModel"
      ],
      "metadata": {
        "id": "7KBCEvKEGgkp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the subsequent code we convert a BERTNLIModel consisting of transformers.BertModel and torch.nn.Linear to a transformers.BertForSequenceClassification(PretrainedModel) that enables saving and loading it without the need of the class definition of BERTNLIModel, thus increases the interoperability."
      ],
      "metadata": {
        "id": "tEwb8iI50u1Z"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcY5vlbakHbB"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# get model from bucket\n",
        "!gsutil cp -r gs://{BUCKET}/{MODEL_FILE_NAME}  {MODEL_FILE_NAME} "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_nli_model = BERTNLIModel(SciBERT.model(), output_dim=len(CLASSES)).to(torch.device('cpu'))\n",
        "bert_nli_model.load_state_dict(torch.load(MODEL_FILE_NAME, map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "id": "RBY9aRtKbsNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "bert_classification = BertForSequenceClassification.from_pretrained(\n",
        "    'allenai/scibert_scivocab_uncased',\n",
        "     num_labels=len(CLASSES),\n",
        "     label2id=CLASSES,\n",
        "     id2label={value:key for key, value in CLASSES.items()},\n",
        "     problem_type='single_label_classification'\n",
        "     )\n",
        "\n",
        "state_dict = {}\n",
        "for key, value in bert_nli_model.state_dict().items():\n",
        "  if key == 'out.weight':\n",
        "    state_dict['classifier.weight'] = value\n",
        "  elif key == 'out.bias':\n",
        "    state_dict['classifier.bias'] = value\n",
        "  else:\n",
        "    state_dict[key] = value\n",
        "\n",
        "bert_classification.load_state_dict(state_dict)"
      ],
      "metadata": {
        "id": "EvdwiEsbCMit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verify conversion results\n",
        "premise = 'ontology learning from data sources has dataset application domain input format output format accuracy f1 score precision knowledge source has data source learning purpose terms learning axiom learning properties learning properties hierarchy learning rule learning class hierarchy learning learning method learning tool evaluation metrics related work validation tool training corpus testing corpus class learning implemented technologies relationships learning instance learning taxonomy learning validation comment assessment of acquired knowledge recall f measure'\n",
        "hypothesis = 'multimedia ontology learning for automatic annotation and video browsing in this work, we offer an approach to combine standard multimedia analysis techniques with knowledge drawn from conceptual metadata provided by domain experts of a specialized scholarly domain, to learn a domain specific multimedia ontology from a set of annotated examples a standard bayesian network learning algorithm that learns structure and parameters of a bayesian network is extended to include media observables in the learning an expert group provides domain knowledge to construct a basic ontology of the domain as well as to annotate a set of training videos these annotations help derive the associations between high level semantic concepts of the domain and low level mpeg 7 based features representing audio visual content of the videos we construct a more robust and refined version of this ontology by learning from this set of conceptually annotated videos to encode this knowledge, we use mowl, a multimedia extension of web ontology language (owl) which is capable of describing domain concepts in terms of their media properties and of capturing the inherent uncertainties involved we use the ontology specified knowledge for recognizing concepts relevant to a video to annotate fresh addition to the video database with relevant concepts in the ontology these conceptual annotations are used to create hyperlinks in the video collection, to provide an effective video browsing interface to the user'\n",
        "bert_tokenizer = SciBERT.tokenizer()\n",
        "\n",
        "scibert_model_prediction = predict_similar_template(bert_nli_model, bert_tokenizer, premise, hypothesis, device=torch.device('cpu')).argmax(dim=-1).item()\n",
        "scibert_classification_model_prediction = predict_similar_template(bert_classification, bert_tokenizer, premise, hypothesis, device=torch.device('cpu')).logits.argmax(dim=-1).item()\n",
        "assert scibert_model_prediction == scibert_classification_model_prediction"
      ],
      "metadata": {
        "id": "XSZyADL5GsiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "pretrained_dir = os.path.splitext(MODEL_FILE_NAME)[0]\n",
        "bert_classification.save_pretrained(pretrained_dir)\n",
        "\n",
        "!zip -r {pretrained_dir}.zip ./{pretrained_dir}\n",
        "\n",
        "from google.colab import files\n",
        "files.download('{}.zip'.format(pretrained_dir))"
      ],
      "metadata": {
        "id": "0Bp2CBPBM86j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}